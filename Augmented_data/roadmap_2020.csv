Feedback reference,Submitted on,Submitted by,User type,Organisation,Organisation size,Transparency register number,Country of origin,Initiative,Paragraph,pdf
F551055,10 September 2020,Douglas Johnson,Wirtschaftsverband,Consumer Technology Association,mittel (50 bis 249 Beschäftigte),-,Vereinigten Staaten,Künstliche Intelligenz – ethische und rechtliche Anforderungen,CTA's comments are in the attached file.,"['September 10, 2020 \n \nConsumer Technology Association  \nComments on \nEuropean Commission “Inception Impact Assessment” on Artificial Intelligence  \nRef. Ares(2020)3896535 \n \nThe Consumer Technology Association® (“CTA”)®1 respectfully submits these \ncomments in response to the European Commission (“Commission”) Inception Impact \nAssessment2 (“the Impact Assessment”) analyzing the potential impact of adopting sweeping \nnew legislation on artificial intelligence (“AI”), as outlined in the Commission’s White Paper.3  \nAs explained in prior comments, CTA applauds the Commission’s thoughtful work on these \nissues, including release of the Impact Assessment, which raises important questions concerning \nthe potential pecuniary impact of adopting new legislation in this area.  \nRespectfully, CTA urges the Commission to undertake additional review and analysis of \nthe potential economic impact of formal Commission action.  This is necessary to better \nunderstand and evaluate the potential economic costs, and benefits, of taking formal action that \nmay lead to new rules and restrictions on the development, use and sale of AI.  In order to meet \nthe Commission’s own high standard for analyzing the impact of potential legislation, the \nCommission must (in its own words) “analyse in more detail the issue to be addressed, whether \naction should be taken at EU level and the potential economic, social and environmental effects \nof the different solutions outlined.”4   \nTo meet this high bar the Commission must recognize the very significant potential \neconomic costs of imposing burdensome new rules upon a still nascent and emerging \ntechnology.  In recognition of this fact, the Commission should further develop its Impact \nAssessment to more fully consider the potential economic impact of adopting new legislation in \nthis area.  That further assessment should specifically analyze the potential impact on small to \n                                                           \n1 CTA® is the tech sector. Our members are the world’s leading innovators—from startups to global brands—\nhelping support millions of jobs. CTA owns and produces CES®—the largest, most influential tech event on \nthe planet. \n2 White Paper on Artificial Intelligence, A European Approach to Excellence and Trust; COM(2020) 65 Final, \nBrussels 2.19.2020 (“White Paper”).   \n3 European Commission Inception Impact Assessment on Artificial Intelligence, Ref. Ares(2020)3896535 \n23.07.2020 (“Impact Assessment”).  \n4 See European Commission, Planning and Proposing Law https://ec.europa.eu/info/law/law-making-\nprocess/planning-and-proposing-law_en#how-their-scope-is-defined (emphasis added).', 'medium sized businesses, the potential costs of adopting AI-specific liability schemes (such as a \nstrict liability framework), and how to avoid creating potentially costly new requirements by \nleveraging consensus-based standards, voluntary governance and risk assessment processes in \nlieu of broad new AI legislation. \n \nI.  Certain Preliminary Findings in the Impact Assessment Will be Central to the \nDevelopment of a More Fulsome Analysis by the Commission \nThe Commission’s Impact Assessment correctly makes preliminary findings for several \nimportant issues, including the need to: 1) properly define the scope of any potential legislation; \n2) avoid fragmentation amongst EU member states; and, 3) recognize that if legislation is \noverreaching and pervasive, the costs of compliance will outweigh the potential opportunities \nand create disincentives for developing this new technology.  CTA agrees with these findings \nand offers the following additional considerations for these three points. \nIn the Impact Assessment the Commission recognizes that a “core question” relates to the \nscope of the initiative, i.e., “how should AI be defined (narrowly or broadly).”5  CTA submits \nthat addressing this core question at the outset is critically important to completion of a probative \nand meaningful final impact assessment.  How can the Commission assess the impact of \npotential additional compliance costs, if it has not yet determined how narrowly or broadly those \ncosts may extend?  Without addressing this threshold question the Commission’s broad, general \npronouncements in the White Paper are too vague to properly assess or calibrate (let alone \nimplement).  To address this issue the Commission should adopt a nuanced approach that \nrecognizes that the breadth of AI use cases will require a focused response, which may differ by \napplication.   \nTo that end, the Commission should avoid using broad, ill-defined concepts in any future \nlegislation.  Use of untethered terms such as “automated decision making” conflicts with the \nthoughtful and focused risk-based constructs outlined in the White Paper.  Similarly, the \nCommission must retain the risk/harm framework articulated in the White Paper and avoid \nadoption of new rules or requirements that do not distinguish between AI applications that may \npose risk of injury or harm to individuals, and those applications that pose little, or no, risk.  \nRetention of this risk-based framework is essential to maintaining proportionality in any new \nframework. \nFurther, the Commission should differentiate types of harm and carefully calibrate the \nresponses to such harm.  Indeed, as CTA explained in its prior comments to the Commission, the \nCommission’s approach should begin with a narrow focus by identifying challenges that are \nunique to AI and not already covered by existing regulation.6  To begin with, the Commission \nshould adopt a definition of AI that narrowly focuses on those unique aspects of the technology \n(i.e., systems that are capable of learning on their own), but which avoids capturing general \nconcepts or constructs used in computer science (i.e., algorithms) that do not, on their own, \nconstitute AI.  The Commission can do so by utilizing the definition set forth in its April 2018 \n                                                           \n5 Impact Assessment at 5. \n6 CTA Consumer Technology Association Comments on European Commission White Paper on Artificial \nIntelligence – A European Approach (filed June 12, 2020) (“CTA AI Comments”). \n2', 'Communication on AI.7 A more nuanced approach would embrace an application-specific \nframework for consideration of any new mandates, which would also ensure that AI policy does \nnot duplicate existing policy frameworks in those fields in which AI technologies are already in \nuse today (such as healthcare, financial services or energy).   \nCTA also agrees with the Commission’s finding that it must avoid fragmentation \namongst EU member states that may consider differing rules as one potential benefit of this \nprocess.8  While sovereignty must be respected, the potential emergence of varied regulatory or \nlegislative proposals within the EU would create an unworkable environment for many \ntechnology providers, who would be forced to operate and comply with numerous different rules \nand regulations.  The operational costs and burdens of doing so are well documented.  Indeed, as \nthe Commission itself noted: “… increasing fragmentation can hamper the confidence of \nEuropean businesses to innovate and the development of beneficial AI solutions, and jeopardize \nthe goals of a single digital market.”9  Narrow, focused Commission action supported by a \ndetailed, and evidence-based assessment will reduce the potential for fragmentation among \nmember states. \nFinally, CTA applauds the Commission for recognizing that if compliance costs outweigh \nthe benefits, some desirable AI systems may not be developed at all.  Specifically, the \nCommission finds that new legislation may impose additional compliance costs because AI \nsystems may have to adhere to “new requirements and processes.”10  Indeed, this finding is \nfollowed by an acknowledgement that “[i]f compliance costs outweigh the benefits, it may even \nbe the case that some desirable AI systems may not be developed at all.”  This principle should \nbe the touchstone of any truly accurate and meaningful impact assessment: over-regulation will \nlead to under-investment and loss of innovation.  Thus, the Commission must be mindful of the \npotential for taking action that has the effect of simply discontinuing further development of this \ntechnology in the EU. \n \nII.  Additional Analysis Is Necessary to Fully Assess the Impact on Nascent and \nDeveloping Technology  \nA.    Further Development of the Cost-Benefit Analysis Is Necessary to Ensure \nCommission Action Does Not Undermine Investment and Innovation \nAlthough the Impact Assessment acknowledges the potentially deleterious impact on \ninvestment and innovation, it does not give sufficient consideration to the potential economic \nimpact of adopting over-reaching new rules.  The Commission’s analysis of the potential \neconomic impact of new rules11 is insufficient and must be more fully developed.  The \nassessment does not give necessary, or sufficient, consideration to the significant potential costs \nof “new requirements and processes,” and specifically any new requirements that may include ex \n                                                           \n7 “Artificial intelligence (AI) refers to systems that display intelligent behaviour by analysing their environment and \ntaking actions – with some degree of autonomy – to achieve specific goals. AI- based systems can be purely \nsoftware-based, acting in the virtual world (e.g. voice assistants, image analysis software, search engines, speech and \nface recognition systems) or AI can be embedded in hardware devices (e.g. advanced robots, autonomous cars, \ndrones or Internet of Things applications).”  COM(2018) 237 Final, p. 1. \n8 Impact Assessment at 4. \n9 Id. at 3. \n10 Id. at 5. \n11 Id. at 5-6. \n3', 'ante regulation.  If ex ante rules are adopted, many companies developing this technology will \nlikely face costly and burdensome obligations that would deter innovation and discourage \ninvestment. \nThe need for a fuller impact assessment of the potential economic costs of regulations is \nmost clearly framed by the Commission’s consideration of a restrictive and complex “Option 3” \nregulatory approach, as described in the Impact Assessment.12  As outlined in numerous \ncomments before the Commission, new recordkeeping and disclosure obligations could greatly \nincrease the operational and compliance costs of AI developers, including many small emerging \ncompanies in this market.  Indeed, the Impact Assessment significantly underestimates the \npotential administrative burdens of operating under a new regulatory regime for AI, as reflected \nin the finding that “binding requirements could create some administrative burden that is not \ntotally compensated by additional benefits.”13  This finding ignores the likely significant \nadministrative burden if recordkeeping, reporting, training and oversight obligations are \nimposed. \nMore significantly, the Impact Assessment does not fully analyze the potential impact of \nadopting limits on the use of certain training data, which may impact the accuracy and robustness \nof these systems; and the impact of imposing new obligations to utilize human oversight of \nsystems that may be unnecessary or redundant.14  Nor does the analysis give sufficient \nconsideration to the numerous societal, economic and other benefits provided by emerging AI \nsystems and technologies, such as the numerous new AI-powered technology solutions being \ndeployed to respond to COVID-19.15  Accordingly, the Commission should further develop its \nassessment to ensure that it considers the impact of policies that may deter the continued \ndevelopment and deployment of AI products and services which enhance the lives, safety and \ninterests of European consumers.   \nThe analysis also fails to fully evaluate the potential impact of providers that may need to \noperate under a regime which creates duplicative or overlapping obligations, and the legal and \nadministrative costs of operating in that environment.  As CTA demonstrated in its comments, \nthe potential for conflicting or duplicative laws in certain markets such as autonomous vehicles, \nwhich may already have existing duties and obligations, is particularly problematic.16  For \nexample, the creation of conformity assessment procedures for high-risk AI applications, if \napplied to autonomous vehicles, could be duplicative of other existing requirements applicable to \nautonomous vehicles. To mitigate this potential overlap the Commission should identify those \nareas, such as autonomous vehicles, which may already have existing duties and obligations and \ncarve out such areas that are subject to other regulatory requirements. \n \n \n                                                           \n12 Id. at 4-5. \n13 Id. at 7 (emphasis added). \n14 See CTA Comments at 11-12 (noting that imposing an obligation of human oversight may be appropriate in \ncertain circumstances, but is fundamentally at odds with the objective of developing and deploying fully automated \nvehicles intended to operate free from human intervention). \n15 As previously noted, during the global COVID-19 pandemic AI is driving important research and testing \nnecessary to defeat the virus.  For example, French AI company Iktos has partnered with SRI International, based in \nMenlo Park, to discover and develop new anti-viral therapies using deep-learning models. Id. at 1-2. \n16 Id. at 11. \n4', 'B.    Cost-benefit Analysis Should Specifically Consider the Impact on Small and \nMedium Sized Business \nIn discussing potential compliance costs for potentially affected entities,17 the \nCommission focuses primarily on the aggregate costs on the industry (i.e., the greater the \npotential costs of compliance, the higher the costs that industry must assume).  But the Impact \nAssessment fails to sufficiently account for the disproportionate impact of significant compliance \ncosts on small to mid-sized entities that do not have the margin or operational budget to assume \nsignificantly greater compliance costs.  For that reason the final Impact Assessment must \nspecifically consider how such compliance costs would impact small to medium-sized \nbusinesses.   \nWhile some AI developers are large concerns with significant resources, this is an area \nwith many new entrants (small and medium sized enterprises) that may not have the same \nresources to devote to respond to new compliance, paperwork or reporting obligations.  The \npotential impact on overreaching compliance costs must consider the proportional impact on \nsuch small and medium sized businesses. \nC.    Final Impact Assessment Must Recognize that AI Specific Liability Schemes \nCould Create a Competitive Disadvantage for AI Technologies \nThe Impact Assessment fails to explore the potential impact of adopting AI-specific \nliability rules, including a strict liability scheme for AI.  CTA believes that adoption of AI-\nspecific liability rules could lead to a competitive disadvantage for AI technologies competing \nagainst traditional technologies that may create similar risks.  For example, many studies have \nrecognized that implementation of autonomous vehicle systems is likely to increase road safety \nin the years ahead, but if such systems are subjected to new or greater liability duties than that \nwhich applies to existing systems, that could create a significant disincentive for further \ninvestment in autonomous vehicle systems.  In this way, an AI-specific-liability scheme that \ncreates an uneven playing field may hinder the development and deployment of many different \ntypes of AI technologies in the EU. \nSimilarly, the Impact Assessment fails to explore the potential impact of adopting a strict \nliability scheme for AI.  To address that shortcoming the Commission should acknowledge that a \nstrict liability scheme would, if adopted, likely increase economic and societal costs across the \nboard, which would require a very different cost-benefit framework and analysis.  In exploring \nsome of the issues surrounding the adoption of a strict liability scheme, the Impact Assessment \nfails to recognize that there are numerous economic actors involved in the lifecycle of an AI \nsystem including entities: developing the technology, those deploying the technology and \npotentially others, such as distributors, resellers, service providers and individuals who use the \ntechnology in an integrated platform or solution.  Under this framework numerous private \neconomic actors play a role in the development and delivery chain and will be impacted by the \nadoption of an overly broad, ill-conceived strict liability system.  This shortcoming may stem \nfrom the fact that neither the White Paper nor the accompanying Report on the Safety and \nLiability Implications of Artificial Intelligence, the Internet of Things and Robotics,18 properly \nevaluates the potential ramifications of adopting a strict liability framework for all AI systems.  \n                                                           \n17 Impact Assessment at 5. \n18 COM(2020)64, Report on the Safety and Liability Implications Of Artificial Intelligence, The Internet of Things \nand Robotics (rel. 19 Feb. 2020). \n5', 'Such a decision would have wide-ranging implications that could seriously undermine the \ndevelopment and expansion of important AI systems, and must be considered in any further \nImpact Assessment. \nD.  Final Impact Assessment Should Rely Upon Evidence from Companies Leading in \nthe Development and Deployment of this Technology \nAlthough the Impact Assessment states that there is “not a lot of currently valid evidence \n[] available at this stage” this belies the very robust record already developed by the Commission \nitself in this proceeding.  There is significant evidence already in the record in the form of public \ncomments made in response to the White Paper.  Many leading developers of AI systems and \ntechnology, such as Google, Microsoft and others have filed comments with the Commission \nwhich provide significant factual record to address the many important questions raised in the \nWhite Paper.  To fail to account for, and rely upon, such evidence would be a mistake. \nE.    Final Impact Assessment Should Further Analyze Value of Industry Consensus \nand Voluntary Standards in Lieu of New Rules \nWhile the Impact Assessment acknowledges the potential value of adopting a “soft law” \napproach (“Option 1”), the analysis does not fully consider the potential value of leveraging \nvoluntary standards and industry consensus in lieu of more formal rules.  This approach should \nbe used whenever possible to leverage existing voluntary standards and ethical frameworks \nadopted by many actors.  As CTA has explained, voluntary consensus-based standards can \nreduce the burden of complying with ill-formed regulation, eliminate the administrative costs of \ndeveloping state-mandated standards, and decrease the overall cost of goods procured and the \nburden of complying with agency regulation.  This is precisely why CTA and its members have \ntaken a leadership position in AI standard-setting, both in North America19 and internationally.20  \nConsensus-based standards, like those being developed by CTA and its member companies, \noften have broad support from industry, are more likely to reflect the most current technological \ndevelopments, and reflect the most practical solutions available to the marketplace. \n \nIII.  Conclusion \nCTA and its members have a significant interest in ensuring that European consumers \nbenefit from AI-powered products and services.  The Commission should further develop its \nassessment of the economic impact of over-reaching and burdensome new legislation in this \narea.  Instead, the Commission should proceed carefully to ensure that its policies promote \ncontinued development and deployment of AI products and services that enhance the lives, \nsafety and interests of European consumers.  CTA stands ready to continue its central role in the \n                                                           \n19 For example, CTA is developing standards focused on AI in healthcare.  See The Use of Artificial Intelligence in \nHealth Care: Trustworthiness; or Definitions and Characteristics of Artificial Intelligence; and, Riya Anandwala and \nDanielle Cassagnol, CTA, Press Release, CTA Launches First-Ever Industry-Led Standard for AI in Health Care \n(rel. Feb. 25, 2020), available at https://www.cta.tech/Resources/Newsroom/Media-Releases/2020/February/CTA-\nLaunches-First-Ever-Industry-Led-Standard. \n20 For example, CTA has been actively participating in ISO/IEC JTC 1/SC 42, the international standards committee \nresponsible for standardization in the area of Artificial Intelligence.  \n6', 'development of consensus-based standards that advance these goals and promote continued \ndynamic growth and innovation throughout the consumer technology industry. \n/s/ Douglas K. Johnson   \nDouglas K. Johnson \nVice President, Technology Policy \n \n/s/ Michael Petricone    \nMichael Petricone \nSr. Vice President, Government and Regulatory Affairs \n \n \nFiled: September 10, 2020 \n \n \n \n \n7']"
F551054,10 September 2020,Stan Adams,NRO (Nichtregierungsorganisation),Center for Democracy & Technology,klein (10 bis 49 Beschäftigte),-,Vereinigten Staaten,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"CDT welcomes the opportunity to provide input to the Commission's choice of regulatory approaches for artificial intelligence. We respectfully offer the following suggestions:
1) Be more precise: No single approach to regulating “AI” will make sense in all scenarios. Instead the commission should start by clearly identifying potential or existing harms and work backward to identify the most effective points to apply legislative or regulatory solutions. After identifying and articulating these harms and appropriate points of intervention, the Commission should consider how to measure their likelihood of occurrence for clearly articulated use cases of well-defined types of applications. This assessment will help distinguish and prioritise which use cases should be further addressed through rules and regulations according to their relative urgency. 

Although we support the idea that some applications pose higher risks of harm than others, we urge the Commission to invest further effort in articulating which use cases of which applications it seeks to include if it chooses regulatory approaches targeting “high risk” applications. This additional clarity will help provide greater certainty for potentially impacted entities and will help guide regulators and other stakeholders as they develop and assess concrete regulatory proposals. We are doubtful, even with a narrowly drawn scope for “high risk” applications, that a single legislative instrument could effectively address the range of harms presented by the diverse array of applications and their use cases. As an alternative, we suggest that regulatory approaches might be more effective if they are geared towards preventing common harms, rather than aimed at governing classes of applications. We suggest that, through a harms-based approach, much of the EU’s existing law could be brought to bear through clarification rather than through development of new rules.

2) Clarify where existing law applies and identify gaps: Many harms have already been addressed through legislation. Clarifying existing law for novel applications should be step one, and we support the Commission’s intent to apply current law in its efforts to address risks posed by AI applications. Before pursuing additional legislative instruments, we suggest that the Commission and relevant regulatory bodies should make a coordinated effort to assess which parts of existing law might be used to address risks posed by AI applications. 

For example, in our response to the Commission’s AI Whitepaper consultation, we suggest that remote biometric surveillance is likely prohibited in most cases because it cannot be justified under Art. 9 GDPR and conflicts with both the European Convention on Human Rights and the Charter of Fundamental Rights. Similarly, the Employment Equality Directive should apply whether or not employers use algorithmic tools, but the Commission may wish to clarify employers’ obligations such as how they can satisfy their burdens of proof when using algorithmic application processing systems.

After mapping existing laws onto the use of AI applications, the Commission can then better identify any gaps in the law or areas for which novel legislative solutions may be necessary. We suggest that this effort should take place before moving forward with a new legislative instrument to help avoid redundancies, confusion, and uncertainty.

3) Refine existing soft-law approaches: CDT generally supports a combination of clarifying and enforcing existing law alongside voluntary adherence to principles expressed in soft law documents such as the findings of the High Level Expert Group on AI. As with our suggestions for the clarification and application of existing law, we believe that the principles expressed in various soft law documents would benefit from refinement through their application to concretely defined use cases of applications. "
F551053,10 September 2020,Paolo Grassia,Wirtschaftsverband,ETNO - European Telecommunications Network Operators' Association,sehr klein (1 bis 9 Beschäftigte),08957111909-85,Belgien,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"ETNO welcomes the Commission’s objective to foster the uptake of Artificial Intelligence
technologies and products that abide by European ethical norms, legal requirements, and fundamental rights.

Today, no specific EU legal framework to regulate AI exists. The development, deployment and use of AI are subject to a range of horizontal laws and principles such as on data protection and privacy, consumer protection, product safety and liability.

We agree that the goals of the legal act should be to pursue a harmonised approach to Trustworthy AI applications, bolstering the EU’s capacity to innovate and remain competitive with other regions of the world. Divergent national requirements that raise barriers to the development and the uptake of AI technology across the single market should be avoided.

We encourage the Commission to examine the existing regulatory framework and assess whether adjustments are necessary to address the emerging risks posed by AI. We agree that several risks examined in the IIA are not necessarily specific to AI and new measures should be embedded in the broader framework. A new instrument could provide additional safeguards and clarifications necessary for a truly safe and lawful AI that respects fundamental rights.

Considering the above, ETNO would support a proportionate combination of soft law, e.g. a voluntary labelling scheme, and mandatory requirements that were guided by a risk-based approach (Option 4). Our views on the various options comprised in this mix are described as follows.

A soft law approach that enable industry initiatives through additional accountability and coordination (Option 1) should be preferred wherever possible, to build on the existing efforts already deployed by public and private organisations and make them more effective.

A voluntary labelling scheme (Option 2) could encourage the uptake of Trustworthy AI. Criteria underpinning the labels could relate to e.g. transparency, robustness, and human oversight. They must be meaningful to users, while avoiding excessive burden on businesses to not discourage adoption of labels. However, it is crucial to clearly define their assignment mechanism, and the role and the authority of testing centres. Governance and enforcement are key to ensuring that a single scheme is implemented across the EU and that labels are valued in the market.

Finally, we suggest examining existing regulation and whether adjustments are necessary, before introducing additional legislation. Any new legislative instrument establishing mandatory requirements should particularly be targeted to “high-risk” AI applications (Option 3b), avoid a sweeping regulatory intervention as in Option 3c, and build on the principles and criteria developed for the voluntary labelling scheme in order to ensure consistency.

To define the scope of mandatory requirements, we agree with the cumulative criteria of high-risk sectors and high-risk applications of AI in those sectors as suggested in the White Paper, with some adjustments and clarifications that consider the impact on the whole AI value chain. For our detailed views on the identification of high-risk AI, as well as on the requirements that could be set out in the legal act, we refer to ETNO’s position paper on the White Paper.

Enforcement mechanisms are key to ensure effective compliance with the various voluntary and mandatory provisions. We envisage a flexible and open EU governance structure, which delegate enforcement responsibilities to Member States. 

Lastly, we concur that a robust definition of AI is the key to an enabling framework that build legal certainty. We recommend to carefully delineate its perimeter, drawing from the definition rendered by the High-Level Expert Group in its Ethics Guidelines for Trustworthy AI.","[""June 2020 \nEuropean Commission's White Paper on Artificial Intelligence - A European \napproach to excellence and trust \nETNO position paper \n \nETNO welcomes the publication of the European Commission’s White Paper on “Artificial Intelligence – A \nEuropean approach to excellence and trust” (the “White Paper”), a key pillar of the Commission’s digital \nstrategy for the next five years. \nThe European telecommunications industry will be a key enabler of the future AI ecosystems.  5G and fibre \nconnectivity will accelerate the digitisation of services and industrial processes, enabling the rapid expansion \nof the Internet of Things (IoT). The massive amounts of data generated by IoT connections and devices will \nopen up new growth opportunities for data analytics and AI services in Europe.  High-class, secure \nconnectivity will then drive IoT, and IoT will in turn fuel European AI. Together, they can form a truly \npowerful virtuous circle that our industry is committed to nurturing. Digital network providers themselves \nincreasingly deploy AI solutions in various areas, typically to improve efficiency in network operations (e.g., \nnetwork security and predictive maintenance), to enhance customer experience, and for better product and \nservice development. \nWe welcome the two-fold approach described in the White Paper, which aims at fostering the uptake of AI \ntechnologies and products in Europe as part of an “ecosystem of excellence”, while ensuring their \ncompliance with European ethical norms, legal requirements and fundamental rights that together form an \n‘‘ecosystem of trust’’1. These two elements are mutually supportive, as long as new requirements to build a \nculture of trustworthy AI are proportionate to the objective of establishing European excellence.  \nThe overarching goal of the European AI strategy should be to pursue a coordinated approach to AI across \nthe EU, bolstering the Union’s capacity to keep pace and remain competitive with other regions of the world \nin the development and deployment of AI applications. Divergent national requirements that raise barriers \nto the development and the uptake of AI technology across the single market should be avoided. When \ndefining new rules, it is equally important to consider the whole AI value chain in order to target in the first \nplace those segments that are most suitable to bear responsibility in line with the “polluter pays principle”.  \nWhereas we largely commend the overarching vision outlined in the White Paper, we would like to offer our \nrecommendations to further strengthen the promotion of a trustworthy AI as a competitive advantage and \ninvestment incentives in Europe. \n \n \n \n1 ETNO’s contribution to the “Ethics Guidelines for Trustworthy Artificial Intelligence’’ is available at \nhttps://www.etno.eu/library/389-etno-response-to-the-stakeholders-consultation-on-draft-ai-ethics-guidelines.html. \n1"", ""1.  An Ecosystem of Excellence \n \nWe welcome the emphasis on the objective of enabling an “ecosystem of excellence” for AI; however, we \nalso believe that in fact the White Paper focuses too much on the regulatory aspects related to trustworthy \nAI. The document lacks bold and actionable measures to mobilise resources that can achieve the excellence \necosystem along the whole value chain. Furthermore, the paper does not sufficiently expand on the details \nof actions that would enable the uptake and scalability of AI technology and products across Europe to drive \nEU competitiveness on the global scene.  \nThe world is witnessing an intensifying race for global dominance in AI between Asia and the United States, \nwhich have both been outspending Europe in research funding, patent applications and technological \ndevelopment (see Fig. 1 and Fig. 2). The strong leadership role of the US and some Asian countries in AI \ntechnology undermines Europe’s goal to become a strategically autonomous global power. European \ntechnology companies suffer from insufficient dynamism and scale compared to their Asian and US \ncounterparts, not least because of the historical fragmentation of the European market. \n \n60000\n50000\n40000\n30000\n20000\n10000\n0\nSU RK epo NC AC PJ NI BG ED LI RF WT UA HC TI EI IF LN UR SE ES GS EB RB LP TA KD ON ZC ZN OR\nr\nu\nE\n \nFig 1. A.I. Patents granted by inventor's country (2010-2020). Source: Analysys Mason, 2020. \n \n20000\n18000\n16000\n14000\n12000\n10000\n8000\n6000\n4000\n2000\n0\nSU RK epo AC NC NI BG PJ ED LI EI RF IF UA WT TI HC SE ES EB LN UR HT ZC GS LP RB KD TP TA KH\nr\nu\nE\n \nFig.2 AI inventors in Telecom by country (2010-2020). Source: Analysys Mason, 2020. \n2"", 'Nonetheless, although Europe is lagging behind in the race for technological leadership in AI, it remains a \nlarge and attractive market for AI technologies. There certainly is a window of opportunity for the EU to act.  \nTo seize the trustworthy AI opportunity, Europe needs to attract a larger pool of public and private \ninvestments in AI technology development and to overcome fragmentation in the access to and retention of \ntalents. Cross-national efforts and collaboration between public and private partners must be reinforced. \nMore importantly, a European approach should foster the creation of technology ecosystems around AI \napplications in the strategic industry sectors where Europe has been traditionally strong, such as \nmanufacturing. \nEven though public spending accounts for nearly half of the EU’s GDP, a reference to the crucial lever of \npublic procurement is missing in the White Paper. Similarly, the paper fails to expand on EU-funded national \nand transnational lighthouse projects that would establish a pathway for further development and \nimplementation of AI. \n \n \n2.  An Ecosystem of Trust: Regulatory Framework For AI \n \nDefinition of AI \nThe White Paper gives a broad definition of AI that exacerbates, rather than minimise, legal uncertainty. \nAccording to the document, “AI is a collection of technologies that combine data, algorithms and computing \npower” pointing to the “the main elements that compose AI” being “data” and “algorithms”.  This definition \nis too broad, and it is unclear whether it refers to AI, machine learning or automated decision making. It \ncould potentially apply to any given piece of software, going much beyond the scope of the White Paper. \nFurthermore, the proposed definition does not correspond to the one given by the Commission’s High-Level \nExpert Group on Artificial Intelligence (AI HLEG) in its Ethics Guidelines for Trustworthy Artificial Intelligence, \nwhich was widely endorsed by industry, civil society, and academia. Inconsistent definitions at EU level may \ncreate confusion and uncertainty, in view of the planned next steps that will need to build on a shared \nunderstanding of AI. \nWe recommend to carefully review the definition of AI and carefully delineate its perimeter, drawing from \nthe definition rendered by the HLEG guidelines. An accurate definition of an AI system is the key to a proper \nproblem assessment. \n \nExisting regulatory framework and AI \nToday, no specific legal framework to regulate AI exists in Europe. The development, deployment and use of \nAI systems are subject to a range of horizontal laws and principles such as rules on personal data protection \nand privacy, consumer protection, product safety and liability. \n3', 'We support the proposed approach to first examine the existing regulatory framework and assess whether \nadjustments are necessary to address the emerging risks potentially posed by AI systems, before designing \nadditional legislation. A profound review of current regulation is also crucial to ensure consistency of any AI-\nspecific rules with the broader EU legal framework. We appreciate that legislative review is a time-\nconsuming and sensitive exercise that must be undertaken with caution, involving a relevant corpus of \ncompetencies across various domains, and engaging with stakeholders through a transparent public \nconsultation process.  \n \nRisk-based approach \nETNO supports the proposed risk-based approach to AI regulation, including a conformity assessment \nrequirement exclusively targeted to “high-risk” applications of AI. However, for the sake of greater legal \ncertainty, further adjustments and clarifications are needed.  \nThe cumulative criteria of (1) high-risk sectors and (2) high-risk applications of AI in those sectors are very \nsensible. However, while the former criterion provides regulatory certainty on specific sectors, the latter \ncriterion requires further specification. \nIt should be very clear why certain sectors and applications specifically pose a direct risk of damage, death or \nsignificant physical or non-physical harm to people. The principles that identify a sector or application as \nhigh-risk should then be clearly defined, e.g. by specifying the “significant” impact on affected parties or \nother “exceptional instances”. The White Paper considers significant risks from a very broad viewpoint of \nsafety protection, consumer rights and fundamental rights. \nThe Commission’s intention to periodically review the list of high-risk sectors could bring legal uncertainty to \nall sectors that currently are not placed this category, which could eventually have negative effects on their \ninvestment plans. Therefore, the review periods should be appropriate. \nFurthermore, it should not be forgotten that AI technologies are an integral part of larger value chains, \nwhere different players play different roles. Those players (and sectors at large) may be exposed to different \nlevels of risk, across the value chains, for which more clarity is warranted. How the responsibility for \ncomplying with the mandatory requirements suggested in the paper would be shared among the different \nactors in the chain remains an outstanding issue.  For instance, AI technology can be developed by a \nresearch institution, integrated as part of a service by a commercial player, and deployed and/or sold to end-\nusers by a government agency in a high-risk application. How would responsibility for complying with the \nmandatory requirements be shared between these different actors? The research institution or commercial \nplayer may not know at development stage that the AI system will end up in high-risk application. \nMore specifically, it is not clear whether the providers of AI-enabled business-to-business (B2B) solutions \n(and services to consumer markets) belong to the non-exhaustive list of sectors at p. 17 of the White Paper. \nWould, for example, telecom operators providing AI-enabled ‘smart connectivity’ solutions that guarantee \nquality of the service to hospitals fall under the high-risk sector definition of the White Paper, hence be \nsubject to ex-ante conformity requirements? This could be clarified if the definition of high risk specified that \nthe risk of damage, death or harm should be directly posed by the system itself. This would avoid that a \nsingle AI-based subsystem be considered high-risk depending on where it is deployed. If indirect risks were \nalso factored in, legal uncertainly for all providers and the complexity of ex-ante conformity assessment \nwould increase exponentially.  \n4', 'Finally, it remains unclear whether this definition of high-risk AI application will be consistent with the high-\nrisk definition in the scope of the proposed liability provisions for AI systems2. \n \nRequirements for High-Risk AI Applications \nThe White Paper proposes new measures to remedy “specific features of AI (e.g. opacity) [that] can make \nthe application and enforcement of this legislation more difficult.” Those requirements relate to training \ndatasets, record-keeping, provision of information on AI applications, robustness and accuracy, human \noversight, and specific requirements for facial recognition. \n•  Training data. If it becomes mandatory to check the adherence of trained datasets to EU values and \nrules, the suppliers of such datasets should have an obligation to certify that their data lives up to \nthe standards. Nonetheless, it could often be challenging and impractical to prove the absence of \nbias in the datasets: checking for absence of bias against sensitive information requires that this \ninformation is available in the dataset3. Due to data minimisation requirements provided by the \nGeneral Data Protection Regulation (GDPR) and the possible risk of unwanted access to or disclosure \nof data, organisations may actively decide not to store such sensitive information. It is even more \nchallenging to prove the absence of bias that violates “EU’s values and rules” based on the \nrequirements set out by the White Paper, since they would compel a great level of granularity of \ndatasets involving the processing of special categories of data. The importance of distinguishing the \nresponsibilities of the dataset owner and of the user who applies the dataset must also be made \nclearer. It should be on the AI engineer to validate and explain the training data used. The dataset \nowner should describe and disclose how the dataset was created. \n•  Keeping of records and data. The keeping of records could be a reasonable means to help users \nprove mistakes and harm in the context of liability claims. This requirement should be in accordance \nwith the GDPR’s documentation obligations. As such, the requirement to keep the datasets \nthemselves in certain justified cases should be strictly defined. Additionally, this approach should be \nconsistent with the broader liability framework. \n•  Information provision. We welcome the acknowledgment that the information to be provided \nshould be tailored to the particular context. We believe that it is proportionate for competent \nauthorities to impose reasonable requirements and demand access to results and decisions of the AI \nsystems. Such interventions should respect intellectual property rights and business secrets, to not \ncause harm to competition and innovation. On the contrary, it would be disproportionate to grant \ncompetent authorities’ access to and review of algorithms and data models, unless this is justified by \na risk to public health or national security posed by the AI application at hand.  Additionally, \nrequesting that the data used to train and periodically retrain algorithms be stored for an undefined \namount of time would be disproportionate as the amount of data may be enormous. \n \n2 The report of DG JUST’s Expert Group on Liability and New Technologies is available here: \nhttps://ec.europa.eu/transparency/regexpert/index.cfm?do=groupDetail.groupMeetingDoc&docid=36608 \n3 https://business.blogthinkbig.com/is-your-ai-system-discriminating-without-knowing-it-the-paradox-between-fairness-and-\nprivacy/?_thumbnail_id=5164  \n5', '•  Robustness and accuracy. Alongside the ex-ante conformity assessment requirement, specific \nconditions to ensure resilience to attacks and security (e.g. security-by-design), general safety, \naccuracy and reliability should be considered. Appropriate technical standards could be approved, \npossibly based on the certification mechanisms provided by the Cyber Security Act. However, this \nmust not lead to constraining innovation and growth and preventing EU undertakings from \ndeveloping and applying the AI technology across high-risk applications and sectors, such as public \nhealth, cybersecurity or critical transport infrastructure.  \n•  Human oversight. We generally support such obligation, which should remain limited to high-risk \napplications. The degree of oversight, human in the loop (HITL) or human on the loop (HOTL) could \nvary from one application to another, for the sake of proportionality depending on the risks \ninvolved. Further details are needed on when HITL or HOTL should be adopted. \n•  Biometric identification in the public sphere. We consider such practices as highly problematic, \nespecially against the objective of creating an ecosystem of trust. If biometric identification in public \nspaces were to be permitted in principle, it should always be considered as a “high risk” application. \nAdditional rules should restrain its application, considering its severe impact on individuals and \nsociety. \n \nEx-ante Conformity Assessment \nWe agree that AI systems should reflect European values and rules, and conformity assessments for high-risk \nAI applications would help confirm that this expectation is met. Nonetheless, greater legal certainty around \nthe concrete mechanisms for the ex-ante conformity assessment is needed, ensuring that Europe does not \ncreate red tape for AI industrialization and growth. Ex-ante conformity should remain limited to high-risk AI \nand be carefully designed to avoid inhibiting the development of AI applications in Europe versus other \nregions of the world. If the ex-ante conformity assessment process implied the intervention of external \nentities (e.g., regulatory authorities or assessment bodies) during the development process, the time to \nmarket of AI developed in Europe would be severely slowed down. Finally, ex-ante conformity should also \napply to AI systems developed in third countries before they are deployed in the EU’s single market. \n \nSafety and Liability \nThe emerging AI technologies are set to challenge the current legal framework for product safety and \nproduct liability. At the same time, digital products and services typically include different and interlinked \ntechnologies, with little user awareness. We believe that consumers should be able to rely on consistent and \neasy-to-understand safety and liability rules. As a general principle, ETNO recommends that decisionmakers \nrefrain from creating AI-specific regulation and that they focus on guaranteeing consistency by applying \nhorizontal rules in a technology-agnostic way. This would also allow to apply the EU’s innovation-friendly \napproach to liability to other crucial technologies, such as the IoT. \nTo address potential issues around liability, undertakings along the value chain should ensure clarity within \ntheir contractual arrangements as to which party is responsible for any physical or non-physical harm caused \nby an AI-enabled product or service to individuals. Yet, anticipating the outcomes of fully autonomous AI \nsystems and attributing liability for their damages will become inherently more difficult. \n6', 'Beyond the Product Liability Directive, liability in business-to-consumer (B2C) relations regarding defective \nproducts or services is also based on contractually agreed terms, according for instance to the Sales of Goods \nDirective and the Digital Content Directive. Sector-specific laws, such as the European Electronic \nCommunication Code and the Telecom Single Market Regulation, lay down liability provisions concerning \nconnectivity services. Therefore, consumers can increasingly rely on EU rules that influence their contractual \nrights and that should be considered when assessing potential gaps in the Product Liability Directive or \nProduct Safety Directive.  \nIf liability rules are tightened, the last link in the value chain such as the deployer or trader must not be left \nalone. A better and fairer distribution of liability throughout the value chain, with a focus on AI technology \ndevelopers like hardware manufacturers and software developers, would enable the downstream value \nchain to fulfil their obligations, preventing unfair burdens on them, and guaranteeing security safeguards for \nconsumers. In line with the “polluter pays principle”, obligations to mitigate the risks should target the \nactors that are best suited to held responsibility.  \nReforms to improve consumer safety should rather focus on “ex-ante” mechanisms based on the Product \nSafety Directive, which would set higher safety requirements for getting access to the EU market. This will \nalso directly address those parts of the value chain that should be responsible for the safety of their products \nin the first place. \nFor high-risk AI systems, the framework of ex-ante safeguards via a conformity test before a product is \nplaced onto the market is a reasonable complement to applicable ex-post safeguards that apply after \nplacement in the market, according to liability rules. As already noted, applying the conformity test to AI \napplications that are developed in third countries before they are offered in the EU market or to EU citizens \ncould be a value-add. \nThe review of the Product Safety Directive should take into account the whole cycle of development, \ndeployment and use of emerging AI technology. The introduction of mandatory security updates supply \nshould be an important area of intervention. This needs to go hand in hand with updated liability rules. For \nlow-risk AI applications, users should be obliged to install the software updates made available to them, in \norder to preserve their safety and security and to not lose redress claims in case of damage.  For high-risk \napplications, an obligation for the producer to provide and install such updates should be considered. \n \nPrivacy related matters \nThe GDPR is a principles-based law designed to be future-proof and adaptable to emerging technologies, \nsuch as AI. In addition, the GDPR embeds a risk–based approach that allows for the consideration of risks \nand harms to individuals and to calibrate compliance accordingly. Therefore, as far as personal data is \nconcerned, the risk-based and technology-neutral approach of the GDPR provides a level of data protection \nthat is adequate to the risk of the respective processing also with regards to AI development, deployment \nand use. \nStill, as already observed, several of the GDPR’s provisions (e.g., compatible use, data minimisation, \nautomated decision-making) may lead to tensions with AI applications. New EU guidelines that clarify how \nthe existing data protection and privacy framework applies in the AI context would be more suitable than \nadditional AI-centred privacy legislation. \n7', 'Although the White Paper seems to call for both high-quality training data and strict restrictions on personal \ndata, in practice the two might be in contradiction and need to be balanced. The calibration between \naccuracy, sufficiency, representativeness, and personal data protection should be carefully considered \nbecause limitations to personal data processing may undermine the quality of training data. Training AI \nsystems by processing personal data cannot solely rely on the use of anonymous or non-personal data. \nFurthermore, the application of consent requirements for the use personal data for AI training often results \nin insufficient training datasets in practice. \nUnder the current data protection legislation, AI training on personal data usually falls under the “legitimate \ninterests” legal ground for processing, or can sometimes be considered as compatible further processing. \nThese options should not be further limited. However, the sectors subject more specific privacy laws in \naddition to the GDPR (such as telecoms with the e-Privacy Directive, but also healthcare, finance etc.) often \nare subject to stricter legal grounds and conditions for processing certain types of personal data, which \nseriously undercuts their possibility to use personal data for AI training. To ensure high quality of AI training \ndata, it would be important to have clear and consistent legal grounds for using personal data in AI training, \nwhich could also apply to data subject to sector-specific regulation. \n \nVoluntary labelling \nA voluntary labelling framework aimed at sustaining the uptake of trustworthy AI in Europe is a reasonable \noption.  However, it is necessary to clearly define labels and their assignment mechanism. Criteria \nunderpinning the labels could relate to e.g. transparency, robustness, and human oversight. The role and the \nauthority of testing centres shall be further defined. \nThe voluntary nature of this labelling system should not become a de facto legal requirement for market \naccess. Criteria to comply with such labels must be meaningful to users, while avoiding excessive burdens on \nbusinesses to not discourage them from adopting these labels.  \n \nGovernance  \nSeveral governance and enforcement aspects of the AI framework need to be resolved, as the White Paper \ndoes not provide strong guidance. For example, it is not clear who decides whether an AI application falls \nunder the high-risk regime – whether the EU or national institutions, or rather dedicated watchdogs for AI. \nAlso, regulatory “forum shopping”, whereby a non-EU actor could pick the national regulator of its own \nchoice to gain access to the internal market, should be avoided. \nA flexible and open EU-level governance structure, which delegate enforcement responsibilities to Member \nStates, could be a pragmatic way forward. Such structures must be sufficiently funded and powerful enough \nfor their monitoring and enforcement authority to be effective and consistent across borders. \n \n \n \n \n8', 'Policy contacts:  \nPaolo Grassia, Director of Public Policy \ngrassia@etno.eu   \n \nSara Ghazanfari, Regulation & Economics Manager  \nghazanfari@etno.eu \n \n \n \n9']"
F551052,10 September 2020,Ena Salihovic,Wirtschaftsverband,EuroCommerce,klein (10 bis 49 Beschäftigte),-,Belgien,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"Artificial Intelligence (AI) can offer significant benefits not just to businesses, but also to consumers and society. Retailers and wholesalers are central actors in the supply chain and are in daily contact with Europe’s 450 million consumers. Many retailers and wholesalers use and develop Artificial Intelligence applications to operate sophisticated and efficient systems ensuring reliable and safe sourcing and distribution of goods to meet consumers’ demand and keep them safe. In most cases, AI applications used by retailers and wholesalers carry no direct impact or risks for individuals but improve shopping experiences and internal efficiencies.

EuroCommerce welcomes the opportunity to provide input to the Commission’s Inception Impact Assessment for the upcoming legislative framework on AI. We believe that the combination of set objectives in the Option 1 and Option 3.b are the best to support and foster further AI developments in the EU.

To secure a future-proof framework that will support an innovative and competitive retail and wholesale sector, EuroCommerce believes that:
•	Having a positive narrative towards AI technologies is a prerequisite to unlock Europe’s tech sovereignty. 
•	The future European framework for AI should be technology-neutral and focus more on achieving desirable outcomes rather than regulating AI tools, as it is already the case with existing legislation such as the General Data Protection Regulation. 
•	The future EU framework for AI should support the digital development of SMEs. SMEs need to be supported in their digital transformation and provided with the right set of digital skills and training that will help them responsibly use the potentials offered by AI. 
•	The future European framework for AI should rely on a simple, narrow, clear, and harmonised definition of ‘Artificial Intelligence’.
•	The use of already existing AI applications should not be disrupted.
•	Careful attention should be paid to avoid overregulation – especially considering recently adopted EU and national legislation, support the use of existing AI technologies and bolster innovation.
•	Regulatory sandboxes for testing high-risk AI solutions could potentially enable businesses of all sizes to explore the potential of Artificial Intelligence.
•	The European Commission’s priority should be to work towards a global framework that would secure a level-playing field beyond EU borders.
•	Competition coming from outside the EU should not be ignored.
•	B2B data sharing for the purpose of AI development should remain on a voluntary basis.
•	Investing in skills, digital education and research should be a priority of the EU institutions. 

Please find attached more detailed input to the roadmap on Artificial Intelligence – ethical and legal requirements."
F551051,10 September 2020,Michal Kardos,Wirtschaftsverband,Slovak Alliance for Innovation Economy,sehr klein (1 bis 9 Beschäftigte),-,Slowakei,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"Smart government approaches to regulation will play an important role in boosting public confidence and ensuring that AI is used responsibly, while also encouraging innovation. However, it is important that a proportionate, risk-based approach is taken - balancing potential harms with the social and economic benefits that will be created by AI. In the following points we are providing our feedback to the policy options presented and share our comments on additional points of concern raised by the Commission's initiative.
Policy options:
No EU policy change
We welcome smart approach on setting up rules for AI. However, it is imperative to maintain clear focus in delivering EU policy change while acknowledging a number of already existing rules, including GDPR, medical devices regulation, and fundamental rights aquis. New prescriptive rules should only be considered in the areas where existing regulation is clearly insufficient.
Option 1: EU “soft law”
We are certain that the European AI industry would benefit greatly from the European Commission providing us with guiding principles for self-regulation and co-regulation, as they would play an important role helping European businesses to develop advanced technologies responsibly. We would expect this kind of support from the European Commission even if other policy options are pursued by the Union.
Option 2: EU legislative instrument setting up a voluntary labelling scheme
We are concerned that even a voluntary labelling scheme is likely to create a heavy administrative burden for AI innovators that are often SMEs with limited resources. As a result, the costs of such a scheme could quickly outweigh the benefits of encouraging uptake of AI across Europe. We would especially like to caution basing a labeling scheme on the Assessment List for Trustworthy AI from the EU High-Level Expert Group on AI as its nature inherently limits variation across settings for different cases of application. Just as in Option 1, we would encourage the Commission to work closely with the AI industry to develop a menu of labeling schemes for different AI application settings.
Option 3: EU legislative instrument establishing mandatory requirements for all or certain types of AI applications
We strongly encourage the Commission to factor in the opportunity cost of not using AI when considering any regulatory intervention into AI applications. In deliberating possible options it’s vital to reflect not only potential harms but also societal opportunities. The benefits of AI will often outweigh the risks, especially if risks can be mitigated in a thoughtful way with strong safeguards. Regulation must not discourage AI innovation, development, nor limit its use. Proportionality and clear focus of any regulation will help ensure legal certainty for AI innovators and increase trust in AI without unduly hindering AI-driven innovation.
On the enforcement mechanisms:
We believe a combination of ex-ante risk self-assessment and ex-post enforcement for high risk AI applications would likely achieve desired results within much faster timeframes and without risking unduly stopping innovation and creating unnecessary burdens. We strongly support building on existing industry practices, including ethical, legal and due diligence practices that guide the responsible and trustworthy development of AI. Furthermore, it would be most practical if regulators were to provide clear “due diligence” guidance for quality self-assessment procedures. We also would discourage relying on third party ex-ante assessments as such approach would subject commercial secrets to external exposure risks and due to lack of familiarity could easily misinterpret aspects of an AI system.

For more details please see the attached file. Thank you. Slovak Alliance for Innovation Economy","[""Slovenská aliancia pre inovatívnu ekonomiku \nGrösslingova 6-8, 811 09 Bratislava \n \n   \nResponse to the European Commission’s inception impact assessment on AI ethical and\xa0\n\u200b\nlegal requirements\xa0 \nSmart\xa0government\xa0approaches\xa0to\xa0regulation\xa0will\xa0play\xa0an\xa0important\xa0role\xa0in\xa0boosting\xa0public\xa0confidence\xa0and\xa0\nensuring\xa0that\xa0AI\xa0is\xa0used\xa0responsibly,\xa0while\xa0also\xa0encouraging\xa0innovation.\xa0However,\xa0it\xa0is\xa0important\xa0that\xa0a\xa0\nproportionate,\xa0risk-based\xa0approach\xa0is\xa0taken\xa0-\xa0balancing\xa0potential\xa0harms\xa0with\xa0the\xa0social\xa0and\xa0economic\xa0\nbenefits\xa0that\xa0will\xa0be\xa0created\xa0by\xa0AI.\xa0In\xa0the\xa0following\xa0points\xa0we\xa0are\xa0providing\xa0our\xa0feedback\xa0to\xa0the\xa0policy\xa0\noptions\xa0presented\xa0and\xa0share\xa0our\xa0comments\xa0on\xa0additional\xa0points\xa0of\xa0concern\xa0raised\xa0by\xa0the\xa0Commission's\xa0\ninitiative.\xa0\n \nPolicy options:\xa0\n \nNo EU policy change\xa0\xa0\n \nWe\xa0welcome\xa0smart\xa0approach\xa0on\xa0setting\xa0up\xa0rules\xa0for\xa0AI.\xa0However,\xa0it\xa0is\xa0imperative\xa0to\xa0maintain\xa0clear\xa0focus\xa0\nin\xa0delivering\xa0EU\xa0policy\xa0change\xa0while\xa0acknowledging\xa0a\xa0number\xa0of\xa0already\xa0existing\xa0rules,\xa0including\xa0GDPR,\xa0\nmedical\xa0 devices\xa0 regulation,\xa0 and\xa0 fundamental\xa0 rights\xa0 aquis.\xa0 New\xa0 prescriptive\xa0 rules\xa0 should\xa0 only\xa0be\xa0\nconsidered in the areas where existing regulation is clearly insufficient.\xa0\xa0\n \nOption 1: EU “soft law”\xa0\nWe\xa0are\xa0 certain\xa0that\xa0the\xa0European\xa0AI\xa0industry\xa0would\xa0benefit\xa0greatly\xa0from\xa0the\xa0European\xa0Commission\xa0\nproviding\xa0us\xa0with\xa0guiding\xa0principles\xa0for\xa0self-regulation\xa0and\xa0co-regulation,\xa0as\xa0they\xa0would\xa0play\xa0an\xa0important\xa0\nrole\xa0helping\xa0European\xa0businesses\xa0to\xa0develop\xa0advanced\xa0technologies\xa0responsibly.\xa0We\xa0would\xa0expect\xa0this\xa0\nkind of support from the European Commission even if other policy options are pursued by the Union.\xa0\xa0\n \nOption 2: EU legislative instrument setting up a voluntary labelling scheme\xa0\xa0\n \nWe\xa0are\xa0concerned\xa0that\xa0even\xa0a\xa0voluntary\xa0labelling\xa0scheme\xa0is\xa0likely\xa0to\xa0create\xa0a\xa0heavy\xa0administrative\xa0burden\xa0\nfor\xa0AI\xa0innovators\xa0that\xa0are\xa0often\xa0SMEs\xa0with\xa0limited\xa0resources.\xa0As\xa0a\xa0result,\xa0the\xa0costs\xa0of\xa0such\xa0a\xa0scheme\xa0could\xa0\nquickly\xa0outweigh\xa0the\xa0benefits\xa0of\xa0encouraging\xa0uptake\xa0of\xa0AI\xa0across\xa0Europe.\xa0We\xa0would\xa0especially\xa0like\xa0to\xa0\ncaution\xa0basing\xa0a\xa0labeling\xa0scheme\xa0on\xa0the\xa0Assessment\xa0List\xa0for\xa0Trustworthy\xa0AI\xa0from\xa0the\xa0EU\xa0High-Level\xa0Expert\xa0\nGroup\xa0on\xa0AI\xa0as\xa0its\xa0nature\xa0inherently\xa0limits\xa0variation\xa0across\xa0settings\xa0for\xa0different\xa0cases\xa0of\xa0application.\xa0Just\xa0\nas\xa0in\xa0Option\xa01,\xa0we\xa0would\xa0encourage\xa0the\xa0Commission\xa0to\xa0work\xa0closely\xa0with\xa0the\xa0AI\xa0industry\xa0to\xa0develop\xa0a\xa0\nmenu of labeling schemes for different AI application settings.\xa0\xa0\n \nOption\xa03:\xa0EU\xa0legislative\xa0instrument\xa0establishing\xa0mandatory\xa0requirements\xa0for\xa0all\xa0or\xa0certain\xa0types\xa0of\xa0AI\xa0\napplications\xa0\xa0\n \nWe\xa0strongly\xa0encourage\xa0the\xa0Commission\xa0to\xa0factor\xa0in\xa0the\xa0opportunity\xa0cost\xa0of\xa0not\xa0using\xa0AI\xa0when\xa0considering\xa0\nany\xa0regulatory\xa0intervention\xa0into\xa0AI\xa0applications.\xa0In\xa0deliberating\xa0possible\xa0options\xa0it’s\xa0vital\xa0to\xa0reflect\xa0not\xa0\nonly\xa0potential\xa0harms\xa0but\xa0also\xa0societal\xa0opportunities.\xa0The\xa0benefits\xa0of\xa0AI\xa0will\xa0often\xa0outweigh\xa0the\xa0risks,\xa0\nespecially\xa0if\xa0risks\xa0can\xa0be\xa0mitigated\xa0in\xa0a\xa0thoughtful\xa0way\xa0with\xa0strong\xa0safeguards.\xa0Regulation\xa0must\xa0not\xa0\ndiscourage\xa0AI\xa0innovation,\xa0development,\xa0nor\xa0limit\xa0its\xa0use.\xa0Proportionality\xa0and\xa0clear\xa0focus\xa0of\xa0any\xa0regulation\xa0\nwill\xa0help\xa0 ensure\xa0 legal\xa0 certainty\xa0for\xa0AI\xa0innovators\xa0and\xa0increase\xa0trust\xa0in\xa0AI\xa0without\xa0unduly\xa0hindering\xa0\nAI-driven innovation.\xa0\n \n3a) EU legislative instrument limited to a specific category of AI applications\xa0\xa0\n \nWe\xa0welcome\xa0 the\xa0Commission's\xa0 approach\xa0 on\xa0 singling\xa0 out\xa0 clear\xa0 areas\xa0of\xa0 AI\xa0 application,\xa0 like\xa0remote\xa0\nbiometric\xa0identification\xa0or\xa0facial\xa0recognition,\xa0when\xa0considering\xa0future\xa0mandatory\xa0requirements.\xa0In\xa0this\xa0\nparticular\xa0case\xa0there\xa0appears\xa0to\xa0be\xa0a\xa0widespread\xa0consensus\xa0across\xa0the\xa0European\xa0society\xa0that\xa0the\xa0use\xa0of"", 'Slovenská aliancia pre inovatívnu ekonomiku \nGrösslingova 6-8, 811 09 Bratislava \n \n   \nfacial\xa0recognition\xa0technologies\xa0for\xa0mass\xa0surveillance\xa0should\xa0be\xa0classified\xa0as\xa0a\xa0high-risk\xa0application\xa0and\xa0\nsubjected\xa0to\xa0straightforward\xa0mandatory\xa0regulation.\xa0We\xa0encourage\xa0the\xa0Commission\xa0to\xa0approach\xa0further\xa0\nhigh-risk AI regulation discussion in a similarly focused manner.\xa0\xa0\n \n3b) EU legislative instrument limited to “high-risk” AI applications\xa0\n \nWe\xa0support\xa0the\xa0proposed\xa0option\xa0to\xa0limit\xa0future\xa0AI\xa0regulation\xa0to\xa0“high-risk”\xa0AI\xa0applications\xa0only.\xa0However,\xa0\nwe\xa0would\xa0like\xa0to\xa0emphasise\xa0the\xa0need\xa0for\xa0proportionality\xa0when\xa0defining\xa0“high-risk”\xa0applications\xa0of\xa0AI.\xa0In\xa0\ndoing\xa0so\xa0it\xa0is\xa0important\xa0to\xa0reflect\xa0the\xa0probability\xa0of\xa0harm\xa0and\xa0not\xa0just\xa0the\xa0possible\xa0severity\xa0of\xa0the\xa0harm.\xa0It\xa0\nshould\xa0 also\xa0 take\xa0 account\xa0 of\xa0 the\xa0 wider\xa0 operational\xa0 context\xa0 when\xa0assessing\xa0 risk,\xa0 since\xa0the\xa0same\xa0AI\xa0\napplication\xa0used\xa0for\xa0the\xa0same\xa0purpose\xa0will\xa0pose\xa0different\xa0risks\xa0depending\xa0on\xa0the\xa0way\xa0it\xa0is\xa0integrated\xa0into\xa0\nbusiness\xa0operations\xa0(e.g.,\xa0extent\xa0of\xa0human\xa0oversight,\xa0 additional\xa0safeguards\xa0such\xa0as\xa0monitoring).\xa0We\xa0\nsuggest using a combination of sector and use/application as criteria to set up the risk-based approach.\xa0\xa0\n\xa0\n \n3c) EU legislative act covering all AI applications\xa0\n \nAs\xa0mentioned\xa0in\xa0the\xa0previous\xa0feedback,\xa0we\xa0believe\xa0that\xa0future\xa0regulation\xa0on\xa0AI\xa0should\xa0be\xa0limited\xa0to\xa0\n“high-risk”\xa0 applications\xa0 only.\xa0 Otherwise\xa0 AI\xa0 applications\xa0posing\xa0no\xa0significant\xa0risk\xa0or\xa0harm\xa0would\xa0be\xa0\nsubjected\xa0to\xa0disproportionate\xa0rules\xa0that\xa0in\xa0no\xa0way\xa0would\xa0advance\xa0development,\xa0trust\xa0and\xa0adoption\xa0of\xa0AI\xa0\nsolutions\xa0in\xa0Europe.\xa0Over\xa0regulating\xa0application\xa0of\xa0such\xa0a\xa0promising\xa0technology\xa0would\xa0likely\xa0result\xa0in\xa0\nsevere\xa0opportunity\xa0 costs\xa0 for\xa0 the\xa0 society\xa0 and\xa0 quite\xa0 possibly\xa0lower\xa0 the\xa0 bar\xa0 for\xa0AI\xa0applications\xa0with\xa0\nsignificant risks.\xa0 \xa0\n \nOn the enforcement mechanisms:\xa0\n \nWe\xa0believe\xa0 a\xa0 combination\xa0of\xa0 ex-ante\xa0 risk\xa0self-assessment\xa0and\xa0ex-post\xa0enforcement\xa0for\xa0high\xa0risk\xa0AI\xa0\napplications\xa0would\xa0 likely\xa0 achieve\xa0desired\xa0results\xa0within\xa0much\xa0faster\xa0timeframes\xa0and\xa0without\xa0risking\xa0\nunduly\xa0stopping\xa0innovation\xa0and\xa0creating\xa0unnecessary\xa0burdens.\xa0We\xa0strongly\xa0support\xa0building\xa0on\xa0existing\xa0\nindustry\xa0practices,\xa0including\xa0ethical,\xa0legal\xa0and\xa0due\xa0diligence\xa0practices\xa0that\xa0guide\xa0the\xa0responsible\xa0and\xa0\ntrustworthy\xa0development\xa0of\xa0AI.\xa0Furthermore,\xa0it\xa0would\xa0be\xa0most\xa0practical\xa0if\xa0regulators\xa0were\xa0to\xa0provide\xa0\nclear\xa0“due\xa0diligence”\xa0guidance\xa0for\xa0quality\xa0self-assessment\xa0procedures.\xa0We\xa0also\xa0would\xa0discourage\xa0relying\xa0\non\xa0third\xa0 party\xa0 ex-ante\xa0 assessments\xa0as\xa0such\xa0approach\xa0would\xa0subject\xa0commercial\xa0secrets\xa0to\xa0external\xa0\nexposure risks and due to lack of familiarity could easily misinterpret aspects of an AI system.\xa0\xa0\n \nOn the scope of the initiative:\xa0\n \nA\xa0clear\xa0and\xa0widely\xa0understood\xa0definition\xa0of\xa0AI\xa0is\xa0critical\xa0to\xa0the\xa0effectiveness\xa0of\xa0the\xa0future\xa0regulatory\xa0\nframework.\xa0Just\xa0as\xa0the\xa0Commission’s\xa0White\xa0Paper\xa0on\xa0AI\xa0described\xa0the\xa0main\xa0elements\xa0of\xa0AI\xa0as\xa0data\xa0and\xa0\nalgorithms\xa0so\xa0is\xa0this\xa0Inception\xa0Impact\xa0Assessment\xa0is\xa0trying\xa0to\xa0suggest\xa0too\xa0broad\xa0of\xa0a\xa0scope\xa0for\xa0future\xa0AI\xa0\nregulation.\xa0For\xa0example\xa0if\xa0AI\xa0were\xa0to\xa0be\xa0defined\xa0as\xa0“automated\xa0decision-making”\xa0it\xa0would\xa0miss\xa0out\xa0on\xa0the\xa0\ndesired\xa0risk-based\xa0focus\xa0and\xa0would\xa0include\xa0automated\xa0systems\xa0that\xa0do\xa0not\xa0pose\xa0any\xa0risk.\xa0Any\xa0future\xa0AI\xa0\nregulation\xa0should\xa0avoid\xa0disproportional\xa0and\xa0unjustified\xa0regulatory\xa0obligations\xa0as\xa0it\xa0would\xa0otherwise\xa0have\xa0\nadverse effects on the development and deployment of AI-based applications in Europe.\xa0\xa0\n \n\xa0\n\xa0\nOn the immaterial harm:\xa0\xa0\n \nImmaterial\xa0harm\xa0is\xa0not\xa0a\xa0known\xa0legal\xa0concept\xa0and\xa0could\xa0mean\xa0anything\xa0from\xa0economic\xa0loss\xa0to\xa0hurt\xa0\nemotions,\xa0and\xa0 could\xa0lead\xa0to\xa0legal\xa0uncertainty,\xa0discouraging\xa0investment\xa0and\xa0innovation.\xa0In\xa0order\xa0to\xa0bring', 'Slovenská aliancia pre inovatívnu ekonomiku \nGrösslingova 6-8, 811 09 Bratislava \n \n   \npractical\xa0clarity\xa0for\xa0the\xa0AI\xa0innovators\xa0it\xa0would\xa0be\xa0best\xa0to\xa0consider\xa0an\xa0alternative\xa0concept\xa0-\xa0“significantly\xa0\nrestricting\xa0the\xa0exercise\xa0of\xa0fundamental\xa0rights,”\xa0which\xa0we\xa0believe\xa0would\xa0be\xa0easier\xa0to\xa0interpret\xa0and\xa0align\xa0\nwith the existing legislative framework.']"
F551050,10 September 2020,Denise Amram,Universität/Forschungseinrichtung,LIDER Lab Scuola Superiore Sant'Anna,groß (250 oder mehr Beschäftigte),-,Italien,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"Feedback for the EU Commission Inception Impact Assessment towards a “Proposal for a Regulation of the European Parliament and the Council laying down requirements for Artificial Intelligence”
Denise Amram – Giovanni Comandé 
*LIDER Lab, DIRPOLIS Institute, Scuola Superiore Sant’Anna (Pisa- Italy)

Table of contents: 1. Introduction. 2. Selecting Option 4 with option 3.c as a baseline (Option 4+3.c) 3. A combined approach towards AI Regulation. 4. A role for the GDPR.

1.	Introduction.
This feedback is provided considering the ongoing studies undertaken within the research lines RIGHTS in the classifying society (https://www.lider-lab.sssup.it/lider/rights/) and ETHOS (EThics & law witH and fOr reSearch, https://www.lider-lab.sssup.it/lider/ricerca/linee/ethos-ethics-law-with-and-for-research/) developed within the LIDER Lab research activities (www.lider-lab.eu)  at Scuola Superiore Sant’Anna (SSSA; www.santannapisa.it).
Our remarks focus on two main issues: 1) providing operational tools to link the ethics and the legal dimension of a Trustworthy AI avoiding risks of ethics washing; 2) the role that the EU Regulation 2016/679 (General Data Protection Regulation, hereinafter “GDPR”) may play to achieve the purposes of the EU Strategy on Artificial Intelligence, providing a multilevel legal framework that may include a General Regulation on Artificial Intelligence and specific safeguards both in terms of national and domain legislation, as well as  in terms of soft law. 

Please see the attachment for further details.","['Feedback for the EU Commission Inception Impact Assessment towards a “Proposal \nfor a Regulation of the European Parliament and the Council laying down requirements \nfor Artificial Intelligence” \nDenise Amram – Giovanni Comandé1 \n*LIDER Lab, DIRPOLIS Institute, Scuola Superiore Sant’Anna (Pisa- Italy) \n \nTable of contents: 1. Introduction. 2. Selecting Option 4 with option 3.c as a baseline \n(Option 4+3.c) 3. A combined approach towards AI Regulation. 4. A role for the GDPR. \n \n1.  Introduction. \nThis feedback is provided considering the ongoing studies undertaken within the research lines \nRIGHTS in the classifying society (https://www.lider-lab.sssup.it/lider/rights/) and ETHOS \n(EThics & law witH and fOr reSearch, https://www.lider-lab.sssup.it/lider/ricerca/linee/ethos-\nethics-law-with-and-for-research/)  developed  within  the  LIDER  Lab  research  activities \n(www.lider-lab.eu)2 at Scuola Superiore Sant’Anna (SSSA; www.santannapisa.it). \nOur remarks focus on two main issues: 1) providing operational tools to link the ethics and the \nlegal dimension of a Trustworthy AI avoiding risks of ethics washing; 2) the role that the EU \nRegulation 2016/679 (General Data Protection Regulation, hereinafter “GDPR”) may play to \nachieve the purposes of the EU Strategy on Artificial Intelligence, providing a multilevel legal \n \n1 Denise Amram, PhD (SSSA, is currently Affiliate Researcher at Scuola Superiore Sant’Anna and Data Protection \nOfficer (denise.amram@santannapisa.it) – Giovanni Comandé, PhD (SSSA) LLM (Harvard), is Full Professor of \nPrivate  Comparative  Law  at  Scuola  Superiore  Sant’Anna  and  Director  of  LIDER  –  Lab \n(giovanni.comande@santannapisa.it). \n2 This position paper has been developed within the “SoBigData Plus Plus: European Integrated Infrastructure for \nSocial Mining and Big Data Analytics” Project, funded by the EU Commission under the H2020 INFRAIA-1-\n2019 programme (GA 871042), starting from the ideas developed in Denise Amram. The Role of the GDPR in \nDesigning  the  European  Strategy  on  Artificial  Intelligence:  Law-Making  Potentialities  of  a  Recurrent \nSynecdoche.  Opinio  Juris  in  Comparatione,  [S.l.],  jul.  2020.  ISSN  2281-5147.  Available  at: \nhttp://www.opiniojurisincomparatione.org/opinio/article/view/145/153 and Giovanni Comandé. Unfolding the \nlegal component of trustworthy AI: a must to avoid ethics washing. In Annuario di diritto comparato, ESI, 2020, \nforthcoming (available at https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3690633). \n1', 'framework that may include a General Regulation on Artificial Intelligence and specific \nsafeguards both in terms of national and domain legislation, as well as  in terms of soft law.  \n \n2.  Selecting Option 4 with option 3.c as a baseline  (Option 4+3.c) \nBoth addressed issues lead to suggest selecting option 4 (combination of the other options) \nusing as a starting point option 3 (we call this Option 4+3.c).  \nIndeed, establishing general principles on mandatory requirements on issues such as training \ndata, record-keeping about datasets and algorithms, information to be provided, robustness \nand accuracy and human oversight would offer an horizontal light set of rules enabling \nguidance for a reliable uptake on the market of AI solutions (option 4 based on 3.c as a \nbaseline)  on  which  to  innervate  eventual  specific  (high-risk  or  category  specific  AI \napplications). Such a combination would a) offer a clear framework, easily linked to the \noverarching protection of fundamental rights and  flexible enough to enable malleable degrees \nof self-regulation to be adapted according to the specificities (risks, typology of AI, sector of \ndeployment,…). \nOption 4+3.c would help to determine whether existing and applicable legal rules suffice \naccording to the contexts and manners in which the AI is developed and deployed, enabling \nthe unfolding of the legal leg of trustworthy AI as a scalable and flexible approach that \nranges effectively between soft-law, labeling and mandatory requirements. These efforts \nneed to consider that only some “requirements are already reflected in existing legal or \nregulatory  regimes  while  most  of  those  regarding  transparency,  traceability  and  human \noversight are not addressed by the legislation in many economic sectors”3.  Thus, Option 4+3.c \nwould downplay requirements for mere industrial applications to optimize processes with \nlittle or no human involvement/impact. Also, it should take into account the relationships \nbetween  different  intelligent  agents  and  between  different  players  to  avoid  the \ncompartmentation of tasks or ownerships which could (intentionally or inadvertently) hide \nhigh human impacts in a setting of Chinese walls. Such an approach would at the same \n \n3 White paper at 11. \n2', ""time avoid the costs of an unclear and/or fragmented legal framework while reaping the \nbenefits of avoiding unnecessary regulatory costs (e.g. to an AI system where trust is not \nrelevant and thus a human impact assessment results are low). \nIn fact, different domains of AI application require more or less different rules, but also \ndifferent applications in the same domain might need dissimilar rules. From the end-users \nperspective, AI could exploit known as well as new vulnerabilities with a different impact on \nfundamental rights. While a one rule fits all is not advisable, the absence of a general set of \nrules able to operationalize general principles might lead to further fragmentation both \nin  legal  rules  with  ensuing  market  fragmentations  and  barriers.  There  are  intuitive \ndifferences in the deployment of an AI based solution for illness screening and diagnosis or of \nan AI for designing the appropriate treatment for individual patients. Similarly, there are \ndifferences if AI is deployed in a high-level institution or at smaller institution / general \npractitioner level. The implications in terms of automation and translational biases are diverse4, \nas well as the implications in terms of liability. Conversely, in other instances, it is different if \nthe AI exploits a known vulnerability5 or not. \nRelying only on ethical rules, voluntary labelling or soft-law, can end up in many problems \nfor all stakeholders. First of all, it gives the impression that fulfilling certain ethical \nguidelines absolves one of regulatory obligations, leading to a potential legal disaster for \nbusinesses that might find themselves exposed to liability or fines, for instance. A very \nsimple example is offered by possible liabilities that emerge in the use of certain personal \ndatasets to produce an AI system, a use that needs to adhere to the GDPR to avoid \ntriggering its art. 82 (liability) and art. 83 (fines). Of course, indiscriminate data use leaves \ndata subjects exposed and limits the trustworthiness of AI and its uptake. \nConversely, several players can establish their own ethical guidelines on the pretext of a \nloose and unclear legal framework which, even when followed, might not produce any \n \n4  G.  COMANDÉ,  Intelligenza  artificiale  e  responsabilità  tra  «liability»  e  «accountability».  Il  carattere \ntrasformativo dell’IA e il problema della responsabilità in Analisi Giuridica dell'Economia, 1, 2019, pp 169-188 \n5 This is very well illustrated in the automated decision making context for which the WP29 clarified that an \nimportant factor in the art. 22 GDPR analysis is whether “knowledge of the vulnerabilities of the data subjects \ntargeted” is used. Article 29 Working Party, Guidelines on Automated individual decision-making and Profiling \nfor the purposes of Regulation 2016/679, WP 251 rev.01, 22. \n3"", 'actual constraints to abuses, while giving end users, customers, consumers, and regulators \nthe impression of  a trustworthy solution. Such  an illusion  causes  a general societal \nspillover in case of litigation related to the AI solutions, and fueling micro-harms to \nindividuals and groups. \nIn any cases, inaction and uncertainty are inefficient as they leave to ex post legal rules, to case \nlaw and litigation, the solution of individual cases in order to offer guidelines for the future.   \nIncidentally, it is worth noting that the establishment of legal parameters helps to level the \nregulatory playing field, thus enabling a better governance of competition. \nWe stress the urgent need to give content to the legal dimension of Trustworthy AI and \nthus avoid risks of ethics washing for a much required regulatory framework for the entire \nindustry. To date the debate has been swinging between downplaying the role of legal rules \nup to ethics washing6 (an exercise on which the HLEGAI Guidelines are accused as well7) \nand stressing the threats versus the opportunities of AI8. The initiative “Proposal for a legal act \nof  the  European  Parliament  and  the  Council  laying  down  requirements  for  Artificial \nIntelligence” has the opportunity to overcome these criticalities. \nA very large number of initiatives, both public and private, are targeting ethical principles \nfor AI. Any list can only be incomplete9. On the other hand, to maintain a sufficiently high \n \n6  See  B.  WAGNER,  Ethics  as  an  Escape  from  Regulation:  From  ethics-washing  to  ethics-shopping?, \nhttps://www.privacylab.at/wp-content/uploads/2018/07/Ben_Wagner_Ethics-as-an-Escape-from-\nRegulation_2018_BW9.pdf; G. COMANDÉ, Unfolding the legal component of trustworthy AI: a must to avoid \nethics washing, in Annurio di diritto comparato, 2020 , forthcoming . \n7 See M. VEALE, A Critical Take on the Policy Recommendations of the EU High-Level Expert Group on Artificial \nIntelligence, in European Journal of Risk Regulation, 2020 doi:10/ggjdjs.  \n8GIUSEPPE CONTISSA, FRANCESCA LAGIOIA, MARCO LIPPI, HANS-WOLFGANG MICKLITZ PRZEMYSŁAW PAŁKA, \nGIOVANNI SARTOR AND PAOLO TORRONI, Proceedings of the Twenty-Seventh International Joint Conference on \nArtificial Intelligence, IJCAI-18. \n9 The following is just a first incomplete list: AI Now Institute (AINI), Association for Computing Machinery \n(ACM) with its Committee on Professional Ethics (https://ethics.acm.org/2018-code-draft-2/), the Public Policy \nCouncil (https://acm.org/public-policy/usacm), the so called the Asilomar Principles of the Future of Life Institute \n(https://futureoflife.org/ai-principles/), the Foundation for Responsible Robotics (http://responsiblerobotics.org), \nGoogles AI Principles (https://www.blog.google/technology/ai/ai-principles), The Institute of Electrical and \nElectronics Engineers (IEEE) Global Initiative on Ethics of Autonomous and Intelligent Systems (IEEE Global \nInitiative on Ethics of Autonomous and Intelligent Systems, Ethically Aligned Design: A Vision for Prioritizing \nHuman  Well-being  with  Autonomous  and  Intelligent  Systems,  Version  2, \nhttp://standards.ieee.org/develop/indconn/ec/autonomous_  systems.html),  OpenAI  (https://openai.com/), \nPartnership on AI (a partnership on AI industry-led set up by Google, Apple, Facebook, Amazon, IBM, and \nMicrosoft https://www.partnershiponai.org/),  Software and Information Industry Association (SIIS) (SIIS, Ethical \nPrinciples  for  Artificial  Intelligence  and  Data  Analytics,  2017, \nhttp://www.siia.net/LinkClick.aspx?fileticket=b46tNqJuiJA%3d&tabid=577&portalid=0&mid=17113),  The \n4', 'level of normativity, some declarations/guidelines anchor them to the normativity of \nfundamental rights as received in legally binding documents. This is the case for the \nCouncil of Europe (CoE)10, the HLEGAI, and the European Group on Ethics in Science and \nNew Technologies11. In these cases, however, there is a risk of collapsing different levels \nof  normativity  (ethics  and  law).  It  is  also  for  this  reason  that  the  debate  and  the \nnational/international actions still lack a sufficient regulatory grip. By regulatory grip we \ndo not mean strings and laces for businesses but clear guidance. An abstract declaration of \na principle, even if it can ideally find almost unanimous agreement, does not provide \nstakeholders with certainties to act upon, leading either to inaction – waiting for a clear-\ncut framework – or to abuses, and the exploitation of an apparent regulatory gap. \nAsking to be “ethically correct” or merely rely on soft-law and/or labelling schemes at this \nstage of AI development is like asking car drivers to be attentive in an era in which speed limits, \ntraffic lights and rules were not in place, and driver license or ban on drunk-driving were not \nenacted12. Can an active and trusted AI industry unfold in such a vacuum? Is it not this \nregulatory gap the real roadblock to innovation and uptake? \nWith no attempt to operationalize in legal terms the aspiration to technical correctness and legal \nfairness of data production and use, such a demand is toothless. This operationalization is much \ndemanded and can come only through the legal system.  \nAny labelling or soft law tool will be more effective and procompetitive if deployed against a \nsolid bedrock of legal rules. \nLast but not least, uncertainty has a cost, and trial and error is not a very efficient and reassuring \nsolution for businesses or individuals. Indeed, a cloudy framework does not offer individuals \nthe required awareness of their rights, nor of something bad that might happen to them (micro-\n \nWorld Economic Forum’s Center for the Fourth Industrial Revolution (https://www.weforum.org/center-for-the-\nfourth-industrial-revolution/areas-of-focus). See also https://ethicsinaction.ieee.org/. \n10 EUROPEAN COMMISSION FOR THE ADMINISTRATION OF JUSTICE (CEPEJ). (2018). European ethical charter on \nthe  use  of  Artificial  Intelligence  in  judicial  systems  and  their  environment.  Retrieved  from: \nhttps://rm.coe.int/ethical-charter-en-for-publication-4-december-2018/16808f699c \n11  EUROPEAN  GROUP  ON  ETHICS  IN  SCIENCE  AND  NEW  TECHNOLOGIES.  (2018).  Statement  on  Artificial \nIntelligence,  robotics  and  ‘autonomous  systems’.  Retrieved  from: \nhttps://ec.europa.eu/research/ege/pdf/ege_ai_statement_2018.pdf  Electronic  copy  available  at: \nhttps://ssrn.com/abstract=3414805 proposed ‘a set of basic principles and democratic prerequisites, based on the \nfundamental values laid down in the EU Treaties and in the EU Charter of Fundamental Rights \n12 Incidentally to have all these “common sense” rules took decades in the 20th century. \n5', 'harms hardly are perceived, but they accumulate). Thus, Option 0, as a scenario, delays the \nprocess of regulating by ex post litigation and benefits the least compliant players on the \nmarket, while disadvantaging both the most compliant ones and SMEs and start-ups that either \nincur higher costs or do not have sufficient resources to compete in a fully compliant way. \n \n3.  A combined approach towards AI Regulation. \nThe Inception Impact Assessment lists the EU initiatives in terms of soft law regulation that \nhelped to define the purposes of the legislative initiatives of AI: namely to build up a \n“trustworthy” ecosystem, considering the need to protect fundamental rights both within the \ndevelopment and the application of the given AI technology.  \nAs known, the High-Level Expert Group on Artificial Intelligence (HLEGAI) distinguished \nthe three pillars 1) lawful, 2) ethical and 3) robust that are clearly interconnected, following an \noverall approach grounded on fundamental rights protection.  \nThe option we suggested above (lines 24 and ff) should not be a straightjacket, but an enabler \nof innovation and fast uptake against a background of clear protection for fundamental rights \nand liberties. It is misleading to think legal rules in terms of roadblocks to innovation. To the \ncontrary, a weak or unclear legal framework can very easily backfire against the development \nand deployment of AI-based products and services by undermining trust in them.  \nA clear example supporting our considerations is offered by the new vulnerability related to AI \nbased solutions. Vulnerability of individuals and groups is often evoked in ethical charters \nand declarations, and the HLEGAI Guidelines are no exception, requiring to “pay particular \nattention  to  situations  involving  more  vulnerable  groups  such  as  children,  persons  with \ndisabilities and others that have historically been disadvantaged or are at risk of exclusion, and \nto situations which are characterised by asymmetries of power or information, such as between \nemployers and workers, or between businesses and consumers” and to acknowledge that  “AI \nsystems also pose certain risks and may have a negative impact, including impacts which may \nbe  difficult  to  anticipate,  identify  or  measure  (e.g.  on  democracy,  the  rule  of  law  and \ndistributive justice, or on the human mind itself.)”. \n6', 'Once again, the ethical call can be seen as a means to distract attention from regulatory needs, \nimplying the sufficiency of abiding by ethical principles. On the contrary, the mentioned claims \nfor ethics can offer interpretative guidance for legal rules applicable to AI-based solutions \nturning the risk of ethics washing into a path to unleash the evolution of legal rules requiring \nonly to frame the general principles applicable to all AI solutions and leave to specific \nregulatory or soft-law interventions special situations (high-risks or specific sectors). \nIndeed, more in general we should ask ourselves why AI should require a different (allegedly \nharsher) treatment  than other technologies which “benefitted” from  years of loose legal \nframeworks. The reason maybe because, as anticipated, it enables the exploitation of a number \nof unseen vulnerabilities that have not (yet) triggered a legal reaction, and it does so in a \nmassive,  personalized  way13,  silently  clearing  activities  and  results  that  are  ethically \nquestionable but not yet framed by the law, at least because their specific impact as such might \nnot trigger existing laws, while its effect on individuals accumulate. In this sense, ethics and \nlaw are mutually complementary, but  this complementarity needs to  be operationalized, \notherwise it will only lead to potential ethics washing of the regulatory needs. \nThus, if AI offers the opportunity to exploit specific biases, individual and even transient \nvulnerabilities to establish contingent and lasting asymmetries of powers, a set of reactions \nneed to be put in place to prevent these risks and to reap all the immense benefits \nTrustworthy AI can bring to humanity \nA similar interplay of the “legal leg” of Trustworthy AI with its “ethics leg” can be \nidentified with its robustness component “both from a technical and social perspective, since, \neven with good intentions, AI systems can cause unintentional harm”.  \nThe call for robustness of AI is more or less a call for security from internal flaws and external \nattacks14 to avoid expanding the unacceptable reach to old and new vulnerabilities.  \nYet, while technical robustness relates to the appropriateness of technical safeguards “in a \ngiven context, such as the application domain or life cycle phase”, and links to the legal \ndimension in terms of safety and security-related rules, the “social perspective” of robustness \n– by relating to the context and environment in which the system operates – is open again to \n \n13 R. CALO, Digital Market Manipulation, in George Washington Law Review, 82, 2014, 995. \n14 “3. it should be robust, both from a technical and social perspective, since, even with good intentions, AI \nsystems can cause unintentional harm”. \n7', ""law but in a different way, requesting to consider closely the actual deployment of the AI \nsolution:  calls  for  legal  solutions  for  AI  in  the  context.  Robustness  relates  to  how  the \noperational context may render an AI solution “unfair”, how it may lead to asymmetries worth \nrebalancing, and how it may replicate past biases and discriminations or create new ones15. It \nreflects the characteristic of AI solutions of creating/exploiting context-specific vulnerabilities \nor undiscovered ones. \nReference to old and new vulnerabilities recalls a dynamic but slowly evolving divide between \nthose vulnerabilities that have emerged in the domain of legal relevance over time (gender, \nreligion,  political  opinions, age,   status,  ethnic origins,  etc.), gaining  different levels  of \nsectoral/vertical legal recognition and protection for instance in terms of antidiscrimination \nlaw, consumer law, and the like. On the other hand, it recalls the “new” vulnerabilities \nenhanced  by  AI-led  solutions  (e.g.  the  exploitation  of  transient  mood  statuses,  specific \nsituations distinguishing purchase assumed to be for personal or business reasons, the perils of \nmass  personalization  for  individual  autonomy,  the  massive  personalized  exploitation  of \ncognitive biases via dark patterns16, etc.) that would not be relevant under colour of law as \nsuch. \nOnly against a set of general principles it is possible to enable the coordinated reading of \nexisting applicable legal rules. Indeed, in the interplay between fundamental rights enshrined \nin constitutions, treaties or charters, and material rules, there is a large scope for reinterpretation \nand balancing; but this balancing approach cannot be left to simple ethical considerations, since \nthis may harm at the same time a rising industry for lack of clear red tapes and individual rights \nfor lack of enforceability. \n4.  A role for the GDPR \nTo deal with the lawfulness component shall not be limited to the compliance with the \napplicable data protection regulation or to the existing framework, even if the GDPR approach \n \n15 Especially on exposing the discrimination risks see S. RUGGIERI, D. PEDRESCHI, F. TURINI, Data mining for \ndiscrimination discovery.  ACM Transactions on Knowledge Discovery from Data, 4(2):9:1–9:40, May 2010. \n16 Dark patterns are tricks used mostly in websites and apps that make the user do things that she didn't mean to, \nlike buying or signing up for something. \n8"", 'might be considered as the first ally in the setting of a fundamental rights-based legal \nframework. Yet, the GDPR can offer some useful hints. \nIn this regard, the structure of the GDPR may influence the new legislative process, as follows. \n-  AI solutions are based on information: collection, generation, understanding, and \nexploitation are the main processing activities.  \n-  The risk-based approach aiming at reaching an acceptable level of fundamental rights \nprotection is functional to induce the development of trustworthy AI solutions by design \nand by default. \n-  It would be useful to identify roles and responsibilities while designing the AI system, \nincluding who is responsible, accountable, and liable to assess risks that may occur in \nthe given AI solution; who is monitoring its development, and who has to comply with \nthe domain regulatory framework according to the current standards and scientific \nknowledge. \n-  While the GDPR assigns obligations to the one who determines “means and purposes \nof the personal data processing”, within the AI context, an “AI controller” could be \nidentified  as  the  one  who  determines  methods  for  data  acquisition  (i.e.  what \nfunction/algorithm is chosen and which data train the algorithm), actions required (i.e. \nwhat task shall the AI perform), and goals (i.e. which is the final purpose of the \nautomated decision making/reasoning activity). \n-  In addition, a series of roles could be identified to support the AI controller in the \nassessment and monitoring activities: likewise the GDPR, an internal distribution of \nroles and responsibilities, a so-called RACI matrix - that identifies who is Responsible, \nAccountable, Consultable, and Informed of the AI processing.  \n-  Synergies between other roles could be included (i.e. collaboration between the AI \ncontroller  and  the  data  protection  officer  as  well  as  the  identification  of  “AI \nofficer/advisor” and the IT manager). \n9', '-  As a consequence, an AI external governance could be defined in case more than one \nAI controller is involved in determining methods, actions, and goals17. \nThese features might be included in a binding harmonised regulation, while possible safeguards \nmight be introduced (as well as confirmed) at national level, according to specific domains \n(e.g. workplace, healthcare, etc.), as well as in terms of soft law regulation. \nSpecial attention shall be given to the fact that AI-based technologies enable the identification \nand  exploitation  of  many  conditions  of  weakness  even  unknown  to  the  individuals \nthemselves18. Vulnerability can derive from the development of AI (e.g. bias) or its application. \nThis very simple fact makes AI-based vulnerabilities contextual and possibly endless, leading \nto situations of inferiority, dependency, and even unidentified subjugation19. Note also that AI \noperates on correlations without much reliance on causal explanations opening to further \nunderstudied vulnerabilities and or discriminations20. \nThis topic recalls the interplay between the robustness requirement of AI, and the technical \nsecurity from internal flaws and external attacks21 (see above also). A hard law approach \nmight seek the banning of non-explainable solutions22, while a tailored reading of art. 22 \n \n17 On the human control on algorithms, see STEFANO RODOTÀ, Il mondo in rete (Roma-Bari, 2017), REMO BODEI, \nDominio e sottomissione. Schiavi, animali, macchine e l’intelligenza artificiale (Bologna, 2019), and GIUSEPPE \nZACCARIA, ‘Figure del giudicare: calcolabilità, precedenti, decisione robotica’ (2020), Riv. Dir. Civ., 277 ff. \n18 On the idea to identify layers of vulnerability see F. LUNA, Elucidating the Concept of Vulnerability: Layers \nNot  Labels,  in  International  Journal  of  Feminist  Approaches  to  Bioethics  II,  no.  1,  2009,  121–39, \nhttps://doi.org/10.3138/ijfab.2.1.121. On the unaccountability of the possible exploitations of vulnerabilities see \nD. CITRON – F. PASQUALE, The Scored Society: Due Process for Automated Predictions, 2014, Faculty \nScholarship https://digitalcommons.law.umaryland.edu/fac_pubs/1431. \n19 On the undetermined nature of vulnerability see R. E. GOODIN, Protecting the Vulnerable: A Reanalysis of Our \nSocial Responsibilities, Chicago, 1985, 112. Also, examples of discriminations in access to services and goods \nare numerous V. EUBANKS, Automating Inequality: How High-Tech Tools Profile, Police, and Punish the Poor, \nNew York: St Martin’s Press, 2018; F. PASQUALE, The Black Box Society: The Secret Algorithms That Control \nMoney and In-formation. Cambridge: Harvard University Press, 2015; J. ANGWIN – J. LARSON, Bias in Criminal \nRisk  Scores  is  Mathematically  Inevitable,  Researchers  Say,  ProPublica,  December  2016, \nhttps://www.propublica.org/article/bias- in- criminal-risk-scores-is-mathematically-inevitable- researchers-say. \n20 Giovanni Comandé; “The Rotting Meat Error: From Galileo to Aristotle in Data Mining?”, in European Data \nProtection Law Review (2018), Volume 4, Issue 3, pages 270-277. \n21 “3. it should be robust, both from a technical and social perspective, since, even with good intentions, AI systems \ncan cause unintentional harm”. \n22 As argued in the data protection context by J. POWLES, The Seductive Diversion of ‘Solving’ Bias in Artificial \nIntelligence, 2018, https://medium.com/s/story/ the-seductive-diversion-of-solving-bias-in-artificial-intelligence- \n890df5e5ef53 . \n10', 'of the GDPR23 might balance things in a different way, activating for instance a right to \noppose the data processing preceding the bidding itself or enabling forms of information \ncapable of effectively reducing the impact on the individual decision-making context. \n \nTo sum up our feedback, we argued in favor of Option 4, using Option 3.c as a basis for a \nGeneral AI Regulation able to bridge lawfulness, robustness and ethics concerns in AI \nwhile remaining flexible enough to avoid roadblocks to innovation and uptake for AI \nsolutions. In addition, we briefly proposed some contents for a General AI Regulation, \naccording to a risk-based oriented system of check and balance aimed at ensuring the \ndevelopment of any AI-solutions in light of fundamental rights protection. The Regulation \nshall  consider  the  peculiarities  emerging  within  the  different  domains  and  therefore \nprovide the opportunity for each AI controller to allocate tasks, roles, and responsibilities. \nUnder this system, independent authorities shall provide assistance and promote awareness \nand trustworthiness among data subjects/end-users/stakeholders.  \nTo boost the cultural and inclusive challenge that the AI is driving, the interdisciplinary \ndialogue shall be maintained. \n \n23 G. COMANDÉ - G. MALGIERI, Why a Right to Legibility of Automated Decision-Making Exists in the General \nData Protection Regulation, in International Data Privacy Law, cit. \n11']"
F551049,10 September 2020,Niek Brunsveld,Universität/Forschungseinrichtung,Amsterdam AI Technology for People,groß (250 oder mehr Beschäftigte),764189838543-45,Niederlande,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"The Amsterdam coalition 'AI Technology for People' strongly supports the European Parliament and Council in their efforts to ensure that AI is safe, lawful and in line with EU fundamental rights. The overall goal of stimulating the uptake of trustworthy AI in the EU economy connects closely to the goals of Amsterdam coalition 'AI Technology for People'.

The attached document provides a succinct response to the consultation document by 'AI Technology for People'. In it, we explain why, in laying down requirements for AI, 'AI Technology for People' encourages the EP and the Council to ensure that:
•	The development, use and regulation of AI go hand in hand, because they are interdependent;
•	Regulatory interventions are evidence-based, proportionate, and respond to identified  regulatory gaps pro-actively and without undue delay so they can create legal certainty without stifling innovation;
•	Regulatory intervention is based on careful analyses of where actual conflicts between certain uses of AI and fundamental rights emerge or where AI has negative effects for a fair and inclusive society;
•	Regulatory interventions should take into account the broader economic-institutional environment and implications for society, and ensure that the way AI is being used and implemented in products and services is fair and complies with fundamental rights;
•	There is a fair legal playing field for European vis-à-vis non-European players; 
•	Compliance costs are distributed fairly not only between low risk and high-risk AI but also directly related to the size of a company and the institutional dependencies it might create. 

The lead author of the attached document is Prof.Dr. Natali Helberger, University Chair in in Law and Digital Technology at the University of Amsterdam

Partners in the coalition AI technology for people: 
Amsterdam Economic Board, Amsterdam UMC, Antoni van Leeuwenhoek (of which the Netherlands Cancer Institute is part), Centrum Wiskunde & Informatica, Municipality of Amsterdam, Amsterdam University of Applied Sciences, Sanquin, University of Amsterdam, Free University Amsterdam","['Consultation “Proposal for a legal act of the European Parliament                                                        \nand the Council laying down requirements for Artificial Intelligence”  \n– Response by the coalition Amsterdam: AI Technology for People \nMain author:  \nProf.Dr. Natali Helberger, University Professor in Law and Digital Technology  \nUniversity of Amsterdam \n \nIn laying down requirements for AI, we encourage the EP and the Council to ensure that: \n•  The development, use and regulation of AI go hand in hand, because they are interdependent; \n•  Regulatory interventions are evidence-based, proportionate, and respond to identified  \nregulatory gaps pro-actively and without undue delay so they can create legal certainty without \nstifling innovation; \n•  Regulatory intervention is based on careful analyses of where actual conflicts between certain \nuses of AI and fundamental rights emerge or where AI has negative effects for a fair and \ninclusive society; \n•  Regulatory interventions should take into account the broader economic-institutional \nenvironment and implications for society, and ensure that the way AI is being used and \nimplemented in products and services is fair and complies with fundamental rights; \n•  There is a fair legal playing field for European vis-à-vis non-European players;  \n•  Compliance costs are distributed fairly not only between low risk and high-risk AI but also \ndirectly related to the size of a company and the institutional dependencies it might create.  \n \nThe need for normative guidance on the side of companies, governments, research institutions and \nsociety is exemplified by the growing number of ethical guidelines, checklists and self-regulatory \ncommitments. At the same time, the diversity and sheer number of these initiatives can also have a \nbackfire effect, as it causes legal uncertainty and unpredictability and can signal a lack of leadership. \nThis is also and particularly true in the larger geo-political context where non-European AI countries, \nsuch as China and Japan, strive to establish themselves as the new leading nations on AI, not only at \nthe technical level but also at the level of ethics and regulation.  \nRegulatory intervention at an EU level (option 3) can have an important role in preventing legal \nfragmentation, creating legal certainty and standardization, and in so doing stimulate innovation, \ninvestment and adoption of AI-based applications. In order to not stifle innovation, it is paramount \nthat regulatory interventions are limited to situations where regulation is the most effective \ngovernance tool, that they are evidence-based, proportionate and respond to clearly identified \nregulatory gaps. Insofar option 3, in combination with the general line of the EU strategy to clarify, \nand where necessary adjust the existing framework, focus on effective enforcement and adopt \nadditional regulation only where necessary is to be welcomed.  \nIn the light of these considerations, the proposal for a legal act could benefit from a sharper focus \nand clearer analysis of the problems the initiative(s) are meant to tackle. Essentially, the document \ndistinguishes three challenges: enforcement, (product) liability and conflicts with human rights. \nWhere the problem is enforcement, it remains unclear how labelling schemes or soft law can help. \nThese are challenges that are best addressed through a combination of  non-regulatory measures \n(staffing up regulatory authorities, training staff) and regulatory intervention  to support the tasks of \nNRAs (like transparency, reporting rules and auditing rules). Also, particularly in the light of the high \nmonitoring and enforcement costs, incentive-based regulations can be a preferable alternative to \nmore traditional command-and-control schemes.', 'Where the problem is the creation of new safety risks it would seem that an update of existing rules \non product safety and clarification of the duties of care of the different operators in the value chain \nare the preferred option, which can involve  creating new rules (option 3). Soft law (option 1) and \nlabelling schemes (option 2) will not take away the need for an updated and foreseeable legal \nframework.  \nThen there is the case in which the application of AI can create challenges for the realization of \nfundamental rights. The Commission refers to an entire range of very different fundamental rights, \nfrom the right to privacy, non-discrimination and freedom of expression. It is still unclear how the \nCommission intends to create a (unified) legal framework that can address all these issues. Instead, \nregulatory intervention would need to be based on a careful analysis of where the actual conflicts \nbetween certain uses of AI and for fundamental rights are, but also where the introduction of AI \nchallenges the realization of broader social values, such as a fair and inclusive society. \nSo far, the guidance document seems to very much focus on risks that are inherent in the technology \nitself (e.g. use of biased data sets, explain ability, documentation, accuracy, etc.). AI-based \napplications, however, do not operate in isolation but in the context of institutions and societal \nentities. Therefore, effective AI regulation must be careful to not ascribe malfunctioning of the \nunderlying institutional set-up to the technology itself. Put differently, to ensure truly trustworthy \nAI that is used responsibly and in accordance with human rights it is necessary to formulate not \nonly requirements for the technology, but duties of care and legal requirements for the way it is \nused, and those using it. To give but one example: to ensure compliance with fundamental rights, it \nis not enough to issue mandatory obligations with respect to the quality of training data or explain \nability. Instead, there is a role for the law to identify situations in which the use of such technologies \nis inacceptable from a fundamental right stand point, and stipulate the rights and obligations for \nthose using these systems and those affected by them. The development, use and regulation of AI \nneed to go hand in hand, because they are interdependent.  \nFinally, one problem that the proposal does not take sufficiently into account is the challenge from \nnon-European players, that come with their own set of values and interpretations of what ethical \nand responsible AI is, as well as the need to create a fair legal playing field, also and particularly for \nEuropean start-ups. Without such a fair legal playing field it will be extremely difficult to compete on \n“European values”. The winner-takes-it-all-dynamics in current technology markets makes it unlikely \nthat smaller SMEs will benefit disproportionally from a higher level of trust than larger companies, as \nthe Commission suggests. The opposite is more likely, which is an argument in favor of distributing \ncompliance costs fairly not only between low risk and high-risk AI (a distinction that will be very \ndifficult to maintain in practice) but also directly related to the size of a company and the \ninstitutional dependencies it might create.  \n \n \n \n \n \n \nPartners in AI technology for people \nAmsterdam Economic Board, Amsterdam UMC, Antoni van Leeuwenhoek (of which the Netherlands Cancer Institute is \npart), Centrum Wiskunde & Informatica, Municipality of Amsterdam, Amsterdam University of Applied Sciences, Sanquin, \nUniversity of Amsterdam, Free University Amsterdam.', 'Amsterdam: AI technology for people \nAI is changing the world – rapidly and in many ways. Amsterdam focuses on developing and deploying responsible \nAI technologies to optimally serve people working in three areas: health, business innovation and citizen support. \nWith people being central to our approach, we believe it’s of utmost importance to develop these technologies in \nan accountable manner. In short, the Amsterdam approach drives AI technology for people. The key to \ncontinuing this purpose-driven development is to attract, develop and retain talent. \nAmsterdam has the largest science and innovation ecosystem in the Netherlands. Over 100,000 students attend \nlocal knowledge institutions, which have roughly 10,000 employees and 5,000 PhD students. With a long tradition \nof public-private partnerships, the region has many long-standing collaborations between academia and the private \nsector. In the field of artificial intelligence, the Amsterdam region builds on three decades of research, education \nand innovation. To further boost AI developments in the Amsterdam region, joint knowledge institutes have \ncommitted themselves to ambitious targets for the next ten years: \n\uf0b7 At least 1 billion euros in financial resources committed to AI\n\uf0b7 At least 800 people working in AI education, research and innovation\n\uf0b7 At least 5,000 students trained in AI technology at the BSc, MSc and PhD levels\n\uf0b7 At least 10,000 students following an AI minor\n\uf0b7 At least 100 SMEs impacted through collaborative spin-off projects\n\uf0b7 At least 100 AI startups\nWe identified three key intertwined technologies, on which we are academic leaders in the Netherlands, that will \ndrive future AI developments, and are expected to have a positive societal and business impact: \n1. Machine learning has been a main driver in the emergence of AI – and will continue pushing it forward.\nRelevant techniques include data-driven deep learning methods for computer vision, text analysis and search\napproaches that make large datasets accessible. Other related activities include the analysis of complex\norganisational processes, and knowledge representation and reasoning techniques to work with symbolic\ninformation.\n2. Responsible AI is key to assuring that technology is fair, accountable and transparent. Methods should\nprevent bias and all outcomes should be explainable through the identification of comprehensible parameters\nthat decisions are based on. When high-impact decisions are involved, the reasoning behind them must be\nunderstandable to allow for ethical considerations and professional judgements.\n3. Hybrid intelligence combines the best of two worlds. It builds on the superiority of AI technology in many\npattern recognition and machine learning tasks and combines it with the strengths of humans to deploy general\nknowledge, common sense reasoning and human capabilities such as collaboration, adaptivity, responsibility\nand explainability. Therefore, we combine human and machine intelligence to expand on human intellect\ninstead of replacing it.\nWithin AI, we observe that the classic juxtaposition of fundamental research vs applied research is fading. While \nthere is ample opportunity for fundamental research across these three key technologies, we see that practical \napplication strengthens the learning processes. In AI Technology for People, we concentrate on three application \ndomains:  \nAI for business innovation: Excellence in research has already inspired several international partners to start \nresearch labs in Amsterdam within ICAI. Other companies, both regional and international, continue to follow \nsuit. As Amsterdam hosts the headquarters of major companies that rely on AI to innovate, many small- and \nmedium-sized high-tech AI businesses and a strong creative industry, the city is in an ideal position to push forward \nbusiness innovations both small and large. \nAI for citizens: With its multitude of cultures, large numbers of tourists, rich history, criminal element and intense \nhousing market, Amsterdam has all the challenges and opportunities of other major world cities, but in a far smaller \narea. With the excellent availability of open data in the city, AI can be directly applied to improve the wellbeing \nof citizens – with the city itself becoming a living lab.', ""AI for health: Here, we are building on the work of renowned medical research organisations such as Amsterdam \nUMC, NKI, Sanquin and the Netherlands Institute for Neuroscience. The cross-sectoral health-AI collaboration \nhas also been institutionalised in other ways, such as through ecosystem mapping and Amsterdam Medical Data \nScience meet-ups, with all initiatives being bundled under Smart Health Amsterdam.\xa0\xa0 \nSome factors that help foster further development include: \n\uf0b7 Infrastructure: The city boasts great infrastructure for enabling AI innovation. This includes technical\ninfrastructure (e.g. high performance computing and Internet capabilities at SURFsara and Amsterdam\nInternet  Exchange,  access  to  European  computing  nodes),  institutional  infrastructure  (e.g.  CWI,\neSciencecenter, UvA, VU and HvA), and network infrastructure (e.g. AmsterdamDataScience and\nAmsterdam Medical Data Science).\n\uf0b7 Value creation support: The city’s two universities have strong academic expertise regarding the\njudicial, ethical and social aspects of AI, and active networks and institutions to guide innovation and\nfacilitate societal value creation. We have a range of related services and offerings, including academic\ntechnology assessments, platforms such as TADA – open about data, the development of new and\nimproved regulations, and arenas that support public-private experimentation, such as Amsterdam Smart\nCity.\n\uf0b7 Public-private  partnerships:  There  is  an  ever-increasing  number  of  long-lasting,  high-impact\ncollaborations between academia and different organisations, with many of them involving the Innovation\nCentre for Artificial Intelligence. Examples include providing support for police investigations, the AIM\nlab for medical imaging, AI for Retail (AIR) with Ahold-Delhaize, labs with Bosch and Qualcomm\nfocused on computer vision and machine learning, Elsevier’s innovations in publishing and TomTom’s\ncreation of high-quality maps.\n\uf0b7 National  and  international  collaboration:  Amsterdam  is  home  to  the  NWO's  ‘Zwaartekracht’\nprogramme on hybrid intelligence (HI), a large, nationally-funded academic research project. Amsterdam\nis also home to the international network TNW (which reaches over 10 million techies worldwide) and\nWorld Summit AI. The region has an ELLIS (European Laboratory for Learning and Intelligent Systems)\nunit and participates in CLAIRE (Confederation of Laboratories for Artificial Intelligence Research in\nEurope).\n\uf0b7 Business: Amsterdam has a vibrant startup and scale-up scene, with academic incubators such as ACE,\nand CWI Inc, as well as private ones such as TQ, all backed by a healthy investor climate. It has a thriving\nprivate sector of companies reliant on data and AI, including Booking.com, Adyen and Tiqets (the latter\ntwo were founded in Amsterdam) as well as a data-intensive and agile financial sector that includes ING\nand ABN AMRO, in addition to multinational companies such as Philips, IBM and Databricks.\nTo top it off, Amsterdam is also a great place to work and live and has an excellent quality of life, highly-connected \ninfrastructure and a diverse, globally-minded population. Amsterdam consistently ranks amongst leading cities in \nterms of innovation, tech savviness, English language skills, friendliness, entrepreneurial spirit, equality and \ninclusiveness, and is often named as one of the best places to live. \nOur past investments and successes have created a great AI ecosystem, but in this highly competitive world, we \nmust continually foster it and  assure we educate and attract top talent and remain in control of the development \nof advanced technology while adhering to our national and European values. This is why we are asking you to join \nus and develop solutions that will drive your success and empower people. Come and connect with the age-old \nAmsterdam tradition of public-private collaborations and work with the brightest minds on meaningful innovation. \nBring your ideas, talent and resources, and become part of Amsterdam’s AI technology for people.  \nPartners in AI technology for people\nAmsterdam Economic Board, Amsterdam UMC, Antoni van Leeuwenhoek (of which the Netherlands Cancer \nInstitute is part), Centrum Wiskunde & Informatica, Municipality of Amsterdam, Amsterdam University of Applied \nSciences, Sanquin, University of Amsterdam, Free University Amsterdam.""]"
F551048,10 September 2020,Anders Pall Skött,Universität/Forschungseinrichtung,"Department of Computer Science, University of Copenhagen",groß (250 oder mehr Beschäftigte),-,Dänemark,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"Department of Computer Science (DIKU) at the University of Copenhagen (UCPH) welcomes the opportunity to provide feedback to the European Commission’s (EC) Inception Impact Assessment for a Proposal for a legal act laying down requirements for Artificial Intelligence. 

EC is proposing four policy options. Below, we provide feedback on each option and some general considerations.

Option 1 may likely lead to a lot of confusion about which guidelines and standards to adopt - and to the interpretation of the assessment lists. 

Option 2 seems attractive. However, this will greatly favour large corporations with RA/QA/QMS already in place. SMEs need assistance to be able to enter markets and get this AI quality label. A labelling based on assessment lists alone may provide very little trustworthiness in reality, since it is difficult to assess the quality of the process followed to complete an assessment list.

Option 3, having a legislative instrument establishing mandatory requirements also implies well established means to determine whether the requirements have been fulfilled, which could provide more trustworthiness than option 1 and 2. However, this may become a large burden, particularly for SMEs. There are already ISO standards for quality assured software development, risk assessment, and so on. Possibly, these could be slightly expanded to cover AI concerns instead of adding a separate AI Regulative?

An argument for expanding standards and legislation for general software products and development as opposed to making special legislation for AI is that AI is in itself a poorly defined term with no agreement on definitions among standardisation bodies. A legislation targeting the functioning and use of general software systems would be more robust than a legislation targeting AI applications. 

Regarding option 3 and 4, and in general, a risk-based approach could mitigate the burden to comply with the legislation but of course put demands on the standardisation and legislative framework for risk assessment. 

An argument for expanding standards and legislation for general software products and development as opposed to making special legislation for AI is that AI is in itself a poorly defined term with no agreement on definitions among standardisation bodies. A legislation targeting the functioning and use of general software systems would be more robust than a legislation targeting AI applications.
"
F551047,10 September 2020,William Hanway,Unternehmen/Unternehmensverband,Clifford Chance LLP,groß (250 oder mehr Beschäftigte),-,Vereinigtes Königreich,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"Clifford Chance LLP welcomes the opportunity to respond to the European Commission's inception impact assessment on a ""Proposal for a legal act of the European Parliament and the Council laying down requirements for Artificial Intelligence"". Our submissions are (i) made on our own behalf only, and (ii) based on our substantial experience as a global (including European Union) law firm, advising on issues relevant to the consultation for a diverse range of clients within and outside of the European Union. The comments below and our submissions do not necessarily represent the views of every Clifford Chance lawyer, nor do they purport to represent the views of our clients. 

Please see the attached PDF for our complete response.
","['CLIFFORD CHANCE LLP \nProposal for a legal act of the European Parliament and the Council laying \ndown  requirements  for  Artificial  Intelligence  –  Clifford  Chance  LLP \nResponse \nClifford Chance LLP welcomes the opportunity to respond to the European Commission\'s (the \n""Commission"") inception impact assessment, on a ""Proposal for a legal act of the European \nParliament and the Council laying down requirements for Artificial Intelligence"" (the ""AI \nRoadmap""). Our submissions are (i) made on our own behalf only, and (ii) based on our \nsubstantial experience as a global (including European Union) law firm, advising on issues \nrelevant to the consultation for a diverse range of clients within and outside of the European \nUnion. The comments below and our submissions do not necessarily represent the views of \nevery Clifford Chance lawyer, nor do they purport to represent the views of our clients. \nWe have considered and assessed the options to the baseline scenarios set out in the AI \nRoadmap, i.e.:  \n• Baseline/Option 0: no EU policy change.\n• Option 1: EU “soft law” (non-legislative) approach to facilitate and spur industry-led\nintervention (no EU legislative instrument).\n• Option 2: EU legislative instrument setting up a voluntary labelling scheme.\n• Option 3: EU legislative instrument establishing mandatory requirements for all or certain\ntypes of AI applications.\n• Option 4: combination of any of the options above taking into account the different levels\nof risk that could be generated by a particular AI application.\nIn our view, Option 4, i.e. a combination of any of the options taking into account the different \nlevels of risk that could be generated by a particular AI application, is the most appropriate \noption to be considered by the Commission.  \nAs set out in our response to the Commission\'s white paper titled ‘Artificial Intelligence – A \nEuropean  approach  to  excellence  and  trust’,  an  agile  regulatory  framework  may  help \norganisations and regulators respond to new risks (e.g. accelerated review procedures for (new) \nhigh risk activities). \nBuilding on this approach, and in order to ensure that existing regulatory and industry efforts \nin relation to AI are adequately accounted for, an ideal approach would be one that combines \nhigh-level principles and obligations that are sector-agnostic (including a robust yet flexible \ndefinition and scope of what constitutes \'artificial intelligence\') with industry-led norms, \nstandards and codes of conduct developed in conjunction with close cooperation with EU \nregulators. This approach would also focus on the outcomes for individuals using (or impacted \nby the use of) AI in terms of their guaranteed rights and freedoms. \nThe principles-led approach would align any proposed AI regulation with existing law and \nguidance, such as the General Data Protection Regulation (GDPR), as well as Ethics Guidelines \nfor Trustworthy Artificial Intelligence presented by the High-Level Expert Group on AI. This \napproach, in our view, would also ensure that a balance is struck between setting up requisite \n1', 'CLIFFORD CHANCE LLP \nguardrails for a critical technology and promoting innovation in the interests of industry and \nsociety, allowing AI developers in the EU to continue to innovate and remain competitive in \nthe global market while adequately accounting for regulatory requirements. \nFurthermore, when presented in conjunction with appropriate guidelines that can be kept up to \ndate to account for changes in use cases as well as technological updates, this approach can \nhelp to ensure that regulations in relation to a fast-changing field do not become outdated. \nWhile the principles could apply to the development and deployment of AI as a whole, \nguidelines can be drafted for specific categories of AI applications which are either \'high risk\', \nhave otherwise significant impacts on the rights and freedoms of individuals, or both. We \nsubmit that in relation to identifying \'high risk\' AI, a binary approach (whereby AI applications \nentailing a high risk are curtailed, while the others are deemed explicitly viable) would likely \nbe over simplistic. Hence, we see a need that the mere statement of principle regarding ""high \nrisk""  is  supported  by  other  factors.  We  support  the  Commission\'s  suggestion  that  any \nmandatory requirements in relation to \'high risk\' AI account for factors such as training data, \nhuman oversight, record keeping, transparency, robustness and accuracy. \nWe also agree that EU legislative instruments should account for enforcement mechanisms to \nensure compliance with regulatory requirements. We support the Commission\'s proposal for \ncarefully  considered  ex-ante mechanisms  consist ing  of  conformity/safety  assessment \nprocedures aligned with procedures that already exist in EU product safety legislation, and \nbelieve others could be developed with the aid of industry adopted standards and internal \nadvisory and review boards. We also note that the Commission is currently in the process of \nconsidering (among others), the liability regime for AI and other emerging technologies. Whilst \nthis may not be the object of this consultation, we agree that it is a key part of any meaningful \nregulatory  framework  for  AI.  Devising  the  right  liability  regime  requires  guiding  and \ncontextualising existing liability regimes, whilst carefully assessing the extent to which they \nmay need to be adapted and complemented to account for the specificities of AI and other new \ntechnologies.  \nDrawing parallels with the GDPR where, while certification mechanisms are envisaged, in the \ntwo years since its implementation none have so far been approved. In order to address this for \nany proposed legislative framework relating AI, it is critical that mechanisms for close \npartnership between regulators and industry (e.g. IEEE, ISO and others) in the development \nand implementation of standards and guidelines are baked in from the beginning, potentially \nby specific inclusion in the mandate for any regulator(s) established under such a framework. \nVoluntary  labelling  schemes  as  envisaged  in  the  Commission\'s  white  paper,  while  a \nconceptually viable option, in our view may not be the best way forward until the industry and \nregulators have achieved further maturity on the adoption of cohesive standards. Schemes of \nthis nature introduced alongside a nascent regulatory regime, in our view, run the risk of having \nthe unintended effect of diluting consumer trust and potentially opening the schemes up for \nmisinterpretation (or in some cases, misuse). Quite the reverse of the objective sought. This is \nsomething that can be usefully  assessed in due time, with the perspective and knowledge \nneeded to be able to define the right use cases and appropriate safeguards and conditions. Only \nthen will this actually help enhance user trust and promote the uptake of the technology, as \ncontemplated by the Commission in its white paper. \n-2 -', ""CLIFFORD CHANCE LLP \nIn terms of ex-post enforcement, this should only be implemented in a manner that balances \nwell against ex-ante approaches, once their efficacy has been established, and post a potential \ntransition period for implementation, within which specific guidelines in relation to compliance \nand enforcement could also be made available. The legislative framework may also consider \nthe need for a nodal regulator or supervisory authority at the EU and/or member state level. At \nthis stage, it may be that such an authority should primarily play the role of helping to achieve \na guided implementation of regulations, and coordinating the efforts of sectoral regulators, as \nopposed to taking an active position in enforcement of AI regulations, which may present \ncomplex challenges in terms of a lack of available expertise in adequately staffing such an \nauthority, as well as overlaps with the mandate of other regulators.  \nIn relation to specific guidelines that will be required alongside a principles-based regulatory \nframework, we believe that illustratively, these should include:  \n• Guidelines and standard on the broad, and context-specific definitions of AI.\n• Guidelines on sectoral applications and 'high-risk/impact' AI.\n• Guidelines on public-private partnerships in AI (including the development of standards).\n• Guidelines on AI governance addressing the data lifecycle as it applies to the development\nof AI systems.\n• Technical guidelines on approved/recommended methodologies for the prevention and\nmitigation of bias, discrimination and lack of transparency in how AI is deployed.\n• Auditing and certification standards for sectoral/use-case specific AI.\n• Guidelines contextualising the application of relevant existing EU laws for AI (such as\nthose relating to consumer protection, product liability and public sector use of AIS).\n• Guidelines on sustainable adoption of AI in the EU (contextualising, for instance the, UN\nSustainable Development Goals).\nFinally, and as a more general observation, we support the objective of seeking to streamline \nand harmonise legislation and other requirements at the European level and, beyond, on the \ninternational scene, where feasible and appropriate, whilst also taking account of industry \nspecificities. \n-3 -""]"
F551045,10 September 2020,Kristin Little,Wirtschaftsverband,IEEE Standards Association,groß (250 oder mehr Beschäftigte),-,Vereinigten Staaten,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"The IEEE SA appreciates the opportunity to respond to the Commission’s IIA. As a resource, we suggest our publication, Ethically Aligned Design, First Edition. The law chapter provides commentary about how the law should respond to a number of specific ethical and legal challenges raised by the development and deployment of AIS and the benefits/risks of the incorporation of AIS into a society’s legal system. 

Please find our comments, by section, below.

Context
Care should be taken to prioritize social elements. We recommend language such as, ""the use of AI should prioritize socially and environmentally beneficial outcomes as a means for companies to demonstrate their key competitive focus on long term business sustainability while upholding key EU values.""

We recommend the use of the term “Artificial Intelligence Systems” as used by the OECD and IEEE, as opposed to “Artificial Intelligence.”

Problem the initiative aims to tackle
While material harm may relate to the “safety and health of individuals and their property,” we would suggest that material harm to the natural habitat also be considered.

Another element to consider is data portability. The context of sovereign data should also be discussed.

The Commission recognizes that AI systems can evolve. Such a situation may require a dynamic supervision not static repeated evaluation of conformity. In the event that safety risks materialize, the Commission notes that it could be difficult to determine the person, particular human action, or omission responsible for the damage. IEEE submits that evaluation and potential certification of accountability may address this concern.

Testing is not adequate for trust, nor can an adaptive learning system be tested only once as a basis for continual trust. We posit that a more dynamic environment for trust when it comes to autonomous learning systems is needed.

Objective
Regarding the Commission’s aim of preventing/minimizing significant risks to fundamental rights and safety, we note that some matters of safety and security are protected by existing regulations, and we suggest that ethical risks and the undermining of rights and values be considered.  

Policy option 2
Caution should be exercised for giving a fixed label to an essentially adaptive system, because such a scheme can qualify a product or system partially not wholly.

Policy option 3, sub-option 2
Limiting a legislative instrument to “high-risk” on the basis of two criteria (as set out in the White Paper) one should consider that AI safety is no different to any other class of safety. Safety of products and services is the most extensively addressed and covered facet of the legal system. A gap exists in the values and rights.

Likely economic impacts
Concerning trust, Government's approach to indicating and upholding it should also be transparent and achievable among the multiple stakeholders in order to realize the full economic benefits. 

SMEs may not be able to comply with the regulatory framework mechanisms. Incentives could also be considered.

Likely social impacts
To expect “no direct significant negative social impacts” is perhaps misguided. Ex ante this is not known. The impact on employment can be concerning as jobs lost in specific sectors may not be easily filled by potential new employment opportunities based on vastly different competencies.

Likely impacts on simplification and/or administrative burden
In addition to the downsides of legal fragmentation outlined, another negative impact to note is the free flow of products and services approved by different non-harmonized criteria.

Impact assessment
With respect to the preparation of an impact assessment for December 2020, an objective assessment framework should consider the likelihood as well as the potential outcomes of various scenarios. This amounts to a risk and reward profiling of the technology and can contribute to can contribute to  a better decision support framework.
","['ETHICALLY  \nALIGNED DESIGN\nFirst Edition\nA Vision for Prioritizing Human Well-being \nwith Autonomous and Intelligent Systems', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems \nTable of Contents\nThe views and opinions expressed in this collaborative work  \nare those of the authors and do not necessarily reflect the official \npolicy or position of their respective institutions or of the Institute  \nof Electrical and Electronics Engineers (IEEE).  This work is published \nunder the auspices of the IEEE Global Initiative on Ethics of \nAutonomous and Intelligent Systems for the purposes of furthering \npublic understanding of the importance of addressing ethical \nconsiderations in the design of autonomous and intelligent systems.  \nPlease see page 290, How the Document Was Prepared,  \nfor more details regarding the preparation of this document.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License.', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems \nTable of Contents\nIntroduction  2 \nExecutive Summary  3-6 \nAcknowledgements  7-8\nEthically Aligned Design\nFrom Principles to Practice  9-16 \nGeneral Principles  17-35\nClassical Ethics in A/IS  36-67\nWell-being   68-89\nAffective Computing   90-109 \nPersonal Data and Individual Agency  110-123\nMethods to Guide Ethical Research and Design  124-139\nA/IS for Sustainable Development  140-168\nEmbedding Values into Autonomous and Intelligent Systems  169-197\nPolicy  198-210\nLaw   211-281\nAbout Ethically Aligned Design\nThe Mission and Results of The IEEE Global Initiative  282\nFrom Principles to Practice—Results of Our Work to Date   283-284 \nIEEE P7000™ Approved Standardization Projects  285-286\nWho We Are  287\nOur Process  288-289\nHow the Document was Prepared  290\nHow to Cite Ethically Aligned Design  290\nKey References  291 \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License.', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nIntroduction\nAs the use and impact of autonomous and intelligent systems (A/IS) become pervasive, we need to \nestablish societal and policy guidelines in order for such systems to remain human-centric, serving \nhumanity’s values and ethical principles. These systems must be developed and should operate in \na way that is beneficial to people and the environment, beyond simply reaching functional goals and \naddressing technical problems. This approach will foster the heightened level of trust between people \nand technology that is needed for its fruitful use in our daily lives.\nTo be able to contribute in a positive, non-dogmatic way, we, the techno-scientific communities, need  \nto enhance our self-reflection. We need to have an open and honest debate around our explicit or \nimplicit values, including our imaginary1 around so-called “Artificial Intelligence” and the institutions, \nsymbols, and representations it generates. \nUltimately, our goal should be eudaimonia, a practice elucidated by Aristotle that defines human  \nwell-being, both at the individual and collective level, as the highest virtue for a society. Translated \nroughly as “flourishing”, the benefits of eudaimonia begin with conscious contemplation, where  \nethical considerations help us define how we wish to live.\nWhether our ethical practices are Western (e.g., Aristotelian, Kantian), Eastern (e.g., Shinto, 墨家/School \nof Mo, Confucian), African (e.g., Ubuntu), or from another tradition, honoring holistic definitions of \nsocietal prosperity is essential versus pursuing one-dimensional goals of increased productivity or gross \ndomestic product (GDP). Autonomous and intelligent systems should prioritize and have as their goal \nthe explicit honoring of our inalienable fundamental rights and dignity as well as the increase of human \nflourishing and environmental sustainability. \nThe goal of The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems (“The IEEE \nGlobal Initiative”) is that Ethically Aligned Design will provide pragmatic and directional insights and \nrecommendations, serving as a key reference for the work of technologists, educators and policymakers \nin the coming years.  \nEthically Aligned Design sets forth scientific analysis and resources, high-level principles, and actionable \nrecommendations. It offers specific guidance for standards, certification, regulation or legislation for \ndesign, manufacture, and use of A/IS that provably aligns with and improves holistic societal well-being.\n1The symbols, values, institutions, and norms of a societal group through which people imagine their lives and constitute their societies.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 2', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nIntroduction\nExecutive Summary\nI. Purpose of Ethically Aligned Design, First Edition (EAD1e) \nAutonomous and intelligent technical systems are specifically designed to reduce the necessity for \nhuman intervention in our day-to-day lives. In so doing, these new systems are also raising concerns \nabout their impact on individuals and societies. Current discussions include advocacy for a positive \nimpact, such as optimization of processes and resource usage, more informed planning and decisions, \nand recognition of useful patterns in big data. Discussions also include warnings about potential harm to \nprivacy, discrimination, loss of skills, adverse economic impacts, risks to security of critical infrastructure, \nand possible negative long-term effects on societal well-being.\nBecause of their nature, the full benefit of these technologies will be attained only if they are aligned \nwith society’s defined values and ethical principles. Through this work we intend, therefore, to establish \nframeworks to guide and inform dialogue and debate around the non-technical implications of these \ntechnologies, in particular related to ethical aspects. We understand “ethical” to go beyond moral \nconstructs and include social fairness, environmental sustainability, and our desire for self-determination.\nOur analyses and recommendations in Ethically Aligned Design address values and intentions as well \nas implementations, both legal and technical. They are both aspirational, what we hope or wish should \nhappen, and practical, what we—the techno-scientific community and every group involved with and/or \naffected by these technologies—could do for society to advance in positive directions. The analyses and \nrecommendations in EAD1e are offered as guidance for consideration by governments, businesses, and \nthe public at large in the advancement of technology for the benefit of humanity.\nChapters in Ethically Aligned Design, First Edition\n1.  From Principles to Practice 7.  Methods to Guide Ethical Research and Design\n2.  General Principles 8.  A/IS for Sustainable Development\n3.  Classical Ethics in A/IS 9.  Embedding Values into Autonomous  \n  and Intelligent Systems\n4.  Well-being \n10. Policy\n5.  Affective Computing\n11.  Law\n6.  Personal Data and Individual Agency\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 3', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nIntroduction\nII. General Principles  III. Ethical Foundations \nThe ethical and values-based design,  Classical Ethics\ndevelopment, and implementation of \nBy drawing from over two thousand five \nautonomous and intelligent systems should be \nhundred years of classical ethics traditions, the \nguided by the following General Principles:\nauthors of Ethically Aligned Design explored \nestablished ethics systems, addressing both \n1. Human Rights \nscientific and religious approaches, including \n  A/IS shall be created and operated to respect,  \nsecular philosophical traditions, to address human \n  promote, and protect internationally  \nmorality in the digital age. Through reviewing the \n  recognized human rights.\nphilosophical foundations that define autonomy \n2. Well-being \nand ontology, this work addresses the alleged \n  A/IS creators shall adopt increased human  \npotential for autonomous capacity of intelligent \n  well-being as a primary success criterion  \ntechnical systems, morality in amoral systems, \n  for development.\nand asks whether decisions made by amoral \n3. Data Agency \nsystems can have moral consequences.\n  A/IS creators shall empower individuals with  \n  the ability to access and securely share their  \n  data, to maintain people’s capacity to have  \nIV. Areas of Impact\n  control over their identity.\n4. Effectiveness   A/IS for Sustainable Development\n  A/IS creators and operators shall provide  \nThrough affordable and universal access to \n  evidence of the effectiveness and fitness  \ncommunications networks and the Internet, \n  for purpose of A/IS.\nautonomous and intelligent systems can be \n5. Transparency  made available to and benefit populations \n  The basis of a particular A/IS decision should  \nanywhere. They can significantly alter institutions \n  always be discoverable.\nand institutional relationships toward more \n6. Accountability  human-centric structures, and they can address \n  A/IS shall be created and operated to provide   humanitarian and sustainable development \n  an unambiguous rationale for all decisions made.\nissues resulting in increased individual societal \n7. Awareness of Misuse  and environmental well-being. Such efforts could \n  A/IS creators shall guard against all potential   be facilitated through the recognition of and \n  misuses and risks of A/IS in operation. adherence to established indicators of societal \nflourishing such as the United Nations Sustainable \n8. Competence \nDevelopment Goals so that human well-being is \n  A/IS creators shall specify and operators shall  \nutilized as a primary success criteria for  \n  adhere to the knowledge and skill required  \nA/IS development. \n  for safe and effective operation.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 4', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nIntroduction\nPersonal Data Rights and Agency   •  The manifestations generated by autonomous \nOver Digital Identity and intelligent technical systems should, in \ngeneral, be protected under national and \nPeople have the right to access, share, and \ninternational laws.\nbenefit from their data and the insights it \nprovides. Individuals require mechanisms to  •  Standards of transparency, competence, \nhelp create and curate the terms and conditions  accountability, and evidence of effectiveness \nregarding access to their identity and personal  should govern the development of \ndata, and to control its safe, specific, and finite  autonomous and intelligent systems.\nexchange. Individuals also require policies and \nPolicies for Education and Awareness\npractices that make them explicitly aware of \nconsequences resulting from the aggregation or  Effective policy addresses the protection and \nresale of their personal information.  promotion of human rights, safety, privacy, and \ncybersecurity, as well as the public understanding \nLegal Frameworks for Accountability\nof the potential impact of autonomous and \nThe convergence of autonomous and intelligent  intelligent technical systems on society. To ensure \nsystems and robotics technologies has led to  that they best serve the public interest, policies \nthe development of systems with attributes  should:\nthat simulate those of human beings in terms \n•  Support, promote, and enable internationally \nof partial autonomy, ability to perform specific \nrecognized legal norms.\nintellectual tasks, and even a human physical \nappearance. The issue of the legal status of  •  Develop government expertise in related \ncomplex autonomous and intelligent systems  technologies.\nthus intertwines with broader legal questions  •  Ensure governance and ethics are core \nregarding how to ensure accountability and  components in research, development, \nallocate liability when such systems cause harm.  acquisition, and use.\nIt is clear that:\n•  Regulate to ensure public safety and \n•  Autonomous and intelligent technical systems  responsible system design.\nshould be subject to the applicable regimes of  •  Educate the public on societal impacts  \nproperty law. of related technologies.\n•  Government and industry stakeholders should \nidentify the types of decisions and operations \nthat should never be delegated to such \nsystems. These stakeholders should adopt \nrules and standards that ensure effective \nhuman control over those decisions and how \nto allocate legal responsibility for harm caused \nby them. \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 5', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nIntroduction\nare to be deployed and, in particular, norms \nV. Implementation\nrelevant to the kinds of tasks that they are \nWell-being Metrics designed to perform.\nFor autonomous and intelligent systems to  Methods to Guide Ethical  \nprovably advance a specific benefit for humanity,  Research and Design\nthere need to be clear indicators of that benefit. \nTo create autonomous and intelligent technical \nCommon metrics of success include profit, \nsystems that enhance and extend human \ngross domestic product, consumption levels, \nwell-being and freedom, values-based design \nand occupational safety. While important, these \nmethods must put human advancement at the \nmetrics fail to encompass the full spectrum \ncore of development of technical systems. This \nof well-being for individuals, the environment, \nmust be done in concert with the recognition that \nand society. Psychological, social, economic \nmachines should serve humans and not the other \nfairness, and environmental factors matter. Well-\nway around. Systems developers should employ \nbeing metrics include such factors, allowing the \nvalues-based design methods in order to create \nbenefits arising from technological progress to \nsustainable systems that can be evaluated in \nbe more comprehensively evaluated, providing \nterms of not only providing increased economic \nopportunities to test for unintended negative \nvalue for organizations but also of broader social \nconsequences that could diminish human \ncosts and benefits.\nwell-being. A/IS can improve capturing of and \nanalyzing the pertinent data, which in turn  Affective Computing\ncould help identify where these systems would \nAffect is a core aspect of intelligence. Drives \nincrease human well-being, providing new routes \nand emotions such as anger, fear, and joy are \nto societal and technological innovation.\noften the foundations of actions throughout our \nEmbedding Values into Autonomous   lives. To ensure that intelligent technical systems \nand Intelligent Systems will be used to help humanity to the greatest \nextent possible in all contexts, autonomous and \nIf machines engage in human communities as \nintelligent systems that participate in or facilitate \nquasi-autonomous agents, then those agents \nhuman society should not cause harm by either \nmust be expected to follow the community’s \namplifying or dampening human emotional \nsocial and moral norms. Embedding norms in \nexperience.\nsuch quasi-autonomous systems requires a \nclear delineation of the community in which \nthey are to be deployed. Further, even within a \nparticular community, different types of technical \nembodiments will demand different sets of \nnorms. The first step is to identify the norms  \nof the specific community in which the systems \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 6', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nIntroduction\nAcknowledgements \nOur progress and the ongoing positive influence  •  Methodologies to Guide Ethical Research \nof this work are due to the volunteer experts  and Design: Raja Chatila and Corinne Cath\nserving on all our Committees and IEEE P7000™ \n•  Safety and Beneficence of Artificial \nStandards Working Groups, along with the IEEE \nGeneral Intelligence (AGI) and Artificial \nprofessional staff who support our efforts. \nSuperintelligence (ASI): Malo Bourgon  \nThank you for your dedication toward defining, \nand Richard Mallah\ndesigning, and inspiring the ethical principles \n•  Personal Data and Individual Agency: \nand standards that will ensure that autonomous \nKatryna Dow and John C. Havens\nand intelligent systems and the technologies \nassociated with them will positively benefit  •  Reframing Autonomous Weapons \nhumanity. Systems: Peter Asaro\n•  Sustainable Development:  \nWe wish to thank the Executive Committee and \nElizabeth Gibbons \nCommittees of The IEEE Global Initiative on \nEthics of Autonomous and Intelligent Systems: •  Law: Nicolas Economou and John Casey\n•  Affective Computing: John Sullins  \nExecutive Committee Officers\nand Joanna J. Bryson\nRaja Chatila, Chair\n•  Classical Ethics in A/IS: Jared Bielby\nKay Firth-Butterfield, Vice Chair\nJohn C. Havens, Executive Director •  Policy: Peter Brooks and Mina Hannah\n•  Extended Reality: Monique Morrow  \nExecutive Committee Members\nand Jay Iorio\nDr. Greg Adamson, Karen Bartleson, Virginia  \n•  Well-being: Laura Musikanski  \nDignum, Danit Gal, Malavika Jayaram, Sven \nand John C. Havens\nKoenig, Eileen M. Lach, Raj Madhavan, Richard \nMallah, AJung Moon, Monique Morrow, Francesca  •  Editing: Karen Bartleson and Eileen M. Lach \nRossi, Alan Winfield, and Hagit Messer Yaron\n•  Outreach: Maya Zuckerman and Ali Muzaffar\nCommittee Chairs •  Communications: Leanne Seeto  \nand Mark Halverson\n•  General Principles: Mark Halverson,  \nand Peet van Biljon •  High School: Tess Posner\n•  Embedding Values into Autonomous  •  Global Coordination: Victoria Wang,  \nIntelligent Systems: Francesca Rossi   Arisa Ema, Pavel Gotovtsev \nand Bertram F. Malle\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 7', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nIntroduction\nPrograms and Projects Inspired   Finally, we would like to also acknowledge the \nby The IEEE Global Initiative: ongoing work of three Committees of The IEEE \nGlobal Initiative regarding their chapters of \n•  Ethically Aligned Design University \nEthically Aligned Design that, for timing reasons, \nConsortium: Hagit Messer, Chair\nwe were not able to include in Ethically Aligned \n•  Ethically Aligned Design Community:  \nDesign, First Edition. These Committees include: \nLisa Morgan, Program Director, Content  \nReframing Autonomous Weapons Systems, \nand Community \nExtended Reality (formerly Mixed Reality) and \n•  Ethics Certification Program for  Safety and Beneficence of Artificial General \nAutonomous and Intelligent Systems:  Intelligence (AGI) and Artificial Superintelligence \nMeeri Haataja, Chair; Ali Hessami, Vice-Chair (ASI). We would like to thank Peter Asaro, \nMonique Morrow and Jay Iorio, Malo Bourgon \n•  Glossary: Sara M. Jordan, Chair\nand Richard Mallah for their leadership in these \ngroups along with all their Committee Members. \nPeople\nOnce these chapters have completed their review \nWe would like to warmly recognize the leadership \nand been accepted by IEEE they could either  \nand constant support of The IEEE Global Initiative \nbe included in Ethically Aligned Design, published \non Ethics of Autonomous and Intelligent Systems \nby The IEEE Global Initiative, or in other \nby Dr. Ing. Konstantinos Karachalios, Managing \npublications of IEEE. \nDirector of the IEEE Standards Association.\nFor information on disclaimers associated with \nWe would also like to thank Stephen Welby, \nEAD1e, see How the Document Was Prepared.\nExecutive Director and Chief Operating Officer \nof IEEE for his generous and insightful support of \nthe Ethically Aligned Design, First Edition process \nand The IEEE Global Initiative overall. \nWe would especially like to thank Eileen M. Lach, \nthe former IEEE General Counsel and Chief \nCompliance Officer, whose heartfelt conviction \nthat there is a pressing need to focus the global \ncommunity on highlighting ethical considerations \nin the development of autonomous and \nintelligent systems served as a strong catalyst  \nfor the development of the Initiative within IEEE.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 8', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems \nFFrroomm  PPrriinncciipplleess  ttoo  PPrraaccttiiccee \nEEtthhiiccaallllyy  AAlliiggnneedd  DDeessiiggnn  CCoonncceeppttuuaall  FFrraammeewwoorrkk  \nEthically Aligned Design, First Edition (EAD1e) represents more than a comprehensive \nreport, distilling the consensus of its vast community of creators into a set of high-level \nethical principles, key issues, and practical recommendations. EAD1e is an in-depth \nseminal work, a one-of-a-kind treatise, intended not only to inform a broader public but \nalso to inspire its audience and readership of academics, engineers, policy makers, and   \nmanufacturers of autonomous and intelligent systems1 (A/IS) to take action.   \nThis Chapter, “From Principles to Practice”, provides a mapping of the conceptual framework \nof Ethically Aligned Design. It outlines the logic behind “Three Pillars” that form the basis of \nEAD1e, and it connects the Pillars to high-level “General Principles” which guide all manner \nof ethical A/IS design. Following this, the content of the Chapters of EAD1e is mapped to \nthe Principles. Finally, examples of EAD1e already in practice are described.  \nSections in this Chapter: \n•  The Three Pillars of the Ethically Aligned Design Conceptual Framework \n•  The General Principles of Ethically Aligned Design \n•  Mapping the Pillars to the Principles \n•  Mapping the Principles to the Content of the Chapters \n•  From Principles to Practice\n•  Ethically Aligned Design in Implementation \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 9', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nFrom Principles to Practice\nEthically Aligned Design Conceptual Framework \nThe Three Pillars of the Ethically Aligned \nDesign Conceptual Framework  \nThe Pillars of the Ethically Aligned Design Conceptual Framework fall broadly into three areas,  \nreflecting anthropological, political, and technical aspects:  \n1. Universal Human Values: A/IS can be an enormous force for good in society provided they are \ndesigned to respect human rights, align with human values, and holistically increase well-being \nwhile empowering as many people as possible. They should also be designed to safeguard our \nenvironment and natural resources. These values should guide policy makers as well as engineers, \ndesigners, and developers. Advances in A/IS should be in the service of all people, rather than \nbenefiting solely small groups, a single nation, or a corporation. \n2. Political Self-Determination and Data Agency: A/IS—if designed and implemented properly—\nhave a great potential to nurture political freedom and democracy, in accordance with the cultural \nprecepts of individual societies, when people have access to and control over the data constituting \nand representing their identity. These systems can improve government effectiveness and \naccountability, foster trust, and protect our private sphere, but only when people have agency over \ntheir digital identity and their data is provably protected. \n3. Technical Dependability: Ultimately, A/IS should deliver services that can be trusted.2 This trust \nmeans that A/IS will reliably, safely, and actively accomplish the objectives for which they were \ndesigned while advancing the human-driven values they were intended to reflect. Technologies \nshould be monitored to ensure that their operation meets predetermined ethical objectives aligning \nwith human values and respecting codified rights. In addition, validation and verification processes, \nincluding aspects of explainability, should be developed that could lead to better auditability and to \ncertification3 of A/IS.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 10', 'The General Principles  \nof Ethically Aligned Design   \nThe General Principles of Ethically Aligned  4. Effectiveness–A/IS creators and operators \nDesign have emerged through the continuous  shall provide evidence of the effectiveness \nwork of dedicated, open communities in a  and fitness for purpose of A/IS.\nmulti-year, creative, consensus-building process. \n5. Transparency–The basis of a particular A/IS \nThey articulate high-level principles that should \ndecision should always be discoverable.\napply to all types of autonomous and intelligent \n6. Accountability–A/IS shall be created and \nsystems (A/IS). Created to guide behavior and \noperated to provide an unambiguous rationale \ninform standards and policy making, the General \nfor all decisions made.\nPrinciples define imperatives for the ethical \ndesign, development, deployment, adoption, and  7. Awareness of Misuse–A/IS creators shall \ndecommissioning of autonomous and intelligent  guard against all potential misuses and risks of \nsystems. The Principles consider the role of A/IS  A/IS in operation.\ncreators, i.e., those who design and manufacture, \n8. Competence–A/IS creators shall specify and \nof operators, i.e., those with expertise specific \noperators shall adhere to the knowledge and \nto use of A/IS, other users, and any other \nskill required for safe and effective operation.\nstakeholders or affected parties.\nThe General Principles4   \nof Ethically Aligned Design \n1. Human Rights–A/IS shall be created and \noperated to respect, promote, and protect \ninternationally recognized human rights.\n2. Well-being–A/IS creators shall adopt \nincreased human well-being as a primary \nsuccess criterion for development.\n3. Data Agency–A/IS creators shall empower \nindividuals with the ability to access and \nsecurely share their data, to maintain people’s \ncapacity to have control over  \ntheir identity.\n11', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nFrom Principles to Practice\nEthically Aligned Design Conceptual Framework \nMapping the Pillars to the Principles   \nWhereas the Pillars of the Ethically Aligned Design Conceptual Framework represent broad \nanthropological, political, and technical aspects relating to autonomous and intelligent systems,  \nthe General Principles provide contextual filters for deeper analysis and pragmatic implementation. \nIt is also important to recognize that the General Principles do not live in isolation of EAD’s Pillars \nand vice versa. While the General Principle of “Transparency” may inform the design of a specific \nautonomous or intelligent system, the A/IS must also account for universal human values, political  \nself-determination, and data agency. Moreover, Transparency goes beyond technical features. It is  \nan important requirement also for the processes of policy and lawmaking. In this way, EAD1e’s Pillars \nform the holistic ethical grounding upon which the Principles can build, and the latter may apply  \nin various spheres of human activity. \nEAD1e Pillars Mapped to General Principles\nEAD Pillars\n \n   \nPPoolliititiccaall\nUUnniivveerrssaall TTeecchhnniiccaall\nSSeellff--DDeetteerrmmiinnaatitioonn\nHHuummaann  VVaalluueess DDeeppeennddaabbiilliittyy\nDDaattaa  AAggeennccyy  \nHuman Rights n n\nWell-being n n  \ns\ne\npl Data Agency n n n\nci\nn\nPri Effectiveness n\nal \nr\nne Transparency n n n\ne\nG\nD  Accountability n n n\nA\nE\nAwareness of Misuse n\nCompetence n\nn    \nIndicates General Principle mapped to Pillar.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 12', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nFrom Principles to Practice\nEthically Aligned Design Conceptual Framework \nMapping the Principles to  \nthe Content of the Chapters    \nThe Chapters of Ethically Aligned Design provide in-depth subject matter expertise that allows readers  \nto move from the General Principles to more deeply analyze ethical A/IS issues within the context of \ntheir specific work. \nThe mapping or indexing provided in the table below serve as directional starting points since elements \nof a Principle like “Competence” may resonate in several EAD1e Chapters. In addition, where core \nsubjects are primarily covered by specific Chapters, we have done our best to indicate this via our \nmapping below. \nEAD1e General Principles Mapped to Chapters\nEAD Chapters\n General Classical  Affective  Data &  Methods  A/IS for  Embedding \n Principles Ethics in  Well-being Computing Individual  A/IS  Sustainable  Values into  Policy Law\nA/IS Agency Design Dev. A/IS\nHuman Rights n n n n n n n n n n\nWell-being n n n n n n n n n n\ns\ne\ncipl Data Agency n n n n n n n n n\nn\nri\nal P Effectiveness n n n n n n\nr\ne\nn Transparency n n n n n n\ne\nG\nD \nA Accountability n n n n n n n\nE\nAwareness of Misuse n n n n n n n\nCompetence n n n n n n\nn    \nIndicates General Principle mapped to Chapter.\n n  \nIndicates primary EAD Chapter providing elaboration on a General Principle.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 13', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nFrom Principles to Practice\nEthically Aligned Design Conceptual Framework \nFrom Principles to Practice    \nIt is at this step of the Ethically Aligned Design Conceptual Framework that readers will be able to \nidentify the Principles and Chapters of key relevance to their work. Content provided in EAD1e Chapters \nis organized by “Issues” identified as the most pressing ethical matters surrounding A/IS design to \naddress today and “Recommendations” on how it should be done. By reviewing these Issues and \nRecommendations in light of a specific A/IS product, service, or system being designed, readers \nare provided with a simple form of impact assessment and due diligence process to help put their \n“Principles into Practice” for themselves. Of course, more fine-tuned customization and adaptation  \nof the content of EAD1e to fit specific sectors or applications are possible and will be pursued  \nin the near future. See below for some implementation examples already happening.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 14', 'Ethically Aligned Design in Implementation     \nEthically Aligned Design, First Edition represents the culmination of a three-year process guided \nbottom-up since 2015 by the rigor and standards of the engineering profession and by a globally \nopen and iterative process involving hundreds of global experts. The analysis of the Principles, Issues, \nand Recommendations generated as part of an iterative process have already inspired the creation of \nfourteen IEEE Standardization Projects, a Certification Program, A/IS Ethics Courses, and multiple other \naction-oriented programs currently in development. \nIn its earlier manifestations, Ethically Aligned Design informed collaborations on A/IS governance with a \nbroad range of governmental and civil society organizations, including the United Nations, the European \nCommission, the Organization for Economic Cooperation and Development and many national and \nmunicipal governments and institutions.5 Moreover, the engagement in all of these arenas and with such \npartners has put the collective knowledge and creativity of The IEEE Global Initiative in the service of \nglobal policy-making with tangible and visible results. Beyond inspiring the policy arena, EAD1e and this \ngrowing body of work has also been influencing the development of industry-related resources.6 \nIt is time to move “From Principles to Practice” in society regarding the governance of emerging \nautonomous and intelligent systems. The implementation of ethical principles must be validated by \ndependable applications of A/IS in practice while honoring our desire for political self-determination and \ndata agency. To achieve societal progress, the autonomous and intelligent systems we create must be \ntrustworthy, provable, and accountable and must align to our explicitly formulated human values.  \nIt is our hope that Ethically Aligned Design and this conceptual framework will provide action-oriented \ninspiration for your work as well. \nEthically Aligned Design Conceptual Framework—From Principles to Practice\nFor information on disclaimers associated with EAD1e, see How the Document Was Prepared.\n15', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nFrom Principles to Practice\nEthically Aligned Design Conceptual Framework \nEndnotes\n1  We prefer not to use—as far as possible—  4  For their overall framing, see the “General \nthe vague term “AI” and use instead the   Principles” Chapter. \nterm autonomous and intelligent systems  \n5  As an example, the recently published report \n(A/IS). This terminology is applied throughout \nDraft Ethics Guidelines for Trustworthy AI of The \nEthically Aligned Design, First Edition to ensure \nEuropean Commission’s High Level Expert Group \nthe broadest possible application of ethical \non AI explicitly mentions EAD as a major source \nconsiderations in the design of the addressed \nof their inspiration. EAD has also been guiding \ntechnologies and systems. \npolicy creation for efforts of the United Nations \n2  See also Draft Ethics Guidelines for Trustworthy  and the Organization for Economic Cooperation \nAI of The European Commission’s High Level  and Development.\nExpert Group on AI.\n6  Everyday Ethics for Artificial Intelligence: A \n3  A/IS should be subject to specific certification  Practical Guide for Designers and Developers\nprocedures by competent and qualified agencies \nwith participation or control of public authorities \nin the same way other technical systems require \ncertification before deployment. The IEEE has \nlaunched one of the world’s first programs \ndedicated to creating A/IS certification processes. \nThe Ethics Certification Program for Autonomous \nand Intelligent Systems (ECPAIS) offers processes \nby which organizations can seek certified  \nA/IS products, systems, and services. It is  \nbeing developed through an extensive and  \nopen public-private collaboration. \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 16', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems \nGGeenneerraall  PPrriinncciipplleess\nThe General Principles of Ethically Aligned Design articulate high-level ethical principles \nthat apply to all types of autonomous and intelligent systems (A/IS), regardless of whether \nthey are physical robots, such as care robots or driverless cars, or software systems, such as \nmedical diagnosis systems, intelligent personal assistants, or algorithmic chat bots, in real, \nvirtual, contextual, and mixed-reality environments.\nThe General Principles define imperatives for the design, development, deployment, \nadoption, and decommissioning of autonomous and intelligent systems. The Principles \nconsider the role of A/IS creators, i.e., those who design and manufacture, of operators, i.e., \nthose with expertise specific to use of A/IS, other users, and any other stakeholders  \nor affected parties.\nWe have created these ethical General Principles for A/IS that:\n•  Embody the highest ideals of human beneficence within human rights.\n•  Prioritize benefits to humanity and the natural environment from the use of A/IS over \ncommercial and other considerations. Benefits to humanity and the natural environment \nshould not be at odds—the former depends on the latter. Prioritizing human well-being \ndoes not mean degrading the environment.\n•  Mitigate risks and negative impacts, including misuse, as A/IS evolve as socio-technical \nsystems, in particular by ensuring actions of A/IS are accountable and transparent.\nThese General Principles are elaborated in subsequent sections of this chapter of Ethically \nAligned Design, with specific contextual, cultural, and pragmatic explorations which impact \ntheir implementation. \n  \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 17', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nGeneral Principles\nGeneral Principles as Imperatives\nWe offer high-level General Principles in Ethically Aligned Design that we consider to \nbe imperatives for creating and operating A/IS that further human values and ensure \ntrustworthiness. In summary, our General Principles are:\n1. Human Rights–A/IS shall be created and operated to respect, promote, and protect \ninternationally recognized human rights.\n2. Well-being–A/IS creators shall adopt increased human well-being as a primary success \ncriterion for development.\n3. Data Agency–A/IS creators shall empower individuals with the ability to access and \nsecurely share their data, to maintain people’s capacity to have control over their identity.\n4. Effectiveness–A/IS creators and operators shall provide evidence of the effectiveness \nand fitness for purpose of A/IS.\n5. Transparency–The basis of a particular A/IS decision should always be discoverable.\n6. Accountability–A/IS shall be created and operated to provide an unambiguous \nrationale for all decisions made.\n7. Awareness of Misuse–A/IS creators shall guard against all potential misuses and risks \nof A/IS in operation.\n8. Competence–A/IS creators shall specify and operators shall adhere to the knowledge \nand skill required for safe and effective operation.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 18', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nGeneral Principles\nPrinciple 1—Human Rights\ndata, or agency. While the direct coding of human \nrights in A/IS may be difficult or impossible based \nA/IS shall be created and \non contextual use, newer guidelines from The \noperated to respect, promote, \nUnited Nations provide methods to pragmatically \nand protect internationally \nimplement human rights ideals within business \nrecognized human rights. or corporate contexts that could be adapted \nfor engineers and technologists. In this way, \nBackground technologists can take into account human rights \nin the way A/IS are developed, operated, tested, \nHuman benefit is a crucial goal of A/IS, as \nand validated. In short, human rights should be \nis respect for human rights set out in works \npart of the ethical risk assessment of A/IS.\nincluding, but not limited to: The Universal \nDeclaration of Human Rights, the International \nRecommendations\nCovenant on Civil and Political Rights, the \nConvention on the Rights of the Child, the  To best respect human rights, society must \nConvention on the Elimination of all forms of  assure the safety and security of A/IS so that they \nDiscrimination against Women, the Convention  are designed and operated in a way that benefits \non the Rights of Persons with Disabilities, and the  humans. Specifically:\nGeneva Conventions.\n•  Governance frameworks, including standards \nSuch rights need to be fully taken into  and regulatory bodies, should be established \nconsideration by individuals, companies,  to oversee processes which ensure that the \nprofessional bodies, research institutions, and  use of A/IS does not infringe upon human \ngovernments alike to reflect the principle that   rights, freedoms, dignity, and privacy, and \nA/IS should be designed and operated in a way  which ensure traceability. This will contribute \nthat both respects and fulfills human rights,  to building public trust in A/IS.\nfreedoms, human dignity, and cultural diversity.\n•  A way to translate existing and forthcoming \nWhile their interpretation may change over time,  legal obligations into informed policy and \n“human rights”, as defined by international law,  technical considerations is needed. Such \nprovide a unilateral basis for creating any A/IS,  a method should allow for diverse cultural \nas these systems affect humans, their emotions,  norms as well as differing legal and regulatory \nframeworks.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 19', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nGeneral Principles\n•  A/IS should always be subordinate to human  Forms of Discrimination against Women, 1979.\njudgment and control.\n•  The Convention on the Rights of Persons with \n•  For the foreseeable future, A/IS should not be  Disabilities, 2006.\ngranted rights and privileges equal to human \n•  The Geneva Conventions and Additional \nrights.\nProtocols, 1949.\nFurther Resources •  IRTF’s Research into Human Rights Protocol \nConsiderations, 2018.\nThe following documents and organizations are \n•  The UN Guiding Principles on Business and \nprovided both as references and examples of the \nHuman Rights, 2011.\ntypes of work that can be emulated, adapted, \nand proliferated regarding ethical best practices  •  British Standards Institute BS8611:2016, \naround A/IS to best honor human rights: Robots and Robotic Devices. Guide to the \nEthical Design and Application of Robots and \n•  The Universal Declaration of Human Rights, \nRobotic Systems\n1947.\n•  N. Wiener, The Human Use of Human Beings, \nNew York: Houghton Mifflin, 1954.\n•  The International Covenant on Civil and \nPolitical Rights, 1966.\n•  The International Covenant on Economic, \nSocial and Cultural Rights, 1966.\n•  The International Convention on the \nElimination of All Forms of Racial \nDiscrimination, 1965.\n•  The Convention on the Rights of the Child, \n1990.\n•  The Convention on the Elimination of All \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 20', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nGeneral Principles\nPrinciple 2—Well-being\nSince modern societies will be largely constituted \nof A/IS users, we believe these considerations to \nA/IS creators shall adopt \nbe relevant for A/IS creators.\nincreased human well-being as \na primary success criterion for  A/IS technologies can be narrowly conceived \nfrom an ethical standpoint. They can be legal, \ndevelopment.\nprofitable, and safe in their usage, yet not \npositively contribute to human and environmental \nBackground\nwell-being. This means technologies created \nFor A/IS technologies to demonstrably advance  with the best intentions, but without considering \nbenefit for humanity, we need to be able to  well-being, can still have dramatic negative \ndefine and measure the benefit we wish to  consequences on people’s mental health, \nincrease. But often the only indicators utilized  emotions, sense of themselves, their autonomy, \nin determining success for A/IS are avoiding  their ability to achieve their goals, and other \nnegative unintended consequences and  dimensions of well-being.\nincreasing productivity and economic growth for \ncustomers and society. Today, these are largely \nRecommendation\nmeasured by gross domestic product (GDP), \nprofit, or consumption levels. A/IS should prioritize human well-being as an \noutcome in all system designs, using the best \nWell-being, for the purpose of Ethically Aligned  available and widely accepted well-being metrics \nDesign, is based on the Organization for  as their reference point.\nEconomic Co-operation and Development’s \n(OECD) ”Guidelines on Measuring Subjective \nFurther Resources\nWell-being” perspective that, “Being able to \nmeasure people’s quality of life is fundamental  •  IEEE P7010™, Well-being Metric for \nwhen assessing the progress of societies.” There  Autonomous and Intelligent Systems.\nis now widespread acknowledgement that  •  The Measurement of Economic Performance \nmeasuring subjective well-being is an essential  and Social Progress now commonly referred to \npart of measuring quality of life alongside other  as “The Stiglitz Report”, commissioned by the \nsocial and economic dimensions as identified  then President of the French Republic, 2009. \nwithin Nassbaum-Sen’s capability approach  From the report: “…the time is ripe for our \nwhereby well-being is objectively defined in  measurement system to shift emphasis from \nterms of human capabilities necessary for  measuring economic production to measuring \nfunctioning and flourishing.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 21', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nGeneral Principles\npeople’s well-being … emphasizing well-being  •  The International Panel on Social Progress, \nis important because there appears to be  Social Justice, Well-Being and Economic \nan increasing gap between the information  Organization, 2018.\ncontained in aggregate GDP data and what \n•  R. Veenhoven, World Database of Happiness, \ncounts for common people’s well-being.”\nErasmus University Rotterdam, The \n•  OECD Guidelines on Measuring Subjective  Netherlands, Accessed 2018 at: http://\nWell-being, 2013.  worlddatabaseofhappiness.eur.nl.\n•  OECD Better Life Index, 2017. •  Royal Government of Bhutan, The Report \nof the High-Level Meeting on Wellbeing \n•  World Happiness Reports, 2012 – 2018.\nand Happiness: Defining a New Economic \n•  United Nations Sustainable Development Goal \nParadigm, New York: The Permanent Mission \n(SDG) Indicators, 2018.\nof the Kingdom of Bhutan to the United \n•  Beyond GDP, European Commission, 2018.  Nations, 2012.\nFrom the site: “The Beyond GDP initiative is \nabout developing indicators that are as clear \nand appealing as GDP, but more inclusive of \nenvironmental and social aspects of progress.”\n•  Genuine Progress Indicator, State of Maryland \n(first developed by Redefining Progress), \n2015.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 22', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nGeneral Principles\nPrinciple 3—Data Agency\nRegulations like the EU General Data Protection \nRegulation (GDPR) will help improve this lack of \nA/IS creators shall empower \nclarity regarding the exchange of personal data. \nindividuals with the ability \nBut compliance with existing models of consent \nto access and securely share \nis not enough to safeguard people’s agency \ntheir data, to maintain people’s  regarding their personal information. In an era \ncapacity to have control over  where A/IS are already pervasive in society, \ngovernments must recognize that limiting the \ntheir identity.\nmisuse of personal data is not enough. \nBackground Society must also recognize that human rights \nin the digital sphere don’t exist until individuals \nDigital consent is a misnomer in its current \nglobally are empowered with means—including \nmanifestation. Terms and conditions or privacy \ntools and policies—that ensure their dignity through \npolicies are largely designed to provide legally \nsome form of sovereignty, agency, symmetry, or \naccurate information regarding the usage of \ncontrol regarding their identity and personal data. \npeople’s data to safeguard institutional and \nThese rights rely on individuals being able to make \ncorporate interests, while often neglecting the \ntheir choices, outside of the potential influence \nneeds of the people whose data they process. \nof biased algorithmic messaging or bad actors. \n“Consent fatigue”, the constant request for \nSociety also needs to be confident that those who \nagreement to sets of long and unreadable data \nare unable to provide legal informed consent, \nhandling conditions, causes a majority of users \nincluding minors and people with diminished \nto simply click and accept terms in order to \ncapacity to make informed decisions, do not lose \naccess the services they wish to use. General \ntheir dignity due to this.\nobfuscation regarding privacy policies, and \nscenarios like the Cambridge Analytica scandal \nRecommendation\nin 2018, demonstrate that even when individuals \nOrganizations, including governments, should \nprovide consent, the understanding of the value \nimmediately explore, test, and implement \nregarding their data and its safety is out of an \ntechnologies and policies that let individuals \nindividual’s control. \nspecify their online agent for case-by-case \nThis existing model of data exchange has eroded  authorization decisions as to who can process \nhuman agency in the algorithmic age. People  what personal data for what purpose. For \ndon’t know how their data is being used at all  minors and those with diminished capacity to \ntimes or when predictive messaging is honoring  make informed decisions, current guardianship \ntheir existing preferences or manipulating them to  approaches should be viewed to determine their \ncreate new behaviors.  suitability in this context.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 23', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nGeneral Principles\nThe general solution to give agency to the  •  IEEE P7006™, IEEE Standards Project for \nindividual is meant to anticipate and enable  Personal Data Artificial Intelligence (AI) Agent \nindividuals to own and fully control autonomous  describes the technical elements required \nand intelligent (as in capable of learning)  to create and grant access to a personalized \ntechnology that can evaluate data use requests  Artificial Intelligence that will comprise inputs, \nby external parties and service providers. This  learning, ethics, rules, and values controlled by \ntechnology would then provide a form of “digital  individuals. \nsovereignty” and could issue limited and specific \n•  IEEE P7012™, IEEE Standards Project for \nauthorizations for processing of the individual’s \nMachine Readable Personal Privacy Terms \npersonal data wherever it is held in a  \nis designed to provide individuals with a \ncompatible system.\nmeans to proffer their own terms respecting \npersonal privacy in ways that can be read, \nFurther Resources acknowledged, and be agreed to by machines \noperated by others in the networked world. \nThe following resources are designed to provide \ngovernments and other organizations—corporate, \nfor-profit, not-for-profit, B Corp, or any form \nof public institution—basic information on \nservices designed to provide user agency and/or \nsovereignty over their personal data. \n•  The European Data Protection Supervisor \ndefines personal information management \nsystems (PIMS) as: \n•  “...systems that help give individuals more \ncontrol over their personal data...allowing \nindividuals to manage their personal data \nin secure, local or online storage systems \nand share them when and with whom they \nchoose. Providers of online services and \nadvertisers will need to interact with the \nPIMS if they plan to process individuals’ data. \nThis can enable a human centric approach \nto personal information and new business \nmodels.” For further information and ongoing \nresearch regarding PIMS, visit Crtl-Shift’s PIMS \nmonthly archive. \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 24', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nGeneral Principles\nPrinciple 4—Effectiveness\nbuilding A/IS should ensure that the results \nwhen the defined metrics are applied are \nCreators and operators shall \nreadily obtainable by all interested parties, e.g., \nprovide evidence of the \nusers, safety certifiers, and regulators  \neffectiveness and fitness for \nof the system.\npurpose of A/IS.\n2. Creators of A/IS should provide guidance on \nhow to interpret and respond to the metrics \nBackground \ngenerated by the systems. \nThe responsible adoption and deployment of  3. To the extent warranted by specific \nA/IS are essential if such systems are to realize  circumstances, operators of A/IS should follow \ntheir many potential benefits to the well-being  the guidance on measurement provided with \nof both individuals and societies. A/IS will not be  the systems, i.e., which metrics to obtain,  \ntrusted unless they can be shown to be effective  how and when to obtain them, how to respond \nin use. Harms caused by A/IS, from harm to  to given results, and so on.   \nan individual through to systemic damage, can \n4. To the extent that measurements are sample-\nundermine the perceived value of A/IS and delay \nbased, measurements should account for the \nor prevent its adoption.\nscope of sampling error, e.g., the reporting \nOperators and other users will therefore benefit  of confidence intervals associated with the \nfrom measurement of the effectiveness of the  measurements. Operators should be advised \nA/IS in question. To be adequate, effective  how to interpret the results. \nmeasurements need to be both valid and  5. Creators of A/IS should design their systems \naccurate, as well as meaningful and actionable.  such that metrics on specific deployments \nAnd such measurements must be accompanied  of the system can be aggregated to provide \nby practical guidance on how to interpret and  information on the effectiveness of the system \nrespond to them. across multiple deployments. For example, \nin the case of autonomous vehicles, metrics \nRecommendations\nshould be generated both for a specific \n1.  Creators engaged in the development of A/IS  instance of a vehicle and for a fleet of many \nshould seek to define metrics or benchmarks  instances of the same kind of vehicle. \nthat will serve as valid and meaningful gauges \n6. In interpreting and responding to \nof the effectiveness of the system in meeting \nmeasurements, allowance should be made \nits objectives, adhering to standards and \nfor variation in the specific objectives and \nremaining within risk tolerances. Creators \ncircumstances of a given deployment of A/IS. \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 25', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nGeneral Principles\n7.  To the extent possible, industry associations \nor other organizations, e.g., IEEE and ISO, \nshould work toward developing standards \nfor the measurement and reporting on the \neffectiveness of A/IS.  \nFurther Resources\n•  R. Dillmann, KA 1.10 Benchmarks for Robotics \nResearch, 2010. \n•  A. Steinfeld, T.W. Fong, D. Kaber, J. Scholtz, \nA. Schultz, and M. Goodrich, “Common \nMetrics for Human-Robot Interaction”, 2006 \nHuman-Robot Interaction Conference, \nMarch, 2006. \n•  R. Madhavan, E. Messina, and E. \nTunstel, Eds., Performance Evaluation and \nBenchmarking of Intelligent Systems, Boston, \nMA: Springer, 2009.\n•  IEEE Robotics & Automation Magazine, Special \nIssue on Replicable and Measurable Robotics \nResearch, Volume 22, No. 3, September 2015.\n•  C. Flanagin, A Survey on Robotics Systems and \nPerformance Analysis, 2011.\n•  Transaction Processing Performance Council \n(TPC) Establishes Artificial Intelligence Working \nGroup (TPC-AI) tasked with developing \nindustry standard benchmarks for both \nhardware and software platforms associated \nwith running Artificial Intelligence (AI) based \nworkloads, 2017.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 26', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nGeneral Principles\nPrinciple 5—Transparency\nAt the same time, the complexity of A/IS \ntechnology and the non-intuitive way in which \nThe basis of a particular A/IS \nit may operate will make it difficult for users of \ndecision should always \nthose systems to understand the actions of the \nbe discoverable.\nA/IS that they use, or with which they interact. \nThis opacity, combined with the often distributed \nBackground manner in which the A/IS are developed, will \nA key concern over autonomous and intelligent  complicate efforts to determine and allocate \nsystems is that their operation must be  responsibility when something goes wrong. \ntransparent to a wide range of stakeholders  Thus, lack of transparency increases the risk \nfor different reasons, noting that the level of  and magnitude of harm when users do not \ntransparency will necessarily be different for each  understand the systems they are using, or there \nstakeholder. Transparent A/IS are ones in which  is a failure to fix faults and improve systems \nit is possible to discover how and why a system  following accidents. Lack of transparency also \nmade a particular decision,   increases the difficulty of ensuring accountability \nor in the case of a robot, acted the way it did.  (see Principle 6— Accountability).\nThe term “transparency” in the context of  \nAchieving transparency, which may involve a \nA/IS also addresses the concepts of traceability, \nsignificant portion of the resources required \nexplainability, and interpretability.\nto develop the A/IS, is important to each \nA/IS will perform tasks that are far more complex  stakeholder group for the following reasons:\nand have more effect on our world than prior \n1.  For users, what the system is doing and why.\ngenerations of technology. Where the task is \nundertaken in a non-deterministic manner, it  2. For creators, including those undertaking \nmay defy simple explanation. This reality will  the validation and certification of A/IS, the \nbe particularly acute with systems that interact  systems’ processes and input data.\nwith the physical world, thus raising the potential \n3. For an accident investigator, if accidents occur.\nlevel of harm that such a system could cause. \nFor example, some A/IS already have real \n4. For those in the legal process, to inform \nconsequences to human safety or well-being, \nevidence and decision-making.\nsuch as medical diagnosis or driverless car \nautopilots. Systems such as these are safety- 5. For the public, to build confidence in  \ncritical systems.  the technology.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 27', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nGeneral Principles\nRecommendation Further Resources\n•  C. Cappelli, P. Engiel, R. Mendes de Araujo, \nDevelop new standards that describe measurable, \nand J. C. Sampaio do Prado Leite, “Managing \ntestable levels of transparency, so that systems \nTransparency Guided by a Maturity Model,” \ncan be objectively assessed and levels of \n3rd Global Conference on Transparency \ncompliance determined. For designers, such \nResearch 1 no. 3, pp. 1–17, Jouy-en-Josas, \nstandards will provide a guide for self-assessing \nFrance: HEC Paris, 2013.\ntransparency during development and suggest \nmechanisms for improving transparency. The  •  J.C. Sampaio do Prado Leite and C. Cappelli, \nmechanisms by which transparency is provided  “Software Transparency.” Business & \nwill vary significantly, including but not limited to,  Information Systems Engineering 2, no.  \nthe following use cases:  3, pp. 127–139, 2010.\n•  A, Winfield, and M. Jirotka, “The Case for an \n1.  For users of care or domestic robots, a “why-\nEthical Black Box,” Lecture Notes in Artificial \ndid-you-do-that button” which, when pressed, \nIntelligence 10454, pp. 262–273, 2017.\ncauses the robot to explain the action it  \njust took. •  R. R. Wortham, A. Theodorou, and J. J. Bryson, \n“What Does the Robot Think? Transparency \n2. For validation or certification agencies, the \nas a Fundamental Design Requirement for \nalgorithms underlying the A/IS and how they \nIntelligent Systems,” IJCAI-2016 Ethics for \nhave been verified.\nArtificial Intelligence Workshop,  \nNew York, 2016.\n3. For accident investigators, secure storage of \nsensor and internal state data comparable to a  •  Machine Intelligence Research Institute, \nflight data recorder or black box. “Transparency in Safety-Critical Systems,” \nAugust 25, 2013.\nIEEE P7001™, IEEE Standard for Transparency \n•  M. Scherer, “Regulating Artificial Intelligence \nof Autonomous Systems is one such \nSystems: Risks, Challenges, Competencies, \nstandard, developed in response to this \nand Strategies,” Harvard Journal of Law & \nrecommendation.\nTechnology 29, no. 2, 2015.\n  •  U.K. House of Commons, “Decision Making \n  Transparency,” Report of the U.K. House \n  of Commons Science and Technology \n  Committee on Robotics and Artificial \n  Intelligence, pp. 17-18, September 13, 2016.\n \n \n \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 28', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nGeneral Principles\nPrinciple 6—Accountability\nRecommendations\nA/IS shall be created and  To best address issues of responsibility and \naccountability:\noperated to provide an \nunambiguous rationale for  1.  Legislatures/courts should clarify responsibility, \ndecisions made. culpability, liability, and accountability for  \nA/IS, where possible, prior to development \nBackground and deployment so that manufacturers and \nusers understand their rights and obligations.\nThe programming, output, and purpose of A/IS \nare often not discernible by the general public.  2. Designers and developers of A/IS should \nBased on the cultural context, application,  remain aware of, and take into account, the \nand use of A/IS, people and institutions need  diversity of existing cultural norms among the \nclarity around the manufacture and deployment  groups of users of these A/IS.\nof these systems to establish responsibility \nand accountability, and to avoid potential  3. Multi-stakeholder ecosystems including \nharm. Additionally, manufacturers of these  creators, and government, civil, and \nsystems must be accountable in order to  commercial stakeholders, should be \naddress legal issues of culpability. It should, if  developed to help establish norms where \nnecessary, be possible to apportion culpability  they do not exist because A/IS-oriented \namong responsible creators (designers and  technology and their impacts are too new. \nmanufacturers) and operators to avoid confusion  These ecosystems would include, but \nor fear within the general public. not be limited to, representatives of civil \nsociety, law enforcement, insurers, investors, \nAccountability and partial accountability are not  manufacturers, engineers, lawyers, and users. \npossible without transparency, thus this principle  The norms can mature into best practices  \nis closely linked with Principle 5–Transparency. and laws. \n \n   \n   \n \n \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 29', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nGeneral Principles\n4. Systems for registration and record-keeping  Further Resources\nshould be established so that it is always \n•  B. Shneiderman, “Human Responsibility for \npossible to find out who is legally responsible \nAutonomous Agents,” IEEE Intelligent Systems \nfor a particular A/IS. Creators, including \n22, no. 2, pp. 60–61, 2007.\nmanufacturers, along with operators,  \nof A/IS should register key, high-level  •  A. Matthias, “The Responsibility Gap: Ascribing \nparameters, including: Responsibility for the Actions of Learning \nAutomata.” Ethics and Information Technology \n•  Intended use, 6, no. 3, pp. 175–183, 2004.\n•  Training data and training environment,   •  A. Hevelke and J. Nida-Rümelin, “Responsibility \nif applicable, for Crashes of Autonomous Vehicles: An Ethical \n•  Sensors and real world data sources, Analysis,” Science and Engineering Ethics 21, \nno. 3, pp. 619–630, 2015.\n•  Algorithms,\n•  An example of good practice (in relation \n•  Process graphs,\nto Recommendation #3) can be found in \n•  Model features, at various levels, Sciencewise—the U.K. national center for public \n•  User interfaces, dialogue in policy-making involving science and \ntechnology issues.\n•  Actuators and outputs, and\n•  Optimization goals, loss functions,  \nand reward functions.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 30', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nGeneral Principles\nPrinciple 7—Awareness of Misuse\n2. Raise public awareness around the issues \nof potential A/IS technology misuse in an \nCreators shall guard against all \ninformed and measured way by:\npotential misuses and risks of \nA/IS in operation. •  Providing ethics education and security \nawareness that sensitizes society to the \nBackground potential risks of misuse of A/IS. For example, \nprovide “data privacy warnings” that some \nNew technologies give rise to greater risk of \nsmart devices will collect their users’  \ndeliberate or accidental misuse, and this is \npersonal data.\nespecially true for A/IS. A/IS increases the impact \n•  Delivering this education in scalable and \nof risks such as hacking, misuse of personal data, \neffective ways, including having experts with \nsystem manipulation, or exploitation of vulnerable \nthe greatest credibility and impact who can \nusers by unscrupulous parties. Cases of A/IS \nminimize unwarranted fear about A/IS.\nhacking have already been widely reported, with \ndriverless cars, for example. The Microsoft Tay  •  Educating government, lawmakers, and \nAI chatbot was famously manipulated when it  enforcement agencies about these issues \nmimicked deliberately offensive users. In an age  of A/IS so citizens can work collaboratively \nwhere these powerful tools are easily available,  with these agencies to understand safe \nthere is a need for a new kind of education  use of A/IS. For example, the same way \nfor citizens to be sensitized to risks associated  police officers give public safety lectures in \nwith the misuse of A/IS. The EU’s General Data  schools, they could provide workshops on \nProtection Regulation (GDPR) provides measures  safe use and interaction with A/IS.\nto remedy the misuse of personal data.\nFurther Resources\nResponsible innovation requires A/IS creators to \n•  A. Greenberg, “Hackers Fool Tesla S’s_Autopilot \nanticipate, reflect, and engage with users of A/IS. \nto Hide and Spoof Obstacles,”  \nThus, citizens, lawyers, governments, etc., all have \nWired, August 2016.\na role to play through education and awareness \nin developing accountability structures (see  •  C. Wilkinson and E. Weitkamp, Creative Research \nPrinciple 6), in addition to guiding new technology  and Communication: Theory and Practice, \nproactively toward beneficial ends. Manchester, UK: Manchester University Press, \n2016 (in relation to Recommendation #2).\nRecommendations\n•  Engineering and Physical Sciences Research \n1.  Creators should be aware of methods of  Council, “Anticipate, Reflect, Engage and Act \nmisuse, and they should design A/IS in ways to  (AREA),” Framework for Responsible Research \nminimize the opportunity for these. and Innovation, Accessed 2018.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 31', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nGeneral Principles\nPrinciple 8—Competence \nA/IS reach their decisions, the information and \nlogic on which the A/IS rely, and the effects of \nCreators shall specify and \nthose decisions. Even more crucially, operators \noperators shall adhere to the \nshould know when they need to question A/IS and \nknowledge and skill required  for  when they need to overrule them.\nsafe and effective operation.\nCreators of A/IS should take an active role in \nensuring that operators of their technologies have \nthe knowledge, experience, and skill necessary \nBackground\nnot only to use A/IS, but also to use it safely \nA/IS can and often do make decisions that  and appropriately, towards their intended ends. \npreviously required human knowledge, expertise,  Creators should make provisions for the operators \nand reason. Algorithms potentially can make even  to override A/IS in appropriate circumstances. \nbetter decisions, by accessing more information, \nmore quickly, and without the error, inconsistency,  While standards for operator competence are \nand bias that can plague human decision-making.  necessary to ensure the effective, safe, and \nAs the use of algorithms becomes common and  ethical application of A/IS, these standards are \nthe decisions they make become more complex,  not the same for all forms of A/IS. The level of \nhowever, the more normal and natural such  competence required for the safe and effective \ndecisions appear. operation of A/IS will range from elementary, such \nas “intuitive” use guided by design, to advanced, \nOperators of A/IS can become less likely to  such as fluency in statistics. \nquestion and potentially less able to question the \ndecisions that algorithms make. Operators will \nRecommendations\nnot necessarily know the sources, scale, accuracy, \nand uncertainty that are implicit in applications of  1.  Creators of A/IS should specify the types and \nA/IS. As the use of A/IS expands, more systems  levels of knowledge necessary to understand \nwill rely on machine learning where actions are  and operate any given application of A/IS. \nnot preprogrammed and that might not leave a  In specifying the requisite types and levels \nclear record of the steps that led the system to  of expertise, creators should do so for the \nits current state. Even if those records do exist,  individual components of A/IS and for the \noperators might not have access to them or the  entire systems.\nexpertise necessary to decipher those records.\n2. Creators of A/IS should integrate safeguards \nStandards for the operators are essential.  against the incompetent operation of their \nOperators should be able to understand how   systems. Safeguards could include issuing \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 32', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nGeneral Principles\nnotifications/warnings to operators in certain  5. Operators of A/IS should, before operating a \nconditions, limiting functionalities for different  system, make sure that they have access to \nlevels of operators (e.g., novice vs. advanced),  the requisite competencies. The operator need \nsystem shut-down in potentially risky  not be an expert in all the pertinent domains \nconditions, etc. but should have access to individuals with the \nrequisite kinds of expertise.\n3. Creators of A/IS should provide the parties \naffected by the output of A/IS with information \nFurther Resources\non the role of the operator, the competencies \nrequired, and the implications of operator error.  •  S. Barocas and A.D. Selbst, The Intuitive Appeal \nSuch documentation should be accessible   of Explainable Machines, Fordham Law Review, \nand understandable to both experts and the  2018.\ngeneral public.\n•  W. Smart, C. Grimm, and W. Hartzog, “An \nEducation Theory of Fault for Autonomous \n4. Entities that operate A/IS should create \nSystems”, 2017. \ndocumented policies to govern how A/IS \nshould be operated. These policies should \ninclude the real-world applications for such  \nA/IS, any preconditions for their effective use, \nwho is qualified to operate them, what training \nis required for operators, how to measure the \nperformance of the A/IS, and what should be \nexpected from the A/IS. The policies should \nalso include specification of circumstances  \nin which it might be necessary for the  \noperator to override the A/IS.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 33', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nGeneral Principles\nThanks to the Contributors \nWe wish to acknowledge all of the people who  •  Hugo Giordano – Engineering Student at \ncontributed to this chapter. Texas A&M University\n•  Alexei Grinbaum – Researcher at CEA \nThe General Principles Committee (French Alternative Energies and Atomic Energy \nCommission) and Member of the French \n•  Alan Winfield (Founding Chair) – Professor, \nCommission on the Ethics of Digital Sciences \nBristol Robotics Laboratory, University of the \nand Technologies CERNA\nWest of England; Visiting Professor, University \nof York •  Jia He – Independent Researcher, Graduate \nDelft University of Technology in Engineering \n•  Mark Halverson (Co-Chair) – Founder and \nand Public Policy, project member within United \nCEO at Precision Autonomy\nNations, ICANN, and ITU Executive Director of \n•  Peet van Biljon (Co-Chair) – Founder \nToutiao Research (Think Tank), Bytedance Inc.\nand CEO at BMNP Strategies LLC, advisor \n•  Bruce Hedin – Principal Scientist, H5\non strategy, innovation, and business \ntransformation; Adjunct professor at  •  Cyrus Hodes – Advisor AI Office, UAE \nGeorgetown University; Business ethics author Prime Minister’s Office, Co-founder and \nSenior Advisor, AI Initiatives@The Future \n•  Shahar Avin – Research Associate, Centre \nSociety; Member, AI Expert Group at the \nfor the Study of Existential Risk, University of \nOECD, Member, Global Council on Extended \nCambridge\nIntelligence; Co-founder and Senior Advisor, \n•  Bijilash Babu – Senior Manager, Ernst and \nThe AI Initiative @ The Future Society\nYoung, EY Global Delivery Services India LLP\n•  Nathan F. Hutchins – Applied Assistant \n•  Richard Bartley – Senior Director - Analyst, \nProfessor, Department of Electrical and \nSecurity & Risk Management, Gartner, Toronto, \nComputer Engineering, The University of Tulsa\nCanada Security Principal Director, Accenture, \n•  Narayana GPL. Mandaleeka (“MGPL”) – \nToronto, Canada.\nVice President & Chief Scientist, Head, Business \n•  R. R. Brooks – Professor, Holcombe \nSystems & Cybernetics Centre, Tata Consultancy \nDepartment of Electrical and Computer \nServices Ltd.\nEngineering, Clemson University\n•  Vidushi Marda – Programme Officer, ARTICLE 19\n•  Nicolas Economou – Chief Executive Officer, \n•  George T. Matthew – Chief Medical Officer, \nH5; Chair, Science, Law and Society Initiative \nNorth America, DXC Technology   \nat The Future Society Chair, Law Committee, \n \nGlobal Governance of AI Roundtable; Member, \nCouncil on Extended Intelligence (CXI)\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 34', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nGeneral Principles\n•  Nicolas Miailhe – Co-Founder & President,  •  Niels ten Oever – Head of Digital, Article \nThe Future Society; Member, AI Expert Group  19, Co-chair Research Group on Human \nat the OECD; Member, Global Council on  Rights Protocol Considerations in the Internet \nExtended Intelligence; Senior Visiting Research  Research Taskforce (IRTF)\nFellow, Program on Science Technology and \n•  Alan R. Wagner – Assistant Professor, \nSociety at Harvard Kennedy School. Lecturer, \nDepartment of Aerospace Engineering, \nParis School of International Affairs (Sciences \nResearch Associate, The Rock Ethics Institute, \nPo); Visiting Professor, IE School of Global and \nThe Pennsylvania State University.\nPublic Affairs\nFor a full listing of all IEEE Global Initiative \n•  Rupak Rathore – Principal Consultant at ATCS \nMembers, visit standards.ieee.org/content/dam/\nfor Telematics, Connected Car and Internet \nieee-standards/standards/web/documents/other/\nof Things; Advisor on strategy, innovation and \nec_bios.pdf.  \ntransformation journey management; Senior \nMember, IEEE For information on disclaimers associated with \nEAD1e, see How the Document Was Prepared.\n•  Peter Teneriello – Investment Analyst, Private \nEquity and Venture Capital, TMRS\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 35', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems \nClassical Ethics in A/IS\nWe applied classical ethics methodologies to considerations of algorithmic design in \nautonomous and intelligent systems (A/IS) where machine learning may or may not reflect \nethical outcomes that mimic human decision-making. To meet this goal, we drew from \nclassical ethics theories and the disciplines of machine ethics, information ethics, and \ntechnology ethics.\nAs direct control over tools becomes further removed, creators of autonomous systems \nmust ask themselves how cultural and ethical presumptions bias artificially intelligent \ncreations. Such introspection is more necessary than ever because the precise and \ndeliberate design of algorithms in self-sustained digital systems will result in responses \nbased on such design.\nBy drawing from over two thousand years’ worth of classical ethics traditions, we \nexplore established ethics systems, including both philosophical traditions (utilitarianism, \nvirtue ethics, and deontological ethics) and religious and culture-based ethical systems \n(Buddhism, Confucianism, African Ubuntu traditions, and Japanese Shinto) and their stance \non human morality in the digital age.1 In doing so, we critique assumptions around concepts \nsuch as good and evil, right and wrong, virtue and vice, and we attempt to carry these \ninquiries into artificial systems’ decision-making processes.\nThrough reviewing the philosophical foundations that define autonomy and ontology,  \nwe address the potential for autonomous capacity of artificially intelligent systems, posing \nquestions of morality in amoral systems and asking whether decisions made by amoral \nsystems can have moral consequences. Ultimately, we address notions of responsibility \nand accountability for the decisions made by autonomous systems and other artificially \nintelligent technologies.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 36', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nClassical Ethics in A/IS\nSection 1—Definitions for Classical  \nEthics in Autonomous and Intelligent  \nSystems Research\nImmanuel Kant’s ethics located morality within \nthe subject (see: categorical imperative) and \nIssue: Assigning Foundations \nseparated morality from the outside world and \nfor Morality, Autonomy, and \nthe consequences of being a part of it. The \nIntelligence\nmoral autonomous subject of modernity became \nthus a worldless isolated subject. This process is \nimportant to understand in terms of ethics for  \nBackground A/IS since it is, paradoxically, the kind of \nClassical theories of economy in the Western  autonomy that is supposed to be achieved by \ntradition, starting with Plato and Aristotle,  intelligent machines as humans evolve into \nembrace three domains: the individual, the  digitally networked beings.\nfamily, and the polis. The formation of the \nThere lies a danger in uncritically attributing \nindividual character (ethos) is intrinsically \nclassical concepts of anthropomorphic autonomy \nrelated to the others, as well as to the tasks of \nto machines, including using the term “artificial \nadministration of work within the family (oikos). \nintelligence” to describe them since, in the \nEventually, this all expands into the framework \nattempt to make them “moral” by programming \nof the polis, or public space (poleis). When we \nmoral rules into their behavior, we run the risk \ndiscuss ethical issues of A/IS, it becomes crucial \nof assuming economic and political dimensions \nto consider these three traditional economic \nthat do not exist, or that are not in line with \ndimensions, since western classical ethics was \ncontemporary human societies. While the \ndeveloped from this foundation and has evolved \nconcepts of artificial intelligence and autonomy \nin modernity into an individual morality \nare mainly used metaphorically as technical \ndisconnected from economics and politics. This \nterms in computer science and technology, \ndisconnection has been questioned and explored \ngeneral and popular discourse may not share in \nby thinkers such as Adam Smith, Georg W. \nthe same nuanced understanding, and political \nF. Hegel, Karl Marx, and others. In particular, \nand societal discourse may become distorted or \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 37', ""The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nClassical Ethics in A/IS\nmisleading. The question of whether   balance between extremes of excess and \nA/IS and the terminology used to describe them  deficiency, which Aristotle identifies as vices. \nwill have any kind of impact on our conception   In the context of A/IS, virtue ethics has two \nof autonomy depends on our policy toward it.   immediate values. First, it provides a model \nFor example, the commonly held fear that   for iterative learning and growth, and moral \nA/IS will relegate humanity to mere spectators  value informed by context and practice, \nor slaves, whether realistic or not, is informed by  not just as compliance with a given, static \nour view of, and terminology around, A/IS. Such  ruleset. Second, it provides to those who \nattitudes are flexible and can be negotiated.   develop and implement A/IS a framework to \nAs noted above, present human societies are  counterbalance tendencies toward excess, \nbeing redefined in terms of digital citizenship  which are common in economically-driven \nvia online social networks. The present public  environments.\ndebate about the replaceability of human work \n•  Deontological ethics: As developed by \nby “intelligent” machines is a symptom of this \n18th century German philosopher, Immanuel \nlack of awareness of the economic and political \nKant, the basic premise of deontological \ndimensions as defined by classical ethics, \nethics addresses the concept of duty. Humans \nreducing ethical thinking to the “morality”  \nhave a rational capacity to create and abide \nof a worldless and isolated machine. \nby rules that allow for duty-based ethics to \nThere is still value that can be gained by  emerge. Rules that produce duties are said \nconsidering how Western ethical traditions can  to have value in themselves without requiring \nbe integrated into either A/IS public awareness  a greater-good justification. Such rules are \ncampaigns or supplemented in engineering and  fundamental to our existence, self-worth, and \nscience education programs, as noted under the  to creating conditions that allow for peaceful \nissue “Presenting ethics to the creators of A/IS”.  coexistence and interaction, e.g., the duty \nBelow is a short overview of how four different  not to harm others; the duty not to steal. \ntraditions can add value. To identify rules that can be universalized \nand made duties, Kant uses the categorical \n•  Virtue ethics: Aristotle argues, using the \nimperative: “Act only on that maxim through \nconcept of telos, or goal, that the ultimate \nwhich you can at the same time will that \ngoal of humans is “eudaimonia”, roughly \nit should become a universal law.” This \ntranslated as “flourishing”. A moral agent \nmeans the rule must be inherently desirable, \nachieves “flourishing”—since it is an action, \ndoable, valuable, and others must be able to \nnot a state—by constantly balancing factors \nunderstand and follow it. Rules based merely \nincluding social environment, material \non personal choice without wider appeal \nprovisions, friends, family, and one's own self. \nare not capable of universalization. There is \nOne cultivates the self through habituation, \nmutual reciprocity in rule-making and rule \npracticing and strengthening virtuous action as \nadherence; if you “will” that a rule should \nthe “golden mean” (a principle of rationality). \nbecome universal law, you not only contribute \nSuch cultivation requires an appropriate \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 38"", 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nClassical Ethics in A/IS\nto rule creation but also agree to be bound  justifications for their use, while disregarding \nby the same rule. The rule should be action- the treatment of humanity as an end in itself, \nguiding, i.e., recommending, prescribing,  e.g., cutting back on funding rigorous testing of  \nlimiting, or proscribing action. Kant also uses  A/IS before they reach the market and society. \nthe humanity formulation of the categorical  Maintaining human agency in human-machine \nimperative: “Act in such a way that you always  interaction is a manifestation of the duty \ntreat humanity, whether in your own person  to respect human dignity. For example, a \nor in the person of any other, never simply  human has the right to know when they are \nas a means, but always at the same time  interacting with A/IS, and may require consent \nas an end.” This produces duties to respect  for any A/IS interaction. \nhumanity and human dignity, and not to treat \n•  Utilitarian ethics: Also called \neither as a means to an end.\nconsequentialist ethics, this code of ethics \n•  In the context of A/IS, one consideration is to  refers to the consequences of one’s decisions \nwonder if developers are acting with the best  and actions. According to the utility principle, \ninterests of humanity and human dignity in  the right course of action is the one that \nmind. This could possibly be extended to   maximizes the utility (utilitarianism) or \nA/IS whereby they are assisting humanity   pleasure (hedonism) for the greatest number \nas an instrument of action that has an impact  of people. This ethics theory does, however, \non decision-making capabilities, despite being  warn against superficial and short-term \nbased on neural machine learning or set  evaluations of utility or pleasure. Therefore, \nprotocols. The humanity formulation of “the  it is the responsibility of the A/IS developers \ncategorical imperative” has implications for  to consider long-term effects. Social justice \nvarious scenarios. The duty to respect human  is paramount in this instance, thus it must be \ndignity may require some limitations on the  ascertained if the implementation of A/IS will \nfunctions and capability of A/IS so that they  contribute to humanity, or negatively impact \ndo not completely replace humans, human  employment or other capabilities. Indeed, \nfunctions, and/or “human central thinking  where it is deemed A/IS can supplement \nactivities” such as judgment, discretion, and  humanity, it should be designed in such a \nreasoning. Privacy and safeguarding issues  way that the benefits are obvious to all the \naround A/IS assisting humans, e.g., healthcare  stakeholders. \nrobots, may require programming certain \n•  Ethics of care: Generally viewed as an \nvalues so that A/IS do not divulge personal \ninstance of feminist ethics, this approach \ninformation to third parties, or compromise a \nemphasizes the importance of relationships \nhuman’s physical or mental well-being. It may \nwhich is context-bound. Relationships are \nalso involve preventing A/IS from deceiving  \nontologically basic to humanity, according to \nor manipulating humans. \nNel Noddings, feminist and philosopher of \n•  Potential benefits and financial incentives  education; to care for other human beings is \nfrom exploiting A/IS may provide ends-means  one of our basic human attributes. For such \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 39', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nClassical Ethics in A/IS\na theory to have relevance in this context,  •  O. Bendel, “Towards Machine Ethics,” in \none needs to consider two criteria: 1) the  Technology Assessment and Policy Areas \nrelationship with the other person, or entity,  of Great Transitions: Proceedings from the \nmust already exist or must have the potential  PACITA 2013 Conference in Prague, PACITA \nto exist, and 2) the relationship should have  2013, Prague, March 13-15, 2013, T. Michalek, \nthe potential to grow into a caring relationship.  L. Hebáková, L. Hennen, C. Scherz, L. Nierling, \nApplied to A/IS, an interesting question comes  J. Hahn, Eds. Prague: Technology Centre \nto the foreground: Can one care for humans  ASCR, 2014. pp. 321-326.\nand their interests in tandem with non-human \n•  O. Bendel, “Considerations about \nentities? If one expects A/IS to be beneficial to \nthe Relationship between Animal and \nhumanity, as in the instance of robots assisting \nMachine Ethics,” AI & Society, vol. 31, no. 1, \nwith care of the elderly, then can one deduce \npp. 103-108, Feb. 2016.\nthe possibility of humans caring for A/IS? If \nthat possibility exists, do principles of social \n•  N. Berberich and K. Diepold, ""The Virtuous \njustice become applicable to A/IS?  \nMachine - Old Ethics for New Technology?"" \narXiv:1806.10322 [cs.AI], June 2018. \nRecommendations\nBy returning to classical ethics foundations,  •  R. Capurro, M. Eldred, and D. Nagel, Digital \nexpand the discussion on ethics in A/IS to  Whoness: Identity, Privacy and Freedom in the \ninclude a critical assessment of anthropomorphic  Cyberworld. Berlin: Walter de Gruyter, 2013.\npresumptions of ethics and moral rules for  \n•  D. Chalmers, “The Singularity: A Philosophical \nA/IS. Keep in mind that machines do not, in \nAnalysis,” Journal of Consciousness \nterms of classical autonomy, comprehend the \nStudies, vol. 17, pp. 7-65, 2010.\nmoral or legal rules they follow. They move \naccording to their programming, following rules \n•  D. Davidson, “Representation and \nthat are designed by humans to be moral.\nInterpretation,” in Modelling the Mind, K. A. M. \nSaid, W. H. Newton-Smith, R. Viale, and K. V. \nExpand the discussion on ethics for A/IS to \nWilkes, Eds. New York: Oxford University Press, \ninclude an exploration of the classical foundations \n1990, pp. 13-26.\nof economy, outlined above, as potentially \ninfluencing current views and assumptions \n•  N. Noddings, Caring: A Relational Approach \naround machines achieving isolated autonomy.\nto Ethics and Moral Education. Oakland, CA: \nUniversity of California Press, 2013.\nFurther Resources\n•  J. Bielby, Ed., “Digital Global  •  O. Ulgen, “Kantian Ethics in the Age of Artificial \nCitizenship,” International Review of  Intelligence and Robotics,” QIL, vol. 43, pp. \nInformation Ethics, vol. 23, pp. 2-3, Nov. 2015. 59-83, Oct. 2017.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 40', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nClassical Ethics in A/IS\n•  O. Ulgen, “The Ethical Implications of  autonomous, especially in the case of genetic \nDeveloping and Using Artificial Intelligence and  algorithms and evolutionary strategies. However, \nRobotics in the Civilian and Military Spheres,”  attempts to implant true morality and emotions, \nHouse of Lords Select Committee, Sept. 6,  and thus accountability, i.e., autonomy, into  \n2017, UK. A/IS blurs the distinction between agents and \npatients and may encourage anthropomorphic \n•  O. Ulgen, “Human Dignity in an Age of \nexpectations of machines by human beings when \nAutonomous Weapons: Are We in Danger \ndesigning and interacting with A/IS.\nof Losing an ‘Elementary Consideration of \nHumanity’?” in How International Law Works  Thus, an adequate assessment of expectations \nin Times of Crisis, I. Ziemele and G. Ulrich,  and language used to describe the human-A/IS \nEds. Oxford: Oxford University Press, 2018. relationship becomes critical in the early stages \nof its development, where analyzing subtleties is \nnecessary. Definitions of autonomy need to be \nIssue: The Distinction between  clearly drawn, both in terms of A/IS and human \nAgents and Patients autonomy. On one hand, A/IS may in some cases \nmanifest seemingly ethical and moral decisions, \nresulting for all intents and purposes in efficient \nBackground and agreeable moral outcomes. Many human \ntraditions, on the other hand, can and have \nOf particular concern when understanding  \nmanifested as fundamentalism under the guise \nthe relationship between human beings and  \nof morality. Such is the case with many religious \nA/IS is the uncritically applied anthropomorphic \nmoral foundations, where established cultural \napproach toward A/IS that many industry and \nmores are neither questioned nor assessed. In \npolicymakers are using today. This approach \nsuch scenarios, one must consider whether there \nerroneously blurs the distinction between \nis any functional difference between the level of \nmoral agents and moral patients, i.e., subjects, \nautonomy in A/IS and that of assumed agency \notherwise understood as a distinction between \n—the ability to choose and act—in humans via \n“natural” self-organizing systems and artificial, \nthe blind adherence to religious, traditional, \nnon-self-organizing devices. As noted above,  \nor habitual mores. The relationship between \nA/IS cannot, by definition, become autonomous \nassumed moral customs, the ethical critique  \nin the sense that humans or living beings are \nof those customs, and the law are  \nautonomous. With that said, autonomy in \nimportant distinctions.\nmachines, when critically defined, designates \nhow machines act and operate independently \nThe above misunderstanding in definitions of \nin certain contexts through a consideration of \nautonomy arises in part because of the tendency \nimplemented order generated by laws and rules. \nfor humans to shape artificial creations in their \nIn this sense, A/IS can, by definition, qualify as \nown image, and our desire to lend our human \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 41', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nClassical Ethics in A/IS\nexperience to shaping a morphology of artificially  Further Resources\nintelligent systems. This is not to say that such \n•  R. Capurro, “Toward a Comparative Theory of \nterminology cannot be used metaphorically, but \nAgents.” AI & Society, vol. 27, no. 4, pp. 479-\nthe difference must be maintained, especially \n488, Nov. 2012.\nas A/IS begin to resemble human beings more \nclosely. It is possible for terms like “artificial  •  W. J. King and J. Ohya, “The Representation \nintelligence” or “morality of machines” to  of Agents: Anthropomorphism, Agency, and \nbe used as metaphors without resulting in  Intelligence,” in Conference Companion \nmisunderstanding. This is how language works  on Human Factors in Computing Systems. \nand how humans try to understand their natural  Vancouver: ACM, 1996, pp. 289-290.\nand artificial environment.\n•  W. Hofkirchner, “Does Computing Embrace \nHowever, the critical difference between human  Self-Organisation?” in Information and \nautonomy and autonomous systems involves  Computation: Essays on Scientific and \nquestions of free will, predetermination, and  Philosophical Understanding of Foundations \nbeing (ontology). The questions of critical  of Information and Computation, G. Dodig-\nontology currently being applied to machines  Crnkovic and M. Burgin, Eds. London: World \nare not new questions to ethical discourse and  Scientific, 2011, pp. 185-202.\nphilosophy; they have been thoroughly applied \n•  International Center for Information Ethics, \nto the nature of human being as well. John Stuart \n2018.\nMill, for example, is a determinist and claims that \nhuman actions are predicated on predetermined \n•  J. S. Mill, On Liberty. London: Longman, \nlaws. He does, however, argue for a reconciliation \nRoberts & Green, 1869.\nof human free will with determinism through a \ntheory of compatibility. Millian ethics provides  •  P. P. Verbeek, What Things Do: Philosophical \na detailed and informed foundation for defining  Reflections on Technology, Agency, and \nautonomy that could serve to help overcome  Design. University Park, PA: Pennsylvania State \ngeneral assumptions of anthropomorphism in   University Press, 2005.\nA/IS and thereby address the uncertainty  \ntherein (Mill, 1999).\nRecommendations\nWhen addressing the nature of “autonomy” \nin autonomous systems, it is recommended \nthat the discussion first consider free will, civil \nliberty, and society from a Millian perspective \nin order to better grasp definitions of autonomy \nand to address general assumptions of \nanthropomorphism in A/IS.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 42', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nClassical Ethics in A/IS\nIdentifying terms that will be intelligible to all \nrelevant audiences is pragmatic, but care should \nIssue: The Need for an \nbe taken not to dilute or misrepresent concepts \nAccessible, Classical Ethics \nthat are familiar to moral philosophy and ethics. \nVocabulary One way around this is to engage in applied \nethics; illustrate how a particular concept would \nBackground work in the A/IS context or scenario. Another \nway is to understand whether terminology used \nPhilosophers and ethicists are trained in \nacross different disciplines actually has the same \nvocabulary relating to philosophical concepts \nor similar meaning and effect which can be \nand terminology. There is an intrinsic value \nexpressed accordingly.\nplaced on these concepts when discussing ethics \nand A/IS, since the layered meaning behind  Recommendations\nthe terminology used is foundational to these \nSupport and encourage the efforts of groups \ndiscussions and is grounded in a subsequent \nraising awareness for social and ethics \nentrenchment of values. Unfortunately, using \ncommittees, whose roles are to support ethics \nphilosophical terminology in cross-disciplinary \ndialogue within their organizations, seeking \ninstances, i.e., a conversation between \napproaches that are both aspirational and values-\ntechnologists and policymakers, is often \nbased. A/IS technologists should engage in \nineffective since not everyone has the education \ncross-disciplinary exchanges whereby philosophy \nto be able to encompass the abstracted layers of \nscholars and ethicists attend and present in \nmeaning contained in philosophical terminology.\nnon-philosophical courses. This will both raise \nawareness and sensitize non-philosophical \nHowever, not understanding a philosophical \nscholars and practitioners to the vocabulary.\ndefinition does not detract from the necessity \nof its utility. While ethical and philosophical \nFurther Resources\ntheories should not be over-simplified for popular \n•  R. T. Ames, Confucian Role Ethics: A \nconsumption, being able to adequately translate \nVocabulary. Hong Kong: Chinese University \nthe essence of the rich history of ethics will go \nPress, 2011.\na long way in supporting a constructive dialogue \non ethics and A/IS. With access and accessibility \n•  R. Capurro, “Towards an Ontological \nconcerns intricately linked with education in \nFoundation of Information Ethics,” Ethics and \ncommunities, as well as secondary and tertiary \nInformation Technology, vol. 8, no. 4, pp. 175-\ninstitutions, society needs to take a vested \n186, 2006.\ninterest in creating awareness for government \nofficials, rural communities, and school teachers.  •  S. Mattingly-Jordan, R. Day, B. Donaldson, \nCreating a more “user-friendly” vocabulary raises  P. Gray, and L. M. Ingram, ""Ethically Aligned \nawareness on the necessity and application of  Design, First Edition Glossary,"" Prepared \nclassical ethics to digital societies. for The IEEE Global Initiative for Ethically \nAligned Design, Feb. 2019.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 43', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nClassical Ethics in A/IS\n•  B. M. Lowe, Emerging Moral Vocabularies: The  The key is to embed ethics into engineering in \nCreation and Establishment of New Forms  a way that does not make ethics a servant, but \nof Moral and Ethical Meanings. Lanham, MD:  instead a partner in the process. In addition \nLexington Books, 2006. to an ethics-in-practice approach, providing \nstudents and engineers with the tools necessary \n•  D. J. Flinders, “In Search of Ethical \nto build a similar orientation into their inventions \nGuidance: Constructing a Basis for \nfurther entrenches ethical design practices. In \nDialogue,” International Journal of Qualitative \nthe abstract, this is not so difficult to describe, \nStudies in Education, vol. 5, no. 2, pp. 101-115, \nbut is very difficult to encode into systems. This \n1992.\nproblem can be addressed by providing students \nwith job aids such as checklists, flowcharts, \n•  G. S. Saldanha, “The Demon in the Gap of \nand matrices that will help them select and \nLanguage: Capurro, Ethics and Language in \nuse a principal ethical framework, and then \nDivided Germany,” in Information Cultures in \nexercise use of those devices with steadily more \nthe Digital Age. Wiesbaden, Germany: Springer \ncomplex examples. In such an iterative process, \nFachmedien, 2016, pp. 253-268.\nstudents will start to determine for themselves \n•  J. Van Den Hoven and G. J. Lokhorst, ""Deontic  what examples do not allow for perfectly clear \nLogic and Computer‐Supported Computer  decisions, and, in fact, require some interaction \nEthics,"" Metaphilosophy, vol. 33, no. 3, pp.  between frameworks. Produced outcomes such \n376-386, April 2002. as videos, essays, and other formats–such as \nproject-based learning activities–allow for  \na didactic strategy which proves effective in \nIssue: Presenting Ethics to the  artificial intelligence ethics education.\nCreators of Autonomous and \nThe goal is to provide students a means to \nIntelligent Systems\nuse ethics in a manner analogous to how they \nare being taught to use engineering principles \nBackground and tools. In other words, the goal is to help \nengineers tell the story of what they are doing.\nThe question arises as to whether or not classical \nethics theories can be used to produce meta- •  Ethicists should use information flows and \nlevel orientations to data collection and data use  consider at a meta-level what information \nin decision-making. Keeping in mind that the  flows do and what they are supposed to do.\ntask of philosophical ethics should be to examine \n•  Engineers should then build a narrative \ngood and evil, ethics should examine values, not \nthat outlines the iterative process of ethical \nprescribe them. Laws, which arise from ethics, \nconsiderations in their design. Intentions are \nare entrenched mores that have been critically \npart of the narrative and provide a base to \nassessed to prescribe.  \nreflect back on those intentions.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 44', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nClassical Ethics in A/IS\n•  The process then allows engineers to better  •  Short (information) ethics courses specifically \nunderstand their assumptions and adjust their  designed for the science program that can \nintentions and design processes accordingly.  be attended by the current students, alumni, \nThey can only get to these by asking targeted  or professionals. These will function as either \nquestions. introductory, refresher, or specialized courses. \nThis process, one with which engineers are   Courses can also be blended to include students \nquite familiar, is basically Kantian and Millian  and/or practitioners from diverse backgrounds \nethics in play. rather than the more traditional practice of \nhomogenous groups, such as engineering \nThe aim is to produce what is referred to in the \nstudents, continuing education programs directed \ncomputer programming lexicon as a macro. \nat a specific specialization, and the like. \nA macro is code that takes other code as its \ninput(s) and produces unique outputs. This  Recommendations\nmacro is built using the Western ethics tradition \nFind ways to present ethics where the \nof virtue ethics.\nmethodologies used are familiar to engineering \nstudents. As engineering is taught as a collection \nThis further underscores the importance of \nof techno-science, logic, and mathematics, \neducation and training on ethical considerations \nembedding ethical sensitivity into these objective \nrelating to A/IS. Such courses should be \nand non-objective processes is essential. \ndeveloped and presented to students of \nCurricula development is crucial in each \nengineering, A/IS, computer science, and \napproach. In addition to research articles and best \nother relevant fields. These courses do not add \npractices, it is recommended that engineers and \nvalue a posteriori, but should be embedded \npractitioners come together with social scientists \nfrom the beginning to allow for absorption of \nand philosophers to develop case studies, \nthe underlying ethical considerations as well as \ninteractive virtual reality gaming, and additional \nallowing for critical thinking to come to fruition \ncourse interventions that are relevant to students.\nonce the students graduate. There are various \napproaches that can be considered on a   Further Resources\ntertiary level:\n•  T. W. Bynum and S. Rogerson, Computer \n•  Parallel (information) ethics program that is  Ethics and Professional Responsibility. Malden, \npresented together with the science program  MA: Wiley-Blackwell, 2003.\nduring the course of undergraduate and \n•  E. G. Seebauer and R. L. Barry, Fundamentals \npostgraduate study;\nof Ethics for Scientists and Engineers. New \n•  Embedded (information) ethics modules   York: Oxford University Press, 2001. \nwithin the science program, i.e., one module \nper semester;\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 45', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nClassical Ethics in A/IS\n•  C. Whitbeck, “Teaching Ethics to  opinion-gathering exercises to embed ethical \nScientists and Engineers: Moral Agents and  considerations into structures, environments, \nMoral Problems,” Science and Engineering  training, and development. \nEthics, vol. 1, no. 3, pp. 299-308, Sept. 1995.\nAs it stands, classical ethics is not accessible \n•  B. Zevenbergen, et al. “Philosophy  enough to corporate endeavors in ethics, and \nMeets Internet Engineering: Ethics in  as such, are not applicable to tech projects. \nNetworked Systems Research,” GTC Workshop  There is often, but not always, a big discrepancy \nOutcomes Paper. Oxford: Oxford Internet  between the output of engineers, lawyers, \nInstitute, University of Oxford, 2015. and philosophers when dealing with computer \nscience issues; there is also a large difference in \n•  M. Alvarez, “Teaching Information \nhow various disciplines approach these issues. \nEthics,” International Review of Information \nWhile this is not true in all cases—and there \nEthics, vol. 14, pp. 23-28, Dec. 2010.\nare now several interdisciplinary approaches in \nrobotics and machine ethics as well as a growing \n•  P. P. Verbeek, Moralizing Technology: \nnumber of scientists that hold double and \nUnderstanding and Designing the Morality of \ninterdisciplinary degrees—there remains a vacuum \nThings. Chicago, IL: University of Chicago \nfor the wider understanding of classical ethics \nPress, 2011.\ntheories in the interdisciplinary setting. Such an \n•  K. A. Joyce, K. Darfler, D. George, J. Ludwig,  understanding includes that of the philosophical \nand K. Unsworth, “Engaging STEM Ethics  language used in ethics and the translation  \nEducation,” Engaging Science, Technology, and  of that language across disciplines. \nSociety, vol. 4, no. 1-7, 2018.\nIf we take, for instance, the terminology and \nusage of the concept of “trust” in reference \nto technology, the term “trust” has specific \nIssue: Accessing Classical Ethics \nphilosophical, legal, and engineering connotations. \nby Corporations and Companies\nIt is not an abstract concept. It is attributable \nto humans, and relates to claims and actions \nBackground people make. Machines, robots, and algorithms \nlack the ability to make claims and so cannot \nMany companies, from startups to tech giants, \nbe attributed with trust. They cannot determine \nunderstand that ethical considerations in tech \nwhether something is trustworthy or not. Software \ndesign are increasingly important, but are not \nengineers may refer to “trusting” the data, but \nsure how to incorporate ethics into their tech \nthis relates to the data’s authenticity and veracity \ndesign agenda. How can ethical considerations \nto ensure software performance. In the context \nin tech design become an integrated part of \nof A/IS, “trust” means “functional reliability”; it \nthe agenda of companies, public projects, \nmeans there is confidence in the technology’s \nand research consortia? Corporate workshops \npredictability, reliability, and security against \nand exercises will need to go beyond \nhackers or impersonators of authentic users.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 46', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nClassical Ethics in A/IS\nRecommendations •  IEEE P7000™, IEEE Standards Project for \nModel Process for Addressing Ethical Concerns \nIn order to achieve multicultural, multidisciplinary, \nDuring System Design will provide engineers \nand multi-sectoral dialogues between \nand technologists with an implementable \ntechnologists, philosophers, and policymakers, \nprocess aligning innovation management \na nuanced understanding in philosophical \nprocesses, IT system design approaches, and \nand technical language, which is critical \nsoftware engineering methods to minimize \nto digital society from Internet of Things \nethical risk for their organizations, stakeholders \n(IoT), privacy, and cybersecurity to issues of \nand end users.\nInternet governance, must be translated into \nnorms and made available to technicians and \npolicymakers who may not understand the \nIssue: The Impact of Automated \nnuances of the terminology in philosophical, \nSystems on the Workplace\nlegal, and engineering contexts. It is therefore \nrecommended that the translation of the \ncritical-thinking terminology of philosophers, \nBackground\npolicymakers, and other stakeholders on A/IS be \ntranslated into norms accessible to technicians. The impact of A/IS on the workplace and the \nchanging power relationships between workers \nFurther Resources\nand employers requires ethical guidance. Issues \n•  A. Bhimani, “Making Corporate  of data protection and privacy via big data \nGovernance Count: The Fusion of Ethics and  in combination with the use of autonomous \nEconomic Rationality,” Journal of Management  systems by employers are increasing, where \n& Governance, vol. 12, no. 2, pp. 135-147,  decisions made via aggregate algorithms directly \nJune 2008. impact employment prospects. The uncritical \nuse of A/IS in the workplace, and its impact \n•  A. B. Carroll, “A History of Corporate Social \non employee-employer relations, is of utmost \nResponsibility,” in The Oxford Handbook \nconcern due to the high chance of error and \nof Corporate Social Responsibility, A. \nbiased outcome.\nChrisanthi, R. Mansell, D. Quah, and R. \nSilverstone, Eds. Oxford, U.K.: Oxford  The concept of responsible research \nUniversity Press, 2008. and innovation (RRI)is a growing area, particularly \nwithin the EU. It offers potential solutions to \n•  W. Lazonick, “Globalization of the ICT \nworkplace bias and is being adopted by several \nLabor Force,” in The Oxford Handbook \nresearch funders, such as the Engineering and \nof Information and Communication \nPhysical Sciences Research Council (EPSRC), \nTechnologies, A. Chrisanthi, R. Mansell, D. \nwho include RRI core principles in their mission \nQuah, and R. Silverstone, Eds. Oxford, U.K.: \nstatement. RRI is an umbrella concept that draws \nOxford University Press, 2006. \non classical ethics theory to provide tools to \naddress ethical concerns from the outset of a \nproject, from the design stage onwards.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 47', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nClassical Ethics in A/IS\nQuoting Rene Von Schomberg, science and  and embrace this concept as both practical to \ntechnologies studies specialist and philosopher,  implement and enact.\n“Responsible Research and Innovation is a \nRecommendations\ntransparent, interactive process by which \nsocietal actors and innovators become mutually  It is recommended, through the application of \nresponsive to each other with a view to the  RRI as founded in classical ethics theory, that \n(ethical) acceptability, sustainability and  research in A/IS design utilize available tools \nsocietal desirability of the innovation  and approaches to better understand the design \nprocess and its marketable products   process, addressing ethical concerns from the \n(in order to allow a proper embedding   very beginning of the design stage of the project, \nof scientific and technological advances   thus maintaining a stronger, more efficient \nin our society).”2 methodological accountability throughout.\nWhen RRI methodologies are used in the ethical  Further Resources\nconsiderations of A/IS design, especially in \n•  M. Burget, E. Bardone, and M. Pedaste, \nresponse to the potential bias of A/IS in the \n“Definitions and Conceptual Dimensions \nworkplace, theoretical deficiencies are then often \nof Responsible Research and Innovation: A \nexposed that would not otherwise have been \nLiterature Review,” Science and Engineering \nexposed, allowing room for improvement in \nEthics, vol. 23, no. 1, pp. 1-9, 2016.\ndesign at the development stage rather than from \na retroactive perspective. RRI in design increases  •  European Commission Communication, \nthe chances of both relevance and strength in  “Artificial Intelligence for Europe,” COM 237, \nethically aligned design. April, 2018.\nThis emerging and exciting new concept aims to  •  R. Von Schomberg, “Prospects for Technology \nalso push the boundaries to incorporate relevant  Assessment in a Framework of Responsible \nstakeholders whose influence in responsible  Research and Innovation,” in Technikfolgen \nresearch is on a global stage. While this concept  Abschätzen Lehren: Bildungspotenziale \ninitially focuses on the workplace setting,  Transdisziplinärer Methode. Wiesbaden, \nsuccess will only be achieved through the active  Germany: Springer VS, 2011, pp. 39-61.\ninvolvement from private companies of industry, \n•  B. C. Stahl, G. Eden, M. Jirotka, M. \nAI Institutes, and those who are at the forefront in \nCoeckelbergh, “From Computer Ethics to \nA/IS design. Responsible research and innovation \nResponsible Research and Innovation \nwill be achieved through careful research and \nin ICT: The Transition of Reference \ninnovation governance that will ensure research \nDiscourses Informing Ethics-Related Research \npurpose, process, and outcomes that are \nin Information Systems,” Information & \nacceptable, sustainable, and even desirable. It will \nManagement, vol. 51, no. 6, pp. 810-818, \nbe incumbent on RRI experts to engage at a level \nSeptember 2014. \nwhere private companies will feel empowered \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 48', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nClassical Ethics in A/IS\n•  B. C. Stahl, M. Obach, E. Yaghmaei, V. Ikonen,  designed to provide organizations with  \nK. Chatfield, and A. Brem, “The Responsible  a set of clear guidelines and certifications \nResearch and Innovation (RRI) Maturity Model:  guaranteeing they are storing, protecting,  \nLinking Theory and Practice,” Sustainability,  and utilizing employee data in an ethical  \nvol. 9, no. 6, June 2017.  and transparent way. \n•  IEEE P7005™, Standards Project for \nTransparent Employer Data Governance is \nSection 2—Classical Ethics  \nfrom Globally Diverse Traditions\nHowever, this venture poses problematic \nassumptions even before the issue above can be \nIssue: The Monopoly on Ethics \nexplored. In classifying Western values, we group \nby Western Ethical Traditions\ntogether thousands of years of independent \nand disparate ideas originating from the Greco-\nBackground Roman philosophical tradition with their Christian-\ninfused cultural heritage and then the break from \nAs human creators, our most fundamental \nthat heritage with the Enlightenment. What is it \nvalues are imposed on the systems we design. \nthat one refers to by the term “Western ethics”? \nIt becomes incumbent on the global community \nDoes one refer to philosophical ethics (ethics \nto recognize which sets of values guide the \nas a scientific discipline) or is the reference to \ndesign, and whether or not A/IS will generate \nWestern morality?\nproblematic, i.e., discriminatory, consequences \nwithout consideration of non-Western values.  The “West”, however it may be defined, is an \nThere is an urgent need to broaden traditional  individualistic society, arguably more so than \nethics in its contemporary form of “responsible  much of the rest of the world, and thus, in \ninnovation” (RI) beyond the scope of “Western”  some aspects, should be even less collectively \nethical foundations, such as utilitarianism,  defined than “Eastern” ethical traditions. Suggest \ndeontology, and virtue ethics. There is also  instead: If one is referring to Western values, \na need to include other traditions of ethics  one must designate which values and to whom \nin RI, such as those inherent to Buddhism,  they belong. Additionally, there is a danger in the \nConfucianism, and Ubuntu traditions. field of intercultural information ethics, however \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 49', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nClassical Ethics in A/IS\nunconsciously or instinctively propagated, to not  the design and programming of ethics in A/IS \nonly group together all Western traditions under   amidst differing ethical traditions.\na single banner, but to negatively designate any \nWhile experts in Intercultural Information  \nand all Western influence in global exchange to \nEthics, such as Pak-Hang Wong, highlight the \nrepresenting an abusive collective of colonial-\ndangers of the dominance of “Western” ethics in  \ninfluenced ideals. Just because there exists \nA/IS design, noting specifically the appropriation \na monopoly of influence by one system over \nof ethics by liberal democratic values to  \nanother does not mean that said monopoly is \nthe exclusion of other value systems, it should \ndevoid of value, even for systems outside itself. \nbe noted that those same liberal democratic \nIn the same way that culturally diverse traditions \nvalues are put in place and specifically designed \nhave much to offer Western tradition(s), so, too, \nto accommodate such differences. However, \ndo they have much to gain from them.\nwhile the accommodation of differences are, in \nIn order to establish mutually beneficial  theory, accounted for in dominant liberal value \nconnections in addressing globally diverse  systems, the reality of the situation reveals a \ntraditions, it is of critical importance to first  monopoly of, and a bias toward, established \nproperly distinguish between subtleties in  Western ethical value systems, especially when it \nWestern ethics as a discipline and morality as its  comes to standardization. As Wong notes:\nobject or subject matter. It is also important to \nStandardization is an inherently value-laden \ndifferentiate between philosophical or scientific \nproject, as it designates the normative criteria for \nethics and theological ethics. As noted above, \ninclusion to the global network. Here, one of the \nthe relationship between assumed moral \nmajor adverse implications of the introduction of \ncustoms, the ethical critique of those customs, \nvalue-laden standard(s) of responsible innovation \nand the law is an established methodology in \n(RI) appears to be the delegitimization of the \nscientific communities. Western and Eastern \nplausibility of RI based on local values, especially \nphilosophy are very different, just like Western \nwhen those values come into conflict with the \nand Eastern ethics. Western philosophical ethics \nliberal democratic values, as the local values \nuse scientific methods such as the logical, \n(or, the RI based on local values) do not enable \ndiscursive, and dialectical approach (models of \nscientists and technology developers to be \nnormative ethics) alongside the analytical and \nrecognized as members of the global network  \nhermeneutical approaches. The Western tradition \nof research and innovation (Wong, 2016).\nis not about education and teaching of social and \nmoral values, but rather about the application \nIt does, however, become necessary for those \nof fundamentals, frameworks, and explanations. \nwho do not work within the parameters of \nHowever, several contemporary globally relevant \naccepted value monopolies to find alternative \ncommunity mores are based in traditional \nmethods of accommodating different value \nand theological moral systems, requiring a \nsystems. Liberal values arose out of conflicts \nconversation around how best to collaborate in \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 50', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nClassical Ethics in A/IS\nof cultural and subcultural differences and are  Recommendations\ndesigned to be accommodating enough to \nIn order to enable a cross-cultural dialogue of \ninclude a rather wide range of differences.\nethics in technology, discussions on ethics and  \nRI enables policymakers, scientists, technology  A/IS must first return to normative foundations \ndevelopers, and the public to better understand  of RI to address the notion of “responsible \nand respond to the social, ethical, and policy  innovation” from a range of value systems not \nchallenges raised by new and emerging  predominant in Western classical ethics. Together \ntechnologies. Given the historical context from  with acknowledging differences, a special \nwhich RI emerges, it should not be surprising  focus on commonalities in the intercultural \nthat the current discourse on RI is predominantly  understanding of the concept of “relationship” \nbased on liberal democratic values. Yet, the bias  must complement the process.\ntoward liberal democratic values will inevitably \nFurther Resources\nlimit the discussion of RI, especially in the cases \n•  J. Bielby, “Comparative Philosophies in \nwhere liberal democratic values are not taken for \nIntercultural Information Ethics,” Confluence: \ngranted. Against this background, it is important \nJournal of World Philosophies, vol. 2, 2016.\nto recognize the problematic consequences of \nRI solely grounded on, or justified by, liberal \n•  W. B. Carlin and K. C. Strong, ""A Critique of \ndemocratic values.\nWestern Philosophical Ethics: Multidisciplinary \nAlternatives for Framing Ethical Dilemmas,"" \nIn addition, many non-Western ethics traditions, \nJournal of Business Ethics, vol. 14, no. 5, pp. \nincluding the Buddhist and Ubuntu traditions \n387-396, May 1995.\nhighlighted below, view “relationship” as a \nfoundationally important concept to ethical \n•  C. Ess, “Lost in translation”?: Intercultural \ndiscourse. One of the key parameters \ndialogues on privacy and information ethics \nof intercultural information ethics and RI \n(introduction to special issue on privacy and \nresearch must be to identify main commonalities \ndata privacy protection in Asia),"" Ethics and \nof “relationship” approaches from different \nInformation Technology, vol. 7, no. 1, pp. 1-6, \ncultures and how to operationalize them for  \nMarch 2005.\nA/IS to complement classical methodologies of \ndeontological and teleological ethics. Different  •  S. Hongladarom, “Intercultural \ncultural perceptions of time may influence  Information Ethics: A Pragmatic \n“relationship” approaches and impact how   Consideration,” in Information Cultures in the \nA/IS are perceived and integrated, e.g.,  Digital Age. Wiesbaden, Germany: Springer \ntechnology as part of linear progress in the West;  Fachmedien, 2016, pp. 191-206. \ninter-generational needs and principles of respect \n•  L. G. Rodríguez and M. Á. P. Álvarez, \nand benevolence in Chinese culture determining \nÉtica Multicultural y Sociedad en Red. Madrid: \ncurrent and future use of technology. \nFundación Telefónica, 2014. \n \n \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 51', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nClassical Ethics in A/IS\n•  P. H. Wong, “What Should We  in the future, achieving what in Buddhism is \nShare?: Understanding the Aim of  understood as “supreme happiness”. Thus \nIntercultural Information Ethics,” ACM SIGCAS  Buddhist ethics are clearly goal-oriented. In \nComputers and Society, vol. 39, no. 3 pp.   the Buddhist tradition, people attain liberation \n50-58, Dec. 2009. when they no longer endure any unsatisfactory \nconditions, when they have attained the state \n•  S. A. Wilson, “Conformity, Individuality, and \nwhere they are completely free from any \nthe Nature of Virtue: A Classical Confucian \npassions, including desire, anger, and delusion— \nContribution to Contemporary Ethical \nto name the traditional three, that ensnare one’s \nReflection,” The Journal of Religious Ethics,  \nself against freedom. In order to attain liberation, \nvol. 23, no. 2, pp. 263-289, 1995.\none engages oneself in mindful behavior (ethics), \nconcentration (meditation), and what is deemed \n•  P. H. Wong, “Responsible Innovation \nin Buddhism as “wisdom”, a term that remains \nfor Decent Nonliberal Peoples: A \nambiguous in Western scientific approaches  \nDilemma?” Journal of Responsible Innovation, \nto ethics. \nvol. 3, no. 2, pp. 154-168, July 2016.\nThus ethics in Buddhism are concerned \n•  R. B. Zeuschner, Classical Ethics, East \nexclusively with how to attain the goal \nand West: Ethics from a Comparative \nof liberation, or freedom. In contrast to Western \nPerspective. Boston, MA: McGraw-Hill, 2000.\nethics, Buddhist ethics are not concerned with \n•  S. Mattingly-Jordan, “Becoming a Leader in  theoretical questions on the source of normativity \nGlobal Ethics,” IEEE, 2017. or what constitutes the good life. What makes \nan action a “good” action in Buddhism is always \nconcerned with whether the action leads, \neventually, to liberation or not. In Buddhism, there \nis no questioning why liberation is a good thing. \nIssue: The Application of \nIt is simply assumed. Such an assumption places \nClassical Buddhist Ethical \nBuddhism, and ethical reflection from a Buddhist \nTraditions to A/IS Design perspective, in the camp of mores rather than \nscientifically led ethical discourse, and it is \napproached as an ideology or a worldview.\nBackground\nAccording to Buddhism, the field of ethics is  While it is critically important to consider, \nconcerned with behaving in such a way that  understand, and apply accepted ideologies \nthe subject ultimately realizes the goal of  such as Buddhism in A/IS, it is both necessary \nliberation. The question, “How should I act?” is  to differentiate the methodology from Western \nanswered straightforwardly; one should act in  ethics, and respectful to Buddhist tradition, not \nsuch a way that one realizes liberation (nirvana)  to require that it be considered in a scientific \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 52', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nClassical Ethics in A/IS\ncontext. Such assumptions put it at odds with  self actually consists of nothing more than these \nthe Western foundation of ethical reflection on  connecting episodes and parts. To exemplify the \nmores. From a Buddhist perspective, one does  above, one can draw from the concept of privacy \nnot ask why supreme happiness is a good thing;  as often explored via intercultural information \none simply accepts it. The relevant question in  ethics. The Buddhist perspective understands \nBuddhism is not about methodological reflection,  privacy as a protection, not of self-subsisting \nbut about how to attain liberation from the  individuals, because such do not exist ultimately \nnecessity for such reflection. speaking, but of certain values that are found to \nbe necessary for a well-functioning society to \nThus, Buddhist ethics contain potential for conflict \nprosper in the globalized world.\nwith Western ethical value systems which are \nfounded on ideas of questioning moral and  The secular formulation of the supreme \nepistemological assumptions. Buddhist ethics are  happiness mentioned above is that of the \ndifferent from, for example, utilitarianism, which  reduction of the experience of suffering, or \noperates via critical analysis toward providing the  reduction of the metacognitive state of suffering. \nbest possible situation to the largest number of  Such a state is the result of lifelong discipline \npeople, especially as it pertains to the good life.  and meditation aimed at achieving proper \nThese fundamental differences between the  relationships with others and with the world. This \ntraditions need to be, first and foremost, mutually  notion of the reduction of suffering is something \nunderstood and then addressed in one form   that can resonate well with certain Western \nor another when designing A/IS that span   traditions, such as epicureanism ataraxia, i.e., \ncultural contexts. freedom from fear through reason and discipline, \nand versions of consequentialist ethics that \nThe main difference between Buddhist and \nare more focused on the reduction of harm. It \nWestern ethics is that Buddhism is based upon \nalso encompasses the concept of phronesis or \na metaphysics of relation. Buddhist ethics \npractical wisdom from virtue ethics.\nemphasizes how action leads to achieving \na goal, or in the case of Buddhism, the final  Relational ethical boundaries promote ethical \ngoal. In other words, an action is considered a  guidance that focuses on creativity and growth \ngood one when it contributes to the realization  rather than solely on mitigation of consequence \nof the goal. It is relational when the value of an  and avoidance of error. If the goal of the \naction is relative to whether or not it leads to the  reduction of suffering can be formulated in \ngoal, the goal being the reduction and eventual  a way that is not absolute, but collaboratively \ncessation of suffering. In Buddhism, the self is  defined, this leaves room for many philosophies \nconstituted through the relationship between the  and related approaches as to how this goal can \nsynergy of bodily parts and mental activities. In  be accomplished. Intentionally making space \nBuddhist analysis, the self does not actually exist  for ethical pluralism is one potential antidote to \nas a self-subsisting entity. Liberation, or nirvana,  dominance of the conversation by liberal thought, \nconsists in realizing that what is known to be the  with its legacy of Western colonialism.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 53', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nClassical Ethics in A/IS\nRecommendations Cultural Attitudes towards Communication and \nTechnology, pp. 300-314, 2010.\nIn considering the nature of interactions \nbetween human and autonomous systems, the  •  M. Mori, The Buddha in the Robot. Suginami-\nabove notion of “proper relationships” through  ku, Japan: Kosei Publishing, 1989.\nBuddhist ethics can provide a useful platform \nthat results in ethical statements formulated in \na relational way, instead of an absolutist way. It  Issue: The Application of Ubuntu \nis recommended as an additional methodology, \nEthical Traditions to A/IS Design\nalong with Western-value methodologies, to \naddress human/computer interactions.\nBackground\nFurther Resources\nIn his article, “African Ethics and Journalism \n•  R. Capurro, “Intercultural Information Ethics: \nEthics: News and Opinion in Light of Ubuntu”, \nFoundations and Applications,” Journal of \nThaddeus Metz frames the following question: \nInformation, Communication & Ethics in \n“What does a sub-Saharan ethic focused on the \nSociety, vol. 6, no. 2, pp. 116-126, 2008.\ngood of community, interpreted philosophically \nas a moral theory, entail for the duties of various \n•  C. Ess, “Ethical Pluralism and Global \nagents with respect to the news/opinion media?” \nInformation Ethics,” Ethics and Information \n(Metz, 2015, 1). In applying that question to  \nTechnology, vol. 8, no. 4, pp. 215-226, Nov. \nA/IS, it reads: “If an ethic focused on the good \n2006.\nof community, interpreted philosophically as a \n•  S. Hongladarom, “Intercultural Information  moral theory, is applied to A/IS, what would the \nEthics: A Pragmatic Consideration,”  implications be on the duties of various agents?” \nin Information Cultures in the Digital Age, K.  Agents, in this regard, would therefore be  \nM. Bielby, Ed. Wiesbaden, Germany: Springer  the following:\nFachmedien Wiesbaden, 2016, pp. 191-206.\n•  Members of the A/IS research community\n•  S. Hongladarom, J. Britz, \n•  A/IS programmers/computer scientists\n“Intercultural Information Ethics,” International \nReview of Information Ethics, vol. 13, pp. 2-5, \n•  A/IS end-users\nOct. 2010.\n•  A/IS themselves\n•  M. Nakada, “Different Discussions \non Roboethics and Information Ethics   \nBased on Different Contexts (Ba).   \nDiscussions on Robots, Informatics and Life   \nin the Information Era in Japanese Bulletin   \nBoard Forums and Mass Media,” Proceedings \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 54', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nClassical Ethics in A/IS\nUbuntu is a sub-Saharan philosophical tradition.  must identify factors and devices that will form \nIts basic tenet is that a person is a person  part of their lifeworld. If so, will the presence \nthrough other persons. It develops further in the  of A/IS inhibit the process of partaking in a \nnotions of caring and sharing as well as identity  community, or does it create more opportunities \nand belonging, whereby people experience  for doing so? One cannot classify A/IS as only a \ntheir lives as bound up with their community. A  negative or disruptive force; it is here to stay and \nperson is defined in relation to the community  its presence will only increase. Ubuntu ethics \nsince the sense of being is intricately linked with  must come to grips with, and contribute to, the \nbelonging. Therefore, community exists through  body of knowledge by establishing a platform for \nshared experiences and values. It is a commonly  mutual discussion and understanding. Ubuntu, \nheld maxim in the Ubuntu tradition that, “to be  as collective human dignity, may offer a way of \nis to belong to a community and participate.”  understanding the impact of A/IS on humankind, \nAs the saying goes, motho ke motho ka batho  e.g., the need for human moral and legal agency; \nbabang, or, “a person is a person because of  human life and death decisions to be taken by \nother people.” humans rather than A/IS.\nVery little research, if any at all, has been  Such analysis fleshes out the following suggestive \nconducted in light of Ubuntu ethics and A/IS,  comments of Desmond Tutu, renowned former \nbut its focus will be within the following moral  chair of South Africa’s Truth and Reconciliation \ndomains: Commission, when he says of Africans, “(We \nsay) a person is a person through other people... \n1.  Among the members of the A/IS research \nI am human because I belong” (Tutu, 1999). \ncommunity\nAs Tutu notes, “Harmony, friendliness, and \ncommunity are great goods. Social harmony is \n2. Between the A/IS community/programmers/\nfor us the summum bonum—the greatest good. \ncomputer scientists and the end-users\nAnything that subverts or undermines this sought-\n3. Between the A/IS community/programmers/ after good is to be avoided” (2015:78).\ncomputer scientists and A/IS\nIn considering the above, it is fair to state that \n4. Between the end-users and A/IS community remains central to Ubuntu. In \nsituating A/IS within this moral domain, they will \n5. Between A/IS and A/IS\nhave to adhere to the principles of community, \nConsidering a future where A/IS will become  identity, and solidarity with others. On the \nmore entrenched in our everyday lives, one must  other hand, they will also need to be cognizant \nkeep in mind that an attitude of sharing one’s  of, and sensitive toward, the potential for \nexperiences with others and caring for their  community-based ethics to exclude individuals \nwell-being will be impacted. Also, by trying to  on the basis that they do not belong or fail to \nensure solidarity within one’s community, one  meet communitarian standards. For example, \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 55', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nClassical Ethics in A/IS\nwould this mean the excluded individual lacks  designers and programmers must work closely \npersonhood and as a consequence would not   with the end-users and target communities to \nbe able to benefit from community-based   ensure their design objectives, products, and \nA/IS initiatives? How would community-based  services are aligned with the needs of the end-\nA/IS programming avoid such biases against  users and target communities.\nindividuals?\nFurther Resources\nWhile virtue ethics question the goal or purpose \n•  D. W. Lutz, “African Ubuntu Philosophy \nof A/IS and deontological ethics question the \nand Global Management,” Journal of Business \nduties, the fundamental question asked by \nEthics, vol. 84, pp. 313-328, Oct. 2009.\nUbuntu would be, “How does A/IS affect the \ncommunity in which it is situated?” This question  •  T. Metz, “African Ethics and Journalism Ethics: \nlinks with the initial question concerning the  News and Opinion in Light of Ubuntu,” Journal \nduties of the various moral agents within the  of Media Ethics: Exploring Questions of Media \nspecific community. Motivation becomes very  Morality, vol. 30 no. 2, pp. 74-90, April 2015. \nimportant, because if A/IS seek to detract from \n•  T. Metz, ""Ubuntu as a moral theory and \ncommunity, they will be detrimental to the \nhuman rights in South Africa,"" African Human \nidentity of this community when it comes to job \nRights Law Journal, vol. 11, no. 2, pp. 532-559, \nlosses, poverty, lacks in education, and lacks \n2011.\nin skills training. However, should A/IS seek to \nsupplement the community by means of ease \n•  R. Nicolson, Persons in Community: African \nof access, support systems, and more, then it \nEthics in a Global Culture. Scottsville, South \ncannot be argued that they will be detrimental.  \nAfrica: University of KwaZulu-Natal Press, \nIn between these two motivators is a \n2008.\nsafeguarding issue about how to avoid excluding \nindividuals from accessing community-based   •  A. Shutte, Ubuntu: An Ethic for a New South \nA/IS initiatives. It therefore becomes imperative  Africa. Dorpspruit, South Africa: Cluster \nthat whoever designs the systems must work  Publications, 2001.\nclosely both with ethicists and the target \n•  D. Tutu, No Future without Forgiveness. \ncommunity, audience, or end-user to ascertain \nLondon: Rider, 1999.\nwhether their needs are identified and met.\n•  O. Ulgen, “Human Dignity in an Age of \nRecommendations\nAutonomous Weapons: Are We in Danger \nIt is recommended that a concerted effort  of Losing an ‘Elementary Consideration of \nbe made toward the study and publication of  Humanity’?” in How International Law Works \nliterature addressing potential relationships  in Times of Crisis, I. Ziemele and G. Ulrich, \nbetween Ubuntu and other instances of African  Eds. Oxford: Oxford University Press, 2018,  \nethical traditions and A/IS value design. A/IS  pp. 242-272.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 56', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nClassical Ethics in A/IS\nfor intelligent machines and robots, particularly \nfor humanoid ones,” (Tzafestas, 2015, 155) colors \nIssue: The Application of  \nand influences technological development in \nShinto-Influenced Traditions  \nJapan, especially robot culture.\nto A/IS Design\nThe word “Shinto” can be traced to two Japanese \nconcepts: Shin, meaning spirit, and to, the \nBackground\nphilosophical path. Along with the modern \nAlongside the burgeoning African Ubuntu  concept of the android, which can be traced \nreflections on A/IS, other indigenous techno- back to three sources—the first, to its Greek \nethical reflections boast an extensive  etymology that combines andras (“άνδρας”), \nengagement. One such tradition is Japanese  or man, and gynoids/gyni (“γυνή”), or woman; \nShinto indigenous spirituality, or, Kami no michi,  the second, via automatons and toys as per U.S. \noften cited as the catalyst for Japanese robot  patent developers in the 1800s; and the third to \nand autonomous systems culture, a culture that  Japan, where both historical and technological \nnaturally stems from the traditional Japanese  foundations for android development have \nconcept of karakuri ningyo (automata). Popular  dominated the market since the 1970s—Japanese \nJapanese artificial intelligence, robot, and video- Shinto-influenced technology culture is perhaps \ngaming culture can be directly connected to  the most authentic representation of the human-\nindigenous Shinto tradition, from the existence  automaton interface. \nof kami (spirits) to puppets and automata.\nShinto tradition is an animistic religious tradition, \nThe relationship between A/IS and a human  positing that everything is created with, and \nbeing is a personal relationship in Japanese  maintains, its own spirit (kami) and is animated \nculture and, one could argue, a very natural  by that spirit—an idea that goes a long way to \none. The phenomenon of “relationship” in Japan  defining autonomy in robots from a Japanese \nbetween humans and automata stands out as  viewpoint. This includes, on one hand, everything \nunique to technological relationships in world  that Western culture might deem natural, \ncultures, since the Shinto tradition is arguably  including rivers, trees, and rocks, and on the \nthe only animistic and naturalistic tradition that  other hand, everything artificially (read: artfully) \ncan be directly connected to contemporary  created, including vehicles, homes, and automata \ndigital culture and A/IS. From the Shinto  (robots). Artifacts are as much a part of nature \nperspective, the existence of A/IS, whether  in Shinto as animals, and they are considered \nmanifested through robots or other technological  naturally beautiful rather than falsely artificial.\nautonomous systems, is as natural to the world \nA potential conflict between Western and \nas rivers, forests, and thunderstorms. As noted \nJapanese concepts of nature and artifact \nby Spyros G. Tzafestas, author of Roboethics: A \narises when the two traditions are compared \nNavigating Overview, “Japan’s harmonious feeling \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 57', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nClassical Ethics in A/IS\nand contrasted, especially in the exploration  Further Resources\nof artificial intelligence. While in Shinto, the \n•  R. M. Geraci, ""Spiritual Robots: Religion and \nartifact as “artificial” represents creation and \nOur Scientific View of the Natural World,"" \nauthentic being, with implications for defining \nTheology and Science, vol. 4, no. 3, pp.  \nautonomy, the same artifact is designated as \n229-246, 2006.\nsecondary and often times unnatural, false, \nand counterfeit in Western ethical philosophical  •  D. F. Holland-Minkley, “God in the \ntradition, dating back to Platonic and Christian  Machine: Perceptions and Portrayals of \nideas of separation of form and spirit. In both  Mechanical Kami in Japanese Anime.” \ntraditions, culturally presumed biases define our  Ph.D. dissertation, University of Pittsburgh, \nrelationships with technology. While disparate  Pittsburgh, PA, 2010.\nin origin and foundation, both Western classical \n•  C. B. Jensen and A. Blok, “Techno-Animism \nethics traditions and Shinto ethical influences \nin Japan: Shinto Cosmograms, Actor-Network \nin modern A/IS have similar goals and outlooks \nTheory, and the Enabling Powers of Non-\nfor ethics in A/IS, goals that are centered \nHuman Agencies,” Theory, Culture & Society, \nin “relationship”.\nvol. 30, no. 2, pp. 84-115, March 2013.\nRecommendations\n•  F. Kaplan, ""Who Is Afraid of the Humanoid? \nWhere Japanese culture leads the way in \nInvestigating Cultural Differences in the \nthe synthesis of traditional value systems \nAcceptance of Robots,"" International Journal of \nand technology, we recommend that people \nHumanoid Robotics, vol. 1, no. 3, pp. 465-\ninvolved with efforts in A/IS ethics explore \n480, 2004.\nthe Shinto paradigm as representative, though \nnot necessarily as directly applicable, to global  •  S. G. Tzafestas, Roboethics: A Navigating \nefforts in understanding and applying traditional  Overview. Cham, Switzerland: Springer, 2015.\nand classical ethics methodologies to A/IS. \n•  G. Veruggio and K. Abney, ""22 Roboethics:  \n \nThe Applied Ethics for a New Science,"" \n \nin Robot Ethics: The Ethical and Social \n \nImplications of Robotics. Cambridge, MA: MIT \nPress, 2011, p. 347.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 58', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nClassical Ethics in A/IS\nSection 3—Classical Ethics  \nfor a Technical World\nHow will A/IS influence human autonomy in ways \nthat may or may not be advantageous to the \nIssue: Maintaining Human \ngood life, and perhaps—even if advantageous—\nAutonomy\nmay be detrimental at the same time? How \ndo these systems affect human autonomy and \nBackground decision-making through the use of algorithms \nwhen said algorithms tend to inform (“in-form”) \nA/IS present the possibility for a digitally \nvia targeted feedback loops?\nnetworked intellectual capacity that imitates, \nmatches, and supersedes human intellectual  Consider, for example, Google’s autocomplete \ncapacity, including, among other things, general  tool, where algorithms attempt to determine \nskills, discovery, and computing functions. In  one’s search parameters via the user’s initial \naddition, A/IS can potentially acquire functionality  keyword input, offering suggestions based on \nin areas traditionally captured under the rubric  several criteria including search patterns. In this \nof what we deem unique human and social  scenario, autocomplete suggestions influence, in \nability. While the larger question of ethics and  real-time, the parameters the user phrases their \nA/IS looks at the implications of the influence  search by, often reforming the user’s perceived \nof autonomous systems in these areas, the  notions of what it was they were looking for \npertinent issue is the possibility of autonomous  in the first place, versus what they might have \nsystems imitating, influencing, and then  actually originally intended.\ndetermining the norms of human autonomy. \nTargeted algorithms also inform, as per emerging \nThis is done through the eventual negation of \nIoT, applications that monitor the user’s routines \nindependent human thinking and decision-\nand habits in the analog world. Consider for \nmaking, where algorithms begin to inform \nexample that our bioinformation is, or soon will \nthrough targeted feedback loops what it is \nbe, available for interpretation by autonomous \nwe are and what it is we should decide. Thus, \nsystems. What happens when autonomous \nhow can the academic rigor of traditional ethics \nsystems can inform the user in ways the user is \nspeak to the question of maintaining human \nnot even aware of, using one’s bioinformation \nautonomy in light of algorithmic decision-making?\nin targeted advertising campaigns that seek to \ninfluence the user in real-time feedback loops \nbased on the user’s biological reactions such as \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 59', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nClassical Ethics in A/IS\npupil dilation, body temperature, and emotional  Recommendations\nreaction, whether positive or negative, to that very \nA two-step process is recommended to maintain \nsame advertising, using information about our \nhuman autonomy in A/IS. The creation of an \nbeing to in-form and re-form our being? On the \nethics-by-design methodology is the first step \nother hand, it becomes important not to adopt \nto addressing human autonomy in A/IS, where \ndystopian assumptions concerning autonomous \na critically applied ethical design of autonomous \nmachines threatening human autonomy. \nsystems preemptively considers how and \nThe tendency to think only in negative terms  where autonomous systems may or may not \npresupposes a case for interactions between  dissolve human autonomy. The second step is \nautonomous machines and human beings, a  the creation of a pointed and widely applied \npresumption not necessarily based in evidence.  education curriculum that spans grade school \nUltimately, the behavior of algorithms rests solely  through university, one based on a classical ethics \nin their design, and that design rests solely in  foundation that focuses on providing choice and \nthe hands of those who designed them. Perhaps  accountability toward digital being as a priority  \nmore importantly, however, is the matter of  in information and knowledge societies.\nchoice in terms of how the user chooses to \nFurther Resources\ninteract with the algorithm. Users often don’t \n•  B. van den Berg and J. de Mul, “Remote \nknow when an algorithm is interacting with them \nControl. Human Autonomy in the Age of \ndirectly or their data which acts as a proxy for \nComputer-Mediated Agency,” in Law, Human \ntheir identity. Should there be a precedent for the \nAgency and Autonomic Computing: The \nA/IS user to know when they are interacting with \nPhilosophy of Law Meets the Philosophy of \nan algorithm? What about consent? \nTechnology, M. Hildebrandt and A. Rouvroy, \nThe responsibility for the behavior of algorithms  Eds. London: Routledge, 2011, pp. 46-63.\nremains with the designer, the user, and a \n•  L. Costa, “A World of Ambient Intelligence,” \nset of well-designed guidelines that guarantee \nin Virtuality and Capabilities in a World of \nthe importance of human autonomy in any \nAmbient Intelligence. Cham, Switzerland: \ninteraction. As machine functions become more \nSpringer International, 2016, pp. 15-41.\nautonomous and begin to operate in a wider \nrange of situations, any notion of those machines \n•  P. P. Verbeek, “Subject to Technology \nworking for or against human beings becomes \non Autonomic Computing and Human \ncontested. Does the machine work for someone \nAutonomy,” in The Philosophy of Law Meets \nin particular, or for particular groups but not \nthe Philosophy of Technology: Autonomic \nothers? Who decides on the parameters? Is it \nComputing and Transformations of Human \nthe machine itself? Such questions become key \nAgency, M. Hildebrandt and A. Rouvroy, Eds. \nfactors in conversations around ethical standards.\nNew York: Routledge, 2011. \n   \n \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 60', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nClassical Ethics in A/IS\n•  D. Reisman, J. Schultz, K. Crawford, and M.  security. The preservation of national sovereignty \nWhittaker, “Algorithmic Impact Assessments:  depends on the production and domination of \nA practical Framework for Public Agency  knowledge. In the realm of migratory policies, \nAccountability,” AI NOW, April 2018. knowledge is created to measure people in \ntransit: collecting, treating, and transferring \n•  A. Chaudhuri, ""Philosophical Dimensions \ninformation about territory and society.    \nof Information and Ethics in the Internet of \nThings (IoT) Technology,"" EDPACS, vol. 56, no.  Knowledge organization has been the paramount \n4, pp. 7-18, Nov. 2017. pillar of scientific thought and scientific practice \nsince the beginning of written civilization. Any \nscientific and technological development has \nIssue: Implications of Cultural  only been possible through information policies \nMigration in A/IS that include the establishment of management \nprocesses to systematize them, and the \ncodification of language. For the Greeks, this \nBackground\nprocess was closely associated with the concept \nIn addition to developing an understanding of   of arete, or the excellence of one’s self in \nA/IS via different cultures, it is crucial to  politics as congregated in the polis. The notion \nunderstand how A/IS are shaped and reshaped  of polis is as relevant as ever in the digital age \n—how they affect and are affected by—human  with the development of digital technologies and \nmobility and cultural diversity through active  the discussions around morality in A/IS. Where \nimmigration. The effect of human mobility  the systematization of knowledge is potentially \non state systems reliant on A/IS impacts the  freely created, the advent of the Internet and its \nState structure itself, and thus the systems that  flows are difficult to control. Ethical issues about \nthe structure relies on, in the end influencing  the production of information are becoming \neverything from democracy to citizenship. Where  paramount to our digital society. \nthe State, through A/IS, invests in and gathers \nThe advancement of the fields of science and \nbig data through mechanisms for registration \ntechnology has not been followed by innovations \nand identification of people, mainly immigrants, \nin the political community, and the technical \nhuman mobility becomes a foundational \ncommunity has repeatedly tabled academic \ncomponent in a system geared toward the \ndiscussions about the hegemony of technocracy \npreservation of human dignity.  \nover policy issues, restricting the space of the \nTraditional national concerns reflect two  policy arena and valorizing excessively technic \ninformation foundations: information produced  solutions for human problems. This monopoly \nfor human rights and information produced for  alters conceptions of morality, relocating the locus \nnational sovereignty. In the second foundation,  of the Kantian “Categorical Imperative”, causing \nState borders are considered the limits from  the tension among different social and political \nwhich political governance is defined in terms of  contexts to become more pervasive.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 61', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nClassical Ethics in A/IS\nCurrent global migration dynamics have been  moral presumptions of citizenship. Closed Circuit \nmet by unfavorable public opinion based in  Television(CCTV), Unmanned Aerial Vehicles \nideas of crisis and emergency, a response vastly  (UAVs), and satellites allow data transference in \ndisproportionate to what statistics have shown  real time to databases, cementing the backbone \nto be the reality. In response to these views,  that A/IS draws from, often with bias as per \nA/IS are currently designed and applied to  the expectations of developed countries. This \nmeasure, calculate, identify, register, systematize,  centrality of data sources for A/IS expresses a \nnormalize, and frame both human rights and  divide between developed and underdeveloped \nsecurity policies. This is largely no different of a  countries, particularly as relevant to the refugee.\nprocess than what has been practiced since the \nInformation is something that links languages, \nperiod of colonialism. It includes the creation \nhabits, customs, identification, and registration \nand implementation of a set of ancient and new \ntechnologies. It provokes a reshaping of the \ntechnologies. Throughout history, mechanisms \nimmigrants’ and refugees’ citizenship and their \nhave been created firstly to identify and select \nvalue as people in terms of their citizenship,  \nindividuals who share certain biological heritage, \nas they seek forms of surviving in, and against, \nand secondly to individuals and social groups, \nthe restrictions imposed by A/IS for surveillance \nincluding biological characteristics.\nand monitoring in an enlarged and more  \nInformation is only possible when materialized as  complex cosmopolis.\nan infrastructure supported by ideas in action as \nAn understanding of the impact of A/IS on \na “communicative act”, which Habermas (1968) \nmigration and mobile populations, as used in \nidentifies in Hegel’s work, converging three \nstate systems, is a critical first step to consider \nelements in human-in-the-world relationships: \nif systems are to become truly autonomous \nsymbol, language, and labor. Information policies \nand intelligent, especially beyond the guidance \nreveal the importance and the strength in which \nof human deliberation. Digital technology \ntechnologies influence economic, social, cultural, \nsystems used to register and identify human \nidentity, and ethnic interactions. \nmobility, including refugees and other displaced \nTraditional mechanisms used to control migration,  populations, are not autonomous in the intelligent \nsuch as the passport, are associated with globally  sense, and are dependent on the biases of \nestablished walls and fences. The more intense  worldviews around immigration. In this aspect, \nhuman mobility becomes, the more amplified are  language is the locus where this dichotomy has  \nthe discourses to discourage it, restricting human  to be considered to understand the diversity  \nmigrations, and deepening the need for an ethics  of morals when there are contacts among \nrelated to conditions of citizenship. Together with  different cultures. \nthe building of walls, other remote technologies \nare developed to monitor and surveil borders, \nbuildings, and streets, also impacting ideas and \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 62', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nClassical Ethics in A/IS\nRecommendations\nIs it recommended that the State become a  Issue: Applying Goal-Directed \nproactive player in the globalized processes   Behavior (Virtue Ethics) to \nof A/IS for migrant and mobile populations, \nAutonomous and Intelligent \nintroducing a series of mechanisms that limit  \nSystems\nthe segregation of social spaces and groups,  \nand consider the biases inherent in surveillance \nfor control.  Background\nFurther Resources Initial concerns regarding A/IS also include \nquestions of function, purpose, identity, and \n•  I. About and V. Denis, Histoire de \nagency, a continuum of goal-directed behavior \nl’identification des personnes. Paris: La \nwith function being the most primitive expression. \nDécouverte, 2010.\nHow can classical ethics act as a regulating force \n•  I. About, J. Brown, G. Lonergan, Identification  in autonomous technologies as goal-directed \nand Registration Practices in Transnational  behavior transitions from being externally set by \nPerspective: People, Papers and Practices.  operators to being internally set? The question \nLondon: Palgrave Macmillan, 2013, pp. 1-13. is important not just for safety reasons, but for \nmutual productivity. If autonomous systems are \n•  D. Bigo, “Security and Immigration: Toward   to be our trusted, creative partners, then we \na Critique of the Governmentality of Unease,”  need to be confident that we possess mutual \nin Alternatives, Special Issue, no. 27.   anticipation of goal-directed action in a wide \npp. 63-92, 2002. variety of circumstances.\n•  R. Capurro, “Citizenship in the Digital Age,”  A virtue ethics approach has merits for \nin Information Ethics, Globalization and  accomplishing this even without having to posit a \nCitizenship, T. Samek and L. Schultz, Eds.  “character” in an autonomous technology, since \nJefferson NC: McFarland, 2017, pp. 11-30. it places emphasis on habitual, iterative action \nfocused on achieving excellence in a chosen \n•  R. Capurro, “Intercultural Information Ethics,” \ndomain or in accord with a guiding purpose. At \nin Localizing the Internet: Ethical Aspects \npoints on the goal-directed continuum associated \nin Intercultural Perspective, R. Capurro, \nwith greater sophistication, virtue ethics become \nJ. Frühbauer, and T. Hausmanninger, \neven more useful by providing a framework for \nEds. Munich: Fink, 2007, pp. 21-38.\nprudent decision-making that is in keeping with \n•  UN High Commissioner for Refugees  the autonomous system’s purpose, but allows for \n(UNHCR), Policy on the Protection of Personal  creativity in how to achieve the purpose in a way \nData of Persons of Concern to UNHCR,   that still allows for a degree of predictability. An \nMay 2015.  ethics approach that does not rely on a decision \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 63', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nClassical Ethics in A/IS\nto refrain from transgressing, but instead to  •  L. Muehlhauser and L. Helm, ""The \nprudently pursue a sense of purpose informed by  Singularity and Machine Ethics,"" in Singularity \none’s identity, might provide a greater degree of  Hypotheses, A. H. Eden, J. H. Moor, J. H. \ninsight into the behavior of the system. Soraker, and E. Steinhart, Eds. Berlin: Springer, \n2012, pp. 101-126.\nRecommendations\n•  D. Vernon, G. Metta, and G. Sandini, ""A Survey \nProgram autonomous systems to be able to \nof Artificial Cognitive Systems: Implications \nrecognize user behavior for the purposes of \nfor the Autonomous Development of Mental \npredictability, traceability, and accountability \nCapabilities in Computational Agents,"" IEEE \nand to hold expectations, as an operator \nTransactions on Evolutionary Computation, vol. \nand co-collaborator, whereby both user and \n11, no. 2, pp. 151-180, April 2007.\nsystem mutually recognize the decisions of the \nautonomous system as virtue ethics-based.\nFurther Resources\n•  M. A. Boden, Ed. The Philosophy of \nIssue: A Requirement for \nArtificial Life. Oxford, U.K.: Oxford University \nRule-Based Ethics in Practical \nPress, 1996.\nProgramming\n•  C. Castelfranchi, ""Modelling Social Action for \nAI Agents,"" Artificial Intelligence, vol. 103, no.1-\nBackground\n2, pp. 157-182, 1998.\nResearch in machine ethics focuses on simple \n•  W. D. Christensen and C. A. Hooker, \nmoral machines. It is deontological ethics \n""Anticipation in Autonomous Systems: \nand teleological ethics that are best suited to \nFoundations for a Theory of Embodied \nthe kind of practical programming needed for \nAgents,"" International Journal of Computing \nsuch machines, as these ethical systems are \nAnticipatory Systems, vol. 5, pp. 135-154, Dec. \nabstractable enough to encompass ideas of \n2000.\nnon-human agency, whereas most modern \nethics approaches are far too human-centered to \n•  K. G. Coleman, “Android Arete: Toward a \nproperly accommodate the task.\nVirtue Ethic for Computational Agents,” Ethics \nand Information Technology, vol. 3, no. 4,  \nIn the deontological model, duty is the point of \npp. 247-265, 2001.\ndeparture. Duty can be translated into rules. It \ncan be distinguished into rules and metarules. \n•  J. G. Lennox, “Aristotle on the Biological Roots \nFor example, a rule might take the form “Don’t \nof Virtue,” Biology and the Foundations of \nlie!”, whereas a metarule would take the form of \nEthics, J. Maienschein and M. Ruse, Eds. \nKant’s categorical imperative: “Act only according \nCambridge, U.K.: Cambridge University Press, \nto that maxim whereby you can, at the same \n1999, pp. 405-438.\ntime, will that it should become a universal law.”\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 64', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nClassical Ethics in A/IS\nA machine can follow simple rules. Rule-based  based reasoning approaches. Here, the machine \nsystems can be implemented as formal systems,  uses “experience” in the form of similar cases \nalso referred to as “axiomatic systems”, and in the  that it has encountered in the past or uses cases \ncase of machine ethics, a set of rules is used to  which are collected in databases.\ndetermine which actions are morally allowable \nIn the context of the teleological model, the \nand which are not. Since it is not possible to \nconsequences of an action are assessed. The \ncover every situation by a rule, an inference \nmachine must know the consequences of an \nengine is used to deduce new rules from a small \naction and what the action’s consequences \nset of simple rules called axioms by combining \nmean for humans, for animals, for things in the \nthem. The morality of a machine comprises the \nenvironment, and, finally, for the machine itself. \nset of rules that is deducible from the axioms.\nIt also must be able to assess whether these \nFormal systems have an advantage since  consequences are good or bad, or if they are \nproperties such as decidability and consistency  acceptable or not, and this assessment is not \nof a system can be effectively examined. If a  absolute. While a decision may be good for \nformal system is decidable, every rule is either  one person, it may be bad for another; while \nmorally allowable or not, and the “unknown” is  it may be good for a group of people or for \neliminated. If the formal system is consistent, one  all of humanity, it may be bad for a minority \ncan be sure that no two rules can be deduced  of people. An implementation approach that \nthat contradict each other. In other words, the  allows for the consideration of potentially \nmachine never has moral doubt about an action  contradictory subjective interests may be realized \nand never encounters a deadlock. by decentralized reasoning approaches such \nas agent-based systems. In contrast to this, \nThe disadvantage of using formal systems is \ncentralized approaches may be used to assess \nthat many of them work only in closed worlds \nthe overall consequences for all involved parties.\nlike computer games. In this case, what is not \nknown is assumed to be false. This is in drastic   \nconflict with real world situations, where rules can  Recommendations\nconflict and it is impossible to take into account \nBy applying the classical methodologies of \nthe totality of the environment. In other words, \ndeontological and teleological ethics to machine \nconsistent and decidable formal systems that \nlearning, rules-based programming in A/IS can be \nrely on a closed world assumption can be used \nsupplemented with established praxis, providing \nto implement an ideal moral framework for a \nboth theory and a practicality toward consistent \nmachine, yet they are not viable for real  \nand determinable formal systems. \nworld tasks.\n \n \nOne approach to avoiding a closed world scenario \n \nis to utilize self-learning algorithms, such as case-\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 65', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nClassical Ethics in A/IS\nFurther Resources •  B. M. McLaren, “Computational Models \nof Ethical Reasoning: Challenges, Initial \n•  C. Allen, I. Smit, and W. Wallach, ""Artificial \nSteps, and Future Directions,” IEEE Intelligent \nMorality: Top-Down, Bottom-Up, and \nSystems, vol. 21, no. 4, pp. 29-37, July 2006.\nHybrid Approaches,"" Ethics and Information \nTechnology, vol. 7, no. 3, pp. 149-155, 2005. •  M. A. Perez Alvarez, “Tecnologías de \nla Mente y Exocerebro o las Mediaciones \n•  O. Bendel, Die Moral in der Maschine: Beiträge \ndel Aprendizaje,” 2015. \nzu Roboter-und Maschinenethik. Heise \nMedien, 2016. •  E. L. Rissland and D. B. Skalak, ""Combining \nCase-Based and Rule-Based Reasoning: A \n•  O. Bendel, Oliver, Handbuch Maschinenethik. \nHeuristic Approach."" Proceedings of the 11th \nWiesbaden, Germany: Springer VS, 2018.\nInternational Joint Conference on Artificial \n•  M. Fisher, L. Dennis, and M.  Intelligence, IJCAI 1989, Detroit, MI, August \nWebster, “Verifying Autonomous  20-25, 1989, San Francisco, CA: Morgan \nSystems,” Communications of the ACM,  Kaufmann Publishers Inc., 1989. pp. 524-530.\nvol. 56, no. 9, pp. 84-93, Sept. 2013.\n \nThanks to the Contributors \nWe wish to acknowledge all of the people who  Ethics, University of Applied Sciences and Arts \ncontributed to this chapter. Northwestern Switzerland FHNW\nThe Classical Ethics in A/IS Committee •  Dr. John T. F. Burgess – Assistant Professor / \nCoordinator for Distance Education, School of \n•  Jared Bielby (Chair) – President, Netizen \nLibrary and Information Studies, The University \nConsulting Ltd; Chair, International Center for \nof Alabama\nInformation Ethics; editor, Information Cultures \n•  Rafael Capurro – Founder, International \nin the Digital Age\nCenter for Information Ethics\n•  Soraj Hongladarom (Co-chair) – President at \n•  Corinne Cath-Speth – PhD student at \nThe Philosophy and Religion Society of Thailand\nOxford Internet Institute, The University of \n•  Miguel Á. Pérez Álvarez – Professor \nOxford, Doctoral student at the Alan Turing \nof Technology in Education, Colegio de \nInstitute, Digital Consultant at ARTICLE 19\nPedagogía, Facultad de Filosofía y Letras, \n•  Dr. Paola Di Maio – Center for Technology \nUniversidad Nacional Autónoma de México\nEthics, ISTCS.org UK and NCKU Taiwan\n•  Oliver Bendel – Professor of Information \n•  Robert Donaldson – Independent Computer \nSystems, Information Ethics and Machine \nScientist, BMRILLC, Hershey, PA\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 66', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nClassical Ethics in A/IS\n•  Rachel Fischer – Research Officer: African  •  Derek Poitras – Independent Consultant, \nCentre of Excellence for Information Ethics,  Object Oriented Software Development\nInformation Science Department, University of \n•  Samuel T. Segun – PhD Candidate, Department \nPretoria, South Africa. \nof Philosophy, University of Johannesburg. Fellow, \n•  Dr. D. Michael Franklin – Assistant  Philosophy Node of the Centre for Artificial \nProfessor, Kennesaw State University, Marietta  Intelligence Research (CAIR) at the University of \nCampus, Marietta, GA Pretoria and Research fellow at the Conversational \nSchool of Philosophy (CSP)\n•  Wolfgang Hofkirchner – Associate \nProfessor, Institute for Design and Technology  •  Dr. Ozlem Ulgen – Reader in International \nAssessment, Vienna University of Technology Law and Ethics, School of Law, Birmingham \nCity University\n•  Dr. Tae Wan Kim – Associate Professor of \nBusiness Ethics, Tepper School of Business  •  Kristene Unsworth – Assistant Professor, \nCarnegie Mellon University The College of Computing & Informatics, \nDrexel University\n•  Kai Kimppa – University Research Fellow, \nInformation Systems, Turku School of  •  Dr. Xiaowei Wang – Associate professor of \nEconomics, University of Turku Philosophy, Renmin University of China\n•  Sara R. Mattingly-Jordan – Assistant  •  Dr Sara Wilford – Senior Lecturer, Research \nProfessor Center for Public Administration &  Fellow, School of Computer Science and \nPolicy, Virginia Tech Informatics, Centre for Computing and Social \nResponsibility, De Montfort University\n•  Dr Neil McBride – Reader in IT \nManagement, School of Computer Science  •  Pak-Hang Wong – Research Associate, \nand Informatics, Centre for Computing and  Department of Informatics, University of Hamburg\nSocial Responsibility, De Montfort University\n•  Bendert Zevenbergen – Oxford Internet \n•  Bruno Macedo Nathansohn – Perspectivas  Institute, University of Oxford & Center for \nFilosóficas em Informação (Perfil-i); Brazilian  Information Technology Policy, Princeton \nInstitute of Information in Science and  University\nTechnology (IBICT)\nFor a full listing of all IEEE Global Initiative \n•  Marie-Therese Png – PhD Student, Oxford  Members, visit standards.ieee.org/content/dam/\nInternet Institute, PhD Intern, DeepMind Ethics  ieee-standards/standards/web/documents/other/\n& Society  ec_bios.pdf. \n \nFor information on disclaimers associated with \nEndnotes EAD1e, see How the Document Was Prepared.\n1  This edition of “Classical Ethics in A/IS” does not (and could not) aspire to universal coverage of all of the world’s traditions \nin the space available to us. Future editions will touch on several other traditions, including Judaism and Islam.\n2  R. Von Schomberg, “Prospects for Technology Assessment in a Framework of Responsible Research and Innovation” in Technikfolgen \nAbschätzen Lehren: Bildungspotenziale Transdisziplinärer Methode. Wiesbaden, Germany: Springer VS, 2011, pp. 39-61. \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 67', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems \nWell-being\nPrioritizing ethical and responsible artificial intelligence has become a widespread goal for \nsociety. Important issues of transparency, accountability, algorithmic bias, and value systems \nare being directly addressed in the design and implementation of autonomous and intelligent \nsystems (A/IS). While this is an encouraging trend, a key question still facing technologists, \nmanufacturers, and policymakers alike is how to assess, understand, measure, monitor, \nsafeguard, and improve the well-being impacts of A/IS on humans. Finding the answer to this \nquestion is further complicated when A/IS are within a holistic and interconnected framework \nof well-being in which individual well-being is inseparable from societal, economic, and \nenvironmental systems.\nFor A/IS to demonstrably advance well-being, we need consistent and multidimensional \nindicators that are easily implementable by the developers, engineers, and designers who are \nbuilding our future. This chapter is intended for such developers, engineers, and designers—\nreferred to in this chapter as “A/IS creators”. Those affected by A/IS are referred to as  \n“A/IS stakeholders”.  \nA/IS technologies affect human agency, identity, emotion, and ecological systems in new and \nprofound ways. Traditional metrics of success are not equipped to ensure A/IS creators can \navoid unintended consequences or benefit from unexpected innovation in the algorithmic age. \nA/IS creators need expanded ways to evaluate the impact of their products, services, or systems \non human well-being. These evaluations must also be done with an understanding that human \nwell-being is deeply linked to the well-being of society, economies, and ecosystems. \nToday, A/IS creators largely measure success using metrics including profit, gross domestic \nproduct (GDP), consumption levels, and occupational safety. While important, these metrics \nfail to encompass the full spectrum of well-being impacts on individuals and society, such as \npsychological, social, and environmental factors. Where the priority given to these factors is \nnot equal to that given to fiscal metrics of success, A/IS creators risk causing or contributing to \nnegative and irreversible harms to our people and our planet.\nWhen A/IS creators are not aware that well-being indicators, in addition to traditional metrics, \ncan provide guidance for their work, they are also missing out on innovation that can increase \nwell-being and societal value. For instance, while it is commonly recognized that autonomous \nvehicles will save lives when safely deployed, a topic of less frequent discussion is how self-\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 68', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nWell-being\ndriving cars also have the potential to help the environment by reducing greenhouse gas \nemissions and increasing green space. Autonomous vehicles can also positively impact well-\nbeing by increasing work-life balance and enhancing the quality of time spent during commutes. \nUnless A/IS creators are made aware of the existence of alternative measures of progress, \nthe value they provide, and the way they can be incorporated into A/IS work, technology and \nsociety will continue to rely upon traditional metrics of success. In an era where innovation is \ndefined by holistic prosperity, alternative measures are needed more now than ever before. \nThe 2009 Report by the Commission on the Measurement of Economic Performance and \nSocial Progress which contributed substantially to the worldwide movement of governments \nusing wider measures of well-being, states, “What we measure affects what we do; and if our \nmeasurements are flawed, decisions may be distorted.”\nWe believe that A/IS creators can profoundly increase human and environmental flourishing \nby prioritizing well-being metrics as an outcome in all A/IS system designs—now and for the \nfuture. The primary intended audience for this chapter is A/IS creators who are unfamiliar with \nthe term “well-being” as it is used in the field of positive psychology and well-being studies. \nOur initial goal is to provide a broad introduction to qualitative and quantitative metrics and \napplications of well-being to educate and inspire A/IS creators. We do not prioritize or advocate \nfor any specific indicator or methodology. For further elaboration on the definition of  \nwell-being, please see the first Issue listed in Section 1. \nThis chapter is divided into two main sections:\n•  The Value of Well-being Metrics for A/IS Creators\n•  Implementing Well-being Metrics for A/IS Creators\nThe following resources are available online to provide readers with an introduction to existing \nwell-being metrics and tools currently in use:\n•  The State of Well-being Metrics\n•  The Happiness Screening Tool for Business Product Decisions  \n•  Additional Resources: Standards Development Models and Frameworks\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 69', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nWell-being\nSection 1—The Value of Well-being \nMetrics for A/IS Creators\nWell-being metrics provide a broader perspective \nfor A/IS creators than they normally might \nIssue: There is ample and \nbe familiar with in evaluating their products. \nrobust science behind well-\nThis broader perspective unlocks greater \nbeing metrics and their use \nopportunities to assure a positive impact of A/IS \non human well-being, while minimizing the risk  by international and national \nof unintended negative outcomes. This section  institutions. However, A/IS \ndefines well-being, discusses the value of well-\ncreators are often unaware \nbeing metrics to A/IS creators, and notes how \nthat well-being metrics exist, \nsimilar frameworks like sustainability and human \nrights can be complemented by incorporating  or that they can be used to \nwell-being metrics. plan, develop, and evaluate \n  technology.\nDefinition of Well-being\n \nFor the purposes of Ethically Aligned Design, the  Background\nterm “well-being” refers to an evaluation of the \nThe concept of well-being refers to an evaluation \ngeneral quality of life of an individual and the \nof the general goodness of the state of an \nstate of external circumstances. The conception \nindividual or community and is distinct from \nof well-being encompasses the full spectrum \nmoral or legal evaluation. A well-being evaluation \nof personal, social, and environmental factors \ntakes into account major aspects of a person’s \nthat enhance human life and on which human \nlife, such as their happiness, success in their \nlife depend. The concept of well-being shall be \ngoals, and their overall positive functioning in \nconsidered distinct from moral or legal evaluation.\ntheir environment. There is now a thriving area \nof scientific research into the psychological, \n  \nsocial, behavioral, economic, and environmental \n \ndeterminants of human well-being. \n \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 70', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nWell-being\nThe term “well-being” is defined and used in  Among the most important and  \nvarious ways across different contexts and fields.  recognized aspects of well-being are  \nFor example: economists identifying economic  (in alphabetical order):\nwelfare with levels of consumption and economic \n•  Community: Belonging, Crime & Safety, \nvitality, psychologists highlighting subjective \nDiscrimination & Inclusion, Participation,  \nexperience, and sociologists emphasizing \nSocial Support\nliving, labor, political, social, and environmental \n•  Culture: Identity, Values\nconditions. We do not take a stand on any \nspecific measure of well-being. The metrics listed  •  Economy: Economic Policy, Equality & \nbelow are an incomplete list and provided as a  Environment, Innovation, Jobs, Sustainable \nstarting point for further inquiry. Among these  Natural Resources & Consumption & \nare subjective well-being indicators, measures of  Production, Standard of Living\nquality of life, social progress and capabilities, and \n•  Education: Formal Education, Lifelong \nmany more. \nLearning, Teacher Training \nThere is now sufficient consensus among  •  Environment: Air, Biodiversity, Climate Change, \nscientists that well-being can be reliably  Soil, Water\nmeasured. Well-being measures differ in the \n•  Government: Confidence, Engagement, \nnumber and the intricacy of indicators they \nHuman Rights, Institutions \nemploy. Short questionnaires of life satisfaction \n•  Human Settlements: Energy, Food, Housing, \nhave emerged as particularly popular, although \nInformation & Communication Technology, \nthey do not reflect all aspects of well-being.  \nTransportation \nWhile recognizing a scope for differences across \nwell-being indicators, we note that the richest  •  Physical Health: Health Status, Risk Factors, \nconception of well-being encompasses the full  Service Coverage\nspectrum of personal, social, and environmental \n•  Psychological Health: Affect (feelings), \ngoods that enhance human life. \nFlourishing, Mental Illness & Health, \nSatisfaction with Life\nWe encourage A/IS creators to consider the \nwide range of available indicators and select  •  Work: Governance, Time Balance,  \nthose most relevant and revealing for particular  Workplace Environment\nstages of the A/IS technology’s life cycle and \nthe particular context for the technology’s use \nand evaluation. That is, measures of well-being \nthat may be well-suited to wealthy, industrialized \nnations may be less applicable in low- and \nmiddle-income countries, and vice versa.  \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 71', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nWell-being\nIn an effort to provide a basic orientation to  conditions that support the well-being of countries \nwell-being metrics, information about well-being  and populations, and to measure the societal and \nindicators can be segmented into four categories:  environmental impact of companies. They are \nin use by organizations like the OECD with their \n1. Subjective or survey-based indicators \nBetter Life Index, which also includes survey-\nSurvey-based well-being indicators, \nbased well-being indicators and SWB indicators, \nsubjective well-being (SWB) indicators, and \nand the United Nations with their Sustainable \nmultidimensional measurements of aspects of \nDevelopment Goals Indicators (formerly the \nwell-being, are being used by national institutions, \nMillennium Development Goals). For business,  \ninternational institutions, and governments to \nthe Global Reporting Initiative, SDG Compass,  \nbetter understand levels of psychological well-\nand B-Corp provide broad indicator sets.\nbeing within countries and aspects of a country’s \npopulation. These indicators are also being used  3. Composite indicators (indices that \nto understand people’s satisfaction in specific  aggregate multiple metrics)  \ndomains of life. Examples of surveys that include  Aggregate metrics combine subjective and/\nsurvey-based well-being indicators and SWB  or objective metrics to produce one measure \nindicators include the European Social Survey,  reflecting both objective aspects of quality \nBhutan’s Gross National Happiness Indicators,  of life and people’s subjective evaluation of \nwell-being surveys created by The UK Office for  these. Examples of this are the UN’s Human \nNational Statistics, and many more.  Development Index, the Social Progress Index, \nand the United Kingdom’s Office of National \nSurvey-based metrics are also employed in the \nStatistics Measures of National Well-being. Some \nfield of positive psychology and in the World \nsubjective and objective indicators are also \nHappiness Report. The data are employed \ncomposite indicators, such as Bhutan’s Gross \nby researchers to understand the causes, \nNational Happiness Index and the OECD’s Better \nconsequences, and correlates of well-being. Data \nLife Index. \ngathered from surveys tend to address concerns, \nsuch as day-to-day experience, overall satisfaction  4. Social media sourced data \nwith life, and perceived flourishing. The findings  Social media can be used to measure the well-\nof these researchers provide crucial and  being of a geographic region or demographic \nnecessary guidance because they often diverge  group, based on sentiment analysis of \nfrom and complement the understanding of  publicly available data. Examples include the \ntraditional conditions, such as economic growth. Hedonometer and the World Well-being Project.\n2. Objective indicators   \n \nObjective indicators of quality of life have typically \n \nincorporated areas such as income, consumption, \n \nhealth, education, crime, housing, etc. These \n \nindicators have been used to understand \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 72', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nWell-being\nRecommendation\nA/IS creators should prioritize learning about  Issue: Increased awareness \nwell-being concepts, scientific learnings, research  and application of well-being \nfindings, and well-being metrics as potential \nmetrics by A/IS creators can \ndeterminants for how they create, deploy, market, \ncreate greater value, safety, \nand monitor their technologies, and ensuring \ntheir stakeholders learn the same. This process  and relevance to corporate \ncan be expedited if Standards Development  communities and other \nOrganizations (SDOs), such as the IEEE Standards \norganizations in the  \nAssociation, or other institutions such as the \nalgorithmic age.\nGlobal Reporting Initiative (GRI) or B-Corp, create \n \ncertifications, guidelines, and standards that for \nBackground\nthe use of holistic, well-being metrics for A/IS  \nin the public and private sectors. While many organizations in the private and \npublic sectors are increasingly aware of the \n \nneed to incorporate well-being measures as part \nFurther Resources\nof their efforts, the reality is that bottom line, \n•  The IEEE P7010™ Standards Project for Well-\nquarterly-driven shareholder growth remains a \nbeing Metric for Autonomous/Intelligent \ndominant goal and metric. Short term growth is \nSystems, was formed with the aim of \noften the priority in the private sector and public \nidentifying well-being metrics for applicability \nsector. As long as organizations exist in a larger \nto A/IS today and in the future. All are \nsocietal system which prioritizes financial success, \nwelcome to join the working group.\nthese companies will remain under pressure \n•  On 11 April 2017, IEEE hosted a dinner debate  to deliver financial results that do not fully \nat the European Parliament in Brussels to  incorporate societal and environmental impacts, \ndiscuss how the world’s top metric of value,  measurements, or priorities.\ngross domestic product, must move Beyond \nRather than focus solely on the negative \nGDP to holistically measure how intelligent \naspects of how A/IS could harm humans and \nand autonomous systems can hinder or \nenvironments, we seek to explore how the \nimprove human well-being.\nimplementation of well-being metrics can help \n•  Prioritizing Human Well-being in the Age of \nA/IS to have a measurable, positive impact on \nArtificial Intelligence (Report)\nhuman well-being as well as on systems and \n•  Prioritizing Human Well-being in the Age of  organizations. Incorporation of well-being goals \nArtificial Intelligence (Video) and measures beyond what is strictly required \ncan benefit both private sector organizations’ \n  \n  brands and public sector organizations’ stability \nand reputation, as well as help realize financial \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 73', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nWell-being\nsavings, innovation, trust, and many other  PricewaterhouseCoopers defines “total impact” \nbenefits. For instance, a companion robot  as a “holistic view of social, environmental, fiscal \noutfitted to support seniors in assisted living  and economic dimensions—the big picture”. \nsituations might traditionally be launched with  Other thought-leading organizations in the \na technology development model that was  public sector, such as the OECD, demonstrate \npopularized by Silicon Valley known as “move fast  the desire for business leaders to incorporate \nand break things”. The A/IS creator who rushed  metrics of success beyond fiscal indicators for \nto bring the robot to market faster than the  their efforts, exemplified in their 2017 workshop, \ncompetition and who was unaware of well-being  Measuring Business Impacts on People’s Well-\nmetrics, may have overlooked critical needs of  Being. The B-Corporation movement has created \nthe seniors. The robot might actually hurt the  a new legal status for “a new type of company \nsenior instead of helping by exacerbating isolation  that uses the power of business to solve social \nor feelings of loneliness and helplessness. While  and environmental problems”. Focusing on \nthis is a hypothetical scenario, it is intended to  increasing stakeholder value versus shareholder \ndemonstrate the value of linking A/IS design to  returns alone, B-Corps are defining their brands \nwell-being indicators. by provably aligning their efforts with wider \nmeasures of well-being.\nBy prioritizing largely fiscal metrics of success,  \nA/IS devices might fail in the market because of   \nRecommendations\nlimited adoption and subpar reception. However, \nif during use of the A/IS product, success were  A/IS creators should work to better understand \nmeasured in terms of relevant aspects of well- and apply well-being metrics in the algorithmic \nbeing, developers and researchers could be in  age. Specifically:\na better position to attain funding and public \nsupport. Depending on the intended use of the  •  A/IS creators should work directly with \nA/IS product, well-being measures that could be  experts, researchers, and practitioners in well-\nused extend to emotional levels of calm or stress;  being concepts and metrics to identify existing \npsychological states of thriving or depression;  metrics and combinations of indicators that \nbehavioral patterns of engagement in community  would bring support a “triple bottom line”, \nor isolation; eating, exercise and consumption  i.e., accounting for economic, social, and \nhabits; and many other aspects of human  environmental impacts, approach to well-\nwell-being. The A/IS product could significantly  being. However, well-being metrics should \nimprove quality of life guided by metrics from  only be used with consent, respect for privacy, \ntrusted sources, such as the World Health  and with strict standards for collection and use \nOrganization, European Social Survey,   of these data.\nand Sustainable Development Goal Indicators.  •  For A/IS to promote human well-being, \nthe well-being metrics should be chosen \nThought leaders in the corporate arena \nin collaboration with the populations \nhave recognized the multifaceted need \nmost affected by those systems—the A/IS \nto utilize metrics beyond fiscal indicators. \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 74', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nWell-being\nstakeholders—including both the intended \nend-users or beneficiaries and those \nIssue: A/IS creators have \ngroups whose lives might be unintentionally \nopportunities to safeguard \ntransformed by them. This selection process \nshould be iterative and through a learning   human well-being by ensuring \nand continually improving process. In addition,  that A/IS does no harm to \n“metrics of well-being” should be treated as \nearth’s natural systems or that \nvehicles for learning and potential mid- \nA/IS contributes to realizing \ncourse corrections. The effects of A/IS on \nsustainable stewardship, \nhuman well-being should be monitored \ncontinuously throughout their life cycles, by   preservation, and/or restoration \nA/IS creators and stakeholders, and both A/IS  of earth’s natural systems. A/IS \ncreators and stakeholders should be prepared \ncreators have opportunities to \nto significantly modify, or even roll back, \nprevent A/IS from contributing to \ntechnology that is shown to reduce well-being, \nthe degradation of earth’s natural \nas defined by affected populations.\nsystems and hence losses to \n•  A/IS creators in the business or academic, \nengineering, or policy arenas are advised to  human well-being.\nreview the additional resources on standards \ndevelopment models and frameworks at the  Background\nend of this chapter to familiarize themselves \nIt is unwise, and in truth impossible, to separate \nwith existing indicators relevant to their work.\nthe well-being of the natural environment of \nthe planet from the well-being of humanity. \nFurther Resources A range of studies, from the historic to more \nrecent, prove that ecological collapse endangers \n•  PricewaterhouseCoopers (PwC). Managing \nhuman existence. Hence, the concept of \nand Measuring Total Impact: A New Language \nwell-being should encompass planetary well-\nfor Business Decisions, 2017.\nbeing. Moreover, biodiversity and ecological \n•  World Economic Forum. The Inclusive Growth \nintegrity have intrinsic merit beyond simply their \nand Development Report 2017, Geneva, \ninstrumental value to humans.\nSwitzerland: World Economic Forum, January \n16, 2017. Technology has a long history of contributing \nto ecological degradation through its role in \n•  OECD Guidelines on Measuring Subjective \nexpanding the scale of resource extraction \nWell-being, 2013.\nand environmental pollution, for example, the \n•  National Research Council. Subjective Well-\nimmense power needs of network computing, \nBeing: Measuring Happiness, Suffering, and \nwhich leads to climate change, water scarcity, soil \nOther Dimensions of Experience. DC: The \ndegradation, species extinction, deforestation, \nNational Academies Press, 2013.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 75', ""The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nWell-being\nbiodiversity loss, and destruction of ecosystems  being, it is required that not only A/IS creators \nwhich in turn threatens humankind in the long  act along such lines, but also that a systems \nrun. These and other costs are often considered  approach is taken by all A/IS stakeholders to \nexternalities and often do not figure into  find solutions that safeguard human well-being \ndecisions or plans. At the same time, there are  with the understanding that human well-being is \nmany examples, such as photovoltaics and smart  inextricable from healthy social, economic, and \ngrid technology that present potential ways to  environmental systems.\nrestore earth’s ecosystems if undertaken within a \nRecommendations\nsystems approach aimed at sustainable economic \nand environmental development. A/IS creators need to recognize and prioritize \nthe stewardship of the Earth’s natural systems \nEnvironmental justice research demonstrates \nto promote human and ecological well-being. \nthat the negative environmental impacts of \nSpecifically: \ntechnology are commonly concentrated on the \nmiddle class and working poor, as well as those  •  Human well-being should be defined to \nsuffering from abject poverty, fleeing disaster  encompass ecological health, access to \nzones, or otherwise lacking the resources to  nature, safe climate and natural environments, \nmeet their needs. Ecological impact can thus  biosystem diversity, and other aspects of a \nexacerbate the economic and sociological effects  healthy, sustainable natural environment. \nof wealth disparities on human well-being by \n•  A/IS systems should be designed to use, \nconcentrating environmental injustice onto those \nsupport, and strengthen existing ecological \nwho are less well off. Moreover, well-being \nsustainability standards with a certification \nresearch findings indicate that unfair economic \nor similar system, e.g., LEED, Energy Star, \nand social inequality has a dampening effect on \nor Forest Stewardship Council. This directs \neveryone's well-being, regardless of economic or \nautomation and machine intelligence to \nsocial class.\nfollow the principle of doing no harm and \nto safeguard environmental, social, and \nIn these respects, A/IS are no exception; they \neconomic systems. \ncan be used in ways that either help or harm the \necological integrity of the planet. It may be fair to  •  A/IS creators should prioritize doing no harm \nsay that ecological health and human well-being  to the Earth’s natural systems, both intended \nwill, increasingly, depend upon A/IS creators. It  and unintended harm.  \nis imperative that A/IS creators and stakeholders \n•  A committee should be convened to issue \nfind ways to use A/IS to do no harm and to \nfindings on ways in which A/IS can be used by \nreduce the environmental degradation associated \nbusiness, NGOs, and governmental agencies \nwith economic growth–while simultaneously \nto promote stewardship and restoration of \nidentifying applications to restore the ecological \nnatural systems while reducing the harmful \nhealth of the planet and thereby safeguarding \nimpact of economic development on ecological \nthe well-being of humans. For A/IS to reduce \nsustainability and environmental justice.\nenvironmental degradation and promote well-\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 76"", 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nWell-being\nFurther Resources\n•  D. Austin and M. Macauley. ""Cutting  Issue: Human rights law is \nThrough Environmental Issues: Technology  related to, but distinct from, \nas a double-edged sword.” The Brookings \nthe pursuit of well-being. \nInstitution, Dec. 2001 [Online]. Available: \nIncorporating a human-rights \nhttps://www.brookings.edu/articles/cutting-\nthrough-environmental-issues-technology-as- framework as an essential basis \na-double-edged-sword/. [Accessed Dec. 1,  for A/IS creators means A/IS \n2018].\ncreators honor existing law as \n•  J. Newton, Well-being and the Natural  part of their well-being analysis \nEnvironment: An Overview of the Evidence. \nand implementation.\nAugust 20, 2007.\n \n•  P. Dasgupta, Human Well-Being and the  Background\nNatural Environment. Oxford, U.K.: Oxford \nInternational human rights law has been firmly \nUniversity Press, 2001.\nestablished for decades in order to protect \n•  R. Haines-Young and M. Potschin. “The Links \nvarious guarantees and freedoms as enshrined \nBetween Biodiversity, Ecosystem Services and \nin charters such as the United Nations’ Universal \nHuman Well-Being,” in Ecosystem Ecology: A \nDeclaration of Human Rights and the Council \nNew Synthesis, D. Raffaelli, and C. Frid, Eds. \nof Europe’s Convention on Human Rights. In \nCambridge, U.K.: Cambridge University Press, \n2018, the Toronto Declaration on machine \n2010.\nlearning standards was released, calling on both \n•  S. Hart, Capitalism at the Crossroads: Next  governments and technology companies to \nGeneration Business Strategies for a Post- ensure that algorithms respect basic principles \nCrisis World. Upper Saddle River, NJ: Pearson  of equality and non-discrimination. The Toronto \nEducation, 2010. Declaration sets forth an obligation to prevent \nmachine learning systems from discriminating, \n•  United Nations Department of Economic \nand in some cases violating, existing human \nand Social Affairs. “Call for New Technologies \nrights law. \nto Avoid Ecological Destruction.” Geneva, \nSwitzerland, July 5, 2011.\nWell-being initiatives are typically undertaken \n•  Pope Francis. Encyclical Letter Laudato Si’ of  for the sake of public interest. However, any \nthe Holy Father Francis On the Care for Our  metric, including well-being metrics, can be \nCommon Home. May 24, 2015. misused to justify human rights violations. \nEncampment and mistreatment of refugees \n•  “Environment,” The 14th Dalai Lama. Accessed \nand ethnic cleansing undertaken to preserve \nDec. 9, 2018. https://www.dalailama.com/\na nation’s culture (an aspect of well-being) is \nmessages/environment.\none example. Imprisonment or assassination of \n•  Why Islam.org, Environment and Islam, 2018. \njournalists or researchers to ensure the stability \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 77', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nWell-being\nof a government is another. The use of well- Recommendation\nbeing metrics to justify human rights violations  A human rights framework should represent the \nis an unconscionable perversion of the nature  floor, and not the ceiling, for the standards to \nof any well-being metric. It should be noted  which A/IS creators must adhere. Developers \nthat these same practices happen today in  and users of well-being metrics should be aware \nrelation to GDP. For instance, in 2012, according  these metrics will not always adequately address \nto the International Labour Organization (ILO),  human rights.\napproximately 21 million people are victims of \nforced labor (slavery), representing 9% to 56% \nFurther Resources\nof GDP income for various countries. These clear \nhuman rights violations, from sex trafficking and  •  United Nations Universal Declaration of \nuse of children in armies, to indentured farming  Human Rights, 1948.\nor manufacturing labor, can increase a country’s \n•  Council of Europe’s Convention on Human \nGDP while obviously harming human well-being. \nRights, 2018. \nWell-being metrics are designed to measure  •  International Labor Organization (ILO) \nthe efficacy of efforts related to individual  Declaration on Fundamental Principles and \nand societal flourishing. Well-being as a value  Rights at Work, 1998.\ncomplements justice, equality, and freedom. \n•  The regularly updated University of Minnesota \nWell-designed application of well-being \nHuman Rights Library provides a wealth of \nconsiderations by A/IS creators should not \nmaterial on human rights laws, its history, and \ndisplace other issues of human rights or ethical \nthe organizations engaged in promoting them.\nmethodologies, but rather complement them.\n•  The Oxford Human Rights Hub reports on \n  how and why technologies surrounding \n \nartificial intelligence raise human rights issues. \n \n \n \n \n \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 78', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nWell-being\nSection 2—Implementing Well-being \nMetrics for A/IS Creators\nA key challenge for A/IS creators in realizing the  Additionally, internal and external stakeholders \nbenefits of well-being metrics is how to best  should be extensively consulted to ensure that \nincorporate them into their work. This section  impacts are thoroughly considered through an \nexplores current best thinking on how to make  iterative and learning stakeholder engagement \nthis happen.  process. After consultation, A/IS creators should \nselect appropriate well-being indicators based \non the possible scope and impact of their A/IS \nproduct or service. These well-being indicators \nIssue: How can A/IS creators \ncan be drawn from mainstream sources and \nincorporate well-being into  \nmodels and adapted as necessary. They can \ntheir work? be used to engage in pre-assessment of the \nintended user population, projection of possible \nimpacts, and post-assessment. Development of \nBackground\na well-being indicator measurement plan and \nWithout practical ways of incorporating well-being \nrelevant data infrastructure will support a robust \nmetrics to guide, measure, and monitor impact, \nintegration of well-being. A/IS models can also be \nA/IS will likely lack fall short of its potential to \ntrained to explicitly include well-being indicators \navoid harm and promote well-being. Incorporating \nas subgoals. \nwell-being thinking into typical organizational \nprocesses of design, prototyping, marketing, etc.,  Data and discussions on well-being impacts \nsuggests a variety of adaptations.  can be used to suggest improvements and \nmodifications to existing A/IS products and \nOrganizations and A/IS creators should consider \nservices throughout their lifecycle. For example, a \nclearly defining the type of A/IS product or \nteam seeking to increase the well-being of people \nservice that they are developing, including \nusing wheelchairs found that when provided the \narticulating its intended stakeholders and uses. \nopportunity to use a smart wheelchair, some \nBy defining typical uses, possible uses, and finally \nusers were delighted with the opportunity for \nunacceptable uses of the technology, creators \nmore mobility, while others felt it would decrease \nwill help to spell out the context of well-being. \ntheir opportunities for social contact, increase \nThis can help to identify possible harms and risks \ntheir sense of isolation, and lead to an overall \ngiven the different possible uses and end users, \ndecrease in their well-being. Therefore, even \nas well as intended and unintended positive \nthough a product modification may increase \nconsequences.\nwell-being according to one indicator or set of \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 79', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nWell-being\nA/IS stakeholders, it does not mean that this  •  J. Helliwell, R. Layard, and J. Sachs, Eds., “The \nmodification should automatically be adopted.  Objective Benefits of Subjective Well-Being,” \nin World Happiness Report 2013. New York: \nFinally, organizational processes can be modified \nUN Sustainable Development Solutions \nto incorporate the above strategies. Appointment \nNetwork, pp. 54-79, 2013.\nof an organizational lead person for well-being \n•  Global Happiness and Well-being Policy \nimpacts, e.g., a well-being lead, ombudsman,  \nReport by the Global Happiness Council, 2018. \nor officer can help to facilitate this effort.\nRecommendation\nA/IS creators should adjust their existing \nIssue: How can A/IS creators \ndevelopment, marketing, and assessment cycles \ninfluence A/IS goals to ensure \nto incorporate well-being concerns throughout \nwell-being, and what can A/IS \ntheir processes. This includes identification of an \nA/IS lead ombudsperson or officer; identification  creators learn or borrow from \nof stakeholders and end users; determination of  existing models in the well-being \npossible uses, harm and risk assessment; robust \nand other arenas?\nstakeholder engagement; selection of well-being \nindicators; development of a well-being indicator \nBackground\nmeasurement plan; and ongoing improvement \nof A/IS products and services throughout the  Another way to incorporate considerations of \nlifecycle. well-being is to include well-being measures  \nin the development, goal setting, and training  \nof the A/IS systems themselves.\nFurther Resources\n•  Peter Senge and the Learning Organization -  Identified metrics of well-being could be \n(synopsis) Purdue University formulated as auxiliary objectives of the A/IS. As \nthese auxiliary well-being objectives will be only \n•  Stakeholder Engagement: A Good Practice \na subset of the intended goals of the system, \nHandbook for Companies Doing Business \nthe architecture will need to balance multiple \nin Emerging Markets. International Finance \nobjectives. Each of these auxiliary objectives may \nCorporation, May 2007. \nbe expressed as a goal, set of rules, set of values, \n•  Global Reporting Initiative or as a set of preferences, which can be weighted \n•  GNH Certification, Centre for Bhutan   and combined using established methodologies \nand GNH Studies, 2018. from intelligent systems engineering.  \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 80', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nWell-being\nFor example, an educational A/IS tool could  to limit their use of an A/IS product if it leads \nnot only optimize learning outcomes, but also  to increased sustained stress levels, sustained \nincorporate measures of student social and  isolation, development of unhealthy habits, or \nemotional education, learning, and thriving. other decreases to well-being.  \nA/IS-related data relates both to the individual—  Incorporating well-being goals and metrics into \nthrough personalized algorithms, in conjunction  broader organizational values and processes \nwith affective sensors measuring and influencing  would support the use of well-being metrics as \nemotion, and other aspects of individual well-being  there would be institutional support. A key factor \n—and to society as large data sets representing  in industrial, corporate, and societal progress is \naggregate individual subjective and objective data.  cross-dissemination of concepts and models \nAs the exchange of this data becomes more widely  from one industry or field to another. To date, a \navailable via establishing tracking methodologies,  number of successful concepts and models exist \nthe data can be aligned within A/IS products  in the fields of sustainability, economics, industrial \nand services to increase human well-being. For  design and manufacturing, architecture and urban \nexample, robots like Pepper are equipped to  development, and governmental policy. These \nshare data regarding their usage and interaction  concepts and models can provide a foundation  \nwith humans to the cloud. This allows almost  for building a metrics standard and the use of well-\ninstantaneous innovation, as once an action is  being metrics by A/IS creators, from conception \nvalidated as useful for one Pepper robot, all other  and design to marketing, product updates, and \nPepper units (and ostensibly their owners) benefit  improvements to the user experience.  \nas well. As long as this data exchange happens \nwith the predetermined consent of the robots’ \nRecommendation\nowners, this innovation in real time model can \nbe emulated for the large-scale aggregation of  Create technical standards for representing goals, \ninformation relating to existing well-being metrics. metrics, and evaluation guidelines for well-being \nmetrics and their precursors and components \nA/IS creators can also help to operationalize  within A/IS that include:\nwell-being metrics by providing stakeholders \nwith reports on the expected or actual outcomes  •  Ontologies for representing technological \nof the A/IS and the values and objectives  requirements. \nembedded in the systems. This transparency will  •  A testing framework for validating adherence to \nhelp creators, users, and third parties assess the  well-being metrics and ethical principles such \nstate of well-being produced by A/IS and make  as IEEE P7010™ Standards Project for Well-\nimprovements in A/IS. In addition, A/IS creators  being Metric for Autonomous and Intelligent \nshould consider allowing end users to layer on  Systems.\ntheir own preferences, such as allowing users \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 81', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nWell-being\n•  The exploration of models and concepts listed  2017), H. Trautmann, G. Rudolph, K. Klamroth, \nabove as well as others as a basis for a well- O. Schütze, M. Wiecek, Y. Jin, and C. Grimme, \nbeing metrics standard for A/IS creators. (See  Eds., Vol. 10173. Springer-Verlag, Berlin, \npage 191, Additional Resources: Additional  Heidelberg, 406-421, 2017. \nResources: Standards Development Models \n•  PositiveSocialImpact: Empowering people, \nand Frameworks)\norganizations and planet with information \n•  The development of a well-being metrics  and knowledge to make a positive impact to \nstandard for A/IS that encompasses an  sustainable development, 2017.\nunderstanding of well-being as holistic and \n•  D.K. Ura, Bhutan’s Gross National Happiness \ninterlinked to social, economic, and ecological \nPolicy Screening Tool.\nsystems. \n \nFurther Resources\nIssue: Decision processes for \n•  A.F.T Winfield, C. Blum, and W. Liu. “Towards an \ndetermining relevant well-being \nEthical Robot: Internal Models, Consequences \nand Ethical Action Selection,” in Advances in  indicators through stakeholder \nAutonomous Robotics Systems. Springer, 2014,  deliberations need to be \npp. 85–96\nestablished.\n•  R. A. Calvo, and D. Peters. Positive Computing: \nTechnology for Well-Being and Human  Background\nPotential. Cambridge MA: MIT Press, 2014.\nA/IS stakeholder involvement is necessary to \n•  Y. Collette, and P. Slarry. Multiobjective \ndetermine relevant well-being indicators, for a \nOptimization: Principles and Case Studies \nnumber of reasons:\n(Decision Engineering Series). Berlin, Germany: \nSpringer, 2004. doi: 10.1007/978-3-662-08883-8. •  “Well-being” will be defined differently by \ndifferent groups affected by A/IS. The most \n•  J. Greene, et al. “Embedding Ethical Principles \nrelevant indicators of well-being may vary \nin Collective Decision Support Systems,” in \naccording to country, with concerns of wealthy \nProceedings of the Thirtieth AAAI Conference \nnations being different than those of low- and \non Artificial Intelligence, 4147–4151. Palo Alto, \nmiddle-income countries. Indicators may \nCA: AAAI Press, 2016.\nvary based on geographical region or unique \n•  L. Li, I. Yevseyeva, V. Basto-Fernandes, H. \ncircumstances. The indicators may also be \nTrautmann, N. Jing, and M. Emmerich,“Building \ndifferent across social groups, including gender, \nand Using an Ontology of Preference-Based \nrace, ethnicity, and disability status.\nMultiobjective Evolutionary Algorithms.” In 9th \n•  Common indicators of well-being include \nInternational Conference on Evolutionary Multi-\nsatisfaction with life, healthy life expectancy, \nCriterion Optimization—Volume 10173 (EMO \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 82', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nWell-being\neconomic standard of living, trust in  A process of stakeholder engagement and \ngovernment, social support, perceived freedom  deliberation is one model for collective decision-\nto make life decisions, income equality, access  making. Parties in such deliberation come \nto education, and poverty rates. Applying  together as equals. Their goal is to set aside their \nthem in particular settings necessarily requires  immediate, personal interests in order to think \njudgment, to ensure that assessments of  together about the common good. Participants in \nwell-being are in fact meaningful in context  a stakeholder engagement and deliberation learn \nand reflective of the life circumstances of the  from one another’s perspectives and experiences.\ndiverse groups in question.  \nIn the real world, stakeholder engagement \n•  Not all aspects of well-being are easily \nand deliberation may run into the following \nquantifiable. The importance of hard-to-quantify \nchallenges:\naspects of well-being is most likely to become \n•  Individuals with more education, power, or \napparent through interaction with those more \nhigher social status may—intentionally or \ndirectly affected by A/IS in specific settings.\nunintentionally—dominate the discussion, \n•  Engineers and corporate employees frequently  undermining their ability to learn from less \nmisunderstand stakeholders’ needs and  powerful participants.\nexpectations, especially when the stakeholders \n•  Topics may be preemptively ruled “out \nare very different from them in terms of \nof bounds”, to the detriment of collective \neducational and cultural background, social \nproblem-solving. An example would be if, \nlocation, and/or economic status.\nin a deliberation on well-being and A/IS, \nparticipants were told that worries about the \nThe processes through which stakeholders  costs of health insurance were unrelated to  \nbecome involved in determining relevant well- A/IS and thus could not be discussed.\nbeing indicators will affect the quality of the \n•  Engineers and scientists may claim authority \nindicators selected and assessed. Stakeholders \nover technical issues and be willing to \nshould be empowered to define well-being, assess \ndeliberate only on social issues, obscuring \nthe appropriateness of existing indicators and \nthe ways that technical and social issues are \npropose new ones, and highlight context-specific \nintertwined.\nfactors that bear on issues of well-being, whether \n•  Less powerful groups may be unable to keep \nor not the issues have been recognized previously \nmore powerful ones “at the table” when \nor are amenable to measurement. Interactive, \ndiscussions get contentious, and vice versa.\nopen-ended discussions or deliberations among a \nwide variety of stakeholders and system designers  •  Participants may not agree on who can \nare more likely to yield robust, widely-shared  legitimately be involved in the conversation. For \nunderstandings of well-being and how to measure  example, the consensual spirit of deliberation \nit in context. Closed-ended or over-determined  is often used as a justification for excluding \nmethods for soliciting stakeholder input are likely  activists and others who already hold a position \nto miss relevant information that system designers  on the issue.\nhave not anticipated.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 83', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nWell-being\nStakeholder engagement and deliberative  participation and deliberation. With expert \nprocesses can be effective when: guidance, facilitators can provide guidance \nfor how to: take steps to mitigate the effects \n•  Their design is guided by experts or \nof unequal power in deliberative processes; \npractitioners who are experienced in \nincorporate appropriately trained facilitators and \ndeliberation models.\ncoaching participants in deliberations; recognize \n•  Deliberations are facilitated by individuals  and curb disproportionate influence by more-\nsensitive to issues of power and are skilled in  powerful groups; use techniques to maximize \nmediating deliberation sessions. the voices of less-powerful groups.\n•  Less powerful actors participate with the help  •  Leads should use their convening power to \nof allies who can amplify their voices. bring together A/IS creators and stakeholders, \nincluding critics of A/IS, for deliberations on \n•  More powerful actors participate with an \nwell-being indicators, impacts, and other \nawareness of their own power and make a \nconsiderations for specific contexts and \ncommitment to listen with humility, curiosity, \nsettings. Leads’ involvement would help bring \nand open-mindedness.\nactors to the table with a balance of power and \n•  Deliberations are convened by institutions or \nencourage all actors to remain in conversation \nindividuals who are trusted and respected by all \nuntil robust, mutually agreeable definitions  \nparties and who hold all actors accountable for \nare found.\nparticipating constructively. \nFurther Resources\nEthically aligned design of A/IS would be furthered \nby thoughtfully constructed, context-specific  •  D. E. Booher and J. E. Innes. Planning with \ndeliberations on well-being and the best indicators  Complexity: An Introduction to Collaborative \nfor assessing it. Rationality for Public Policy. London:  \n  Routledge, 2010. \nRecommendation\n•  J. A. Leydens and J. C. Lucena. Engineering \nAppoint a lead team or person, “leads”, to facilitate  Justice: Transforming Engineering Education \nstakeholder engagement and to serve as a  and Practice. Wiley-IEEE Press, 2018. \nresource for A/IS creators who use stakeholder-\n•  G. Ottinger. Assessing Community Advisory \nbased processes to establish well-being indicators. \nPanels: A Case Study from Louisiana’s Industrial \nSpecifically: \nCorridor. Center for Contemporary History and \nPolicy, 2008.\n•  Leads should solicit and collect lessons learned \nfrom specific applications of stakeholder  •  Expert and Citizen Assessment of Science and \nengagement and deliberation in order to  Technology (ECAST) Network \ncontinually refine its guidance.  \n•  When determining well-being indicators, the \nleads should enlist the help of experts in public \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 84', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nWell-being\nWhile this is a partial list, it is important to be \naware of and reflect on possible and actual cases. \nIssue: There are insufficient \nFor example:\nmechanisms to foresee and \nmeasure negative impacts, and   •  A prominent concern related to A/IS is of  \nlabor displacement and economic and social \nto promote and safeguard positive \nimpacts at an individual and a systems level.  \nimpacts of A/IS.\nA/IS technologies designed to replicate human \ntasks, behavior, or emotion have the potential \nBackground to increase or decrease human well-being. \nThese systems could complement human work \nA/IS technologies present great opportunity \nand increase productivity, wages, and leisure \nfor positive change in every aspect of society. \ntime; or they could be used to supplement \nHowever, they can—by design or unintentionally—\nand displace human workers, leading to \ncause harm as well. While it is important to \nunemployment, inequality, and social strife.  \nconsider and make sense of possible benefits, \nIt is important for A/IS creators to think about \nharms, and trade-offs, it is extremely challenging  \npossible uses of their technology and whether \nto foresee all of the relevant, direct, and  \nthey want to encourage or design in restrictions \nsecondary impacts.\nin light of these impacts.\nHowever, it is prudent to review case studies of \n•  Another example relates to manipulation. \nsimilar products and the impacts they have had  \nSophisticated manipulative technologies \non well-being, as well as to consider possible  \nutilizing A/IS can restrict the fundamental \ntypes of impacts that could apply. Issues to \nfreedom of human choice by manipulating \nconsider include: \nhumans who consume content without them \nrecognizing the extent of the manipulation. \n•  Economic and labor impacts, including labor \nSoftware platforms are moving from targeting \ndisplacement, unemployment, and inequality,\nand customizing content to much more \n•  Accountability, transparency, and explainability,\npowerful and potentially harmful “persuasive \n•  Surveillance, privacy, and civil liberties,\ncomputing” that leverages psychological data \n•  Fairness, ethics, and human rights,\nand methods. While these approaches may \n•  Political manipulation, deception, “nudging”,  \nbe effective in encouraging use of a product, \nand propaganda,\nthey may come at significant psychological and \n•  Human physical and psychological health,\nsocial costs.\n•  Environmental impacts,\n•  Human dignity, autonomy, and human vs.   •  A/IS may deceive and harm humans by \nA/IS roles, posing as humans. With the increased ability \n•  Security, cybersecurity, and autonomous  of artificial systems to meet the Turing test, an \nweapons, and intelligence test for a computer that allows a \n•  Existential risk and super intelligence. human to distinguish human intelligence from \nartificial intelligence, there is a significant risk \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 85', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nWell-being\nthat unscrupulous operators will abuse the  well-being impacts when designing, using, \ntechnology for unethical commercial or outright  and monitoring A/IS systems. This includes \ncriminal purposes. Without taking action to  being aware of existing cases and possible \nprevent it, it is highly conceivable that A/IS  areas of impact, measuring impacts on well-\nwill be used to deceive humans by pretending  being outcomes, and developing regulations to \nto be another human being in a plethora of  promote beneficent uses of A/IS. Specifically:\nsituations and via multiple mediums.\n•  A/IS creators should protect human dignity, \nautonomy, rights, and well-being of those \nA potential entry point for exploring these  directly and indirectly affected by the \nunintended consequences is computational  technology. As part of this effort, it is important \nsustainability.  to include multiple stakeholders, minorities, \nmarginalized groups, and those often without \nComputational-Sustainability.org defines the \npower or a voice in consultation.\nterm as an “interdisciplinary field that aims \nto apply techniques from computer science,  •  Policymakers, regulators, monitors, and \ninformation science, operations research,  researchers should consider issuing guidance \napplied mathematics, and statistics for balancing  on areas such as A/IS labor and the proper role \nenvironmental, economic, and societal needs  of humans vs. A/IS in work transparency, trust, \nfor sustainable development”. The Institute  and explainability; manipulation and deception; \nof Computational Sustainability states that  and other areas that emerge.\nthe intent of computational sustainability is  •  Ongoing literature review and analysis \nprovide “computational models for a sustainable  should be performed by research and \nenvironment, economy, and society”. Examples of  other communities to curate and aggregate \napplied computational sustainability can be seen in  information on positive and negative A/IS \nthe Stanford University Engineering Department’s  impacts, along with demonstrated approaches \ncourse in computational sustainability presentation.  to realize positive ones and ameliorate  \nComputational sustainability technologies designed  negative ones.\nto increase social good could also be tied to \n•  A/IS creators working toward computational \nexisting well-being metrics.\nsustainability should integrate well-being \nconcepts, scientific findings, and indicators into \nRecommendation current computational sustainability models. \nThey should work with well-being experts, \n•  To avoid potential negative, unintended \nresearchers, and practitioners to conduct \nconsequences, and secure and safeguard \nresearch and develop and apply models in  \npositive impacts, A/IS creators, end-users, \nA/IS development that prioritize and increase \nand stakeholders should be aware of possible \nhuman well-being.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 86', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nWell-being\n•  Cross-pollination should be developed  •  Partnership on AI, “AI, Labor, and the \nbetween computational sustainability and  Economy” Working Group launches in New \nwell-being professionals to ensure integration  York City,” https://www.partnershiponai.org/\nof well-being into computational sustainability  aile-wg-launch/. April 25, 2018. \nframeworks, and vice versa. Where feasible \n•  C.Y. Johnson, “Children can be swayed \nand reasonable, do the same for conceptual \nby robot peer pressure,study says,” The \nmodels such as doughnut economics and \nWashington Post, August 15, 2018. [Online]. \nsystems thinking.\nAvailable: www.WashingtonPost.com. \n[Accessed 2018]. \nFurther Resources\n•  AI Safety Research by The Future of Life  Further Resources for  \nInstitute Computational Sustainability\n•  D. Helbing, et al. “Will Democracy Survive  •  Stanford Engineering Department, Topics \nBig Data and Artificial Intelligence?” Scientific  in Computational Sustainability Course \nAmerican, February 25, 2017. Presentation, 2016.  \n•  J. L. Schenker, “Can We Balance Human  •  Computational Sustainability, Computational \nEthics with Artificial Intelligence?” Techonomy,  Sustainability: Computational Methods for \nJanuary 23, 2017. a Sustainable Environment, Economy, and \n•  M. Bulman, “EU to Vote on Declaring Robots  Society Project Summary.  \nTo Be ‘Electronic Persons’.” Independent,  •  C. P. Gomes, “Computational Sustainability: \nJanuary 14, 2017. Computational Methods for a Sustainable \n•  N. Nevejan, for the European Parliament.  Environment, Economy, and Society” in The \n“European Civil Law Rules in Robotics.”  Bridge: Linking Engineering and Society. \nOctober 2016.  Washington, DC: National Academy of \nEngineering of the National Academies, 2009.\n•  University of Oxford. “Social media \nmanipulation rising globally, new report  •  S.J. Gershman, E. J. Horvitz, and J. B. \nwarns,” https://phys.org/news/2018-07- Tenenbaum. “Computational rationality: A \nsocial-media-globally.html. July 20, 2018. converging paradigm for intelligence in brains, \nminds, and machines,” Science vol. 349, no. \n•  “The AI That Pretends To Be Human,” \n6245, pp. 273–278, July 2015.\nLessWrong blog post, February 2, 2016.\n•  ACM Fairness, Accountability and Transparency \n•  C. Chan, “Monkeys Grieve When Their Robot \nConference\nFriend Dies.” Gizmodo, January 11, 2017.\n \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 87', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nWell-being\nThanks to the Contributors \nWe wish to acknowledge all of the people who  Adjunct faculty at Georgetown University; \ncontributed to this chapter. Business ethics author\nThe Well-being Committee •  Amy Blankson – Author of The Future \nof Happiness and Founder of TechWell, a \n•  John C. Havens (Co-Chair) – Executive \nresearch and consulting firm that aims to help \nDirector, The IEEE Global Initiative on Ethics \norganizations to create more positive digital \nof Autonomous and Intelligent Systems; \ncultures\nExecutive Director, The Council on Extended \n•  Marc Böhlen – Professor, University at \nIntelligence; Author, Heartificial Intelligence: \nBuffalo, Emerging Practices in Computational \nEmbracing Our Humanity to Maximize \nMedia. www.realtechsupport.org\nMachines \n•  Rafael A. Calvo – Professor and ARC Future \n•  Laura Musikanski (Co-Chair) – Executive \nFellow at The University of Sydney. Co-author \nDirector at The Happiness Alliance—home \nof Positive Computing: Technology for Well-\nof The Happiness Initiative & Gross National \nBeing and Human Potential\nHappiness Index\n•  Rumman Chowdhury – PhD Senior \n•  Liz Alexander – PhD Futurist\nPrincipal, Artificial Intelligence, and Strategic \n•  Anna Alexandrova – Senior Lecturer in \nGrowth Initiative Responsible AI Lead, \nPhilosophy of Science at Cambridge University \nAccenture\nand Fellow of Kings College\n•  Dr. Aymee Coget – CEO and Founder of \n•  Christina Berkley – Executive Coach to \nHappiness For HumanKind\nleaders in exponential technologies, cutting-\n•  Danny W. Devriendt – Managing director of \nedge science, and aerospace\nMediabrands Dynamic (IPG) in Brussels, and \n•  Catalina Butnaru – UK AI Ambassador for \nthe CEO of the Eye of Horus, a global think-tank \nglobal community City.AI, and Founder of HAI, \nfor communication-technology related topics\nthe first methodology for applications of AI in \n•  Eimear Farrell – Eimear Farrell, independent \ncognitive businesses\nexpert/consultant on technology and human \n•  Celina Beatriz – Project Director at the \nrights (formerly at OHCHR)\nInstitute for Technology & Society of Rio de \n•  Danit Gal – Project Assistant Professor, Keio \nJaneiro (ITS Rio)\nUniversity; Chair, IEEE Standard P7009 on the \n•  Peet van Biljon – Founder and CEO at \nFail-Safe Design of Autonomous and Semi-\nBMNP Strategies LLC, advisor on strategy, \nAutonomous Systems\ninnovation, and business transformation; \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 88', ""The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nWell-being\n•  Marek Havrda – PhD Strategy Advisor, GoodAI •  Gideon Rosenblatt – Writer, focused on \nwork and the human experience in an era of \n•  Andra Keay – Managing Director of Silicon \nmachine intelligence, at The Vital Edge\nValley Robotics, cofounder of Robohub\n•  Daniel Schiff – PhD Student, Georgia \n•  Dr. Peggy Kern – Senior Lecturer, Centre \nInstitute of Technology; Chair, Sub-Group \nfor Positive Psychology at the University of \nfor Autonomous and Intelligent Systems \nMelbourne's Graduate School of Education\nImplementation, IEEE P7010™ Standards \n•  Michael Lennon – Senior Fellow, Center \nProject for Well-being Metric for Autonomous \nfor Excellence in Public Leadership, George \nand Intelligent Systems\nWashington University; Co-Founder, \n•  Madalena Sula – Undergraduate student \nGovpreneur.org; Principal, CAIPP.org \nof Electrical and Computer Engineering, \n(Consortium for Action Intelligence and Positive \nUniversity of Thessaly, Greece, x-PR Manager \nPerformance); Member, Well-being Metrics \nof IEEE Student Branch of University of \nStandard for Ethical Artificial Intelligence and \nThessaly, Data Scientist & Business Analyst in \nAutonomous Systems Committee\na multinational company\n•  Alan Mackworth – Professor of Computer \n•  Vincent Siegerink – Analyst, OECD Statistics \nScience, University of British Columbia; \nand Data Directorate\nFormer President, AAAI; Co-author of “Artificial \nIntelligence: Foundations of Computational  •  Andy Townsend – Emerging and Disruptive \nAgents” Technology, PwC UK\n•  Richard Mallah – Director of AI Project,  •  Andre Uhl – Research Associate, Director's \nFuture of Life Institute Office, MIT Media Lab\n•  Fabrice Murtin – Senior Economist, OECD  •  Ramón Villasante – Founder of \nStatistics and Data Directorate PositiveSocialImpact. Software designer, \nengineer, CTO & CPO in EdTech for sustainable \n•  Gwen Ottinger – Associate Professor, Center \ndevelopment, social impact and innovation\nfor Science, Technology, and Society and \nDepartment of Politics, Drexel University;  •  Sarah Villeneuve – Policy Analyst; Member, \nDirector, Fair Tech Collective IEEE P7010™ Standards Project for Well-being \nMetric for Autonomous and Intelligent Systems.\n•  Eleonore Pauwels – Research Fellow on \nAI and Emerging Cybertechnologies, United \nFor a full listing of all IEEE Global Initiative \nNations University (NY) and Director of the AI \nMembers, visit standards.ieee.org/content/dam/\nLab, Woodrow Wilson International Center for \nieee-standards/standards/web/documents/\nScholars (DC)\nother/ec_bios.pdf. \n•  Venerable Tenzin Priyadarshi – MIT Media \nLab, Director, Ethics Initiative  For information on disclaimers associated with \nEAD1e, see How the Document Was Prepared.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 89"", 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nAffective Computing\nAffect is a core aspect of intelligence. Drives and emotions, such as excitement and \ndepression, are used to coordinate action throughout intelligent life, even in species that \nlack a nervous system. Emotions are one mechanism that humans evolved to accomplish \nwhat needs to be done in the time available with the information at hand—to satisfice. \nEmotions are not an impediment to rationality; arguably they are integral to rationality in \nhumans. Humans create and respond to both positive and negative emotional influence \nas they coordinate their actions with other individuals to create societies. Autonomous and \nintelligent systems (A/IS) are being designed to simulate emotions in their interactions with \nhumans in ways that will alter our societies.\nA/IS should be used to help humanity to the greatest extent possible in as many contexts \nas are appropriate. While A/IS have tremendous potential to effect positive change, there is \nalso potential that artifacts used in society could cause harm either by amplifying, altering, \nor even dampening human emotional experience. Even rudimentary versions of synthetic \nemotions, such as those already in use within nudging systems, have already altered the \nperception of A/IS by the general public and public policy makers.\nThis chapter of Ethically Aligned Design addresses issues related to emotions and emotion-\nlike control in interactions between humans and design of A/IS. We have put forward \nrecommendations on a variety of topics: considering how affect varies across human \ncultures; the particular problems of artifacts designed for caring and private relationships; \nconsiderations of how intelligent artifacts may be used for “nudging”; how systems can \nsupport human flourishing; and appropriate policy interventions for artifacts designed with \ninbuilt affective systems.\nDocument Sections\n• Section 1—Systems Across Cultures\n• Section 2—When Systems Care\n• Section 3—System Manipulation/Nudging/Deception\n• Section 4—Systems Supporting Human Potential \n• Section 5—Systems with Synthetic Emotions\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 90', ""The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nAffective Computing\nSection 1—Systems Across Cultures\n2.  These include: physical cues such as simulated \nfacial expressions, psychological cues such as \nIssue: Should affective systems \n1simulated humor or other emotions, use of \ninteract using the norms \nlanguage, use of social dynamics like taking \nfor verbal and nonverbal \nturns, and through social roles such as acting \ncommunication consistent with  as a tutor or medical advisor. Further examples \nthe norms of the society in which  are listed below:\nthey are embedded?\na.  Well-designed affective systems will use \nlanguage with affective content carefully \nBackground and within the contemporaneous \nexpectations of the culture. An example \nIndividuals around the world express intentions \nis small talk. Although small talk is useful \ndifferently, including the ways that they make eye \nfor establishing a friendly rapport in \ncontact, use gestures, or interpret silence. These \nmany communities, some communities \nparticularities are part of an individual’s and a \nsee people that use small talk as \nsociety's culture and are incorporated into their \ninsincere and hypocritical. Other cultures \naffective systems in order to convey the intended \nmay consider people that do not use \nmessage. To ensure that the emotional systems \nsmall talk as unfriendly, uncooperative, \nof autonomous and intelligent systems foster \nrude, arrogant, or ignorant. Additionally, \neffective communication within a specific culture, \nspeaking with proper vocabulary, \nan understanding of the norms/values of the \ngrammar, and sentence structure \ncommunity where the affective system will be \nmay contrast with the typical informal \ndeployed is essential.\ninteractions between individuals. For \nexample, the latest trend, TV show, or \nRecommendations other media may significantly influence \nwhat is viewed as appropriate vocabulary \n1.  A well-designed affective system will have a \nand interaction style.\nset of essential norms, specific to its intended \ncultural context of use, in its knowledge base.  b.  Well-designed affective systems will \nResearch has shown that A/IS technologies  recognize that the amount of personal \ncan use at least five types of cues to simulate  space (proxemics) given by individuals \nsocial interactions. in an important part of culturally specific \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 91"", 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nAffective Computing\nhuman interaction. People from varying  culture. Well-developed A/IS will be \ncultures maintain, often unknowingly,  able to recognize, analyze, and even \ndifferent spatial distances between  display facial expressions essential for \nthemselves to establish smooth  culturally specific social interaction.\ncommunication. Crossing these limits \nmay require explicit or implicit consent,  3.  Engineers should consider the need for \nwhich A/IS must learn to negotiate to  cross-cultural use of affective systems.  \navoid transmitting unintended messages. Well-designed systems will have options innate \nto facilitate flexibility in cultural programming. \nc.  Eye contact is an essential component  Mechanisms to enable and disable culturally \nfor culturally sensitive social interaction.  specific “add-ons” should be considered an \nFor some interactions, direct eye  essential part of A/IS development.\ncontact is needed but for others it is \nnot essential and may even generate \nFurther Resources\nmisunderstandings. It is important that \n•  G. Cotton, “Gestures to Avoid in Cross-Cultural \nA/IS be equipped to recognize the role \nBusiness: In Other Words, ‘Keep Your Fingers \nof eye contact in the development of \nto Yourself!’” Huffington Post, June 13, 2013.\nemotional interaction.\n•  “Paralanguage Across Cultures,” Sydney, \nd.  Hand gestures and other non-verbal \nAustralia: Culture Plus Consulting, 2016.\ncommunication are very important \nfor social interaction. Communicative  •  G. Cotton, Say Anything to Anyone, Say \ngestures are culturally specific and  Anything to Anyone, Anywhere: 5 Keys to \nthus should be used with caution in  Successful Cross-Cultural Communication. \ncross-cultural situations. The specificity  Hoboken, NJ: Wiley, 2013.\nof physical communication techniques \nmust be acknowledged in the design  •  D. Elmer, Cross-Cultural Connections: \nof functional affective systems. For  Stepping Out and Fitting In Around the World. \ninstance, although a “thumbs-up” sign  Westmont, IL: InterVarsity Press, 2002.\nis commonly used to indicate approval, \n•  B. J. Fogg, Persuasive Technology. Ubiquity, \nin some countries this gesture can be \nDecember 2, 2002. \nconsidered an insult.\n•  A. McStay, Emotional AI: The Rise of Empathic \ne.  Humans use facial expressions to detect \nMedia. London: Sage, 2018. \nemotions and facilitate communication. \nFacial expressions may not be universal  •  M. Price, “Facial Expressions—Including Fear—\nacross cultures, however, and A/IS  May Not Be as Universal as We Thought.” \ntrained with a dataset from one culture  Science, October 17, 2016.\nmay not be readily usable in another \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 92', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nAffective Computing\nRecommendations\nIssue: It is presently unknown \n1.  Collaborative research teams must research \nwhether long-term interaction  the effects of long-term interaction of people \nwith affective artifacts that lack  with affective systems. This should be \ndone using multiple protocols, disciplinary \ncultural sensitivity could alter \napproaches, and metrics to measure \nhuman social interaction.\nthe modifications of habits, norms, and \nprinciples as well as careful evaluation of the \nBackground downstream cultural and societal impacts.\nSystems that do not have cultural knowledge \n2. Parties responsible for deploying affective \nincorporated into their knowledge base may or \nsystems into the lives of individuals or \nmay not interact effectively with humans for \ncommunities should be trained to detect  \nwhom emotion and culture are significant. Given \nthe influence of A/IS, and to utilize mitigation \nthat interaction with A/IS may affect individuals \ntechniques if A/IS effects appear to be \nand societies, it is imperative that we carefully \nharmful. It should always be possible to  \nevaluate mechanisms to promote beneficial \nshut down harmful A/IS. \naffective interaction between humans and  \nA/IS. Humans often use mirroring in order to \nunderstand and develop their norms for behavior.  Further Resources\nCertain machine learning approaches also \n•  T. Nishida and C. Faucher, Eds., Modelling \naddress improving A/IS interaction with humans \nMachine Emotions for Realizing Intelligence: \nthrough mirroring human behavior. Thus, we \nFoundations and Applications. Berlin, \nmust remember that learning via mirroring can \nGermany: Springer-Verlag, 2010.\ngo in both directions and that interacting with \nmachines has the potential to impact individuals’  •  D. J. Pauleen, et al. “Cultural Bias in \nnorms, as well as societal and cultural norms.  Information Systems Research and Practice: \nIf affective artifacts with enhanced, different,  Are You Coming from the Same Place I \nor absent cultural sensitivity interact with  Am?” Communications of the Association \nimpressionable humans this could alter their  for Information Systems, vol. 17,)pp. 1–36, \nresponses to social and cultural cues and values.  2006. J. Bielby, “Comparative Philosophies in \nThe potential for A/IS to exert cultural influence   Intercultural Information Ethics.” Confluence: \nin powerful ways, at scale, is an area of  Online Journal of World Philosophies 2, no. 1, \nsubstantial concern.  pp. 233–253, 2015.\n•  J. Bryson, “Why Robot Nannies Probably \n  Won’t Do Much Psychological Damage.” A \n  commentary on an article by N. Sharkey \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 93', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nAffective Computing\nand A. Sharkey, The Crying Shame of Robot  end-user values should be actively considered. \nNannies. Interaction Studies, vol. 11, no. 2 pp.  Differences between affective systems and \n161–190, July 2010. societal values may generate conflict situations \nproducing undesirable results, e.g., gestures \n•  A. Sharkey, and N. Sharkey, “Children, the \nor eye contact being misunderstood as rude \nElderly, and Interactive Robots.” IEEE Robotics \nor threatening. Thus, affective systems should \n& Automation Magazine, vol.18, no. 1, pp. \nadapt to reflect the values of the community and \n32–38, March 2011.\nindividuals where they will operate in order to \navoid misunderstanding.\nIssue: When affective systems  Recommendations\nare deployed across cultures,  Assuming that well-designed affective systems \nthey could adversely affect the  have a minimum subset of configurable norms \nincorporated in their knowledge base:\ncultural, social, or religious  \n1.  Affective systems should have capabilities to \nvalues of the community in \nidentify differences between the values they \nwhich they interact.\nare designed with and the differing values of \nthose with whom the systems are interacting. \nBackground\n2. Where appropriate, affective systems will \nSome philosophers argue that there are no \nadapt accordingly over time to better fit the \nuniversal ethical principles and that ethical \nnorms of their users. As societal values change, \nnorms vary from society to society. Regardless \nthere needs to be a means to detect and \nof whether universalism or some form of ethical \naccommodate such cultural change in affective \nrelativism is true, affective systems need to \nsystems.\nrespect the values of the cultures within which \nthey are embedded. How systems should  3. Those actions undertaken by an affective system \neffectively reflect the values of the designers  that are most likely to generate an emotional \nor the users of affective systems is not a  response should be designed to be easily \nsettled discussion. There is general agreement  changed in appropriate ways by the user without \nthat developers of affective systems should  being easily hacked by actors with malicious \nacknowledge that the systems should reflect  intentions. Similar to how software today \nthe values of those with whom the systems are  externalizes the language and vocabulary to be \ninteracting. There is a high likelihood that when  easily changeable based on location, affective \nspanning different groups, the values imbued by  systems should externalize some  \nthe developer will be different from the operator  of the core aspects of their actions.\nor customer of that affective system, and that \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 94', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nAffective Computing\nFurther Resources •  Culture reflects the moral values and ethical \nnorms governing how people should behave \n•  J. Bielby, “Comparative Philosophies in \nand interact with others. “Ethics, an Overview.” \nIntercultural Information Ethics.” Confluence: \nBoundless Management.\nOnline Journal of World Philosophies 2, no. 1, \npp. 233–253, 2015. •  T. Donaldson, “Values in Tension: Ethics Away \nfrom Home Away from Home.” Harvard \n•  M. Velasquez, C. Andre, T. Shanks, and M. J. \nBusiness Review. September– October 1996. \nMeyer. “Ethical Relativism.” Markkula Center \nfor Applied Ethics, Santa Clara, CA: Santa Clara \nUniversity, August 1, 1992.\n \nSection 2—When Systems Care\nfundamental human rights to highlight potential \nethical benefits and risks that may emerge, if  \nIssue: Are moral and ethical \nand when affective systems interact intimately \nboundaries crossed when the \nwith users. \ndesign of affective systems \nAmong the many areas of concern are the \nallows them to develop intimate \nrepresentation of care, embodiment of caring  \nrelationships with their users?\nA/IS, and the sensitivity of data generated \nthrough intimate and caring relationships with  \nBackground A/IS. The literature suggests that there are some \npotential benefits to individuals and to society \nThere are many robots in development or \nfrom the incorporation of caring A/IS, along with \nproduction designed to focus on intimate care \nduly cautionary notes concerning the possibility \nof children, adults, and the elderly2. While \nthat these systems could negatively impact \nrobots capable of participating fully in intimate \nhuman-to-human intimate relations3.\nrelationships are not currently available, the \npotential use of such robots routinely captures  Recommendations\nthe attention of the media. It is important that \nAs this technology develops, it is important \nprofessional communities, policy makers, and \nto monitor research into the development of \nthe general public participate in development \nintimate relationships between A/IS and humans. \nof guidelines for appropriate use of A/IS in this \nResearch should emphasize any technical and \narea. Those guidelines should acknowledge \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 95', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nAffective Computing\nnormative developments that reflect use of   6. Existing laws regarding personal imagery need \nA/IS in positive and therapeutic ways while  to be reconsidered in light of caring A/IS.  \nalso creating appropriate safeguards to mitigate  In addition to other ethical considerations, it \nagainst uses that contribute to problematic  will also be necessary to establish conformance \nindividual or social relationships: with local laws and mores in the context of \ncaring A/IS systems.\n1.  Intimate systems must not be designed \nor deployed in ways that contribute to \nstereotypes, gender or racial inequality,   Further Resources\nor the exacerbation of human misery.\n•  M. Boden, J. Bryson, D. Caldwell, K. \nDautenhahn, L. Edwards, S. Kember, P. \n2. Intimate systems must not be designed \nNewman, V. Parry, G. Pegman, T. Rodden and \nto explicitly engage in the psychological \nT. Sorrell, Principles of robotics: regulating \nmanipulation of the users of these systems \nrobots in the real world. Connection Science, \nunless the user is made aware they are being \nvol. 29, no. 2, pp. 124-129, April 2017.\nmanipulated and consents to this behavior. \nAny manipulation should be governed  \n•  J. J. Bryson, M. E. Diamantis, and T. D. Grant, \nthrough an opt-in system. \n“Of, For, and By the People: The Legal Lacuna \nof Synthetic Persons.” Artificial Intelligence & \n3. Caring A/IS should be designed to avoid \nLaw, vol. 25, no. 3, pp. 273–291, Sept. 2017.\ncontributing to user isolation from society.  \n•  M. Scheutz, “The Inherent Dangers of \n4. Designers of affective robotics must \nUnidirectional Emotional Bonds between \npublicly acknowledge, for example, within \nHumans and Social Robots,” in Robot Ethics: \na notice associated with the product, \nThe Ethical and Social Implications of \nthat these systems can have side effects, \nRobotics, P. Lin, K. Abney, and G. Bekey, Eds., \nsuch as interfering with the relationship \npp. 205. Cambridge, MA: MIT Press, 2011. \ndynamics between human partners, causing \n \nattachments between the user and the A/IS \nthat are distinct from human partnership.\n5. Commercially marketed A/IS for caring \napplications should not be presented to \nbe a person in a legal sense, nor marketed \nas a person. Rather its artifactual, that is, \nauthored, designed, and built deliberately, \nnature should always be made as transparent \nas possible, at least at point of sale and in \navailable documentation, as noted in Section \n4, Systems Supporting Human Potential.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 96', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nAffective Computing\nSection 3— System Manipulation/ \nNudging/Deception\nRecommendations\nIssue: Should affective systems  1.  Systematic analyses are needed that examine \nbe designed to nudge people   the ethics and behavioral consequences of \ndesigning affective systems to nudge human \nfor the user’s personal benefit \nbeings prior to deployment. \nand/or for the benefit of others?\n2. The user should be empowered, through an \nexplicit opt-in system and readily available, \nBackground\ncomprehensible information, to recognize \nManipulation can be defined as an exercise  different types of A/IS nudges, regardless \nof influence by one person or group, with the  of whether they seek to promote beneficial \nintention to attempt to control or modify the  social manipulation or to enhance consumer \nactions of another person or group. Thaler  acceptance of commercial goals. The user \nand Sunstein (2008) call the tactic of subtly  should be able to access and check facts \nmodifying behavior a “nudge4”. Nudging mainly  behind the nudges and then make a conscious \noperates through the affective elements of a  decision to accept or reject a nudge. Nudging \nhuman rational system. Making use of a nudge  systems must be transparent, with a clear chain \nmight be considered appropriate in situations  of accountability that includes human agents: \nlike teaching children, treating drug dependency,  data logging is required so users can know \nand in some healthcare settings. While nudges  how, why, and by whom they were nudged.\ncan be deployed to encourage individuals to \n3. A/IS nudging must not become coercive and \nexpress behaviors that have community benefits, \nshould always have an opt-in system policy \na nudge could have unanticipated consequences \nwith explicit consent.  \nfor people whose backgrounds were not well \n4. Additional protections against unwanted \nconsidered in the development of the nudging \nnudging must be put in place for vulnerable \nsystem5. Likewise, nudges may encourage \npopulations, such as children, or when \nbehaviors with unanticipated long-term effects, \ninformed consent cannot be obtained. \nwhether positive or negative, for the  \nProtections against unwanted nudging should \nindividual and/or society. The effect of  \nbe encouraged when nudges alter long-term \nA/IS nudging a person, such as potentially \nbehavior or when consent alone may not be  \neroding or encouraging individual liberty, or \na sufficient safeguard against coercion  \nexpressing behaviors that are for the benefit \nor exploitation. \nothers, should be well characterized in the design \nof A/IS.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 97', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nAffective Computing\n5. Data gathered which could reveal an individual  •  C.R. Sunstein, The Ethics of Influence: \nor groups’ susceptibility to a nudge or their  Government in the Age of Behavioral Science. \nemotional reaction to a nudge should not be  New York: Cambridge, 2016\ncollected or distributed without opt-in consent, \n•  M. Scheutz, “The Affect Dilemma for Artificial \nand should only be retained transparently, \nAgents: Should We Develop Affective Artificial \nwith access restrictions in compliance with the \nAgents? ” IEEE Transactions on Affective \nhighest requirements of data privacy and law.\nComputing, vol. 3, no. 4,pp. 424–433,  \nSept. 2012.\nFurther Resources\n•  A. Grinbaum, R. Chatila, L. Devillers, J.-\n•  R. Thaler, and C. R. Sunstein, Nudge: \nG. Ganascia, C. Tessier and M. Dauchet. \nImproving Decision about Health, Wealth and \n“Ethics in Robotics Research: CERNA \nHappiness, New Haven, CT: Yale University \nRecommendations,” IEEE Robotics and \nPress, 2008.\nAutomation Magazine, vol. 24, no. 3,pp. \n139–145, Sept. 2017.\n•  L. Bovens, “The Ethics of Nudge,” in \nPreference change: Approaches from \n•  “Designing Moral Technologies: Theoretical, \nPhilosophy, Economics and Psychology, T. \nPractical, and Ethical Issues” Conference July \nGrüne-Yanoff and S. O. Hansson, Eds., Berlin, \n10–15, 2016, Monte Verità, Switzerland.\nGermany: Springer, 2008 pp. 207–219.\n•  S. D. Hunt and S. Vitell. ""A General Theory of \nMarketing Ethics."" Journal of Macromarketing, \nvol.6, no. 1, pp. 5-16, June 1986.\n•  A. McStay, Empathic Media and Advertising: \nIndustry, Policy, Legal and Citizen Perspectives \n(the Case for Intimacy), Big Data & Society, pp. \n1-11, December 2016. \n \n•  J. de Quintana Medina and P. Hermida Justo,   \n“Not All Nudges Are Automatic: Freedom of   \nChoice and Informative Nudges.” Working   \npaper presented to the European Consortium   \nfor Political Research, Joint Session of   \nWorkshops, 2016 Behavioral Change and   \nPublic Policy, Pisa, Italy, 2016.  \n•  M. D. White, The Manipulation of Choice. \nEthics and Libertarian Paternalism. New York: \nPalgrave Macmillan, 2013\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 98', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nAffective Computing\n2. There needs to be transparency regarding who \nthe intended beneficiaries are, and whether \nIssue: Governmental entities \nany form of deception or manipulation is \nmay potentially use nudging \ngoing to be used to accomplish the intended \nstrategies, for example to  goal.\npromote the performance of \ncharitable acts. Does the practice \nFurther Resources\nof nudging for the benefit  \n•  J. Borenstein and R. Arkin, “Robotic Nudges: \nof society, including nudges  \nRobotic Nudges: The Ethics of Engineering a \nby affective systems, raise   More Socially Just Human Being Just Human \nethical concerns? Being.” Science and Engineering Ethics, vol. \n22, no. 1,pp. 31–46, Feb. 2016.\nBackground •  J. Borenstein and R. Arkin. “Nudging for Good: \nRobots and the Ethical Appropriateness of \nA few scholars have noted a potentially \nNurturing Empathy and Charitable Behavior .” \ncontroversial practice of the future: allowing a \nAI and Society, vol. 32, no. 4, pp. 499–507, \nrobot or another affective system to nudge a \nNov. 2016.\nuser for the good of society6. For instance, if \nit is possible that a well-designed robot could \neffectively encourage humans to perform \ncharitable acts, would it be ethically appropriate \nIssue: Will A/IS nudging systems \nfor the robot to do so? This design possibility \nthat are not fully relevant to \nillustrates just one behavioral outcome that a \nrobot could potentially elicit from a user. the sociotechnical context \nin which they are operating \nGiven the persuasive power that an affective \ncause behaviors with adverse \nsystem may have over a user, ethical concerns \nrelated to nudging must be examined. This  unintended consequences?\nincludes the significant potential for misuse.\nBackground\nRecommendations A well-designed nudging or suggestion system \nwill have sophisticated enough technical \n1.  As more and more computing devices subtly \ncapabilities for recognizing the context in which \nand overtly influence human behavior, it \nit is applying nudging actions. Assessment of \nis important to draw attention to whether \nthe context requires perception of the scope \nit is ethically appropriate to pursue this \nor impact of the actions to be taken, the \ntype of design pathway in the context of \nconsequences of incorrectly or incompletely \ngovernmental actions. \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 99', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nAffective Computing\napplied nudges, and acknowledgement of the  •  R. C. Arkin, M. Fujita, T. Takagi, and R. \nuncertainties that may stem from long term  Hasegawa, “An Ethological and Emotional \nconsequences of a nudge7. Basis for Human- Robot Interaction.” Robotics \nand Autonomous Systems, vol. 42, no. 3–4 \n \npp.191–201, March 2003.\nRecommendations\n•  S. Omohundro “Autonomous Technology \n1.  Consideration should be given to the \nand the Greater Human Good.” Journal \ndevelopment of a system of technical \nof Experimental and Theoretical Artificial \nlicensing (“permits”) or other certification \nIntelligence, vol. 26, no. 3, pp. 303–315, 2014.\nfrom governments or non-governmental \norganizations (NGOs) that can aid users to \nunderstand the nudges from A/IS in their lives. \nIssue: When, if ever, and  \n2. User autonomy is a key and essential \nunder which circumstances,  \nconsideration that must be taken into account \nis deception performed by \nwhen addressing whether affective systems \naffective systems acceptable?\nshould be permitted to nudge human beings.\n3. Design features of an affective system that \nBackground\nnudges human beings should include the \nability to accurately distinguish between users,  Deception is commonplace in everyday human-\nincluding detecting characteristics such as  human interaction. According to Kantian ethics, \nwhether the user is an adult or a child. it is never ethically appropriate to lie, while \nutilitarian frameworks indicate that it can be \n4. Affective systems with nudging strategies  acceptable when deception increases overall \nshould incorporate a design system of  happiness. Given the diversity of views on ethics \nevaluation, monitoring, and control for  and the appropriateness of deception, should \nunintended consequences. affective systems be designed to deceive? Does \nthe non-consensual nature of deception restrict \n \nthe use of A/IS in contexts in which deception \nFurther Resources\nmay be required?\n•  J. Borenstein and R. Arkin, “Robotic Nudges: \nRobotic Nudges: The Ethics of Engineering a   \nMore Socially Just Human Being Just Human   \nBeing.” Science and Engineering Ethics, vol.   \n22, no. 1, pp. 31–46, 2016.  \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 100', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nAffective Computing\nRecommendations Further Resources\nIt is necessary to develop recommendations  •  R. C. Arkin, “Robots That Need to Mislead: \nregarding the acceptability of deception  Biologically-inspired Machine Deception.” IEEE \nperformed by A/IS, specifically with respect to  Intelligent Systems 27, no. 6, pp. 60–75, 2012.\nwhen and under which circumstances, if any,  \n•  J. Shim and R. C. Arkin, “Other-Oriented Robot \nit is appropriate.\nDeception: How Can a Robot’s Deceptive \n1.  In general, deception may be acceptable in an  Feedback Help Humans in HRI?” Eighth \naffective agent when it is used for the benefit  International Conference on Social Robotics \nof the person being deceived, not for the  (ICSR 2016), Kansas, MO., November 2016.\nagent itself. For example, deception might be \n•  J. Shim and R. C. Arkin, “The Benefits of \nnecessary in search and rescue operations or \nRobot Deception in Search and Rescue: \nfor elder- or child-care.  \nComputational Approach for Deceptive Action \n2. For deception to be used under any  Selection via Case-based Reasoning.” 2015 \ncircumstance, a logical and reasonable  IEEE International Symposium on Safety, \njustification must be provided by the designer,  Security, and Rescue Robotics (SSRR 2015), \nand this rationale should be certified by an  West Lafayette, IN, October 2015.\nexternal authority, such as a licensing body  \n•  J. Shim and R. C. Arkin, “A Taxonomy of \nor regulatory agency.\nRobot Deception and its Benefits in HRI.” \n  Proceedings of IEEE Systems, Man and \n  Cybernetics Conference, Manchester England, \n  October 2013.\n \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 101', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nAffective Computing\nSection 4—Systems Supporting \nHuman Potential\nEthically aligned design should support, not \nIssue: Will extensive use of  \nhinder, human autonomy or its expression.\nA/IS in society make our \norganizations more brittle by  Recommendations\nreducing human autonomy  1.  It is important that human workers’ \nwithin organizations, and by  interaction with other workers not always be \nintermediated by affective systems (or other \nreplacing creative, affective, \ntechnology) which may filter out autonomy, \nempathetic components  \ninnovation, and communication. \nof management chains?\n2. Human points of contact should remain \navailable to customers and other organizations \nBackground\nwhen using A/IS.\nIf human workers are replaced by A/IS, the \npossibility of corporations, governments,  3. Affective systems should be designed \nemployees, and customers discovering new  to support human autonomy, sense of \nequilibria outside the scope of what the  competence, and meaningful relationships as \norganizations’ past leadership originally foresaw  these are necessary to support a flourishing life. \nmay be unduly limited. A lack of empathy based \n4. Even where A/IS are less expensive, more \non shared needs, abilities, and disadvantages \npredictable, and easier to control than \nbetween organizations and customers causes \nhuman employees, a core network of \ndisequilibria between the individuals and \nhuman employees should be maintained at \ncorporations and governments that exist to \nevery level of decision-making in order to \nserve them. Opportunities for useful innovation \nensure preservation of human autonomy, \nmay therefore be lost through automation. \ncommunication, and innovation.\nCollaboration requires enough commonality  \nof collaborating intelligences to create empathy—  5. Management and organizational theorists \nthe capacity to model the other’s goals based   should consider appropriate use of affective \non one’s own. and autonomous systems to enhance their \nbusiness models and the efficacy of their \nAccording to scientists within several fields, \nworkforce within the limits of the preservation \nautonomy is a psychological need. Without \nof human autonomy. \nit, humans fail to thrive, create, and innovate. \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 102', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nAffective Computing\nFurther Resources 3. Utilization of customers or other end users to \nperform basic corporate business processes \n•  J. J. Bryson, “Artificial Intelligence and Pro-Social \nsuch as data entry as a barter for lower prices \nBehavior,” in Collective Agency and Cooperation \nor access, resulting potentially in reduced tax \nin Natural and Artificial Systems, C. Misselhorn, \nrevenues.\nEd., pp. 281–306, Springer, 2015.\n4. Changes to the expression of individual \n•  D. Peters, R.A. Calvo, and R.M. Ryan, \nautonomy could alter the diversity, creativity, \n“Designing for Motivation, Engagement and \nand cohesiveness of a society. It may also alter \nWellbeing in Digital Experience,” Frontiers in \nperceptions of privacy and security, and social \nPsychology– Human Media Interaction, vol. 9, \nand legal liability for autonomous expressions. \npp 797, 2018.\nRecommendations\nIssue: Does the increased access  1.  Organizations, including governments, must \nto personal information about  put a high value on individuals’ privacy and \nautonomy, including restricting the amount \nother members of our society, \nand age of data held about individuals \nfacilitated by A/IS, alter the \nspecifically.\nhuman affective experience?  \n2. Education in all forms should encourage \nDoes this access potentially \nindividuation, the preservation of autonomy, \nlead to a change in human \nand knowledge of the appropriate uses and \nautonomy?\nlimits to A/IS9.\nBackground\nFurther Resources\nTheoretical biology tells us that we should expect \n•  J. J. Bryson, “Artificial Intelligence and Pro-Social \nincreased communication—which A/IS facilitate—\nBehavior,” in Collective Agency and Cooperation \nto increase group-level investment8. Extensive \nin Natural and Artificial Systems, C. Misselhorn, \nuse of A/IS could change the expression of \nEd., pp. 281–306, Springer, 2015.\nindividual autonomy and in its place increase \ngroup-based identities. Examples of this sort   •  M. Cooke, “A Space of One’s Own: Autonomy, \nof social alteration may include: Privacy, Liberty,” Philosophy & Social Criticism, \nVol. 25, no. 1, pp. 22–53, 1999.\n1.  Changes in the scope of monitoring and \ncontrol of children’s lives by parents. •  D. Peters, R.A. Calvo, R.M. Ryan, “Designing \nfor Motivation, Engagement and Wellbeing in \n2. Decreased willingness to express opinions for \nDigital Experience” Frontiers in Psychology – \nfear of surveillance or long-term consequences \nHuman Media Interaction, vol. 9. pp 797, 2018. \nof past expressions being used in changed \ntemporal contexts.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 103', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nAffective Computing\n•  J. Roughgarden, M. Oishi and E. Akçay,  2. Design restrictions should be placed on \n“Reproductive Social Behavior: Cooperative  the systems themselves to avoid machine \nGames to Replace Sexual Selection.” Science  decisions that may alter a person’s life in \n311, no. 5763, pp. 965–969, 2006. unknown ways. Explanations should be \navailable on demand in systems that may \naffect human well-being.\nIssue: Will use of A/IS adversely \nFurther Resources\naffect human psychological and \n•  K. Kamewari, M. Kato, T. Kanda, H. Ishiguro \nemotional well-being in ways not \nand K. Hiraki. “Six-and-a-Half-Month-Old \notherwise foreseen?\nChildren Positively Attribute Goals to Human \nAction and to Humanoid-Robot Motion,” \nBackground Cognitive Development, vol. 20, no. 2, pp. \n303–320, 2005.\nA/IS may be given unprecedented access to \nhuman culture and human spaces—both physical  •  R.A. Calvo and D. Peters, Positive Computing: \nand intellectual. A/IS may communicate via  Technology for Wellbeing and Human \nnatural language, may move with humanlike form,  Potential. Cambridge, MA: MIT Press, 2014. \nand may express humanlike identity, but they \nare not, and should not be regarded as, human. \nIncorporation of A/IS into daily life may affect   \nhuman well-being in ways not yet anticipated.   \nIncorporation of A/IS may alter patterns of trust   \nand capability assessment between humans, and   \nbetween humans and A/IS.    \n \n \nRecommendations\n \n1.  Vigilance and robust, interdisciplinary, on-going   \nresearch on identifying situations where    \nA/IS affect human well-being, both positively   \nand negatively, is necessary. Evidence of   \ncorrelations between the increased use of    \nA/IS and positive or negative individual or   \nsocial outcomes must be explored.    \n \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 104', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nAffective Computing\nSection 5—Systems  \nwith Synthetic Emotions\nRecommendations\n1.  Commercially marketed A/IS should not be \nIssue: Will deployment of \npersons in a legal sense, nor marketed as \nsynthetic emotions into affective \npersons. Rather their artifactual (authored, \nsystems increase the accessibility \ndesigned, and built deliberately) nature should \nof A/IS? Will increased accessibility  always be made as transparent as possible, \nprompt unforeseen patterns of  at least at point of sale and in available \ndocumentation.\nidentification with A/IS?\n2. Some systems will, due to their application, \nrequire opaqueness in some contexts, e.g., \nBackground\nemotional therapy. Transparency in such \nDeliberately constructed emotions are designed  systems should be available to inspection by \nto create empathy between humans and artifacts,  responsible parties but may be withdrawn for \nwhich may be useful or even essential for  operational needs.\nhuman-A/IS collaboration. Synthetic emotions  \nFurther Resources\nare essential for humans to collaborate with the  \nA/IS but can also lead to failure to recognize that  •  R. C. Arkin, P. Ulam and A. R. Wagner, “Moral \nsynthetic emotions can be compartmentalized  Decision-making in Autonomous Systems: \nand even entirely removed. Potential  Enforcement, Moral Emotions, Dignity, Trust \nconsequences for humans include different  and Deception,” Proceedings of the IEEE, vol. \npatterns of bonding, guilt, and trust, whether  100, no. 3, pp. 571–589, 2012.\nbetween the human and A/IS or between \n•  R. Arkin, M. Fujita, T. Takagi and R. Hasegawa. \nother humans. There is no coherent sense in \n“An Ethological and Emotional Basis for \nwhich A/IS can be made to suffer emotional \nHuman-Robot Interaction,” Robotics and \nloss, because any such affect, even if possible, \nAutonomous Systems, vol.42, no. 3–4, \ncould be avoided at the stage of engineering, \npp.191–201, 2003.\nor reengineered. As such, it is not possible to \n•  R. C. Arkin, “Moving up the Food Chain: \nallocate moral agency or responsibility in the \nMotivation and Emotion in Behavior-based \nsenses that have been developed for human \nRobots,” in Who Needs Emotions: The Brain \nemotional bonding and thus sociality. \nMeets the Robot, J. Fellous and M. Arbib., Eds., \nNew York: Oxford University Press, 2005.\n \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 105', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nAffective Computing\n•  M. Boden, J. Bryson, D. Caldwell, et al.  •  M. Scheutz, “The Affect Dilemma for Artificial \n“Principles of Robotics: Regulating Robots in  Agents: Should We Develop Affective Artificial \nthe Real World.” Connection Science, vol. 29,  Agents?” IEEE Transactions on Affective \nno. 2, pp. 124–129, 2017. Computing, vol. 3, no. 4, pp. 424–433, 2012.\n•  J. J Bryson, M. E. Diamantis and T. D. Grant.  •  A. Sharkey and N. Sharkey. “Children, the \n“Of, For, and By the People: The Legal Lacuna  Elderly, and Interactive Robots.” IEEE Robotics \nof Synthetic Persons,” Artificial Intelligence &  & Automation Magazine, vol. 18, no. 1, pp. \nLaw, vol. 25, no. 3, pp. 273–291, Sept. 2017. 32–38, 2011.\n•  J. Novikova, and L. Watts, “Towards Artificial \nEmotions to Assist Social Coordination in HRI,”   \nInternational Journal of Social Robotics, vol. 7, \nno. 1, pp. 77–88, 2015.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 106', ""The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nAffective Computing\nThanks to the Contributors\nWe wish to acknowledge all of the people who  •  Joost Broekens – Assistant Professor \ncontributed to this chapter.  Affective Computing, Interactive Intelligence \ngroup; Department of Intelligent Systems, \nThe Affective Computing Committee\nDelft University of Technology\n•  Ronald C. Arkin (Founding Co-Chair) – \n•  Rafael Calvo – Professor & ARC Future \nRegents' Professor & Director of the Mobile \nFellow, School of Electrical and Information \nRobot Laboratory; College of Computing \nEngineering, The University of Sydney\nGeorgia Institute of Technology\n•  Laurence Devillers – Professor of Computer \n•  Joanna J. Bryson (Co-Chair) – Reader \nSciences, University Paris Sorbonne, LIMSI-\n(Associate Professor), University of Bath, \nCNRS 'Affective and social dimensions in \nIntelligent Systems Research Group, \nspoken interactions' - member of the French \nDepartment of Computer Science\nCommission on the Ethics of Research in \n•  John P. Sullins (Co-Chair) – Professor of  Digital Sciences and Technologies (CERNA)\nPhilosophy, Chair of the Center for Ethics Law \n•  Jonathan Gratch – Research Professor of \nand Society (CELS), Sonoma State University\nComputer Science and Psychology, Director \n•  Genevieve Bell – Intel Senior Fellow Vice  for Virtual Human Research, USC Institute for \nPresident, Corporate Strategy Office Corporate  Creative Technologie\nSensing and Insights\n•  Mark Halverson – Founder and CEO at \n•  Jason Borenstein – Director of Graduate  Human Ecology Holdings and Precision \nResearch Ethics Programs, School of Public  Autonomy\nPolicy and Office of Graduate Studies, Georgia \n•  John C. Havens – Executive Director, The \nInstitute of Technology\nIEEE Global Initiative on Ethics of Autonomous \n•  Cynthia Breazeal – Associate Professor of  and Intelligent Systems; Executive Director, \nMedia Arts and Sciences, MIT Media Lab;  The Council on Extended Intelligence; Author, \nFounder & Chief Scientist of Jibo, Inc.   Heartificial Intelligence: Embracing Our \n  Humanity to Maximize Machines  \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 107"", 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nAffective Computing\n•  Noreen Herzfeld – Reuter Professor of  •  Matthias Scheutz – Professor, Bernard M. \nScience and Religion, St. John’s University Gordon Senior Faculty Fellow, Tufts University \nSchool of Engineering\n•  Chihyung Jeon – Assistant Professor, Graduate \nSchool of Science and Technology Policy, Korea  •  Robert Sparrow – Professor, Monash \nAdvanced Institute of Science and Technology  University, Australian Research Council “Future \n(KAIST) Fellow”, 2010-15.\n•  Preeti Mohan – Software Engineer at  •  Cherry Tom – Emerging Technologies \nMicrosoft and Computational Linguistics  Intelligence Manager, IEEE Standards \nMaster’s Student at the University of  Association\nWashington\nFor a full listing of all IEEE Global Initiative \n•  Bjoern Niehaves – Professor, Chair of \nMembers, visit standards.ieee.org/content/dam/\nInformation Systems, University of Siegen\nieee-standards/standards/web/documents/other/\n•  Rosalind Picard – Rosalind Picard, (Sc.D,  ec_bios.pdf. \nFIEEE) Professor, MIT Media Laboratory, \nFor information on disclaimers associated with \nDirector of Affective Computing Research; \nEAD1e, see How the Document Was Prepared.\nFaculty Chair, MIT Mind+Hand+Heart; Co-\n       \nfounder & Chief Scientist, Empatica Inc.; Co-\nfounder, Affectiva Inc.\n•  Edson Prestes – Professor, Institute of \nInformatics, Federal University of Rio Grande \ndo Sul (UFRGS), Brazil; Head, Phi Robotics \nResearch Group, UFRGS; CNPq Fellow\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 108', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nAffective Computing\nEndnotes\n1 See B. J. Fogg, Persuasive technology. Ubiquity,  6 See, for example, J. Borenstein and R. Arkin. \nDecember: 2, 2002.  “Robotic Nudges: The Ethics of Engineering a \nMore Socially Just Human Being.” Science and \n2 See S. Turkle, W. Taggart , C.D. Kidd, and O. Daste, \nEngineering Ethics, vol. 22, no. 1 (2016): 31–46.\n“Relational artifacts with children and elders: the \ncomplexities of cybercompanionship, Connection  7 See S. Omohundro, “Autonomous Technology \nScience, vol. 18, no. 4, 2006. and the Greater Human Good.” Journal of \nExperimental and Theoretical Artificial Intelligence \n3 A discussion of intimate robots for therapeutic \n26, no. 3 (2014): 303–315.\nand personal use is outside of the scope of Ethically \nAligned Design, First Edition. For further treatment,  8 See J. Roughgarden, M. Oishi, and E. Akçay. \namong others, see J. P. Sullins, “Robots, Love, and  “Reproductive Social Behavior: Cooperative Games \nSex: The Ethics of Building a Love Machine.” IEEE  to Replace Sexual Selection.” Science 311, no. 5763 \nTransactions on Affective Computing 3, no. 4  (2006): 965–969.\n(2012): 398–409.\n9 See the Well-being chapter of this Ethically \n4 See R. Thaler, and C. R. Sunstein. Nudge:  Aligned Design, First Edition.\nImproving Decision about Health, Wealth and \nHappiness, New Haven, CT: Yale University Press, \n2008.\n5 See J. de Quintana Medina and P. Hermida Justo. \n“Not All Nudges Are Automatic: Freedom of Choice \nand Informative Nudges.” Working paper presented \nto the European Consortium for Political Research, \nJoint Session of Workshops, 2016 Behavioral \nChange and Public Policy, Pisa, Italy, 2016; and \nM. D. White, The Manipulation of Choice. Ethics \nand Libertarian Paternalism. New York: Palgrave \nMacmillan, 2013.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 109', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems \nPersonal Data and Individual Agency\nRegulations like the General Data Protection Regulation (GDPR) and the California Consumer \nPrivacy Act (CCPA) of 2018 are helping to improve personal data protection. But legal \ncompliance is not enough to mitigate the ethical implications and core challenges to human \nagency embodied by algorithmically driven behavioral tracking or persuasive computing.  \nThe core of the issue is one of parity. \nHumans cannot respond on an individual basis to every algorithm tracking their behavior \nwithout technological tools supported by policy allowing them to do so. Individuals may provide \nconsent without fully understanding specific terms and conditions agreements. But they are \nalso not equipped to fully recognize how the nuanced use of their data to inform personalized \nalgorithms affects their choices at the risk of eroding their agency. \nHere we understand agency as an individual’s ability to influence and shape their life trajectory \nas determined by their cultural and social contexts. Agency in the digital arena enables an \nindividual to make informed decisions where their own terms and conditions can be  \nrecognized and honored at an algorithmic level.  \nTo strengthen individual agency, governments and organizations must test and implement \ntechnologies and policies that let individuals create, curate, and control their online agency \nas associated with their identity. Data transactions should be moderated and case-by-case \nauthorization decisions from the individual as to who can process what personal data  \nfor what purpose.  \nSpecifically, we recommend governments and organizations: \n•  Create: Provide every individual with the means to create and project their own terms \nand conditions regarding their personal data that can be read and agreed to at a machine-\nreadable level.\n•  Curate: Provide every individual with a personal data or algorithmic agent which they curate \nto represent their terms and conditions in any real, digital, or virtual environment.\n•  Control: Provide every individual access to services allowing them to create a trusted \nidentity to control the safe, specific, and finite exchange of their data. \nThree sections of this chapter reflect these core ideals regarding human agency. \nA fourth section addresses issues surrounding personal data and individual agency relating to children.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 110', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nPersonal Data and Individual Agency\nSection 1—Create \nTo retain agency in the algorithmic era, each  Most individuals also believe controlling their \nindividual must have the means to create and  personal data only happens on the sites or social \nproject their own terms and conditions regarding  networks to which they belong and have no idea \ntheir personal data. These must be readable and  of the consequences of how that data may be \nusable by both humans and machines.    used by others in the future. Agreeing to most \nstandard terms and conditions on these sites \nlargely means users consent to give up control of \ntheir personal data rather than play a meaningful \nrole in defining and curating its downstream use.  \nIssue: What would it mean for \na person to have individually  The scope of how long one should or could \ncontrolled terms and conditions  control the downstream use of their data can be \ndifficult to calculate as consent-based models \nfor their personal data? \nof personal data have trained users to release \nrights on any claims for use of their data which \nBackground are entirely provided to the service, manufacturer, \nand their partners. However, models like \nPart of providing individually controlled terms \nYouTube’s Content ID provide a form of \nand conditions for personal data is to help each \nprecedent for thinking about how an individual’s \nperson consider what their preferences are \ndata could be technically protected where it is \nregarding their data versus dictating how they \nconsidered as an asset they could control and \nneed to share it. While questions along these \ncopyright. Here is language from YouTube’s site \nlines are framed in light of a person’s privacy, \nabout the service: “Copyright owners can use \ntheir preferences also reveal larger values for \na system called Content ID to easily identify \nindividuals. The ethical issue is whether A/IS act \nand manage their content on YouTube. Videos \nin accordance with these values.\nuploaded to YouTube are scanned against a \nThis process of investigating one’s values to  database of files that have been submitted to us \nidentify these preferences is a powerful step  by content owners.” In this sense, the question of \ntowards regaining data agency. The point is not  how long or how far downstream one’s personal \nonly that a person’s data are protected, but also  data should be protected takes on the same logic \nthat by curating these answers they become  of how long a corporation’s intellectual property \neducated about how important their information  or copyrights could be protected based on initial \nis in the context of how it is shared.  legal terms set. \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 111', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nPersonal Data and Individual Agency\nOne challenge is how to define use of data  Specifically: \nthat can affect the individual directly, versus \n•  Personal data access and consent should be \nuse of aggregated data. For example, an \nmanaged by the individual using their curated \nindividual subway user’s travel card, tracking \nterms and conditions that provide notification \ntheir individual movements, should be protected \nand an opportunity for consent at the time \nfrom uses that identify or profile that individual \ndata are exchanged, versus outside actors \nto make inferences about his/her likes or \nbeing able to access personal data without an \nlocation generally. But data provided by a user \nindividual’s awareness or control.   \ncould be included in an overall travel system’s \nmanagement database, aggregated into patterns  •  Terms should be presented in a way that \nfor scheduling and maintenance as long as  allows a user to easily read, interpret, \nthe individual-level data are deleted. Where  understand, and choose to engage with any \nusers have predetermined via their terms and  A/IS. Consent should be both conditional \nconditions that they are willing to share their data  and dynamic, where “dynamic” means \nfor these travel systems, they can meaningfully  downstream uses of a person’s data must be \narticulate how to share their information.  explicitly called out, allowing them to cancel \na service and potentially rescind or “kill” any \nUnder current business models, it is common  data they have shared with a service to date \nfor people to consent to the sharing of discrete  via the use of a “Smart Contract” or specific \ndata like credit card transaction data, answers  conditions as described in mutual terms and \nto test questions, or how many steps they walk.  conditions between two parties at the time of \nHowever, once aggregated these data and  exchange.\nthe associated insights may lead to complex \n•  For further information on these issues, \nand sensitive conclusions being drawn about \nplease see the following section in regard to \nindividuals. This end use of the individual’s data \nalgorithmic agents and their application. \nmay not have been part of the initial sharing \nagreement. This is why models for terms and \nFurther Resources\nconditions created for user control typically alert \npeople via onscreen or other warning methods  •  IEEE P7012™ - IEEE Standards Project for \nwhen their predetermined preferences are   Machine Readable Personal Privacy Terms.  \nnot being honored.   This approved standardization project \n(currently in development) directly honors  \nRecommendation the goals laid out in Section One of  \nIndividuals should be provided tools that produce  this document.  \nmachine-readable terms and conditions that are  •  The Personalized Privacy Assistant Project \ndynamic in nature and serve to protect their data  Carnegie Mellon University. https://\nand honor their preferences for its use.   privacyassistant.org, 2019.  \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 112', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nPersonal Data and Individual Agency\n•  M. Orcutt, “Personal AI Privacy Watchdog  A Web-based System for Privacy-aware IoT \nCould Help You Regain Control of Your Data”  Service Discovery and Interaction,” 2018 \nMIT Technology Review, May 11, 2017. IEEE International Conference on Pervasive \nComputing and Communications  \n•  M. Hintze, Privacy Statements: Purposes, \nWorkshops (PerCom Workshops),  \nRequirements, and Best Practices. Cambridge, \nAthens, pp. 107-112, 2018.\nU.K.: Cambridge University Press, 2017. \n•  L. Cranor, M. Langheinrich, M. Marchiori, \n•  D. J. Solove, “Privacy self-management and \nM. Presler-Marshall, and J. Reagle, “The \nthe consent dilemma, Harvard Law Review, \nPlatform for Privacy Preferences 1.0 (P3P1.0) \nvol. 126, no. 7, pp. 1880–1903, May 2013.\nSpecification,” W3C Recommendation, \n•  N. Sadeh, M. Degeling, A. Das, A. S. Zhang, A. \n[Online]. Available: www.w3.org/TR/P3P/, Apr. \nAcquisti, L. Bauer, L. Cranor, A. Datta, and D. \n2002.\nSmullen, A Privacy Assistant for the Internet of \n•  L. F. Cranor, “Personal Privacy Assistants in \nThings: https://www.usenix.org/sites/default/\nthe Age of the Internet of Things,” in World \nfiles/soups17_poster_sadeh.pdf\nEconomic Forum Annual Meeting, 2016.\n•  H. Lee, R. Chow, M. R. Haghighat, H. M. \nPatterson and A. Kobsa, “IoT Service Store: \n \nSection 2—Curate\nTo retain agency in the algorithmic era, we must \nprovide every individual with a personal data or \nIssue: What would it mean for \nalgorithmic agent they curate to represent their \na person to have an algorithmic \nterms and conditions in any real, digital, or virtual \nagent helping them actively \nenvironment. This “agent” would be empowered \nto act as an individual’s legal proxy in the digital  represent and curate their terms \nand virtual arena. Oftentimes, the functionality of  and conditions at all times?\nthis agent will be automated, operating along the \nlines of current ad blockers which do not permit \nBackground\nprespecified algorithms to access a user’s data. \nFor other situations that might be unique or new  While it’s essential to create your own terms \nto this agent, a user could specify that notices  and conditions to broadcast your preferences, \nor updates be sent on a case-by-case basis to  it’s also important to recognize that humans do \ndetermine where there could be a concern.  not operate at an algorithmic speed or level. A \nsignificant part of retaining your agency in this \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 113', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nPersonal Data and Individual Agency\nway involves identifying trusted services that  Apple acquired the startup points to the potential \ncan essentially act on your behalf when making  for the successful business model of user-centric \ndecisions about your data.   data exchange and putting individuals at the \ncenter of their data. \nPart of this logic entails putting you “at the center \nof your data”. One of the greatest challenges  A person’s A/IS agent is a proactive algorithmic \nto user agency is that once you give your data  tool honoring their terms and conditions in the \naway, you do not know how it is being used or  digital, virtual, and physical worlds. Any public \nby whom. But when all transactions about your  space where a user may not be aware they are \ndata go through your A/IS agent honoring your  under surveillance by facial recognition, biometric, \npreferences, you have better opportunities to  or other tools that could track, store, and utilize \ncontrol how your information is shared.  their data can now provide overt opportunity for \nconsent via an A/IS agent platform. Even where \nAs an example, with medical data—while it is \nan individual is not sure they are being tracked, \nassumed most would share all their medical data \nby broadcasting their terms and conditions \nwith their spouse—most would also not wish to \nvia digital means, they can demonstrate their \nshare that same amount of data with their local \npreferences in the public arena. Via Bluetooth \ngym. This is an issue that extends beyond privacy, \nor similar technologies, individuals could offer \nmeaning one’s cultural or individual preferences \ntheir terms and conditions in a ubiquitous and \nabout what personal information to share, \nalways-on manner. This means even when \nto utility and clarity. This type of sharing also \nan individual’s terms and conditions are not \nbenefits users or organizations on the receiving \nhonored, people would have the ability to \nend of data from these exchanges. For instance, \ndemonstrate their desire not to be tracked which \nthe local gym in the previous example may only \ncould provide a methodology for the democratic \nneed basic heart or general health information \nright to protest in a peaceful manner. And where \nand would actually not wish to handle or store \nthose terms and conditions are recognized―\nsensitive cancer or other personal health data for \nmeaning technically recognized even if they are \nreasons of liability.  \nnot honored―one’s opinions could be formally \nlogged via GPS and timestamp data.\nA precedent for this type of patient- or user-\ncentric model comes from Gliimpse, a service \nThe A/IS agent could serve as an educator and \ndescribed by Jordan Crook from TechCrunch in \nnegotiator on behalf of its user by suggesting \nhis article, “Apple acquired Gliimpse, a personal \nhow requested data could be combined with \nhealth data startup”: “Gliimpse works by letting \nother data that has already been provided, inform \nusers pull their own medical info into a single \nthe user if data are being used in a way that was \nvirtual space, with the ability to add documents \nnot authorized, or make recommendations to the \nand pictures to fill out the profile. From there, \nuser based on a personal profile. As a negotiator, \nusers can share that data (as a comprehensive \nthe agent could broker conditions for sharing \npicture) to whomever they wish.” The fact that \ndata and could include payment to the user as a \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 114', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nPersonal Data and Individual Agency\nterm, or even retract consent for the use of data  without constraints that would make some \npreviously authorized, for instance, if a breach of  guardians inherently incompatible or subject \nconditions was detected. to censorship.\n•  Vulnerable parts of the population will need \nRecommendations protection in the process of granting access.\nAlgorithmic agents should be developed for \nFurther Resources\nindividuals to curate and share their personal \ndata. Specifically: •  IEEE P7006™ - IEEE Standards Project on \nPersonal Data AI Agent Working Group. \n•  For purposes of privacy, a person must be \nDesigned as a tool to allow any individual \nable to set up complex permissions that \nto create their own personal “terms and \nreflect a variety of wishes.\nconditions” for their data, the AI Agent will also \n•  The agent should help a person foresee  provide a technological tool for individuals to \nand mitigate potential ethical implications of  manage and control their identity in the digital \nspecific machine learning data exchanges. and virtual world.\n•  A user should be able to override his/her  •  Tools allowing an individual to create a form \npersonal agents should he/she decide that  of an algorithmic guardian are often labeled \nthe service offered is worth the conditions  as PIMS, or Personal Information Management \nimposed. Services. Nesta in the United Kingdom was \n•  An agent should enable machine-to-machine  one of the funders of early research about \nprocessing of information to compare,  PIMS conducted by CtrlShift.\nrecommend, and assess offers and services.\n•  Institutional systems should ensure support \nfor and respect the ability of individuals to \nbring their own agent to the relationship \n \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 115', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nPersonal Data and Individual Agency\nSection 3—Control \nTo retain agency in the algorithmic era, we must  Personas, identities that act as proxies, and \nprovide every individual access to services allowing  pseudonymity are also critical requirements for \nthem to create a trusted identity to control the  privacy management and agency. These help \nsafe, specific, and finite exchange of their data.  individuals select an identity that is appropriate \nfor the context they are in or wish to join. In these \nsettings, trust transactions can still be enabled \nIssue: How can we increase  without giving up the “root” identity of the user. \nFor example, it is possible to validate that a user \nagency by providing individuals \nis over eighteen or is eligible for a service. \naccess to services allowing them \nto create a trusted identity to  Attribute verification will play a significant role \nin enabling individuals to select the identity that \ncontrol the safe, specific, and \nprovides access without compromising agency. \nfinite exchange of their data?\nThis type of access is especially important in \ndealing with the myriad of algorithms interacting \nBackground with narrow segments of our identity data. In \nthese situations, individuals typically are not aware \nPervasive behavior-tracking adversely affects \nof the context for how their data will be used.\nhuman agency by recognizing our identity in \nevery action we take on and offline. This is why \nidentity as it relates to individual data is emerging  Recommendation\nat the forefront of the risks and opportunities \nIndividuals should have access to trusted identity \nrelated to use of personal information for A/IS. \nverification services to validate, prove, and \nAcross the identity landscape there is increasing \nsupport the context-specific use of their identity.  \ntension between the requirement for federated \nidentities versus a range of identities. In federated \nFurther Resources\nidentities, all data are linked to a natural and \nidentified person. When one has a range of  •  Sovrin Foundation, The Inevitable Rise of Self-\nidentities, or personas, these can be context  Sovereign Identity, Sept. 29, 2016.\nspecific and determined by the use case. New \n•  T. Ruff, “Three Models of Digital Identity \nmovements, such as “Self-Sovereign Identity”— \nRelationships,” Evernym, Apr. 24, 2018. \ndefined as the right of a person to determine his \n•  C. Pettey, The Beginner’s Guide to \nor her own identity—are emerging alongside legal \nDecentralized Identity. Gartner, 2018.\nidentities, e.g., those issued by governments, \nbanks, and regulatory authorities, to help put  •  C. Allen, The Path to Self-Sovereign Identity. \nindividuals at the center of their data in the  GitHub, 2017. \nalgorithmic age.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 116', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nPersonal Data and Individual Agency\nSection 4—Children’s Data Issues\nWhile the focus of this chapter is to provide all  As children post, click, search, and share \nindividuals with agency regarding their personal  information, their data are linked to various \ndata, some sectors of society have little or no  profiles, grouped into segmented audiences, and \ncontrol. For some elderly individuals or the  fed into machine learning algorithms. Some of \nmentally ill, it is because they have been found  these may be designed to target campaigns that \nto not have “mental capacity”, and for prisoners  increase sales, influence sentiment, encourage \nin the criminal justice system, society has taken  online games, impact social networks, or \ncontrol away as punishment. In the case of  influence religious and political views. Data fed \nchildren, this is because they are considered  into algorithmic advertising is not only gathered \nhuman beings in development with   from children’s online actions but also from \nevolving capacities. their devices. An example of device data is \nbrowser fingerprinting.3 It includes a set of data \nWe examine the issues of children as an example \nabout a child’s browser or operating system. \ncase and recommend either regulation or a \nFingerprinting vastly increases privacy risks \ntechnical architecture that provides a veil and \nbecause it is used to link to an individual. \nbuffer from harm until a child is at an age where \nthey can claim personal responsibility for   Increasingly, children’s beliefs and social \ntheir decisions.  norms are established by what they see and \nexperience online. Their actions reflect what they \nIn many parts of the world, children are viewed \nbelieve is possible and expected. The report, \nby the law as being primarily charges of their \n“Digital Deceit: Technologies Behind Precision \nparents who make choices on their behalf. In \nPropaganda on the Internet”4, explains how \nEurope, however, the state has a role in ensuring \ncompanies collect, process, and then monetize \nthe “best interests of the child”1 2. In schools, the \npersonal preferences, socioeconomic status, \ntwo interests operate side-by-side, with parents \nfears, political and religious beliefs, location,  \nbeing given some control over their child’s \nand patterns of internet use.\neducation but with many decisions being made \nby the schools. Companies, governments, political parties, and \nphilosophical and religious organizations use data \nMany of the issues described above concern \navailable about students and children to influence \nchoices around personal data and the future \nhow they spend their time, money, and the \nimpacts of how the data are gathered and shared. \npeople or institutions they trust and with whom \nChildren are at the forefront of technological \nthey spend time and build relationships. \ndevelopments with future educational and \nrecreational technology gathering data from them  Many aspects of a child’s life can be digitized. \nall day at school and intelligent toys throughout  Their behavioral, device, and network data \ntheir time at home.  are combined and used by machine learning \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 117', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nPersonal Data and Individual Agency\nalgorithms to determine the information and  about everything from bullying, loneliness, and \ncontent that best achieve the educational goals  stomachaches. Recently it was disclosed that \nof the schools and the economic goals of the  although the collected data was presented \nadvertisers and platform companies.   as anonymous, they were not. Data were \nstored with social security numbers, correlated \nwith other test data, and even used in case \nmanagement by some Danish municipalities.5 \nIssue: Mass personalization    Commercial profiling and correlation of different \nof instruction sets of personal data may further affect these \nchildren in future job or educational situations.\nBackground \nRecommendation\nThe mass personalization of education offers \nbetter education for all at very low cost through  Educational data offer a unique opportunity \nA/IS-enabled computer-based instruction that  to model individuals’ thought processes and \npromises to free up teachers to work with kids  could be used to predict or change individuals’ \nindividually to pursue their passions. These  behavior in many situations. Governments and \napplications will rely on the continuous gathering  organizations should classify educational data  \nof personal data regarding mood, thought  as being sensitive and implement special \nprocesses, private stories, physiological data,  protective standards. \nand more. The data will be used to construct a \nChildren’s data should be held in “escrow”  \ncomputational model of each child’s interests, \nand not used for any commercial purposes  \nunderstanding, strengths, and weaknesses. The \nuntil a child reaches the age of majority and is \nmodel provides an intimate understanding of \nable to authorize use as they choose.\nhow they think, what they understand, how they \nprocess information, or react to new information; \nall of which can be used to drive instructional  Further Resources\ncontent and feedback. \n•  The journal of the International Artificial \nSharing of this data between classes, enabling it  Intelligence in Education Society:  \nto follow students through their schooling, will  http://iaied.org/journal/\nmake the models more effective and beneficial \n•  Deeper discussion and bibliography of future \nto children, but it also exposes children and their \ntrends of AI-based education with utopian \nfamilies to social control. If performance data are \nand dystopian case scenarios: N. Pinkwart, \ncorrelated with social data on a family, it could \n“Another 25 Years of AIED? Challenges and \nbe used by social authorities in decision-making \nOpportunities for Intelligent Educational \nabout the family. For example, since 2015-\nTechnologies of the Future,” International \n2018, well-being digital tests were performed \nJournal of Artificial Intelligence in Education, \nin schools in Denmark. Children were asked \nvol. 26, no. 2, pp. 771–783, 2016. [Online]. \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 118', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nPersonal Data and Individual Agency\nAvailable: https://doi.org/10.1007/s40593- There are many gaps in current student data \n016-0099-7 [Accessed Dec. 2018]. regulation. In June 2018, CLIP, The Center \non Law and Information Policy at Fordham \n•  Information Commissioners Office (ico.),“What \nLaw School published, ”Transparency and the \nif we want to profile children or make \nMarketplace for Student Data”.6 This study \nautomated decisions about them?” https://ico.\nconcluded that “student lists are commercially \norg.uk/for-organisations/guide-to-the-general-\navailable for purchase on the basis of ethnicity, \ndata-protection-regulation-gdpr/children-and-\naffluence, religion, lifestyle, awkwardness, and \nthe-gdpr/what-if-we-want-to-profile-children-\neven a perceived or predicted need for family \nor-make-automated-decisions-about-them/\nplanning services”. Fordham found that the data \n•  K. Firth-Butterfield, “What happens when your \nmarket is becoming one of the largest and most \nchild’s friend is an AI toy that talks back?” \nprofitable marketplaces in the United States. \nin World Economic Forum: Generation AI, \nData brokers have databases that store billions \nhttps://www.weforum.org/agenda/2018/05/\nof data elements on nearly every United States \ngeneration-ai-what-happens-when-your-childs-\nconsumer. However, information from students \ninvisible-friend-is-an-ai-toy-that-talks-back/,  \nin the pursuit of an education should not be \nMay 22, 2018.\nexploited and commercialized without restraint.\nFordham researchers found at least 14 data \nbrokers who advertise the sale of student \nIssue: Technology choice-making  information. One sold lists of students as young \nin schools as two years old. Another sold lists of student \nprofiles on the basis of ethnicity, religion, \neconomic factors, and even gawkiness. \nBackground\n \nChildren, as minors, have no standing to give  Recommendation\nor deny consent, or to control the use of their \npersonal data. Parents only have limited choices  Local and national educational authorities must \nin what are often school-wide implementations  work to develop policies surrounding students’ \nof educational technology. Examples include the  personal data with all stakeholders: administrators, \nuse of Google applications, face recognition in  teachers, technology providers, students, and \nsecurity systems, and computer driven instruction  parents in order to balance the best educational \nas described above. In many cases, parents’   interests of each child with the best practices to \nonly choice would be to send their children   ensure safety of their personal data. Such efforts \nto a different school, but that choice is   will raise awareness among all stakeholders of the \nseldom available. promise and the compromises inherent in new \neducational technologies. \nHow should schools make these choices? How \n \nmuch input should parents have? Should parents \n \nbe able to demand technology-free teaching?\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 119', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nPersonal Data and Individual Agency\nFurther Resources There is currently little regulatory oversight. In \nthe United States COPPA7 offers some protection \n•  Common Sense Media privacy evaluation \nfor the data of children under 13. Germany has \nproject: https://www.commonsense.org/\noutlawed such toys using legislation banning \neducation/privacy\nspying equipment enacted in 1981. Corporate \n•  D. T. Ritvo, L. Plunkett, and P. Haduong,”Privacy  A/IS are being embodied in toys and given to \nand Student Data: Companion Learning  children to play with, to talk to, tell stories to, and \nTools.” Berkman Klein Center for Internet  to explore all the personal development issues \nand Society at Harvard University, 2017.  that we learn about in private play as children.\n[Online]. Available: http://blogs.harvard.\nedu/youthandmediaalpha/files/2017/03/ Recommendations\nPrivacyStudentData_Companion_Learning_\nChild data should be held in “escrow” and \nTools.pdf [Accessed Dec. 2018].\nnot used for any commercial purposes until a \nchild reaches the age of majority and is able to \n•  F. Alim, N. Cardozo, G. Gebhart, K. Gullo, and \nauthorize use as they choose.\nA. Kalia, “Spying on Students: School-Issued \nDevices and Student Privacy,” Electronic \nGovernments and organizations need to educate \nFrontier Foundation, https://www.eff.org/wp/\nand inform parents of the mechanisms of \nschool-issued-devices-and-student-privacy, \nA/IS and data collection in toys and the possible \nApril 13, 2017.\nimpact on children in the future. \n•  N. C. Russell, J. R. Reidenberg, E. Martin, and \nT. Norton, “Transparency and the Marketplace  Further Resources\nfor Student Data,” Virginia Journal of Law and \n•  K. Firth-Butterfield, “What happens when your \nTechnology, Forthcoming. Available at SSRN: \nchild’s friend is an AI toy that talks back?” \nhttps://ssrn.com/abstract=3191436, June 6, \nin World Economic Forum: Generation AI, \n2018. \nhttps://www.weforum.org/agenda/2018/05/\ngeneration-ai-what-happens-when-your-childs-\ninvisible-friend-is-an-ai-toy-that-talks-back/, \nIssue: Intelligent toys May 22, 2018.\n•  D. Basulto,“How artificial intelligence is \nBackground  moving from the lab to your kid’s playroom,” \nWashington Post, Oct. 15, 2015. [Online]. \nChildren will not only be exposed to A/IS at \nAvailable: https://www.washingtonpost.\nschool but also at home, while they play and \ncom/news/innovations/wp/2015/10/15/\nwhile they sleep. Toys are already being sold that \nhow-artificial-intelligence-is-moving-from-\noffer interactive, intelligent opportunities for play. \nthe-lab-to-your-kids-playroom/?utm_\nMany of them collect video and audio data which \nterm=.89a1431a05a7  \nis stored on company servers and either is or \n[Accessed Dec. 1, 2018].\ncould be mined for profiling or marketing data. \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 120', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nPersonal Data and Individual Agency\n•  S. Chaudron, R. Di Gioia, M. Gemo, D.  European Union, 2017.\nHolloway, J. Marsh, G. Mascheroni J. Peter, \n•  Z. Kleinman, “Alexa, are you friends with our \nand D. Yamada-Rice , http://publications.jrc.\nkids?” BBC News, July 16, 2018. [Online]. \nec.europa.eu/repository/handle/JRC105061, \nAvailable: https://www.bbc.com/news/\n2016. \ntechnology-44847184.%5b. [Accessed Dec. \n•  S. Chaudron, R. Di Gioia, M. Gemo, D.  1, 2018].\nHolloway, J. Marsh, G. Mascheroni, J. Peter, \n•  J. Wakefield, “Germany bans children’s \nD. Yamada-Rice Kaleidoscope on the Internet \nsmartwatches.” BBC News, Nov. 17 2017. \nof Toys - Safety, security, privacy and societal \n[Online]. Available: https://www.bbc.co.uk/\ninsights, EUR 28397 EN, doi:10.2788/05383, \nnews/technology-42030109. [Accessed Dec. \nLuxembourg: Publications Office of the \n2018].\nThanks to the Contributors\nWe wish to acknowledge all of the people who  •  Ajay Bawa – Technology Innovation Lead, \ncontributed to this chapter.  Avanade Inc.\n  •  Ariel H. Brio – Privacy and Data Counsel at \nThe Personal Data and Individual  Sony Interactive Entertainment\nAgency Committee\n•  Walter Burrough – Co-Founder, Augmented \n•  Katryna Dow (Co-Chair) – CEO & Founder at  Choice; PhD Candidate (Computer Science) – \nMeeco Serious Games Institute\n•  John C. Havens (Co-Chair) – Executive  •  Danny W. Devriendt – Managing director of \nDirector, The IEEE Global Initiative on Ethics  Mediabrands Dynamic (IPG) in Brussels, and \nof Autonomous and Intelligent Systems;  the CEO of the Eye of Horus, a global think-\nExecutive Director, The Council on Extended  tank for communication-technology related \nIntelligence; Author, Heartificial Intelligence:  topics \nEmbracing Our Humanity to Maximize  •  Dr. D. Michael Franklin – Assistant \nMachines  Professor, Kennesaw State University, Marietta \n•  Mads Schaarup Andersen – Senior Usable  Campus, Marietta, GA\nSecurity Expert in the Alexandra Institute’s  •  Jean-Gabriel Ganascia – Professor, \nSecurity Lab  University Pierre et Marie Curie; LIP6 \n  Laboratory ACASA Group Leader\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 121', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nPersonal Data and Individual Agency\n•  Bryant Joseph Gilot, MD CM DPhil MSc –  •  Hiroshi Nakagawa – Professor, The \nCenter for Personalised Medicine, University of  University of Tokyo, and AI in Society Research \nTuebingen Medical Center, Germany & Chief  Group Director at RIKEN Center for Advanced \nMedical Officer, Blockchain Health Co., San  Intelligence Project (AIP)\nFrancisco\n•  Sofia C. Olhede – Professor of Statistics and \n•  David Goldstein – Seton Hall University an Honorary Professor of Computer Science \nat University College London, London, U.K; \n•  Adrian Gropper, M.D. – CTO, Patient Privacy \nMember of the Programme Committee of \nRights Foundation; HIE of One Project\nthe International Centre for Mathematical \n•  Marsali S. Hancock – Chair, IEEE Standards \nSciences. \nfor Child and Student Data governance, CEO \n•  Ugo Pagallo – University of Turin Law \nand Co-Foundation EP3 Foundation.F\nSchool; Center for Transnational Legal Studies, \n•  Gry Hasselbalch – Founder DataEthics, \nLondon; NEXA Center for Internet & Society, \nAuthor, Data Ethics - The New Competitive \nPolitecnico of Turin \nAdvantage\n•  Dr. Juuso Parkkinen – Senior Data Scientist, \n•  Yanqing Hong – Graduate, University of \nNightingale Health; Programme Team \nUtrecht Researcher at Tsinghua University\nMember, MyData 2017 conference\n•  Professor Meg Leta Jones – Assistant \n•  Eleonore Pauwels – Research Fellow on \nProfessor in the Communication, Culture & \nAI and Emerging Cybertechnologies, United \nTechnology program at Georgetown University\nNations University (NY) and Director of the AI \n•  Mahsa Kiani – Chair of Student Activities,  Lab, Woodrow Wilson International Center for \nIEEE Canada; Vice Editor, IEEE Canada  Scholars (DC)\nNewsletter (ICN); PhD Candidate, Faculty \n•  Dr. Deborah C. Peel – Founder, Patient \nof Computer Science, University of New \nPrivacy Rights & Creator, the International \nBrunswick\nSummits on the Future of Health Privacy\n•  Brenda Leong – Senior Counsel, Director of \n•  Walter Pienciak – Principal Architect, \nOperations, The Future of Privacy Forum\nAdvanced Cognitive Architectures, Ltd.\n•  Emma Lindley – Founder, Innovate Identity\n•  Professor Serena Quattrocolo – University \n•  Ewa Luger – Chancellor’s Fellow at the  of Turin Law School\nUniversity of Edinburgh, within the Design \n•  Carolyn Robson – Group Data Privacy \nInformatics Group\nManager at Etihad Aviation Group\n•  Sean Martin McDonald – CEO of \n•  Gilad Rosner – Internet of Things Privacy \nFrontlineSMS, Fellow at Stanford’s Digital Civil \nForum; Horizon Digital Economy Research \nSociety Lab, Principal at Digital Public\nInstitute, UK; UC Berkeley Information School\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 122', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nPersonal Data and Individual Agency\n•  Prof. Dr.-Ing. Ahmad-Reza Sadeghi –  For a full listing of all IEEE Global Initiative \nDirector System Security Lab, Technische  Members, visit standards.ieee.org/content/dam/\nUniversität Darmstadt / Director Intel  ieee-standards/standards/web/documents/other/\nCollaborative Research Institute for Secure  ec_bios.pdf. \nComputing\nFor information on disclaimers associated with \n•  Rose Shuman – Partner at BrightFront Group \nEAD1e, see How the Document Was Prepared.\n& Founder, Question Box\n•  Dr. Zoltán Szlávik – Lead/Researcher, IBM \n \nCenter for Advanced Studies Benelux\n•  Udbhav Tiwari – Centre for Internet and \nSociety, India\nEndnotes\n1  Europäische Union, Europäischer Gerichtshof  5  Case described in Danish here https://dataethics.\nfür Menschenrechte, & Europarat (Eds.). (2015).  eu/trivsel-enhver-pris/\nHandbook on European law relating to the rights of \n6  Russell, N. Cameron, Reidenberg, Joel R., Mar-\nthe child. Luxembourg: Publications Office of the \ntin, Elizabeth, and Norton, Thomas, “Transparency \nEuropean Union. https://www.echr.coe.int/Docu-\nand the Marketplace for Student Data” (June 6, \nments/Handbook_rights_child_ENG.PDF\n2018). Virginia Journal of Law and Technology, \n2  Children Act (1989). Retrieved from https://www. Forthcoming. Available at SSRN: https://ssrn.com/\nlegislation.gov.uk/ukpga/1989/41/section/1 abstract=3191436\n3   “Browser fingerprints, and why they are so hard  7  Children’s Online Privacy Protection Act (COPPA) \nto erase | Network World.” 17 Feb. 2015, https:// - https://www.ftc.gov/tips-advice/business-center/\nwww.networkworld.com/article/2884026/securi- privacy-and-security/children%27s-privacy\nty0/browser-fingerprints-and-why-they-are-so-hard-\nto-erase.html. Accessed 25 July. 2018.\n4  D. Gosh and B. Scott, “Digital Deceit: The Tech-\nnologies behind Precision Propaganda on the \nInternet” 23 Jan. 2018, https://www.newamerica.\norg/public-interest-technology/policy-papers/digit-\naldeceit/. Accessed 10 Nov 2018.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 123', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nMethods to Guide Ethical Research and Design\nAutonomous and intelligent systems (A/IS) research and design must be developed  \nagainst the backdrop that technology is not neutral. A/IS embody values and biases that \ncan influence important social processes like voting, policing, and banking. To ensure that \nA/IS benefit humanity, A/IS research and design must be underpinned by ethical and legal \nnorms. These should be instantiated through values-based research and design methods. \nSuch methods put human well-being at the core of A/IS development. \nTo help achieve these goals, researchers, product developers, and technologists across  \nall sectors need to embrace research and development methods that evaluate their \nprocesses, products, values, and design practices in light of the concerns and  \nconsiderations raised in this chapter. This chapter is split up into three sections:\nSection 1—Interdisciplinary Education and Research \nSection 2—Corporate Practices on A/IS \nSection 3—Responsibility and Assessment \nEach of the sections highlights various areas of concern (issues) as well as \nrecommendations and further resources. \nOverall, we address both structural and individual approaches. We discuss how to improve \nthe ethical research and business practices surrounding the development of A/IS and attend \nto the responsibility of the technology sector vis-à-vis the public interest. We also look at that \nwhat can be done at the level of educational institutions, among others, informing engineering \nstudents about ethics, social justice, and human rights. The values-based research and design \nmethod will require a change of current system development approaches for organizations. \nThis includes a commitment of research institutions to strong ethical guidelines for research \nand of businesses to values that transcend narrow economic incentives.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 124', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nMethods to Guide Ethical Research and Design\nSection 1—Interdisciplinary \nEducation and Research\nIntegrating applied ethics into education and  sufficiently integrate applied ethics throughout \nresearch to address the issues of A/IS requires  their curricula. When they do, often ethics is \nan interdisciplinary approach, bringing together  relegated to a stand-alone course or module that \nhumanities, social sciences, physical sciences,  gives students little or no direct experience in \nengineering, and other disciplines. ethical decision-making. Ethics education should \nbe meaningful, applicable, and incorporate best \npractices from the broader field. \nIssue: Integration of ethics in  \nThe aim of these recommendations is to \nA/IS-related degree programs\nprepare students for the technical training \nand engineering development methods that \nBackground\nincorporate ethics as essential so that ethics,  \nA/IS engineers and design teams do not always  and relevant principles, like human rights, \nthoroughly explore the ethical considerations  become naturally a part of the design process.\nimplicit in their technical work and design \nRecommendations\nchoices. Moreover, the overall science, \n•  Ethics training needs to be a core subject  \ntechnology, engineering, and mathematics \nfor all those in the STEM field, beginning at  \n(STEM) field struggles with the complexity of \nthe earliest appropriate level and for all \nethical considerations, which cannot be readily \nadvanced degrees. \narticulated and translated into the formal \nlanguages of mathematics and computer \n•  Effective STEM ethics curricula should be \nprogramming associated with algorithms and \ninformed by experts outside the STEM \nmachine learning. \ncommunity from a variety of cultural and \neducational backgrounds to ensure that \nEthical issues can easily be rendered invisible \nstudents acquire sensitivity to a diversity  \nor inappropriately reduced and simplified in the \nof robust perspectives on ethics and design. \ncontext of technical practice. For the dangers \nof this approach see for instance, Lipton \n•  Such curricula should teach aspiring engineers, \nand Steinhardt (2018), listed under “Further \ncomputer scientists, and statisticians about \nResources”. This problem is further compounded \nthe relevance and impact of their decisions \nby the fact that many STEM programs do not \nin designing A/IS technologies. Effective \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 125', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nMethods to Guide Ethical Research and Design\nethics education in STEM contexts and \nbeyond should span primary, secondary, and \nIssue: Interdisciplinary \npostsecondary education, and include both \ncollaborations\nuniversities and vocational training schools. \n•  Relevant accreditation bodies should reinforce  Background\nthis integrated approach as outlined above. \nMore institutional resources and incentive \nstructures are necessary to bring A/IS engineers \nFurther Resources\nand designers into sustained and constructive \n•  IEEE P7000TM Standards Project for a Model  contact with ethicists, legal scholars, and social \nProcess for Addressing Ethical Concerns  scientists, both in academia and industry. This \nDuring System Design. IEEE P7000 aims to  contact is necessary as it can enable meaningful \nenhance corporate IT innovation practices  interdisciplinary collaboration and shape the \nby providing processes for embedding a  future of technological innovation. More could \nvalues- and virtue-based thinking, culture, and  be done to develop methods, shared knowledge, \npractice into them. and lexicons that would facilitate  \nsuch collaboration.\n•  Z. Lipton and J. Steinhardt, Troubling Trends \nin Machine Learning Scholarship. ICML  This issue relates, among other things, to \nconference paper, July 2018.  funding models as well as the lack of diversity \nof backgrounds and perspectives in A/IS-related \n•  J. Holdren, and M. Smith. “Preparing for the  institutions and companies, which limit cross-\nFuture of Artificial Intelligence.” Washington,  pollination between disciplines. To help bridge \nDC: Executive Office of the President, National  this gap, additional translation work and resource \nScience and Technology Council, 2016.  sharing, including websites and Massive Open \nOnline Courses (MOOCs), need to happen \n•  Comparing the UK, EU, and US approaches \namong technologists and other relevant experts, \nto AI and ethics: C. Cath, S. Wachter, B. \ne.g., in medicine, architecture, law, philosophy, \nMittelstadt, et al., “Artificial Intelligence and \npsychology, and cognitive science. Furthermore, \nthe ‘Good Society’: The US, EU, and UK \nthere is a need for more cross-disciplinary \nApproach.” Science and Engineering Ethics, \nconversation and multi-disciplinary research, as \nvol. 24, pp. 505-528, 2017.\nis being done, for instance, at the annual ACM \n  Fairness, Accountability, and Transparency (FAT*) \n  conference or the work done by the Canadian \n \nInstitute For Advanced Research (CIFAR), which  \n \nis developing Canada’s AI strategy. \n \n \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 126', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nMethods to Guide Ethical Research and Design\nRecommendations\nFunding models and institutional incentive  Issue: A/IS culture and context\nstructures should be reviewed and revised to \nprioritize projects with interdisciplinary ethics  Background\ncomponents to encourage integration of ethics \nA responsible approach to embedding values into \ninto projects at all levels.\nA/IS requires that algorithms and systems are \nFurther Resources created in a way that is sensitive to the variation \nof ethical practices and beliefs across cultures. \n• S. Barocas, Course Material for Ethics and Policy \nThe designers of A/IS need to be mindful \nin Data Science, Cornell University, 2017. \nof cross-cultural ethical variations while also \n• L. Floridi, and M. Taddeo. “What Is Data  respecting widely held international legal norms.\nEthics?” Philosophical Transactions of the Royal \nRecommendation\nSociety, vol. 374, no. 2083, 1–4. DOI10.1098/\nrsta.2016.0360, 2016. Establish a leading role for intercultural \ninformation ethics (IIE) practitioners in ethics \n• S. Spiekermann, Ethical IT Innovation: A Value-\ncommittees informing technologists, policy \nBased System Design Approach. Boca Raton, \nmakers, and engineers. Clearly demonstrate \nFL: Auerbach Publications, 2015.\nthrough examples how cultural variation informs \nnot only information flows and information \n• K. Crawford, “Artificial Intelligence’s White Guy \nsystems, but also algorithmic decision-making \nProblem”, New York Times, July 25, 2016. \nand value by design.\n[Online]. Available: http://www.nytimes.\ncom/2016/06/26/opinion/sunday/artificial- Further Resources\nintelligences-white-guy-problem.html?_r=1. \n•  D. J. Pauleen, et al. “Cultural Bias in \n[Accessed October 28, 2018].\nInformation Systems Research and Practice: \n  Are You Coming From the Same Place I Am? \n \n” Communications of the Association for \n \nInformation Systems, vol. 17, no. 17, 2006. \n \n \n•  J. Bielby, “Comparative Philosophies in \n \nIntercultural Information Ethics,” Confluence: \n \nOnline Journal of World Philosophies 2, no. 1, \n \n  pp. 233–253, 2016. \n   \n   \n \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 127', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nMethods to Guide Ethical Research and Design\nto market are not developed into policies or \nguidance documents from recognized national \nIssue: Institutional ethics \nand international bodies, e.g., U.S. Food and Drug \ncommittees in the A/IS fields\nAdministration (FDA) and EU European Medicines \nAgency (EMA). Second, the bodies that typically \nBackground train individuals to be gatekeepers for the \nresearch ethics bodies are under-resourced in \nIt is unclear how research on the interface \nterms of expertise for A/IS development, e.g., \nof humans and A/IS, animals and A/IS, and \nPublic Responsibility in Medicine and Research \nbiological hazards will impact research ethical \n(PRIM&R) and the Society of Clinical Research \nreview boards. Norms, institutional controls, and \nAssociates (SoCRA). Third, it is not clear whether \nrisk metrics appropriate to the technology are \nthere is sufficient attention paid to A/IS ethics by \nnot well established in the relevant literature and \nresearch ethics board members or by researchers \nresearch governance infrastructure. Additionally, \nwhose projects involve the use of human \nnational and international regulations governing \nparticipants or their identifiable data. \nreview of human-subjects research may explicitly \nor implicitly exclude A/IS research from their \nFor example, research pertinent to the ethics-\npurview on the basis of legal technicalities or \ngoverning research at the interface of animals  \nmedical ethical concerns, regardless of the \nand A/IS research is underdeveloped with \npotential harms posed by the research.\nrespect to systematization for implementation by \nthe Institutional Animal Care and Use Committee \nResearch on A/IS human-machine interaction, \n(IACUC) or other relevant committees. In \nwhen it involves intervention or interaction with \ninstitutions without a veterinary school, it is \nidentifiable human participants or their data, \nunclear that the organization would have the \ntypically falls to the governance of research ethics \nrelevant resources necessary to conduct an \nboards, e.g., institutional review boards. The \nethical review of such research.\nnational level and institutional resources, e.g., \nhospitals and universities, necessary to govern \nSimilarly, research pertinent to the intersection of \nethical conduct of Human-Computer Interaction \nradiological, biological, and toxicological research \n(HCI), particularly within the disciplines pertinent \n—ordinarily governed under institutional biosafety \nto A/IS research, are underdeveloped. \ncommittees—and A/IS research is not often  \nfound in the literature pertinent to research  \nFirst, there is limited international or national \nethics or research governance. \nguidance to govern this form of research. \n \nSections of IEEE standards governing research \n \non A/IS in medical devices address some \nof the issues related to the security of A/IS-\nenabled devices. However, the ethics of testing \nthose devices for the purpose of bringing them \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 128', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nMethods to Guide Ethical Research and Design\nRecommendation •  B. Schneiderman, “The Dangers of Faulty, \nBiased, or Malicious Algorithms Requires \nThe IEEE and other standards-setting bodies \nIndependent Oversight.” Proceedings of the \nshould draw upon existing standards, empirical \nNational Academy of Sciences of the United \nresearch, and expertise to identify priorities  \nStates of America 113, no. 48, 13538–13540, \nand develop standards for the governance of  \n2016.\nA/IS research and partner with relevant national \nagencies, and international organizations,   •  J. Metcalf and K. Crawford, “Where are Human \nwhen possible. Subjects in Big Data Research? The Emerging \nEthics Divide.” Big Data & Society, May 14, \nFurther Resources 2016. [Online]. Available: SSRN: https://ssrn.\ncom/abstract=2779647. [Accessed Nov. 1, \n•  S. R. Jordan, “The Innovation Imperative.” \n2018].\nPublic Management Review 16, no. 1,  \npp. 67–89, 2014.  •  R. Calo, “Consumer Subject Review Boards: \nA Thought Experiment.” Stanford Law Review \nOnline 66 97, Sept. 2013. \n \n \n \n \n \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 129', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nMethods to Guide Ethical Research and Design\nSection 2—Corporate Practices on A/IS\nCorporations are eager to develop, deploy,  For instance, if an ethics review board comes in \nand monetize A/IS, but there are insufficient  at the right time during the A/IS creation process, \nstructures in place for creating and supporting  it would help mitigate the likelihood of creating \nethical systems and practices around A/IS  ethically problematic designs. The institution of \nfunding, development, and use. an ethical A/IS corporate culture would accelerate \nthe adoption of the other recommendations \nwithin this section focused on business practices.\nIssue: Values-based ethical \nFurther Resources\nculture and practices for industry\n•  ACM Code of Ethics and Professional Ethics, \nwhich includes various references to human \nBackground\nwell-being and human rights, 2018. \nCorporations are built to create profit while \n•  Report of UN Special Rapporteur on Freedom \ncompeting for market share. This can lead \nof Expression. AI and Freedom of Expression. \ncorporations to focus on growth at the expense \n2018. \nof avoiding negative ethical consequences. Given \nthe deep ethical implications of widespread \n•  The website of the Benefit corporations \ndeployment of A/IS, in addition to laws and \n(B-corporations) provides a good overview of \nregulations, there is a need to create values-\na range of companies that personify this type \nbased ethical culture and practices for the \nof culture.\ndevelopment and deployment of those systems. \nTo do so, we need to further identify and refine  •  R. Sisodia, J. N. Sheth and D. Wolfe, Firms of \ncorporate processes that facilitate values-based  Endearment, 2nd edition. Upper Saddle River, \ndesign. NJ: FT Press, 2014. This book showcases \nhow companies embracing values and a \nRecommendations\nstakeholder approach outperform their \nThe building blocks of such practices include  competitors in the long run.\ntop-down leadership, bottom-up empowerment, \nownership, and responsibility, along with the \nneed to consider system deployment contexts \nand/or ecosystems. Corporations should identify   \nstages in their processes in which ethical   \nconsiderations, “ethics filters”, are in place before   \nproducts are further developed and deployed. \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 130', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nMethods to Guide Ethical Research and Design\nFurther Resources\nIssue: Values-based leadership •  K. Firth-Butterfield, “How IEEE Aims to Instill \nEthics in Artificial Intelligence Design,” The \nBackground Institute. Jan. 19, 2017. [Online]. Available: \nhttp://theinstitute.ieee.org/ieee-roundup/\nTechnology leadership should give innovation \nblogs/blog/how-ieee-aims-to-instill-ethics-\nteams and engineers direction regarding which \nin-artificial-intelligence-design. [Accessed \nhuman values and legal norms should be \nOctober 28, 2018]. \npromoted in the design of A/IS. Cultivating \nan ethical corporate culture is an essential  •  United Nations, Guiding Principles on Business \ncomponent of successful leadership in the   and Human Rights: Implementing the United \nA/IS domain. Nations “Protect, Respect and Remedy” \nFramework, New York and Geneva: UN, 2011.\nRecommendations\n•  Institute for Human Rights and Business \nCompanies should create roles for senior-level \n(IHRB), and Shift, ICT Sector Guide on \nmarketers, engineers, and lawyers who can \nImplementing the UN Guiding Principles on \ncollectively and pragmatically implement ethically \nBusiness and Human Rights, 2013.\naligned design. There is also a need for more \nin-house ethicists, or positions that fulfill similar \n•  C. Cath, and L. Floridi, “The Design of \nroles. One potential way to ensure values are \nthe Internet’s Architecture by the Internet \non the agenda in A/IS development is to have a \nEngineering Task Force (IETF) and Human \nChief Values Officer (CVO), a role first suggested \nRights.” Science and Engineering Ethics, vol. \nby Kay Firth-Butterfield, see “Further Resources”. \n23, no. 2, pp. 449–468, Apr. 2017.\nHowever, ethical responsibility should not be \ndelegated solely to CVOs. They can support the \ncreation of ethical knowledge in companies, but \nin the end, all members of an organization will \nIssue: Empowerment to raise \nneed to act responsibly throughout the design \nethical concerns\nprocess.\nCompanies need to ensure that their  Background\nunderstanding of values-based system innovation \nEngineers and design teams may encounter \nis based on de jure and de facto international \nobstacles to raising ethical concerns regarding \nhuman rights standards.\ntheir designs or design specifications within \n  their organizations. Corporate culture should \n \nincentivize technical staff to voice the full range \n \n  of ethical questions to relevant corporate actors \n  throughout the full product lifecycle, including \nthe design, development, and deployment \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 131', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nMethods to Guide Ethical Research and Design\nphases. Because raising ethical concerns can be \nperceived as slowing or halting a design project, \nIssue: Ownership and \norganizations need to consider how they can \nresponsibility\nrecognize and incentivize values-based design as \nan integral component of product development.\nBackground\nRecommendations\nThere is variance within the technology \nEmployees should be empowered and \ncommunity on how it sees its responsibility \nencouraged to raise ethical concerns in  \nregarding A/IS. The difference in values and \nday-to-day professional practice. \nbehaviors are not necessarily aligned with \nthe broader set of social concerns raised by \nTo be effective in ensuring adoption of ethical \npublic, legal, and professional communities. \nconsiderations during product development or \nThe current makeup of most organizations has \ninternal implementation of A/IS, organizations \nclear delineations among engineering, legal, and \nshould create a company culture and set of \nmarketing functions. Thus, technologists will often \nnorms that encourage incorporating ethical \nbe incentivized in terms of meeting functional \nconsiderations in the design and implementation \nrequirements, deadline, and financial constraints, \nprocesses. \nbut for larger social issues may say, “Legal will \nNew categories of considerations around these  handle that.” In addition, in employment and \nissues need to be accommodated, along with  management technology or work contexts, \nupdated Codes of Conduct, company value- “ethics” typically refers to a code of conduct \nstatements, and other management principles  regarding professional behavior versus a values-\nso individuals are empowered to share their  driven design process mentality. \ninsights and concerns in an atmosphere of trust. \nAs such, ethics regarding professional conduct \nAdditionally, bottom-up approaches like company \noften implies moral issues such as integrity or \n“town hall meetings” should be explored that \nthe lack thereof, in the case of whistleblowing, for \nreward, rather than punish, those who bring up \ninstance. However, ethics in A/IS design include \nethical concerns.\nbroader considerations about the consequences \nFurther Resources of technologies.\n•  The British Computer Society (BCS), Code  \nRecommendations\nof Conduct, 2019.\nOrganizations should clarify the relationship \n•  C. Cath, and L. Floridi, “The Design of  between professional ethics and applied  \nthe Internet’s Architecture by the Internet  A/IS ethics by helping or enabling designers, \nEngineering Task Force (IETF) and Human  engineers, and other company representatives to \nRights,” Science and Engineering Ethics, vol.  discern the differences between these kinds of \n23, no. 2, pp. 449–468, Apr. 2017. ethics and where they complement each other.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 132', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nMethods to Guide Ethical Research and Design\nCorporate ethical review boards, or comparable  as accessibility, that consider human physical \nmechanisms, should be formed to address   disabilities, which should be incorporated into \nethical and behavioral concerns in relation to   A/IS as they are more widely deployed. It is \nA/IS design, development and deployment. Such  important to continuously consider the impact \nboards should seek an appropriately diverse  of A/IS through unanticipated use and on \ncomposition and use relevant criteria, including  unforeseen interests.\nboth research ethics and product ethics, at the \nRecommendations\nappropriate levels of advancement of research \nand development. These boards should examine  To ensure representation of stakeholders, \njustifications of research or industrial projects. organizations should enact a planned and \ncontrolled set of activities to account for the \nFurther Resources\ninterests of the full range of stakeholders or \n•  HH van der Kloot Meijberg and RHJ ter  practitioners who will be working alongside  \nMeulen, “Developing Standards for Institutional  A/IS and incorporating their insights to build \nEthics Committees: Lessons from the  upon, rather than circumvent or ignore, the  \nNetherlands,” Journal of Medical Ethics 27  social and practical wisdom of involved \ni36-i40, 2001.  practitioners and other stakeholders.\nFurther Resources\n•  C. Schroeter, et al., “Realization and User \nIssue: Stakeholder inclusion Evaluation of a Companion Robot for People \nwith Mild Cognitive Impairments,” Proceedings \nBackground of IEEE International Conference on Robotics \nand Automation (ICRA 2013), Karlsruhe, \nThe interface between A/IS and practitioners, \nGermany 2013. pp. 1145–1151.\nas well as other stakeholders, is gaining broader \nattention in domains such as healthcare  •  T. L. Chen, et al. “Robots for Humanity: Using \ndiagnostics, and there are many other contexts  Assistive Robotics to Empower People with \nwhere there may be different levels of  Disabilities,” IEEE Robotics and Automation \ninvolvement with the technology. We should  Magazine, vol. 20, no. 1, pp. 30–39, 2013.\nrecognize that, for example, occupational \n•  R. Hartson, and P. S. Pyla. The UX Book: \ntherapists and their assistants may have on-the-\nProcess and Guidelines for Ensuring a Quality \nground expertise in working with a patient, who \nUser Experience. Waltham, MA: Elsevier, 2012.\nmight be the “end user” of a robot or social  \nA/IS technology. In order to develop a product \nthat is ethically aligned, stakeholders’ feedback is \ncrucial to design a system that takes ethical and \nsocial issues into account. There are successful   \nuser experience (UX) design concepts, such \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 133', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nMethods to Guide Ethical Research and Design\nRecommendations\nIssue: Values-based design Companies should study design processes \nto identify situations where engineers and \nBackground researchers can be encouraged to raise and \nresolve questions of ethics and foster a proactive \nEthics are often treated as an impediment to \nenvironment to realize ethically aligned design. \ninnovation, even among those who ostensibly \nAchieving a distributed responsibility for ethics \nsupport ethical design practices. In industries \nrequires that all people involved in product \nthat reward rapid innovation in particular, it is \ndesign are encouraged to notice and respond to \nnecessary to develop ethical design practices \nethical concerns. Organizations should consider \nthat integrate effectively with existing engineering \nhow they can best encourage and facilitate \nworkflows. Those who advocate for ethical design \ndeliberations among peers.\nwithin a company should be seen as innovators \nseeking the best outcomes for the company,  Organizations should identify points for formal \nend users, and society. Leaders can facilitate that  review during product development. These \nmindset by promoting an organizational structure  reviews can focus on “red flags” that have been \nthat supports the integration of dialogue about  identified in advance as indicators of risk. For \nethics throughout product life cycles. example, if the datasets involve minors or focus \non users from protected classes, then it may \nA/IS design processes often present moments \nrequire additional justification or alterations to the \nwhere ethical consequences can be highlighted. \nresearch or development protocols.\nThere are no universally prescribed models for \nthis because organizations vary significantly in  Further Resources\nstructure and culture. In some organizations, \n•  A. Sinclair, “Approaches to Organizational \ndesign team meetings may be brief and informal. \nCulture and Ethics,” Journal of Business Ethics, \nIn others, the meetings may be lengthy and \nvol. 12, no. 1, pp. 63–73, 1993.\nstructured. The transition points between \ndiscovery, prototyping, release, and revisions are  •  Al Y. S. Chen, R. B. Sawyers, and P. F. Williams. \nnatural contexts for conducting such reviews.  “Reinforcing Ethical Decision Making Through \nIterative review processes are also advisable, in  Corporate Culture,” Journal of Business Ethics \npart because changes to risk profiles over time  16, no. 8, pp. 855–865, 1997.\ncan illustrate needs or opportunities for improving \n•  K. Crawford and R. Calo, “There Is a Blind Spot \nthe final product.\nin AI Research,” Nature 538, pp. 311–313, \n  2016.\n \n \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 134', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nMethods to Guide Ethical Research and Design\nSection 3—Responsibility and Assessment\nLack of accountability of the A/IS design and  protections for proprietary technology are \ndevelopment process presents a challenge  routinely and effectively balanced with the \nto ethical implementation and oversight. This  need for appropriate oversight standards and \nsection presents four issues, moving from macro  mechanisms to safeguard the public.\noversight to micro documentation practices. \nHuman rights and algorithmic impact \nassessments should be explored as a meaningful \nway to improve the accountability of A/IS.  \nIssue: Oversight for algorithms\nThese need to be paired with public \nconsultations, and the final impact  \nThe algorithms behind A/IS are not subject to  assessments must be made public. \nconsistent oversight. This lack of assessment \nFurther Resources\ncauses concern because end users have no \naccount of how a certain algorithm or system  •  F. Pasquale, The Black Box Society: The \ncame to its conclusions. These recommendations  Secret Algorithms That Control Money \nare similar to those made in the “General  and Information. Cambridge, MA: Harvard \nPrinciples” and “Embedding Values into  University Press, 2016.\nAutonomous and Intelligent Systems” chapters \n•  R. Calo, “Artificial Intelligence Policy: A Primer \nof Ethically Aligned Design, but here the \nand Roadmap,” UC Davis Law Review, 52: pp. \nrecommendations are used as they apply to the \n399–435, 2017.\nnarrow scope of this chapter .\nRecommendations •  ARTICLE 19. “Privacy and Freedom of \nExpression in the Age of Artificial Intelligence,” \nAccountability: As touched on in the General \nPrivacy International, April 2018. [Online]. \nPrinciples chapter of Ethically Aligned Design, \nAvailable: https://www.article19.org/wp-\nalgorithmic transparency is an issue of concern. It \ncontent/uploads/2018/04/Privacy-and-\nis understood that specifics relating to algorithms \nFreedom-of-Expression-In-the-Age-of-Artificial-\nor systems contain intellectual property that \nIntelligence-1.pdf. [Accessed October 28, \ncannot, or will not, be released to the general \n2018].\npublic. Nonetheless, standards providing \noversight of the manufacturing process of A/IS   \ntechnologies need to be created to avoid harm   \n \nand negative consequences. We can look to \n \nother technical domains, such as biomedical, civil, \n \nand aerospace engineering, where commercial \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 135', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nMethods to Guide Ethical Research and Design\nRecommendations\nIssue: Independent   An independent, internationally coordinated \nreview organization body—akin to ISO—should be formed to oversee \nwhether A/IS products actually meet ethical \ncriteria, both when designed, developed, \nBackground\ndeployed, and when considering their evolution \nWe need independent, expert opinions that  after deployment and during interaction with \nprovide guidance to the general public regarding  other products. It should also include  \nA/IS. Currently, there is a gap between how   a certification process.\nA/IS are marketed and their actual performance \nFurther Resources\nor application. We need to ensure that  \nA/IS technology is accompanied by best-use  •  A. Tutt, “An FDA for Algorithms,” Administrative \nrecommendations and associated warnings.  Law Review 69, 83–123, 2016.\nAdditionally, we need to develop a certification \n•  M. U. Scherer, “Regulating Artificial Intelligence \nscheme for A/IS which ensures that the \nSystems: Risks, Challenges, Competencies, \ntechnologies have been independently  \nand Strategies,” Harvard Journal of Law and \nassessed as being safe and ethically sound.\nTechnology vol. 29, no. 2, 354–400, 2016.\nFor example, today it is possible for systems \n•  D. R. Desai and J. A. Kroll, “Trust But Verify: \nto download new self-parking functionality to \nA Guide to Algorithms and the Law.” Harvard \ncars, and no independent reviewer establishes \nJournal of Law and Technology, Forthcoming; \nor characterizes boundaries or use. Or, when \nGeorgia Tech Scheller College of Business \na companion robot promises to watch your \nResearch Paper No. 17-19, 2017. \nchildren, there is no organization that can issue \nan independent seal of approval or limitation on \nthese devices. We need a ratings and approval \nIssue: Use of black-box \nsystem ready to serve social/automation \ntechnologies that will come online as soon as  components\npossible. We also need further government \nfunding for research into how A/IS technologies  Background\ncan best be subjected to review, and how  \nSoftware developers regularly use “black box” \nreview organizations can consider both  \ncomponents in their software, the functioning of \ntraditional health and safety issues, as well  \nwhich they often do not fully understand. “Deep” \nas ethical considerations.\nmachine learning processes, which are driving \n  many advancements in autonomous and intelligent \n  systems, are a growing source of black box \n  software. At least for the foreseeable future,  \n \nA/IS developers will likely be unable to build \nsystems that are guaranteed to operate as intended.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 136', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nMethods to Guide Ethical Research and Design\nRecommendations Further Resources\nWhen systems are built that could impact the  •  M. Ananny and K. Crawford, “Seeing without \nsafety or well-being of humans, it is not enough  Knowing: Limitations of the Transparency \nto just presume that a system works. Engineers  Ideal and Its Application to Algorithmic \nmust acknowledge and assess the ethical risks  Accountability,” New Media & Society, vol. 20, \ninvolved with black box software and implement  no. 3, pp. 973-989, Dec. 13, 2016.\nmitigation strategies.\n•  D. Reisman, J. Schultz, K. Crawford, and M. \nTechnologists should be able to characterize  Whittaker, “Algorithmic Impact Assessments: \nwhat their algorithms or systems are going to  A Practical Framework for Public Agency \ndo via documentation, audits, and transparent  Accountability,” AI NOW 2018. [Online]. \nand traceable standards. To the degree possible,  Available: https://ainowinstitute.org/\nthese characterizations should be predictive,  aiareport2018.pdf.  \nbut given the nature of A/IS, they might need to  [Accessed October 28, 2018].\nbe more retrospective and mitigation-oriented. \n•  J. A. Kroll “The Fallacy of Inscrutability.” \nAs such, it is also important to ensure access to \nPhilosophical Transactions of the Royal Society \nremedy adverse impacts. \nA: Mathematical, Physical and Engineering \nTechnologists and corporations must do their  Sciences, C. Cath, S. Wachter, B. Mittelstadt \nethical due diligence before deploying A/IS  and L. Floridi, Eds., October 15, 2018 DOI: \ntechnology. Standards for what constitutes ethical  10.1098/rsta.2018.0084. \ndue diligence would ideally be generated by \nan international body such as IEEE or ISO, and \nbarring that, each corporation should work to  Issue: Need for better  \ngenerate a set of ethical standards by which their  technical documentation \nprocesses are evaluated and modified. Similar \nto a flight data recorder in the field of aviation, \nBackground\nalgorithmic traceability can provide insights \non what computations led to questionable or  A/IS are often construed as fundamentally \ndangerous behaviors. Even where such processes  opaque and inscrutable. However, lack of \nremain somewhat opaque, technologists should  transparency is often the result of human \nseek indirect means of validating results and  decision. The problem can be traced to a  \ndetecting harms. variety of sources, including poor documentation \nthat excludes vital information about the \n \nlimitations and assumptions of a system.  \n \nBetter documentation combined with  \n \ninternal and external auditing are crucial to  \n \nunderstanding a system’s ethical impact.\n \n \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 137', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nMethods to Guide Ethical Research and Design\nRecommendation Further Resources\nEngineers should be required to thoroughly  •  S. Wachter, B. Mittelstadt, and L. Floridi. \ndocument the end product and related data  “Transparent, Explainable, and Accountable \nflows, performance, limitations, and risks of   AI for Robotics.” Science Robotics, vol. 2, no. \nA/IS. Behaviors and practices that have been  6, May 31, 2017. [Online]. Available: DOI: \nprominent in the engineering processes should  10.1126/scirobotics.aan6080. [Accessed Nov. \nalso be explicitly presented, as well as empirical \n•  S. Barocas, and A. D. Selbst, “Big Data’s \nevidence of compliance and methodology \nDisparate Impact.” California Law Review 104, \nused, such as training data used in predictive \n671-732, 2016. \nsystems, algorithms and components used, and \nresults of behavior monitoring. Criteria for such \n•  J. A. Kroll, J. Huey, S. Barocas, E. W. Felten, \ndocumentation could be: auditability, accessibility, \nJ. R. Reidenberg, D. G. Robinson, and H. \nmeaningfulness, and readability.\nYu. “Accountable Algorithms.” University of \nPennsylvania Law Review 165, no. 1, 633–\nCompanies should make their systems auditable \n705, 2017.\nand should explore novel methods for external \nand internal auditing.  \n•  J. M. Balkin, “Free Speech in the Algorithmic \nSociety: Big Data, Private Governance, and \n \nNew School Speech Regulation.” UC Davis \n \n  Law Review, 2017. \n \n \n \n \n \n \n \n \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 138', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nMethods to Guide Ethical Research and Design\nThanks to the Contributors\nWe wish to acknowledge all of the people who  •  John C. Havens – Executive Director, The \ncontributed to this chapter.  IEEE Global Initiative on Ethics of Autonomous \nand Intelligent Systems; Executive Director, \nThe Methods to Guide Ethical  \nThe Council on Extended Intelligence; Author, \nResearch and Design Committee\nHeartificial Intelligence: Embracing Our \n•  Corinne Cath-Speth (Co-Chair) – PhD  Humanity to Maximize Machines \nstudent at Oxford Internet Institute, The \n•  Sara Jordan – Assistant Professor of Public \nUniversity of Oxford, Doctoral student at the \nAdministration in the Center for Public \nAlan Turing Institute, Digital Consultant at \nAdministration & Policy at Virginia Tech\nARTICLE 19\n•  Jason Millar – Professor, robot ethics at \n•  Raja Chatila (Co-Chair) – CNRS-Sorbonne  Carleton University\nInstitute of Intelligent Systems and Robotics, \n•  Sarah Spiekermann – Chair of the Institute \nParis, France; Member of the French \nfor Information Systems & Society at Vienna \nCommission on the Ethics of Digital Sciences \nUniversity of Economics and Business; Author \nand Technologies CERNA; Past President of \nof the textbook “Ethical IT-Innovation”, the \nIEEE Robotics and Automation Society\npopular book “Digitale Ethik—Ein Wertesystem \nfür das 21. Jahrhundert” and Blogger on ”The \n•  Thomas Arnold – Research Associate at \nEthical Machine” \nTufts University Human-Robot Interaction \nLaboratory •  Shannon Vallor – William J. Rewak Professor \nin the Department of Philosophy at Santa \n•  Jared Bielby – President, Netizen Consulting \nClara University in Silicon Valley and Executive \nLtd; Chair, International Center for Information \nBoard member of the Foundation for \nEthics; editor, Information Cultures in the \nResponsible Robotics\nDigital Age\n•  Klein, Wilhelm E. J., PhD – Senior Research \n•  Reid Blackman, PhD – Founder & CEO  Associate & Lecturer in Technology Ethics, City \nVirtue Consultants, Assistant Professor of  University of Hong Kong\nPhilosophy Colgate University\nFor a full listing of all IEEE Global Initiative \n•  Tom Guarriello, PhD – Founding Faculty  Members, visit standards.ieee.org/content/dam/\nmember in the Master’s in Branding program  ieee-standards/standards/web/documents/other/\nat New York City’s School of Visual Arts, Host  ec_bios.pdf. \nof RoboPsyc Podcast and author of RoboPsych \nFor information on disclaimers associated with \nNewsletter \nEAD1e, see How the Document Was Prepared.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 139', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nA/IS for Sustainable Development \nAutonomous and intelligent systems (A/IS) offer unique and impactful opportunities as well \nas risks both to people living in high-income countries (HIC) and in low-and middle-income \ncountries (LMIC). The scaling and use of A/IS represent a genuine opportunity across the \nglobe to provide individuals and communities—be they rural, semi-urban, or urban—with \nthe means to satisfy their needs and develop their full potential, with greater autonomy \nand choice. A/IS will potentially disrupt economic, social, and political relationships and \ninteractions at many levels. Those disruptions could provide an historical opportunity to \nreset those relationships in order to distribute power and wealth more equitably and thus \npromote social justice.1 They could also leverage quality and better standards of life and \nprotect people’s dignity, while maintaining cultural diversity and protecting the environment.\nOne possible vehicle that can be used to agree on priorities and prioritize resources and \nactions is the United Nations Agenda for Sustainable Development, which was adopted by \nthe UN General Assembly in 2015; 193 nations voted in favor of the Agenda, which also \nincludes 17 Sustainable Development Goals (SDGs) for the world to achieve by 2030. \nThe Agenda challenges all member states to make concerted efforts toward the above \nmentioned goals, and thus toward a sustainable, prosperous, and resilient future for people \nand the planet. These universally applicable goals should be reached by 2030.2 \nThe value of A/IS is significantly associated with the generation of various types of superior \nand unique insights, many of which could help achieve positive socioeconomic outcomes \nfor both HIC and LMIC societies, in keeping with the SDGs. The ethical imperative driving \nthis chapter is that A/IS must be harnessed to benefit humanity, promote equality, and \nrealize the world community’s vision of a sustainable future and the SDGs:\n…….of universal respect for human rights and human dignity, the rule of law, justice, \nequality and nondiscrimination; of respect for race, ethnicity and cultural diversity; and \nof equal opportunity permitting the full realization of human potential and contributing \nto shared prosperity. A world which invests in its children and in which every child grows \nup free from violence and exploitation. A world in which every woman and girl enjoys full \ngender equality and all legal, social and economic barriers to their empowerment have \nbeen removed. A just, equitable, tolerant, open and socially inclusive world in which the \nneeds of the most vulnerable are met.3\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 140', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nA/IS for Sustainable Development \nWe recognize that how A/IS are deployed globally will be a determining factor in whether, \nin fact, “no-one gets left behind”, whether human rights and dignity of all people are \nrespected, whether children are protected, and whether the gap between rich and \npoor, within and between nations, narrows or widens. A/IS can advance the Sustainable \nDevelopment Agenda’s transformative vision, but at the same time, A/IS can undermine  \nit if risks reviewed in this chapter are not managed properly. \nFor example, A/IS create the risk of accelerating inequality within and among nations,  \nif their development and marketing are controlled by a few select companies, primarily in \nHIC. The benefits would largely accrue to the highly educated and wealthier segment of \nthe population, while displacing the less educated workforce, both by automation and by \nthe absence of educational or retraining systems capable of imparting skills and knowledge \nneeded to work productively alongside A/IS. These risks, although differentiated by IT \ninfrastructure, educational attainment, economic, and cultural contexts, exist in HIC and \nLMIC alike. The inequality in accessing and using the internet, both within and among \ncountries, raises questions on how to spread A/IS benefits across humanity. Ensuring  \nA/IS “for the common good” is an ethical imperative and at the core of Ethically Aligned \nDesign, First Edition; the key elements of this “common good” are that it is human-\ncentered, accountable, and ensure outcomes that are fair and inclusive.\nThis chapter explores the imperative for A/IS to serve humanity by improving the quality and \nstandard of life for all people everywhere. It makes recommendations for advancing equal \naccess to this transformative technology, so that it drives the well-being of all people, rather \nthan further concentrating wealth, resources, and decision-making power in the hands of \na few countries, companies, or citizens. The recommendations further reflect policies and \ncollaborative public, private, and people programs which, if implemented, will respect the \nethical imperative embedded in the Sustainable Development Agenda’s transformative vision. \nThe respect of human rights and dignity, and the advancement of “common good” with equal \nbenefit to both HIC and LMIC, are central to every recommendation within this chapter. \n \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 141', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nA/IS for Sustainable Development \nSection 1—A/IS in Service to Sustainable \nDevelopment for All\nA/IS have the potential to contribute to the  citizenship and well-being, and a conscious effort \nresolution of some of the world’s most pressing  to counter the nature of the tech economy, with \nproblems, including: violation of fundamental rights,  its tendency to concentrate wealth within high \npoverty, exploitation, climate change, lack of high- income populations. Implementation of the SDGs \nquality services to excluded populations, increased  should benefit excluded sectors of society in \nviolence, and the achievement of the SDGs. every country, regardless of A/IS infrastructure.\n“The Road to Dignity by 2030” document of the \nUN Secretary General reports on resources and \nIssue: Current roadmaps for \nmethods for implementing the 2030 Agenda \ndevelopment and deployment \nfor Sustainable Development and emphasizes \nof A/IS are not aligned with \nthe importance of science, technology, and \nor guided by their impact in  innovation for a sustainable future.5 The UN \nthe most important challenges  Secretary General posits that: \nof humanity, defined in the \n“A sustainable future will require that we act now to \nseventeen United Nations  phase out unsustainable technologies and to invest \nSustainable Development Goals  in innovation and in the development of clean and \nsound technologies for sustainable development. \n(SDGs), which collectively aspire \nWe must ensure that they are fairly priced, broadly \nto create a more equal world \ndisseminated and fairly absorbed, including to and \nof prosperity, peace, planet  by developing countries.” (para. 120)\nprotection, and human dignity  \nA/IS are among the technologies that can play \nfor all people.4\nan important role in the solution of the deep \nsocial problems plaguing our global civilization, \nBackground contributing to the transformation of society away \nfrom an unsustainable, unequal socioeconomic \nSDGs promoting prosperity, peace, planet \nsystem, towards one that realizes the vision of \nprotection, human dignity, and respect for human \nuniversal human dignity, peace, and prosperity.\nrights of all, apply to HIC and LMIC alike. Yet \nensuring that the benefits of A/IS will accrue to  However, with all the potential benefits of  \nhumanity as a whole, leaving “no one behind”,  A/IS, there are also risks. For example, given  \nrequires an ethical commitment to global  A/IS technology’s immense power needs, without \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 142', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nA/IS for Sustainable Development \nnew sources of sustainable energy harnessed  points.”8 With mobile phones generating much of \nto power A/IS in the future, there is a risk that it  the data needed for developing A/IS applications \nwill increase fossil fuel use and have a negative  in LMIC, unequal phone ownership may build in \nimpact on the environment and the climate. bias. For example, there is a risk of discrimination \nagainst women, who across LMIC are 14% less \nWhile 45% of the world’s population is \nlikely than men to own a mobile phone, and in \nnot connected to the internet, they are not \nSouth Asia where 38% are less likely to own a \nnecessarily excluded from A/IS’ potential \nmobile phone.9\nbenefits: in LMIC mobile networks can provide \ndata for A/IS applications. However, only those  Recommendations\nconnected are likely to benefit from the income-\nThe current range of A/IS applications in sectors \nproducing potential of internet technologies. In \ncrucial to the SDGs, and to excluded populations \n2017, internet penetration in HIC left behind \neverywhere, should be studied, with the \ncertain portions of the population often in rural \nstrengths, weaknesses, and potential of the most \nor remote areas; 12% of U.S. residents and 20% \nsignificant recent applications analyzed, and the \nof residents across Europe were unable to access \nbest ones developed at scale. Specific objectives \nthe internet. In Asia with its concentration of \nto consider include:\nLMIC, 52% of the population, on average, had no \naccess, a statistic skewed by the large population  •  Identifying and experimenting with  \nof China, where internet penetration reached  A/IS technologies relevant to the SDGs,  \n45% of the population. In numerous other  such as: big data for development relevant to, for \ncountries in the region, 99% of residents had  example, agriculture and medical tele-diagnosis; \nno access. This nearly total exclusion also exists  geographic information systems needed in \nin several countries in Africa, where the overall  public service planning, disaster prevention, \ninternet penetration is only 35%: 2 of every 3  emergency planning, and disease monitoring; \nresidents in Africa have no access.6 Those with  control systems used in, for example, \nno internet access also do not generate data  naturalizing intelligent cities through energy \nneeded to “train” A/IS, and are thereby excluded  and traffic control and management of urban \nfrom benefits of the technology, the development  agriculture; applications that promote human \nof which risks systematic discriminatory bias,  empathy focused on diminishing violence and \nparticularly against people from minority  exclusion and increasing well-being.\npopulations, and those living in rural areas, or \n•  Promoting the potential role of A/IS in \nin low-income countries. As a comparison, one \nsustainable development by collaboration \nstudy estimated that “in the US, just one home \nbetween national and international \nautomation product can generate a data point \ngovernment agencies and nongovernmental \nevery six seconds.”7 In Mozambique, where about \norganizations (NGOs) in technology sectors.\n90% of the population lack internet access, “the \naverage household generates zero digital data \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 143', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nA/IS for Sustainable Development \n•  Analyzing the cost of and proposing strategies  Further Resources\nfor publicly providing internet access for  \n•  R. Van Est and J.B.A. Gerritsen, with assistance \nall, as a means of diminishing the gap in  \nof L. Kool, Human Rights in the Robot Age: \nA/IS’ potential benefit to humanity, particularly \nChallenges arising from the use of Robots, \nbetween urban and rural populations in HIC \nArtificial Intelligence and Augmented Reality \nand LMIC alike.\nExpert Report written for the Committee on \n•  Investing in the documentation and  Culture, Science, Education and Media of the \ndissemination of innovative applications of   Parliamentary Assembly of the Council of \nA/IS that advance the resolution of identified  Europe (PACE), The Hague: Rathenau Instituut \nsocietal issues and the SDGs.  2017.\n•  Researching sustainable energy to power A/IS  •  World Economic Forum Global Future Council \ncomputational capacity. on Human Rights 2016-18, “White Paper: \nHow to Prevent Discriminatory Outcomes in \n•  Investing in the development of transparent \nMachine Learning,” World Economic Forum, \nmonitoring frameworks to track the concrete \nMarch 2018.\nresults of donations by international \norganizations, corporations, independent  •  United Nations General Assembly, \nagencies, and the State, to ensure efficiency  Transforming Our World: The 2030 Agenda \nand accountability in applied A/IS. for Sustainable Development (A/RES/70/1: 21 \nOctober 2015) Preamble. http://www.un.org/\n•  Developing national legal, policy, and fiscal \nen/development/desa/population/migration/\nmeasures to encourage competition in the  \ngeneralassembly/docs/globalcompact/ \nA/IS domestic markets and the flourishing  \nA_RES_70_1_E.pdf.\nof scalable A/IS applications.\n•  United Nations Global Pulse, Big Data for \n•  Integrating the SDGs into the core of private \nDevelopment: Challenges and Opportunities, \nsector business strategies and adding SDG \n2012. \nindicators to companies’ key performance \n \nindicators, going beyond corporate social \n \nresponsibility (CSR). \n \n \n•  Applying the well-being indicators10 to evaluate \n \nA/IS’ impact from multiple perspectives in HIC \n \nand LMIC alike.\n \n   \n \n \n \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 144', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nA/IS for Sustainable Development \nRecommendations\nIssue: A/IS are often viewed  To understand the impact of A/IS on society, \nonly as having impact in market  it is necessary to consider product and process \ninnovation, as well as wider sociocultural and \ncontexts, yet these technologies \nethical implications, from a global perspective, \nalso have an impact on social \nincluding the following: \nrelations and culture.\n•  Exploring the development of algorithms \ncapable of detecting and reporting \nBackground\ndiscrimination, cyberbullying, deceptive \nA/IS are expected to have an impact beyond \ncontent and identities, etc., and of notifying \nmarket domains and business models, diffusing \ncompetent authorities; recognizing that the \nthroughout the global society. For instance,  \nuse of such algorithms must address ethical \nA/IS have and will impact social relationships  \nconcerns related to algorithm explainability \nin a way similar to how mobile phones changed \nas well as take into account the risk to certain \nour daily lives, reflecting directly on our culture, \naspects of human rights, notably to privacy \ncustoms, and language. The extent and direction \nand freedom from oppression.\nof this impact is not yet clear, but documented \nexperience in HIC and high internet-penetration  •  Developing a globally recognized professional \nenvironments of trolls, “fake news,” and  Code of Ethics with and for technology \ncyberbullying on social media offer a cautionary  companies.\ntale.11 Depression, social isolation, aggression, \n•  Identifying social disorders, such as depression, \nand the dissemination of violent behavior with \nanxiety, psychological violence, political \ndamage to human relations, so extreme that, \nmanipulation, etc., correlated with the use of  \nin some cases, it has resulted in suicide, are all \nA/IS-based technologies as a world health \ncorrelated with the internet.12 As an example, \nproblem; monitoring and measuring their impact.\nthe technology for “smart homes” has been \nused for inflicting domestic violence by remotely  •  Elaborating metrics measuring how, where and \nlocking doors, turning off heat/AC, and otherwise  on whom there is a cultural impact of new  \nharassing a partner. This problem could be easily  A/IS-based technologies. \nextended to include elder and child abuse.13   \nMeasures need to be developed to prevent   \nA/IS from contributing to the emergence or   \namplification of social disorders.  \n \n \n \n \n \n \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 145', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nA/IS for Sustainable Development \nFurther Resources etc., and are currently in the vanguard of \nthe movement toward customized/targeted \n•  T. Luong, “Thermostats, Locks and Lights: \ninformation based on user profiling that involves \nDigital Tools of Domestic Abuse,” The New \nsignificant use of A/IS techniques. Analysis of \nYork Times, June 23, 2018, https://www.\nopinion polls and trends in social networks, \nnytimes.com/2018/06/23/technology/smart-\nblogs, etc., and of the emotional response to \nhome-devices-domestic-abuse.html.\nnews items can be used for the purposes of \n•  J. Naughton, “The internet of things has  manipulation, facilitating both the selection of \nopened up a new frontier of domestic abuse.”  news that guides public opinion in the desired \nThe Guardian, July 2018. direction and the practice of sensationalism.\n•  M. Pianta, Innovation and Employment,  The ""personalization of the consumer \nHandbook of Innovation. Oxford, U.K.: Oxford  experience"", that is, the adaptation of articles \nUniversity Press, 2003. to the interests, political vision, cultural level, \neducation, and geographic location of the reader, \n•  M.J. Salganik, Bit by Bit. Princeton, NJ: \nis a new challenge for the journalism profession \nPrinceton University Press 2018.\nthat expands the possibilities of manipulation.\n•  J. Torresen, “A Review of Future and Ethical \nThe information infrastructure is currently \nPerspectives of Robotics and AI” Frontiers \nlacking in transparency, such that it is difficult \nin Robotics and AI, Jan. 15, 2018. [Online]. \nor impossible to know (except perhaps for the \nAvailable: https://doi.org/10.3389/\ninfrastructure operator):\nfrobt.2017.00075. [Accessed Nov. 1, 2018].\n•  what private information is being collected for \nuser profiling and by whom,\nIssue: The right to truthful \n•  which groups are targeted and by whom, \ninformation is key to a \n•  what information has been received by any \ndemocratic society and \ngiven targeted group,\nto achieving sustainable \ndevelopment and a more equal  •  who financed the creation and dissemination \nof this information,\nworld, but A/IS poses risks to \nthis right that must be managed. •  the percentage of the information being \n  disseminated by bots, and \nBackground\n•  who is financing these bots.\nSocial media have become the dominant \ntechnological infrastructure for the dissemination  Many actors have found this opaque \nof information such as news, opinion, advertising,  infrastructure ideal for spreading politically \nmotivated disinformation, which has a negative \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 146', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nA/IS for Sustainable Development \neffect on the creation of a more equal world,  •  Promoting the right to information in \ndemocracy, and the respect for fundamental  official documents, and developing A/IS \nrights. This disinformation can have tragic  techniques to automate journalistic tasks \nconsequences. For instance, human rights  such as verification of sources and checking \ngroups have unearthed evidence that the military  the accuracy of the information in official \nauthorities of Myanmar used Facebook for inciting  documents, or in the selection, hierarchy, \nhatred against the Rohingya Muslim minority,  assessment, and development of news, \nhatred which facilitated an ethnic cleansing  thereby contributing to objectivity and \ncampaign and the murder of up to 50,000  reliability.\npeople.14 The UN determined that these actions \nFurther Resources\nconstituted genocide, crimes against humanity, \nand war crimes.15 •  M. Broussard, “Artificial Iintelligence for \nInvestigative Reporting: Using an expert \nRecommendations\nsystem to enhance journalists’ ability to \nTo protect democracy, respect fundamental  discover original public affairs stories.” Digital \nrights, and promote sustainable development,  Journalism, vol. 3, no. 6, pp. 814-831, 2015.\ngovernments should implement a legislative \n•  M. Carlson, “The robotic reporter: Automated \nagenda which prevents the spread of \njournalism and the redefinition of labor, \nmisinformation and hate speech, by:\ncompositional forms, and journalistic authority.” \n•  Ensuring more control and transparency in  Digital Journalism, vol. 3, no. 3, pp. 416-431, \nthe use of A/IS techniques for user profiling  2015.\nin order to protect privacy and prevent user \n•  A. López Barriuso, F. de la Prieta Pintado, Á. \nmanipulation.\nLozano Murciego, , D. Hernández de la Iglesia \n•  Using A/IS techniques to detect untruthful  and J. Revuelta Herrero, JOUR-MAS: A Multi-\ninformation circulating in the infrastructures,  agent System Approach to Help Journalism \noverseen by a democratic body to prevent  Management, vol. 4, no. 4, 2015.\npotential censorship.\n•  P. Mozur, ”A Genocide Incited on Facebook \n•  Obliging companies owning A/IS  with Posts from Myanmar’s Military,” The \ninfrastructures to provide more transparency  New York Times, Oct. 15 2018. https://\nregarding their algorithms, sources of funding,  www.nytimes.com/2018/10/15/technology/\nservices, and clients. myanmar-faceboo.k-genocide.html\n•  Defining a new legal status somewhere  •  UK Parliament, House of Commons, Digital, \nbetween ""platforms"" and ""content providers""  Culture, Media and Sport Committee \nfor A/IS infrastructures. Disinformation and ‘fake news’: Interim \nReport, Fifth Report of Session 2017–19UK \n•  Reformulating the deontological codes of the \nParliament, Published on July 29, 2018.\njournalistic profession to take into account the \nintensive use of A/IS techniques foreseen  \nin the future. \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 147', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nA/IS for Sustainable Development \nSection 2—Equal Availability\nFurthermore, technological innovation in LMIC \ncomes up against many potential obstacles, \nIssue: Vastly different power \nwhich could be considered when undertaking \nstructures among and within \ninitiatives aimed at enhancing LMIC access:\ncountries create risk that A/IS \ndeployment accelerates, rather  •  Reluctance to provide open source licensing of \ntechnological development innovations,\nthan reduces, inequality in the \npursuit of a sustainable future.  •  Lack of the human capital and knowledge \nIt is unclear how LMIC can best  required to adapt HIC-developed technologies \nto resolving problems in the LMIC context, \nimplement A/IS via existing \nor to develop local technological solutions to \nresources and take full advantage \nthese problems,\nof the technology’s potential to \n•  Retention of A/IS capacity in LMIC due to \nachieve a sustainable future.\nglobally uncompetitive salaries,\nBackground •  Lack of infrastructure for deployment, and \ndifficulties in taking technological solutions to \nThe potential use of A/IS to create sustainable \nwhere they are needed,\neconomic growth for LMIC is uniquely powerful. \nYet, many of the debates surrounding A/IS take \n•  Lack of organizational and business models for \nplace within HIC, among highly educated and \nadapting technologies to the specific needs of \nfinancially secure individuals. It is imperative that \ndifferent regions,\nall humans, in any condition around the world, \nare considered in the general development and  •  Lack of active participation of the target \napplication of these systems to avoid the risk of  population,\nbias, excessive inequality, classism, and general \n•  Lack of political will to allow people to have \nrejection of these technologies. With much of \naccess to technological resources,\nthe financial and technical resources for A/IS \ndevelopment and deployment residing in HIC, not \n•  Existence of oligopolies that hinder new \nonly are A/IS benefits more difficult to access for \ntechnological development,\nLMIC populations, but those A/IS applications that \nare deployed outside of HIC realities may not be  •  Lack of inclusive and high-quality education at \nappropriate. This is for reasons of cultural/ethnic  all levels, and\nbias, language difficulties, or simply an inability to \n•  Bureaucratic policies ill-adapted to highly \nadapt to local internet infrastructure constraints. \ndynamic scenarios.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 148', ""The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nA/IS for Sustainable Development \nFor A/IS capacities and benefits to become  •  Supporting LMIC in the development of their \nequally available worldwide, training, education,  own A/IS strategies, and in the retention or \nand opportunities should be provided particularly  return of their A/IS talent to prevent “brain drain”.\nfor LMIC. Currently, access to products that \n•  Encouraging global standardization/\nfacilitate A/IS research of timely topics is quite \nharmonization and open source A/IS software.\nlimited for researchers in LMIC, due to cost \n•  Promoting distribution of knowledge and \nconsiderations.\nwealth generated by the latest A/IS, including \nIf A/IS capacity and governance problems, such  through formal public policy and financial \nas relevant laws, policies, regulations, and anti- mechanisms to advance equity worldwide.\ncorruption safeguards, are addressed, LMIC \n•  Developing public datasets to facilitate the \ncould have the ability to use A/IS to transform \naccess of people from LMIC to data resources \ntheir economies and leapfrog into a new era \nto facilitate their applied research, while \nof inclusive growth. Indeed, A/IS itself can \nensuring the protection of personal data.\ncontribute to good governance when applied \n•  Creating A/IS international research centers \nto the detection of corruption in state and \nin every continent, that promote culturally \nbanking institutions, one of the most serious \nappropriate research, and allow the remote \nrecognized constraints to investment in LMIC. \naccess of LMIC's communities to high-end \nParticular attention, however, must be paid to \ntechnology.16\nensure that the use of A/IS is for the common \ngood—especially in the context of LMIC—and  •  Facilitating A/IS access in LMIC through online \ndoes not reinforce existing socioeconomic  courses in local languages.\ninequities through systematic discriminatory bias \n•  Ensuring that, along with the use of A/IS, \nin both design and application, or undermine \ndiscussions related to identity, platforms, and \nfundamental rights through, among other issues, \nblockchain are conducted, such that core \nlax data privacy laws and practice.\nenabling technologies are designed to meet the \neconomic, social, and cultural needs of LMIC.\nRecommendations\n•  Diminishing the barriers and increase LMIC \nA/IS benefits should be equally available to  access to technological products, including the \npopulations in HIC and LMIC, in the interest of  formation of collaborative networks between \nuniversal human dignity, peace, prosperity, and  developers in HIC and LMIC, supporting the \nplanet protection. Specific measures for LMIC  latter in attending global A/IS conferences.17\nshould include:  •  Promoting research into A/IS-based \ntechnologies, for example, mobile lightweight \n•  Deploying A/IS to detect fraud and corruption, \nA/IS applications, that are readily available  \nto increase the transparency of power \nin LMIC.\nstructures, to contribute to a favorable \ninvestment, governance, and innovation  •  Facilitating A/IS research and development in \nenvironment.  LMIC through investment incentives, public-\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 149"", 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nA/IS for Sustainable Development \nprivate partnerships, and/or joint grants,  Countries. Industrialization in the 21st Century. \nand collaboration between international  New York: United Nations, 2006.\norganizations, government bodies, universities, \n•  M. Fong, Technology Leapfrogging for Developing \nand research institutes.\nCountries. Encyclopedia of Information Science \n•  Prioritizing A/IS infrastructure in international  and Technology, 2nd ed. Hershey, PA: IGI Global, \ndevelopment assistance, as necessary to  2009 (pp. 3707– 3713). \nimprove the quality and standard of living and \n•  C. B. Frey and M. A. Osborne. “The Future of \nadvance progress towards the SDGs in LMIC. \nEmployment: How Susceptible Are Jobs to \n•  Recognizing data issues that may be particular  Computerisation?” (working paper). Oxford, \nto LMIC contexts, i.e., insufficient sample size  U.K.: Oxford University, 2013.\nfor machine learning which sometimes results \n•  B. Hazeltine and C. Bull. Appropriate \nin de facto discrimination, and inadequate \nTechnology: Tools, Choices, and Implications. \nlaws for, and the practice of, data protection.  \nNew York: Academic Press, 1999. \n•  Supporting research on the adaptation of  \n•  McKinsey Global Institute. “Disruptive \nA/IS methods to scarce data environments  \nTechnologies: Advances That Will Transform \nand other remedies that facilitate an optimal  \nLife, Business, and the Global Economy” \nA/IS enabling environment in LMIC.\n(report), May 2013.\nFurther Resources •  D. Rotman, “How Technology Is Destroying \nJobs.” MIT Technology Review, June 12, 2013. \n•  A. Akubue, “Appropriate Technology for \nSocioeconomic Development in Third World  •  R. Sauter and J. Watson. “Technology \nCountries.” The Journal of Technology Studies  Leapfrogging: A Review of the Evidence, A \n26, no. 1, pp. 33–43, 2000. Report for DFID.” Brighton, England: University \nof Sussex. October 3, 2008.\n•  O. Ajakaiye and M. S. Kimenyi. “Higher \nEducation and Economic Development in  •  “The Rich and the Rest.” The Economist. \nAfrica: Introduction and Overview.” Journal of  October 13, 2012. \nAfrican Economies 20, no. 3, iii3–iii13, 2011.\n•  “Wealth without Workers, Workers without \n•  D. Allison-Hope and M. Hodge, ""Artificial  Wealth.” The Economist. October 4, 2014. \nIntelligence: A Rights-Based Blueprint for \n•  World Bank. “Global Economic Prospects \nBusiness,” San Francisco: BSF, Aug. 28, 2018\n2008: Technology Diffusion in the Developing \n•  D. E. Bloom, D. Canning, and K. Chan. Higher  World.” Washington, DC: World Bank, 2008. \nEducation and Economic Development in Africa \n•  World Development Report 2016: Digital \n(Vol. 102). Washington, DC: World Bank, 2006.\nDividends. Washington, DC: World Bank. \n•  N. Bloom, “Corporations in the Age of Inequality.”  doi:10.1596/978-1-4648-0671-1.\nHarvard Business Review, April 21, 2017.\n•  World Wide Web Foundation “Artificial Intelligence: \n•  C. Dahlman, Technology, Globalization, and  The Road ahead in Low and Middle-income \nCompetitiveness: Challenges for Developing  Countries,” webfoundation.org, June 2017.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 150', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nA/IS for Sustainable Development \nSection 3—A/IS and Employment\nHIC, where such jobs have already disappeared. \nIn addition, the qualities which made certain \nIssue: A/IS are changing the \njobs easy to outsource to LMIC where wages \nnature of work, disrupting \nare lower are those that may make them easy \nemployment, while technological  to automate.19 An offsetting factor is the reality \nchange is happening too fast for  that many LMIC lack the communication, energy, \nexisting methods of (re)training  and IT infrastructure required to support highly \nautomated industries.20 Notwithstanding this \nthe workforce.\nreality, the World Bank estimated the automatable \nshare of employment, unadjusted for adoption \nBackground\ntime lag, for LMIC ranges from 85% in Ethiopia \nThe current pace of technological development  to 62% in Argentina, compared to the OECD \nwill heavily influence changes in employment  average of 57%.21\nstructure. In order to properly prepare the \nIn the coming decades, the automation wave \nworkforce for such evolution, actions should be \ncalls for higher investment and the transformation \nproactive and not only reactive. The wave of \nof labor market capacity development programs. \nautomation caused by the A/IS revolution will \nInnovative and fair ways of funding such an \ndisplace a very large share of jobs across domains \ninvestment are required; the solutions should \nand value chains. The U.S. “automated vehicle” \nbe designed in cooperation with the companies \ncase study analyzed in the White House 2016 \nbenefiting from the increase of profitability, \nreport Artificial Intelligence, Automation, and the \nthanks to automation. This should be done \nEconomy is emblematic of what is at stake: “2.2 \nin a responsible way so that the innovation \nto 3.1 million existing part- and full-time U.S. jobs \ncycle is not broken, and yet workforce capacity \nare exposed over the next two decades, although \ndoes not fall behind the needs of 21st century \nthe timeline remains uncertain.”18 \nemployment. At the same time, A/IS and other \nThe risk of unemployment for LMIC is more  digital technologies offer real potential to innovate \nserious than for developed countries. The  new approaches to job-search assistance, \nindustry of most LMIC is labor intensive. While  placement, and hiring processes in the age of \nlabor may be cheap(er) in LMIC economies, the  personalized services. The efficiency of matching \nripple effects of A/IS and automation will be  labor supply and demand can be tremendously \nfelt much more than in the HIC economies. The  enhanced by the rise of multisided platforms \n2016 World Bank Development Report stated  and predictive analytics, provided they do not \nthat the share of occupations susceptible to  entrench discrimination.22 The case of platforms, \nautomation and A/IS is higher in LMIC than in  such as LinkedIn, for instance, with its 470 million \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 151', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nA/IS for Sustainable Development \nregistered users, and online job consolidators  •  Offering new technical programs, possibly \nsuch as indeed.com and Simply Hired, are  earlier than high school, to increase the \ninteresting as an evolution in hiring practices,   workforce capacity to close the skills gap and \nat least for those able to access the internet.  thrive in employment alongside A/IS. \n•  Creating opportunities for apprenticeships, \nTailored counseling and integrated retraining \npilot programs, and scaling up data-driven \nprograms also represent promising grounds \nevidence-based solutions that increase \nfor innovation. In addition, much will have to \nemployment and earnings.\nbe done to create fair and effective lifelong \nskill development/training, infrastructures, and  •  Supporting new forms of public-private \nmechanisms capable of empowering millions  partnerships involving civil society, as well as \nof people to viably transition jobs, sectors, and  new outcome-oriented financial mechanisms, \npotentially locations, and to address differential  e.g., social impact bonds, that help scale up \ngeographic impacts that exacerbate income  successful innovations.\nand wealth disparities. Effectively enabling the \n•  Supporting partnerships between universities, \nworkforce to be more mobile—physically, legally, \ninnovation labs in corporations, and \nand virtually—will be crucial. This implies systemic \ngovernments to research and incubate \npolicy approaches which encompass housing, \nstartups for A/IS graduates.23\ntransportation, licensing, tax incentives, and \n•  Developing regulations to hold corporations \ncrucially in the age of A/IS, universal broadband \nresponsible for employee retraining necessary \naccess, especially in rural areas of both HIC  \ndue to increased automation and other \nand LMIC.\ntechnological applications having impact  \non the workforce. \nRecommendations\n•  Facilitating private sector initiatives by public \nTo thrive in the A/IS age, workers must be \npolicy for co-investment in training and \nprovided training in skills that improve their \nretraining programs through tax incentives.\nadaptability to rapid technological changes; \n•  Establishing and resourcing public policies that \nprograms should be available to any worker, with \nassure the survival and well-being of workers, \nspecial attention to the low-skilled workforce. \ndisplaced by A/IS and automation, who cannot \nThose programs can be private, that is, sponsored \nbe retrained.\nby the employer, or publicly and freely offered \nthrough specific public channels and government  •  Researching complementary areas, to lay solid \npolicies, and should be available regardless of  foundations for the transformation outlined \nwhether the worker is in between jobs or still  above.\nemployed. Specific measures include: \n•  Requiring more policy research on the \n \ndynamics of professional transitions in \n \ndifferent labor market conditions. \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 152', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nA/IS for Sustainable Development \n•  Researching the fairest and most efficient \npublic-private options for financing labor \nIssue: Analysis of the  \nforce transformation due to A/IS.\nA/IS impact on employment \n•  Developing national and regional future of  is too focused on the number \nwork strategies based on sound research \nand category of jobs affected, \nand strategic foresight.\nwhereas more attention should \nbe addressed to the complexities \nFurther Resources\nof changing the task content  \n•  V. Cerf and D. Norfors, The People-centered \nof jobs. \nEconomy: The New Ecosystem for Work. \nCalifornia: IIIJ Foundation, 2018.\nBackground\n•  Executive Office of the President. Artificial \nCurrent attention on automation and \nIntelligence, Automation, and the Economy. \nemployment tends to focus on the sheer number \nDecember 20, 2016. \nof jobs lost or gained. It is important to focus \n•  S. Kilcarr, “Defining the American Dream for  the analysis on how employment structures \nTrucking ... and the Nation, Too,” FleetOwner,  will be changed by A/IS, rather than solely \nApril 26, 2016.  dwelling on the number of jobs that might be \nimpacted. For example, rather than carrying out \n•  M. Mason, “Millions of Californians’ Jobs could  a task themselves, workers will need to shift \nbe Affected by Automation—a Scenario the  to supervision of robots performing that task. \nnext Governor has to Address,”Los Angeles  Other concerns include changes in traditional \nTimes, October 14, 2018. employment structures, with an increase \nin flexible, contract-based temporary jobs, \n•  OECD, “Labor Market Programs: Expenditure \nwithout employee protection, and a shift in \nand Participants,” OECD Employment and \ntask composition away from routine/repetitive \nLabor Market Statistics (database), 2016. \nand toward complex decision-making. This \n•  M. Vivarelli, “Innovation and Employment: A  is in addition to the enormous need for the \nSurvey,” Institute for the Study of Labor (IZA)  aforementioned retraining. Given the extent of \nDiscussion Paper No. 2621, February 2007.  disruption, workforce trends will need to measure \ntime spent unemployed or underemployed, labor \n \nforce participation rates, and other factors beyond \n \nsimple unemployment numbers. \n \n \n \n \n \n \n \n \n \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 153', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nA/IS for Sustainable Development \nThe Future of Jobs 2018 report of the World  augmentation becomes increasingly common \nEconomic Forum highlights:   over the coming years as a way to supplement \nand complement human labour.”24\n“...the potential of new technologies to create as \nwell as disrupt jobs and to improve the quality  The report predicts the shift in skill demand \nand productivity of the existing work of human  between today and 2022 will be significant and \nemployees. Our findings indicate that, by 2022,  that “proactive, strategic and targeted efforts will \naugmentation of existing jobs through technology  be needed to map and incentivize workforce \nmay free up workers from the majority of data  redeployment… [and therefore]... investment \nprocessing and information search tasks—and  decisions [on] whether to prioritize automation or \nmay also increasingly support them in high-value  augmentation and the question of whether or not \ntasks such as reasoning and decision-making as  to invest in workforce reskilling.”25\nComparing Skills Demand, 2018 Versus 2022, Top Ten \nTODAY, 2018 TRENDING, 2022 DECLINING, 2022\n1.  Analytical thinking   1.  Analytical thinking   1.  Manual dexterity, \nand innovation and innovation endurance, and precision\n2.  Complex problem- 2.  Active learning and  2.  Memory, verbal, auditory, \nsolving learning strategies and spatial abilities\n3.  Critical thinking   3.  Creativity, originality,   3.  Management of financial \nand analysis and initiative and material resources\n4.  Active learning   4.  Technology design   4.  Technology installation \nand learning strategies and programming and maintenance\n5.  Creativity, originality,   5.  Critical thinking   5.  Reading, writing, math, \nand initiative and analysis and active listening\n6.  Attention to detail,  6.  Complex problem- 6.  Management  \ntrustworthiness solving of personnel\n7.  Emotional Intelligence 7.  Leadership and   7.  Quality control and \nsocial influence safety awareness\n8.  Reasoning, problem-\nsolving, and ideation 8.  Emotional intelligence 8.  Coordination and  \ntime-management\n9.  Leadership and   9.  Reasoning, problem- \nsocial influence solving, and ideation 9.  Visual, auditory,  \nand speech abilities\n10. Coordination and   10. Systems analysis  \ntime management and evaluation 10. Technology use, \nmonitoring, and control\nSource: Future of Jobs Survey 2018, World Economic Forum, Table 4 \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 154', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nA/IS for Sustainable Development \nRecommendations •  Considering both product and process \ninnovation, and looking at them from a global \nWhile there is evidence that robots and \nperspective in order to understand properly \nautomation are taking jobs away in various \nthe global impact of A/IS on employment.\nsectors, a more balanced, granular, analytical, \nand objective treatment of A/IS impact on  •  Proposing mechanisms for redistribution of \nthe workforce is needed to effectively inform  productivity increases and developing an \npolicy making and essential workforce reskilling.  adaptation plan for the evolving labor market.\nSpecifics to accomplish this include:\nFurther Resources\n•  Creating an international and independent \n•  E. Brynjolfsson and A. McAfee. The Second \nagency able to properly disseminate objective \nAge of Machine Intelligence: Work Progress \nstatistics and inform the media, as well as the \nand Prosperity in a Time of Brilliant \ngeneral public, about the impact of robotics \nTechnologies. New York, NY: W. W. Norton & \nand A/IS on jobs, tax revenue, growth,26 and \nCompany, 2014.\nwell-being.\n•  Analyzing and disseminating data on how  •  P.R. Daugherty, and H.J. Wilson, Human + \ncurrent task content of jobs have changed,  Machine: Reimagining Work in the Age of AI. \nbased on a clear assessment of the  Watertown, MA: Harvard Business Review \nautomatability of the occupational   Press, 2018. \ndescription of such jobs.\n•  International Federation of Robotics. “The \n•  Promoting automation with augmentation, as \nImpact of Robots on Productivity, Employment \nrecommended in the Future of Jobs Report \nand Jobs,” A positioning paper by the \n2018 (see chart on page 154), to maximize \nInternational Federation of Robotics, April \nthe benefit of A/IS to employment and \n2017. \nmeaningful work.\n•  RockEU. “Robotics Coordination Action for \n•  Integrating more granulated dynamic mapping \nEurope Report on Robotics and Employment,” \nof the future jobs, tasks, activities, workplace-\nDeliverable D3.4.1, June 30, 2016. \nstructures, associated work-habits, and \nskills base spurred by the A/IS revolution, \n•  World Economic Forum, Centre for the New \nin order to innovate, align, and synchronize \nEconomy and Society, The Future of Jobs \nskill development and training programs with \n2018, Geneva: WEF 2018.\nfuture requirements. This workforce mapping \nis needed at the macro, but also crucially at \nthe micro, levels where labor market programs  \nare deployed.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 155', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nA/IS for Sustainable Development \nSection 4—Education for the A/IS Age\nvery limited, in view of the enormous ethical and \nsocial problems associated with technology.28 \nIssue: Education to prepare \nEnhancing the global dimension of engineering  \nthe future workforce, in both \nin undergraduate and postgraduate A/IS \nHIC and LMIC, to design ethical \neducation is necessary, so that students can  \nA/IS applications or to have  be prepared as technical professionals, aware  \na comparative advantage in  of the opportunities and risks that A/IS present, \nand ready for work anywhere in the world in  \nworking alongside A/IS, is either \nany sector. \nlacking or unevenly available, \nrisking inequality perpetuated  Engineering studies at the university and \npostgraduate levels is just one dimension of the \nacross generations, within and \nA/IS education challenge. For instance, business, \nbetween countries, constraining \nlaw, public policy, and medical students will \nequitable growth, supporting  also need to be prepared for professions where \na sustainable future, and  A/IS are a partner, and to have internalized \nethical principles to guide the deployment of \nachievement of the SDGs.\nsuch technologies. LMIC need financial and \nacademic support to incorporate global A/IS \nBackground\nprofessional curricula in their own universities, \nMultiple international institutions, in particular  and all countries need to develop the pipeline \neducational engineering organizations,27 have  by preparing elementary and secondary \ncalled on universities to play an active role,  school students to access such professional \nboth locally and globally, in the resolution of  programs. While the need for curriculum reform \nthe enormous problems that the world faces in  is recognized, the impact of A/IS on various \nsecuring peace, prosperity, planet protection,  professions and socioeconomic contexts is, at this \nand universal human dignity: armed conflict,  time, both evolving and largely undocumented. \nsocial injustice, rapid climate change, abuse  Thus, the overhaul of education systems at all \nof human rights, etc. Addressing global social  levels should be preceded by A/IS research. \nproblems is one of the central objectives of many \nMuch of LMIC education is not globally \nuniversities, transversal to their other functions, \ncompetitive today, so there is a risk that the \nincluding research in A/IS. UNESCO points out \nglobal advent of A/IS could negatively affect \nthat universities’ preparation of future scientists \nthe chances of young people in LMIC finding \nand engineers for social responsibility is presently \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 156', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nA/IS for Sustainable Development \nproductive employment, further fueling global  Recommendations \ninequality. Education systems worldwide have \nEducation with respect to A/IS must be targeted \nto be reformed and transformed to fit the new \nto three sets of students: the general public, \ndemands of the information age, in view of \npresent and future professionals in A/IS, and \nthe changing mix of skills demanded from the \npresent and future policy makers. To prepare the \nworkforce.29 In 21st century education, it has \nfuture workforce to develop culturally appropriate \nbeen observed that children need less rote \nA/IS, to work productively and ethically alongside \nknowledge, given so much is instantly accessible \nsuch technologies, and to advance the UN SDGs, \non the web and more tools to network and \nthe curricula in HIC and LMIC universities and \ninnovate are available; less memory and more \nprofessional schools require innovation. Equally \nimagination should be developed; and fewer \nimportantly, preuniversity education systems, \nphysical books and more internet access is \nstarting with early childhood education, need to \nrequired. Young people everywhere need to \nbe reformed to prepare society for the risks and \ndevelop their capacities for creativity, human \nopportunities of the A/IS age, rather than the \nempathy, ethics, and systems thinking in order \ncurrent system which prepares society for work \nto work productively alongside robots and A/IS \nin an industrial age that ended with the 20th \ntechnologies. Science, Technology, Engineering, \ncentury. Specific recommendations include:\nArt/design, and Math (STEAM) subjects need to \nbe more extensive and more creatively taught.30  •  Preparing future managers, lawyers, \nIn addition, research is needed to establish ways  engineers, civil servants, and entrepreneurs \nthat a new subject, empathy, can be added to  to work productively and ethically as global \nthese crucial 21st century subjects in order to  citizens alongside A/IS, through reform of \neducate the future A/IS workforce in social skills.  undergraduate and graduate curricula as \nInstead, in rich and poor countries alike, children  well as of preschool, primary, and secondary \nare continuing to be educated for an industrial  school curricula. This will require:\nage which has disappeared or never even arrived. \n•  Fomenting interaction between universities \nLMIC education systems, being less entrenched \nand other actors such as companies, \nin many countries, may have the potential to be \ngovernments, NGOs, etc., with respect to \nmore flexible than those in HIC. Perhaps A/IS \nA/IS research through definition of research \ncan be harnessed to help educational systems \npriorities and joint projects, subcontracts to \nto leapfrog into the 21st century, just as mobile \nuniversities, participation in observatories, \nphone technology enabled LMIC leapfrog over \nand co-creation of curricula, cooperative \nthe phase of wired communication infrastructure.   \nteaching, internships/service learning, and \n  conferences/seminars/courses.\n \n•  Establishing and supporting more \nmultidisciplinary degrees that include  \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 157', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nA/IS for Sustainable Development \nA/IS, and adapting university curricula to  •  Training teachers in teaching \nprovide a broad, integrated perspective  methodologies suited to addressing \nwhich allows students to understand the  challenges imposed in the age of A/IS. \nimpact of A/IS in the global, economic, \n•  Stimulating STEAM courses in preuniversity \nenvironmental, and sociocultural domains \neducation. \nand trains them as future policy makers in \nA/IS fields. \n•  Encouraging high-quality HIC-LMIC \ncollaborative A/IS research in both private \n•  Integrating the teaching of ethics and  \nand public universities.\nA/IS across the education spectrum, from \npreschool to postgraduate curricula, instead \n•  Conducting research to support innovation in \nof relegating ethics to a standalone module \neducation and business for the A/IS world, \nwith little direct practical application.\nwhich could include:\n•  Promoting service learning opportunities \n•  Researching the impact of A/IS on the \nthat allow A/IS undergraduate and graduate \ngovernance and macro/micro strategies of \nstudents to apply their knowledge to meet \ncompanies and organizations, together with \nthe needs of a community. \nthose companies, in an interdisciplinary \nmanner which harnesses expertise of both \n•  Creating international exchange programs, \nsocial scientists and technology experts.\nthrough both private and public institutions, \nwhich expose students to different cultural \n•  Researching the impact of A/IS on the \ncontexts for A/IS applications in both HIC \nbusiness model for the development of \nand LMIC.\nnew products and services through the \ncollaborative efforts of management, \n•  Creating experimental curricula to prepare \noperations, and the technical research and \npeople for information-based work in \ndevelopment function.\nthe 21st century, from preschool through \npostgraduate education.\n•  Researching how empathy can be taught \nand integrated into curricula, starting at the \n•  Taking into account transversal \npreschool level.\ncompetencies students need to acquire to \nbecome ethical global citizens, i.e., critical \n•  Researching how schools and education \nthinking, empathy, sociocultural awareness, \nsystems in low-income settings of both \nflexibility, and deontological reasoning  \nHIC and LMIC can leverage their less-\nin the planning and assessment of  \nentrenched interests to leapfrog into a 21st \nA/IS curricula. \ncentury-ready education system. \n \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 158', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nA/IS for Sustainable Development \n•  Establishing ethics observatories in  Further Resources\nuniversities with the purpose of fostering \n•  ABET Computing and Engineering \nan informed public opinion capable  \nAccreditation Criteria 2018. Available \nof participating in policy decisions  \nat: http://www.abet.org/accreditation/\nregarding the ethics and social impact  \naccreditation-criteria/\nof A/IS applications.\n•  ABET, 2017 ABET Impact Report, Working \n•  Creating professional continuing education \nTogether for a Sustainable Future, 2017.\nand employment opportunities in A/IS for \ncurrent professionals, including through  •  emlyon business school, Artificial Intelligence \nonline and executive education courses. in Management (AIM) Institute http://aim.\nem-lyon.com\n•  Creating educative mass media campaigns \nto elevate society’s ongoing baseline  •  UNESCO, The UN Decade of Education \nlevel of understanding of A/IS systems,  for Sustainable Development, Shaping the \nincluding what it is, if and how it can be  Education of Tomorrow. UNESCO 2012.\ntrusted in various contexts, and what are its \nlimitations.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 159', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nA/IS for Sustainable Development \nSection 5—A/IS and Humanitarian Action\nconflict zones and to enable early warning \nsystems.35 For example, Microsoft has partnered \nIssue: A/IS are contributing to \nwith the UN Human Rights Office of the High \nhumanitarian action to save lives, \nCommissioner (OHCHR) to use big data in order \nalleviate suffering, and maintain \nto track and analyze human rights violations \nhuman dignity both during and  in conflict zones.36 Machine learning is being \nin the aftermath of man-made  used for improved decision-making regarding \nasylum adjudication and refugee resettlement, \ncrises and natural disasters, as \nwith a view to increasing successful integration \nwell as to prevent and strengthen \nbetween refugees and host communities.37 In \npreparedness for the occurrence  addition, there is evidence that a recent growth in \nof such situations. However,  human empathy has increased well-being while \ndiminishing psychological and physical violence,38 \nthere are ethical concerns with \ninspiring some researchers to look for ways of \nboth the collection and use \nharnessing the power of A/IS to introduce more \nof data during humanitarian \nempathy and less violence into society. \nemergencies.\nThe design and ethical deployment of these \ntechnologies in crisis settings are both essential \nBackground\nand challenging. Large volumes of both personally \nThere have been a number of promising  identifiable and demographically identifiable \nA/IS applications that relieve suffering in  data are collected in fragile environments, where \nhumanitarian crises, such as extending the  tracking of individuals or groups may compromise \nreach of the health system by using drones  their security if data privacy cannot be assured. \nto deliver blood to remote parts of Rwanda,31  Consent to data use is also impractical in such \nlocating and removing landmines,32 efforts to  environments, yet crucial for the respect of \nuse A/IS to track movements and population  human rights. \nsurvival needs following a natural disaster, and \n \nto meet the multiple management requirements \n \nof refugee camps.33 There are also promising \n \ndevelopments using A/IS and robotics to assist   \npeople with disabilities to recover mobility, and   \nrobots to rescue people trapped in collapsed   \nbuildings.34 A/IS are also being used to monitor \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 160', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nA/IS for Sustainable Development \nRecommendations •  Setting up clear ethical frameworks for \nexceptional use of A/IS technologies in life-\nThe potential for A/IS to contribute to \nsaving humanitarian situations, compared  \nhumanitarian action to save and improve \nto ""normal"" situations.41\nlives should be prioritized for research and \ndevelopment, including by organizing global  •  Stimulating the development of low-cost  \nresearch challenges, while also building in  and open source solutions based on A/IS  \nsafeguards to protect the creation, collection,  to address specific humanitarian problems.\nprocessing, sharing, use, and disposal \nof information, including data from and  •  Training A/IS experts in humanitarian action \nabout individuals and populations. Specific  and norms, and humanitarian practitioners  \nrecommendations include: to catalyze collaboration in designing,  \npiloting, developing, and implementing  \nA/IS technologies for humanitarian purposes. \n•  Promoting awareness of the vulnerable \nForging public-private A/IS participant alliances \ncondition of certain communities around the \nthat develop crisis scenarios in advance.\nglobe and the need to develop and use A/IS \napplications for humanitarian purposes.\n•  Working on cultural and contextual acceptance \nof any A/IS introduced during emergencies.\n•  Elaborating competitions and challenges \nin high impact conferences and university \n•  Documenting and developing quantifiable \nhackathons to engage both technical and \nmetrics for evaluating the outcomes of \nnontechnical communities in the development \nhumanitarian digital projects, and educating \nof A/IS for humanitarian purposes and to \nthe humanitarian ecosystem on the same. \naddress social issues.\n \n \n•  Support civil society groups who organize \n \nthemselves for the purpose of A/IS research \n \nand advocacy to develop applications to \n \nbenefit humanitarian causes.39\n \n•  Developing and applying ethical standards for   \nthe collection, use, sharing, and disposal of   \ndata in fragile settings.  \n \n•  Following privacy protection frameworks for \n \npressing humanitarian situations that ensure \n \nthe most vulnerable are protected.40  \n \n \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 161', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nA/IS for Sustainable Development \nFurther Resources\n•  J.A. Quinn, et al., “Humanitarian applications \n•  E. Prestes et al., ""The 2016 Humanitarian \nof machine learning with remote-sensing data: \nRobotics and Automation Technology Challenge \nreview and case study in refugee settlement \n[Competitions],"" in IEEE Robotics & Automa-\nmapping” Philosophical Transactions of \ntion Magazine, vol. 23, no. 3, pp. 23-24, Sept. \nthe Royal Society A, 376 20170363; DOI: \n2016. http://ieeexplore.ieee.org/stamp/\n10.1098/rsta.2017.0363. Aug. 6, 2018.\nstamp.jsp?tp=&arnumber=7565695&isnum-\nber=7565655 •  Humanitarian Innovation Guide: https://\nhiguide.elrha.org/, 2019.\n•  L. Marques et al., ""Automation of humanitarian \ndemining: The 2016 Humanitarian Robotics \n•  P. Meier, Digital Humanitarians: How Big \nand Automation Technology Challenge,"" 2016 \nData is Changing the Face of Humanitarian \nInternational Conference on Robotics and \nResponse. Florida: CRC Press, 2015. \nAutomation for Humanitarian Applications \n(RAHA), Kollam, 2016, pp. 1-7. http://ieeex- •  “Technology for human rights: UN Human \nplore.ieee.org/stamp/stamp.jsp?tp=&arnum- Rights Office announces landmark partnership \nber=7931893&isnumber=7931858 with Microsoft” https://www.ohchr.org/\nEN/NewsEvents/Pages/DisplayNews.\n•  CYBATHLON 2020 Preliminary Race Task \naspx?NewsID=21620&LangID=E\nDescriptions http://www.cybathlon.ethz.\nch/cybathlon-2020/preliminary-race-task-\n•  M. Luengo-Oroz, “10 big data science \ndescriptions.html\nchallenges facing humanitarian organizations,” \nUNHCR, Nov. 22, 2016. http://www.\n•  CYBATHLON Scientific Publications  \nunhcr.org/innovation/10-big-data-science-\nhttp://www.cybathlon.ethz.ch/\nchallenges-facing-humanitarian-organizations/\n•  Immigration Policy Lab (IPL), “Harnessing \n•  Optic Technologies, Press Release, Vatican \nBig Data to Improve Refugee Resettlement” \nHack 2018—Results, 18 March 2018, which \nhttps://immigrationlab.org/project/harnessing-\nannounced winning AI applications to benefit \nbig-data-to-improve-refugee-resettlement/\nmigrants and refugees as well as social \n•  Harvard Humanitarian Initiative, The Signal  inclusion and interfaith dialogue,  \nCode, https://signalcode.org  http://optictechnology.org/index.php/en/\n  news-en/151-vhack-2018winners-en \n   \n \n \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 162', ""The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nA/IS for Sustainable Development \nThanks to the Contributors\nWe wish to acknowledge all of the people who  •  Renaud Champion – Director of Emerging \nIntelligences, emlyon business school; \ncontributed to this chapter. \n  Founder of Robolution Capital & CEO of \nThe A/IS for Sustainable   PRIMNEXT\nDevelopment Committee\n•  Chandramauli Chaudhuri – Senior Data \n•  Elizabeth D. Gibbons (Chair) – Senior  Scientist; Fractal Analytics\nFellow and Director of the Child Protection \n•  Rozita Dara – Assistant Professor, Principal \nCertificate Program, FXB Center for Health and \nInvestigator of Data Management and Data \nHuman Rights, Harvard T.H. Chan School of \nGovernance program, School of Computer \nPublic Health\nScience, University of Guelph, Canada\n•  Kay Firth-Butterfield (Founding Co-Chair) \n•  Scott L. David – Director of Policy at \n– Project Head, AI and Machine Learning \nUniversity of Washington—Center for Data \nat the World Economic Forum. Founding \nManagement and Privacy Governance \nAdvocate of AI-Global; Senior Fellow and \nLabInformation Assurance and Cybersecurity\nDistinguished Scholar, Robert S. Strauss Center \nfor International Security and Law, University \n•  Jia He – Executive Director of Toutiao \nof Texas, Austin; Co-Founder, Consortium for \nResearch (Think Tank), Bytedance Inc. \nLaw and Ethics of Artificial Intelligence and \nRobotics, University of Texas, Austin; Partner,  •  William Hoffman – Associate director and \nCognitive Finance Group, London, U.K. head of Data-Driven Development, The World \nEconomic Forum\n•  Raj Madhavan (Founding Co-Chair) – \nFounder & CEO of Humanitarian Robotics  •  Michael Lennon – Senior Fellow, Center \nTechnologies, LLC, Maryland, U.S.A. for Excellence in Public Leadership, George \nWashington University; Co-Founder, \n•  Ronald C. Arkin – Regents' Professor & \nGovpreneur.org; Principal, CAIPP.org \nDirector of the Mobile Robot Laboratory; \n(Consortium for Action Intelligence and \nAssociate Dean for Research & Space \nPositive Performance); Member, Wellbeing \nPlanning, College of Computing, Georgia \nMetrics Standard for Ethical Artificial \nInstitute of Technology\nIntelligence and Autonomous Systems \nCommittee\n•  Joanna J. Bryson – Reader (Associate \nProfessor), University of Bath, Intelligent \n•  Miguel Luengo-Oroz – Chief Data Scientist, \nSystems Research Group, Department of \nUnited Nations Global Pulse. \nComputer Science \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 163"", 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nA/IS for Sustainable Development \n•  Angeles Manjarrés – Professor of the  •  Simon Pickin – Professor, Dpto. de Sistemas \nDepartment of Artificial Intelligence of the  Informáticos y Computación, Facultad de \nSpanish National Distance-Learning University Informática, Universidad Complutense de \nMadrid, Spain\n•  Nicolas Miailhe – Co-Founder & \nPresident, The Future Society; Member, AI  •  Rose Shuman – Partner at BrightFront Group \nExpert Group at the OECD; Member, Global  & Founder, Question Box\nCouncil on Extended Intelligence; Senior \n•  Hruy Tsegaye – One of the founders of \nVisiting Research Fellow, Program on Science \niCog Labs; a pioneer company in East Africa \nTechnology and Society at Harvard Kennedy \nto work on Research and Development of \nSchool. Lecturer, Paris School of International \nArtificial General Intelligence, Ethiopia\nAffairs (Sciences Po). Visiting Professor, IE \nSchool of Global and Public Affairs\nFor a full listing of all IEEE Global Initiative \nMembers, visit standards.ieee.org/content/dam/\n•  Roya Pakzad – Research Associate and \nieee-standards/standards/web/documents/other/\nProject Leader in Technology and Human \nec_bios.pdf. \nRights, Global Digital Policy Incubator (GDPi), \nStanford University\nFor information on disclaimers associated with \nEAD1e, see How the Document Was Prepared.\n•  Edson Prestes – Professor, Institute of \nInformatics, Federal University of Rio Grande \ndo Sul (UFRGS), Brazil; Head, Phi Robotics \nResearch Group, UFRGS; CNPq Fellow \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 164', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nA/IS for Sustainable Development \nEndnotes\n1   See, for example, the writing of T. Piketty, Capital  Goal 2. End hunger, achieve food security and \nin the Twenty-First Century (Cambridge: Belknap  improved nutrition and promote sustainable \nPress 2014). agriculture\n2  See preamble of the United Nations General  Goal 3. Ensure healthy lives and promote well-being \nAssembly, Transforming our world: the   for all at all ages\n2030 Agenda for Sustainable Development  \nGoal 4. Ensure inclusive and equitable quality \n(A/RES/70/1: 21 October 2015): “This Agenda is \neducation and promote lifelong learning \na plan of action for people, planet and prosperity. \nopportunities for all\nIt also seeks to strengthen universal peace in \nlarger freedom. We recognize that eradicating \nGoal 5. Achieve gender equality and empower all \npoverty in all its forms and dimensions, including \nwomen and girls\nextreme poverty, is the greatest global challenge \nand an indispensable requirement for sustainable  Goal 6. Ensure availability and sustainable management \ndevelopment. All countries and all stakeholders,  of water and sanitation for all\nacting in collaborative partnership, will implement \nGoal 7. Ensure access to affordable, reliable, \nthis plan. We are resolved to free the human \nsustainable and modern energy for all\nrace from the tyranny of poverty and want and to \nheal and secure our planet. We are determined  Goal 8. Promote sustained, inclusive and sustainable \nto take the bold and transformative steps which  economic growth, full and productive employment \nare urgently needed to shift the world on to a  and decent work for all\nsustainable and resilient path. As we embark on \nthis collective journey, we pledge that no one will  Goal 9. Build resilient infrastructure, promote inclusive \nbe left behind. The 17 Sustainable Development  and sustainable industrialization and foster innovation\nGoals and 169 targets which we are announcing \nGoal 10. Reduce inequality within and among countries\ntoday demonstrate the scale and ambition of this \nnew universal Agenda.” Goal 11. Make cities and human settlements inclusive, \nsafe, resilient and sustainable\n3  Ibid, paragraph 8.\nGoal 12. Ensure sustainable consumption and \n4  A/IS has the potential to advance positive \nproduction patterns\nchange toward all seventeen 2030 Sustainable \nDevelopment Goals, which are:  Goal 13. Take urgent action to combat climate change \nand its impacts\nGoal 1. End poverty in all its forms everywhere\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 165', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nA/IS for Sustainable Development \nGoal 14. Conserve and sustainably use the oceans,  8  World Economic Forum Global Future Council \nseas and marine resources for sustainable  on Human Rights 2016-18 “White Paper: How \ndevelopment to Prevent Discriminatory Outcomes in Machine \nLearning” (WEF: March 2018).\nGoal 15. Protect, restore and promote sustainable \nuse of terrestrial ecosystems, sustainably manage  9  World Wide Web Foundation Artificial Intelligence: \nforests, combat desertification, and halt and reverse  the Road ahead in Low and Middle-income \nland degradation and halt biodiversity loss Countries (June 2017: webfoundation.org) p.13\nGoal 16. Promote peaceful and inclusive societies for  10  See the Well-being chapter of Ethically Aligned \nsustainable development, provide access to justice  Design, First Edition\nfor all and build effective, accountable and inclusive \n11  See, for example, S. Vosougi, D. Roy, and S. Aral, \ninstitutions at all levels\n“The spread of true and false news online” Science \nGoal 17. Strengthen the means of implementation   09 Mar 2018: Vol. 359, Issue 6380, pp. 1146-1151 \nand revitalize the global partnership for   and M. Fox, “Fake News:Lies spread faster on \nsustainable development social media than Truth does” NBC Health News, \n8 March 2018 https://www.nbcnews.com/health/\nSource: United Nations General Assembly, \nhealth-news/fake-news-lies-spread-faster-social-\nTransforming our world: the 2030 Agenda  \nmedia-truth-does-n854896; Cyberbullying Research \nfor Sustainable Development   \nCenter: Summary of Cyberbullying Research 2004-\n(A/RES/70/1: 21 October 2015) p. 14\n2016 https://cyberbullying.org/summary-of-our-\ncyberbullying-research and TeenSafe “Cyberbullying \n5  United Nations Secretary General “The road \nFacts and Statistics” TeenSafe October 4, 2016, \nto dignity by 2030: ending poverty, transforming \nhttps://www.teensafe.com/blog/cyber-bullying-\nall lives and protecting the planet” United \nfacts-and-statistics/\nNations, A/69/700, 4 December 2014, pp. 25-\n27 http://www.un.org/ga/search/view_doc.\nA. Hutchison, “Social Media Still Has a Fake News \nasp?symbol=A/69/700&Lang=E \nProblem―and Digital Literacy is Largely to Blame” \nSocial Media Today, October 5, 2018 https://\n6  Internet World Stats https://www.\nwww.socialmediatoday.com/news/social-media-\ninternetworldstats.com/stats.htm, accessed  \nstill-has-a-fake-news-problem-and-digital-literacy-\n17 May 2018.  \nis-largel/538930/; D.D. Luxton, J.D. June, and \n7  (“Internet of Things, Privacy and Security in  J. M. Fairall, “Social Media and Suicide: A Public \na Connected World,” FTC, https:// www.ftc. Health Perspective”, Am J Public Health. 2012 May; \ngov/system/ les/documents/reports/federal- 102(Suppl 2): S195–S200. J. Twege, T. E. Joiner, \ntrade-commission-staff- report-november- M.L. Rogers, “Increases in Depressive Symptoms, \n2013-workshop-entitled-internet-things- Suicide-Related Outcomes, and Suicide Rates \nprivacy/150127iotrpt.pdf) Among U.S. Adolescents After 2010 and Links \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 166', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nA/IS for Sustainable Development \nto Increased New Media Screen Time” Clinical  18  Executive Office of the President of the United \nPsychological Science, November 14, 2017 https:// States. Artificial Intelligence, Automation, and the \ndoi.org/10.1177/2167702617723376 Economy. December 20, 2016. page 21.\n12  D.D. Luxton, J.D. June, and J. M. Fairall, “Social  19 From World Wide Web Foundation Artificial \nMedia and Suicide: A Public Health Perspective”,  Intelligence: The Road ahead in Low and Middle-\nAm J Public Health. 2012 May; 102(Suppl 2):  income Countries (June 2017: webfoundation.org) \nS195–S200. J. Twege, T. E. Joiner, M.L. Rogers,  page 8.\n“Increases in Depressive Symptoms, Suicide-\n20  Ibid.\nRelated Outcomes, and Suicide Rates Among U.S. \nAdolescents After 2010 and Links to Increased \n21  World Bank, 2016. World Development Report \nNew Media Screen Time” Clinical Psychological \n2016: Digital Dividends. Washington, DC: World \nScience, November 14, 2017 https://doi.\nBank. doi:10.1596/978-1-4648-0671-1 page 129.\norg/10.1177/2167702617723376\n22  See for example: J. Dasten, “Amazon scraps \n13  T. Luong, “Thermostats, Locks and Lights: \nsecret AI recruiting tool that showed bias against \nDigital Tools of Domestic Abuse.” The New York \nwomen” Reuters Business News October 9, 2018, \nTimes, June 23, 2018, https://www.nytimes.\nhttps://www.reuters.com/article/us-amazon-com-\ncom/2018/06/23/technology/smart-home-\njobs-automation-insight/amazon-scraps-secret-ai-\ndevices-domestic-abuse.html\nrecruiting-tool-that-showed-bias-against-women-\nidUSKCN1MK08G\n14  P. Mozur, ”A Genocide incited on Facebook with \nposts from Myanmar’s Military”, The New York \n23  For example, The Vector Institute, CIFAR and \nTimes, October 15, 2018. https://www.nytimes.\nthe Legal Innovation Group at Ryerson University.  \ncom/2018/10/15/technology/myanmar-facebook-\nSee https://vectorinstitute.ai and http://www.\ngenocide.html\nlegalinnovationzone.ca.\n15  United Nations Human Rights Council “Human \n24  World Economic Forum, Centre for the New \nrights situations that require the Council’s attention \nEconomy and Society the Future of Jobs 2018 \nReport of the independent international fact-\n(Geneva: WEF 2018) p. 3.\nfinding mission on Myanmar*” (A/HRC/39/64, 12 \nSeptember 2018) 25  Ibid, page 9\n16  See for example Google AI in Ghana https:// 26  It must be noted that the OECD is already \nwww.blog.google/around-the-globe/google-africa/ engaged in this work as well as are some \ngoogle-ai-ghana/ government bodies. See http://www.oecd.org/\nemployment/future-of-work/ \n17   See Artificial Intelligence: the Road ahead in \n \nLow and Middle-income Countries \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 167', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nA/IS for Sustainable Development \n27  UNESCO, WHO, ABET, Bologna Follow-Up Group  https://www.worldbank.org/en/news/press-\nSecretariat for the European Higher Education Area  release/2018/09/23/united-nations-world-bank-\nhumanitarian-organizations-launch-innovative-\n28  UNESCO, The UN Decade of Education for \npartnership-to-end-famine\nSustainable Development, Shaping the Education  \nof Tomorrow. (UNESCO: Paris 2012).  36  ""United Nations Human Rights Office of the \nHigh Commissioner, press release, ""Technology for \n29  See Future of Jobs Report 2018 Survey table,  \nhuman rights: UN Human Rights Office announces \np. 154.\nlandmark partnership with Microsoft” 16 May 2017."" \nhttps://www.ohchr.org/EN/NewsEvents/Pages/\n30  National Math and Science Initiative, STEM \nDisplayNews.aspx?NewsID=21620&LangID=E\nEducation and Workforce, 2014 https://www.nms.\norg/Portals/0/Docs/STEM%20Crisis%20Page%20\n37  For example, researchers at Stanford University \nStats%20and%20References.pdf\nare running a pilot project to develop machine \nlearning algorithms for a better resettlement \n31 https://www.bloomberg.com/news/\nprogram. To train their algorithm, the Immigration \narticles/2018-08-16/this-27-year-old-launches-\nPolicy Lab (IPL) at Stanford University and ETH \ndrones-that-deliver-blood-to-rwanda-s-hospitals\nZurich gathered data from refugee resettlement \n32 https://www.theguardian.com/sustainable- agencies in the US and Switzerland. The model \nbusiness/2015/may/25/robots-rescue-lethal- is optimized based on refugees’ background and \nrehabilitation-landmines-drones skill sets to match them to a host city in which \nthe individual has a higher chance of finding \n33  See for example, C. Fey, “Tech can improve lives \nemployment. \nin refugee camps” Cambridge Network, 10 May \n2018 https://www.cambridgenetwork.co.uk/news/ 38  See for example S. Pinker, The Better Angels of \ntech-can-improve-lives-in-refugee-camps/; https:// Our Nature: Why Violence has Declined (Penguin \ngithub.com/qcri-social/AIDR/wiki/AIDR-Overview 2012) and R. Krznaric, Empathy: How it matters and \nhow to get it. (Perigee 2015).\n34  https://www.sciencemag.org/news/2017/10/\n39  See for example TechToronto: https://www.\nsearching-survivors-mexico-earthquake-snake-\ntechtoronto.org and #AI and Big Data \nrobots\nhttps://www.livescience.com/48473-search-and- 40  See for example Harvard Humanitarian Initiative \nrescue-robot-algorithm.html Signal Code https://signalcode.org\n41  See Humanitarian Innovation Guide:  \n35  http://focus.barcelonagse.eu/can-machine-\nhttps://higuide.elrha.org/\nlearning-help-policymakers-detect-conflict/ \n \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 168', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nEmbedding Values into Autonomous and Intelligent Systems\nSociety has not established universal standards or guiding principles for embedding human \nvalues and norms into autonomous and intelligent systems (A/IS) today. But as these \nsystems are instilled with increasing autonomy in making decisions and manipulating their \nenvironment, it is essential that they are designed to adopt, learn, and follow the norms \nand values of the community they serve. Moreover, their actions should be transparent in \nsignaling their norm compliance and, if needed, they must be able to explain their actions. \nThis is essential if humans are to develop appropriate levels of trust in A/IS in the specific \ncontexts and roles in which A/IS function.\nAt the present time, the conceptual complexities surrounding what “values” are (Hitlin and \nPiliavin 20041; Malle and Dickert 20072; Rohan 20003; Sommer 20164) make it difficult to \nenvision A/IS that have computational structures directly corresponding to social or cultural \nvalues such as “security,” “autonomy,” or “fairness”. It may be a more realistic goal to embed \nexplicit norms into such systems. Since norms are observable in human behavior, they can \ntherefore be represented as instructions to act in defined ways in defined contexts, for a \nspecific community—from family to town to country and beyond. A community’s network  \nof social and moral norms is likely to reflect the community’s values, and A/IS equipped \nwith such a network would, therefore, also reflect the community’s values. For discussion  \nof specific values that are critical for ethical considerations of A/IS, see the chapters  \nof Ethically Aligned Design, “Personal Data and Individual Agency” and “Well-being”.\nNorms are typically expressed in terms of obligations and prohibitions, and these can \nbe expressed computationally (Malle, Scheutz, and Austerweil 20175; Vázquez-Salceda, \nAldewereld and Dignum 20046). They are typically qualitative in nature, e.g., do not stand too \nclose to people. However, the implementation of norms also has a quantitative component—\nthe measurement of the physical distance we mean by “too close”, and the possible \ninstantiations of the quantitative component technically enable the qualitative norm. \n \n \n \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 169', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nEmbedding Values into Autonomous and Intelligent Systems\nTo address the broad objective of embedding  Further Resources\nnorms and, by implication, values into A/IS, this \n•  S. Hitlin and J. A. Piliavin, “Values: Reviving \nchapter addresses three more concrete goals:\na Dormant Concept.” Annual Review of \n1.  Identifying the norms of the specific  Sociology 30, pp.359–393, 2004.\ncommunity in which the A/IS operate,\n•  B. F. Malle, and S. Dickert. “Values,” in \n2. Computationally implementing the norms   Encyclopedia of Social Psychology, edited by \nof that community within the A/IS, and R. F. Baumeister and K. D. Vohs. Thousand \nOaks, CA: Sage, 2007.\n3. Evaluating whether the implementation  \nof the identified norms in the A/IS are   •  B. F. Malle, M. Scheutz, and J. L. Austerweil. \nindeed conforming to the norms reflective   “Networks of Social and Moral Norms in \nof that community. Human and Robot Agents,” in A World with \nRobots: International Conference on Robot \nPursuing these three goals represents an  Ethics: ICRE 2015, edited by M. I. Aldinhas \niterative process that is sensitive to the  Ferreira, J. Silva Sequeira, M. O. Tokhi, E. E. \npurpose of the A/IS and to its users within a  Kadar, and G. S. Virk, 3–17. Cham, Switzerland: \nspecific community. It is understood that there  Springer International Publishing, 2017.\nmay be conflicts of values and norms when \nidentifying, implementing, and evaluating these  •  M. J. Rohan, “A Rose by Any Name? The \nsystems. Such conflicts are a natural part of  Values Construct.” Personality and Social \nthe dynamically changing and renegotiated  Psychology Review 4, pp. 255–277, 2000.\nnorm systems of any community. As a result, \n•  U. Sommer, Werte: Warum Man Sie Braucht, \nwe advocate for an approach in which systems \nObwohl es Sie Nicht Gibt. [Values. Why We \nare designed to provide transparent signals \nNeed Them Even Though They Don’t Exist.] \ndescribing the specific nature of their behavior \nStuttgart, Germany: J. B. Metzler, 2016.\nto the individuals in the community they serve. \nSuch signals may include explanations or offers  •  J. Vázquez-Salceda, H. Aldewereld, and F. \nfor inspection and must be in a language or form  Dignum. “Implementing Norms in Multiagent \nthat is meaningful to the community. Systems,” in Multiagent System Technologies. \nMATES 2004, edited by G. Lindemann, \nDenzinger, I. J. Timm, and R. Unland. (Lecture \nNotes in Computer Science, vol. 3187.) Berlin: \nSpringer, 2004. \n \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 170', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nEmbedding Values into Autonomous and Intelligent Systems\nSection 1—Identifying Norms for  \nAutonomous and Intelligent Systems\nWe identify three issues that must be  documented and therefore easy to identify, so \naddressed in the attempt to identify norms and  they can be incorporated into A/IS as long as \ncorresponding values for A/IS. The first issue asks  they do not violate humanitarian or community \nwhich norms should be identified and with which  moral principles. Social and moral norms \nproperties. Here we highlight context specificity  are more difficult to ascertain, as they are \nas a fundamental property of norms. Second,  expressed through behavior, language, customs, \nwe emphasize another important property  cultural symbols, and artifacts. Most important, \nof norms: their dynamically changing nature  communities ranging from families to whole \n(Mack 20187), which requires A/IS to have the  nations differ to various degrees in the norms \ncapacity to update their norms and learn new  they follow. Therefore, generating a universal \nones. Third, we address the challenge of norm  set of norms that applies to all A/IS in all \nconflicts that naturally arise in a complex social  contexts is not realistic, but neither is it advisable \nworld. Resolving such conflicts requires priority  to completely tailor the A/IS to individual \nstructures among norms, which help determine  preferences. We suggest that it is feasible to \nwhether, in a given context, adhering to one  identify broadly observed norms of communities \nnorm is more important than adhering to another  in which a technology is deployed.\nnorm, often in light of overarching standards, e.g., \nFurthermore, the difficulty of generating a \nlaws and international humanitarian principles.\nuniversal set of norms is not inconsistent with \nthe goal of seeking agreement over Universal \nHuman Rights (see the “General Principles” \nchapter of Ethically Aligned Design). However, \nIssue 1: Which norms should  \nthese universal rights are not sufficient for \nbe identified?\ndevising A/IS that conform to the specific norms \nof its community. Universal Human Rights must, \nBackground\nhowever, constrain the kinds of norms that are \nIf machines engage in human communities,  implemented in the A/IS (cf. van de Poel 20168).\nthen those agents will be expected to follow \nEmbedding norms in A/IS requires a careful \nthe community’s social and moral norms. \nunderstanding of the communities in which the \nA necessary step in enabling machines to \nA/IS are to be deployed. Further, even within a \ndo so is to identify these norms. But which \nparticular community, different types of A/IS will \nnorms should be identified? Laws are publicly \ndemand different sets of norms. The relevant \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 171', ""The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nEmbedding Values into Autonomous and Intelligent Systems\nnorms for self-driving vehicles, for example,   unspoken, that typically apply in a particular \nmay differ greatly from those for robots used   community. Such a set of empirically identified \nin healthcare. Thus, we recommend that to  norms should then guide system design. This \ndevelop A/IS capable of following legal, social,  process of norm identification and implementation \nand moral norms, the first step is to identify the  must be iterative and revisable. A/IS with an initial \nnorms of the specific community in which the   set of implemented norms may betray biases \nA/IS are to be deployed and, in particular, norms  of original assessments (Misra, Zitnick, Mitchell, \nrelevant to the kinds of tasks and roles for which  and Girshick 20169) that can be revealed by \nthe A/IS are designed. Even when designating  interactions with, and feedback from, the relevant \na narrowly defined community, e.g., a nursing  community. This leads to a process of norm \nhome, an apartment complex, or a company,  updating, which is described next in Issue 2.\nthere will be variations in the norms that apply, or \nRecommendation\nin their relative weighting. The norm identification \nprocess must heed such variation and ensure that  To develop A/IS capable of following social and \nthe identified norms are representative, not only  moral norms, the first step is to identify the \nof the dominant subgroup in the community but  norms of the specific community in which the  \nalso of vulnerable and underrepresented groups. A/IS are to be deployed and, in particular, norms \nrelevant to the kinds of tasks and roles that the \nThe most narrowly defined “community” is a \nA/IS are designed for. This norm identification \nsingle person, and A/IS may well have to adapt \nprocess must use appropriate scientific methods \nto the unique expectations and needs of a given \nand continue through the system's life cycle.\nindividual, such as the arrangement of a disabled \nperson’s living accommodations. However,  Further Resources \nunique individual expectations must not violate \n•  Mack, Ed., “Changing social norms.” Social \nnorms in the larger community. Whereas the \nResearch: An International Quarterly, 85, no.1, \narrangement of someone’s kitchen or the \n1–271, 2018.\nfrequency with which a care robot checks in with \na patient can be personalized without violating  •  I. Misra, C. L. Zitnick, M. Mitchell, and R. \nany community norms, encouraging the robot  Girshick, (2016). Seeing through the human \nto use derogatory language to talk about certain  reporting bias: Visual Classifiers from Noisy \nsocial groups does violate such norms. In the  Human-Centric Labels. In Proceedings of the \nnext section, we discuss how A/IS might handle  2016 IEEE Conference on Computer Vision \nsuch norm conflicts. and Pattern Recognition (CVPR), pp. 2930–\n2939. doi:10.1109/CVPR.2016.320\nInnovation projects and development efforts for \nA/IS should always rely on empirical research,  •  I. van de Poel, “An Ethical Framework for \ninvolving multiple disciplines and multiple  Evaluating Experimental Technology,” Science \nmethods; to investigate and document both  and Engineering Ethics, 22, no. 3,pp. 667-\ncontext- and task-specific norms, spoken and  686, 2016.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 172"", 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nEmbedding Values into Autonomous and Intelligent Systems\n \nLikewise, A/IS need multiple capacities to \nIssue 2: The need for  \nimprove their own norm knowledge and to adapt \nnorm updating\nto a community’s dynamically changing norms. \nThese capacities include:\nBackground\n•  Processing behavioral trends by members of \nNorms are not static. They change over time, in \nthe target community and comparing them to \nresponse to social progress, political change, new \ntrends predicted by the baseline norm system,\nlegal measures, or novel opportunities (Mack \n201810). Norms can fade away when, for whatever  •  Asking for guidance from the community \nreasons, fewer and fewer people adhere to them.  when uncertainty about applicable norms \nAnd new norms emerge when technological inno- exceeds a critical threshold,\nvation invites novel behaviors and novel standards, \n•  Responding to instruction from the community \ne.g., cell phone use in public.\nmembers who introduce a robot to a \nA/IS should be equipped with a starting set of \npreviously unknown context or who notice the \nsocial and legal norms before they are deployed \nA/IS’ uncertainty in a familiar context, and\nin their intended community (see Issue 1), \nbut this will not suffice for A/IS to behave  •  Responding to formal or informal feedback \nappropriately over time. A/IS or the designers of  from the community when the A/IS violate  \nA/IS, must be adept at identifying and adding  a norm.\nnew norms to its starting set, because the initial \nThe modification of a normative system can \nnorm identification process in the community \noccur at any level of the system: it could involve \nwill undoubtedly have missed some norms and \naltering the priority weightings between individual \nbecause the community’s norms change.\nnorms, changing the qualitative expression of a \nHumans rely on numerous capacities to update  norm, or altering the quantitative parameters that \ntheir knowledge of norms and learn new ones.  enable the norm.\nThey observe other community members’ \nWe recommend that the system’s norm changes \nbehavior and are sensitive to collective norm \nbe transparent. That is, the system or its \nchange; they explicitly ask about new norms \ndesigner should consult with users, designers, \nwhen joining new communities, e.g., entering \nand community representatives when adding \ncollege or a job in a new town; and they respond \nnew norms to its norm system or adjusting the \nto feedback from others when they exhibit \npriority or content of existing norms. Allowing \nuncertainty about norms or have violated a norm.\na system to learn new norms without public or \n  expert review has detrimental consequences \n  (Green and Hu 201811). The form of consultation \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 173', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nEmbedding Values into Autonomous and Intelligent Systems\nand the specific review process will vary by \nmachine sophistication―e.g., linguistic capacity \nIssue 3: A/IS will face norm \nand function/role, or a flexible social companion \nconflicts and need methods to \nversus a task-defined medical robot―and \nbest practices will have to be established. In  resolve them.\nsome cases, the system may document its \n \ndynamic change, and the user can consult this \nBackground\ndocumentation as desired. In other cases, explicit \nOften, even within a well-specified context, no \nannouncements and requests for discussion with \naction is available that fulfills all obligations and \nthe designer may be appropriate. In yet other \nprohibitions. Such situations—often described \ncases, the A/IS may propose changes, and the \nas moral dilemmas or moral overload (Van \nrelevant human community, e.g., drawn from a \nden Hoven 201212)—must be computationally \nrepresentative crowdsourced panel, will decide \ntractable by A/IS; they cannot simply stop in their \nwhether such changes should be implemented  \ntracks and end on a logical contradiction. Humans \nin the system.  \nresolve such situations by accepting trade-offs \nbetween conflicting norms, which constitute \nRecommendation\npriorities of one norm or value over another in a \nTo respond to the dynamic change of norms in  given context. Such priorities may be represented \nsociety A/IS or their designers must be able to  in the norm system as hierarchical relations.\namend their norms or add new ones, while being \nAlong with identifying the norms within a specific \ntransparent about these changes to users,  \ncommunity and task domain, empirical research \ndesigners, broader community representatives, \nmust identify the ways in which people prioritize \nand other stakeholders. \ncompeting norms and resolve norm conflicts, and \nthe ways in which people expect A/IS to resolve \nFurther Resources \nsimilar norm conflicts. These more local conflict \n•  B. Green and L. Hu. “The Myth in the  resolutions will be further constrained by some \nMethodology: Towards a Recontextualization  general principles, such as the “Common Good \nof Fairness in ML.” Paper presented at the  Principle” (Andre and Velasquez 199213) or local \nDebates workshop at the 35th International  and national laws. For example, a self-driving \nConference on Machine Learning, Stockholm,  vehicle’s prioritization of one factor over another \nSweden 2018. in its decision-making will need to reflect the laws \nand norms of the population in which the A/IS \n•  Mack, Ed., “Changing social norms,” Social \nare deployed, e.g., the traffic laws of a U.S. state \nResearch: An International Quarterly, 85  \nand the United States as a whole.\n(1, Special Issue), 1-271, 2018.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 174', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nEmbedding Values into Autonomous and Intelligent Systems\nSome priority orders can be built into a given  In general, we recommend that the system’s \nnorm network as hierarchical relations, e.g.,   resolution of norm conflicts be transparent—that \nmore general prohibitions against harm to  is, documented by the system and ready to be \nhumans typically override more specific norms  made available to users, the relevant community \nagainst lying. Other priority orders can stem from  of deployment, and third-party evaluators. Just \nthe override that norms in the larger community   like people explain to each other why they made \nexert on norms and preferences of an individual  decisions, they will expect any A/IS to be able \nuser. In the earlier example discussing  to explain their decisions and be sensitive to \npersonalization (see Issue 1), the A/IS of a racist  user feedback about the appropriateness of the \nuser who demands the A/IS use derogatory  decisions. To do so, design and development \nlanguage for certain social groups will have  of A/IS should specifically identify the relevant \nto resist such demands because community  groups of humans who may request explanations \nnorms hierarchically override an individual user’s  and evaluate the systems’ behaviors. In the \npreferences. In many cases, priority orders are  case of a system detecting a norm conflict, the \nnot built in as fixed hierarchies because the  system should consult and offer explanations \npriorities are themselves context-specific or may  to representatives from the community, e.g., \narise from net moral costs and benefits of the  randomly sampled crowdsourced members \nparticular case at hand. A/IS must have learning  or elected officials, as well as to third-party \ncapacities to track such variations and incorporate  evaluators, with the goal of discussing and \nuser and community input, e.g., about the subtle  resolving the norm conflict. \ndifferences between contexts, so as to refine the \nRecommendation\nsystem’s norm network (see Issue 2).\nA/IS developers should identify the ways in which \nTension may sometimes arise between a \npeople resolve norm conflicts and the ways in \ncommunity’s social and legal norms and the \nwhich they expect A/IS to resolve similar norm \nnormative considerations of designers or \nconflicts. A system’s resolution of norm conflicts \nmanufacturers. Democratic processes may need \nmust be transparent—that is, documented by the \nto be developed that resolve this tension—\nsystem and ready to be made available to users, \nprocesses that cannot be presented in detail \nthe relevant community of deployment, and \nin this chapter. Often such resolution will favor \nthird-party evaluators. \nthe local laws and norms, but in some cases \n \nthe community may have to be persuaded to \n \naccept A/IS favoring international law or broader \n \nhumanitarian principles over, say, racist or sexist \n \nlocal practices. \n \n \n \n \n \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 175', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nEmbedding Values into Autonomous and Intelligent Systems\nFurther Resources •  M. Flanagan, D. C. Howe, and H. Nissenbaum, \n“Embodying Values in Technology: Theory and \n•  M. Velasquez, C. Andre, T. Shanks, S.J., and \nPractice.” Information Technology and Moral \nM. J. Meyer, “The Common Good.” Issues in \nPhilosophy, J. van den Hoven and J. Weckert, \nEthics, vol. 5, no. 1, 1992.\nEds., Cambridge University Press, 2008, \n•  J. Van den Hoven, “Engineering and the  pp. 322–53. Cambridge Core, Cambridge \nProblem of Moral Overload.” Science and  University Press. Preprint available at  \nEngineering Ethics, vol. 18, no. 1, pp.   http://www.nyu.edu/projects/nissenbaum/\n143–155, 2012. papers/Nissenbaum-VID.4-25.pdf\n•  D. Abel, J. MacGlashan, and M. L. Littman.  •  B. Friedman, P. H. Kahn, A. Borning, and \n“Reinforcement Learning as a Framework for  A. Huldtgren. “Value Sensitive Design and \nEthical Decision Making.” AAAI Workshop AI,  Information Systems,” in Early Engagement \nEthics, and Society, Volume WS-16-02 of 13th  and New Technologies: Opening up the \nAAAI Workshops. Palo Alto, CA: AAAI   Laboratory, N. Doorn, Schuurbiers, I. van de \nPress, 2016. Poel, and M. Gorman, Eds., vol. 16, pp. 55–95. \nDordrecht: Springer, 2013. \n•  O. Bendel, Die Moral in der Maschine: \n•  A comprehensive introduction into Value \nBeiträge zu Roboter- und Maschinenethik. \nSensitive Design and three sample \nHannover, Germany: Heise Medien, 2016. \napplications \n•  Accessible popular-science contributions \nto philosophical issues and technical  •  G. Mackie, F. Moneti, E. Denny, and H. \nimplementations of machine ethics Shakya. “What Are Social Norms? How Are \nThey Measured?” UNICEF Working Paper. \n•  S. V. Burks, and E. L. Krupka. “A Multimethod \nUniversity of California at San Diego: UNICEF, \nApproach to Identifying Norms and Normative \nSept. 2014. https://dmeforpeace.org/sites/\nExpectations within a Corporate Hierarchy: \ndefault/files/4%2009%2030%20Whole%20\nEvidence from the Financial Services Industry.” \nWhat%20are%20Social%20Norms.pdf\nManagement Science, vol. 58, pp. 203–217, \n•  A broad survey of conceptual and \n2012. \nmeasurement questions regarding social \n•  Illustrates surveys and incentivized \nnorms.\ncoordination games as methods to elicit \nnorms in a large financial services firm •  J. A. Leydens and J. C. Lucena. Engineering \nJustice: Transforming Engineering Education \n•  F. Cushman, V. Kumar, and P. Railton, “Moral \nand Practice. Hoboken, NJ: John Wiley & Sons, \nLearning,” Cognition, vol. 167, pp. 1–282, \n2018.\n2017. \n•  Identifies principles of engineering for \n \nsocial justice.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 176', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nEmbedding Values into Autonomous and Intelligent Systems\n•  B. F. Malle, “Integrating Robot Ethics and  •  S. H. Schwartz, “An Overview of the Schwartz  \nMachine Morality: The Study and Design of  Theory of Basic Values.” Online Readings in \nMoral Competence in Robots.” Ethics and  Psychology and Culture 2, 2012. \nInformation Technology, vol. 18, no. 4, pp. \n•  Comprehensive overview of a specific \n243–256, 2016. \ntheory of values, understood as \n•  Discusses how a robot’s norm capacity fits \nmotivational orientations toward abstract \nin the larger vision of a robot with moral \noutcomes (e.g., self-direction, power, \ncompetence.\nsecurity).\n•  K. W. Miller, M. J. Wolf, and F. Grodzinsky, “This \n•  S. H. Schwartz and K. Boehnke. “Evaluating the \n‘Ethical Trap’ Is for Roboticists, Not Robots: On \nStructure of Human Values with Confirmatory \nthe Issue of Artificial Agent Ethical Decision-\nFactor Analysis.” Journal of Research in \nMaking.” Science and Engineering Ethics, vol. \nPersonality, vol. 38, pp. 230–255, 2004. \n23, pp. 389–401, 2017. \n•  Describes an older method of subjective \n•  This article raises doubts about the \njudgments of relations among valued \npossibility of imbuing artificial agents with \noutcomes and a newer, formal method of \nmorality, or of claiming to have done so.\nanalyzing these relations.\n•  Open Roboethics Initiative: www.\n•  W. Wallach and C. Allen. Moral Machines: \nopenroboethics.org. A series of poll results on \nTeaching Robots Right from Wrong. New York: \ndifferences in human moral decision-making \nOxford University Press, 2008. \nand changes in priority order of values for \n•  This book describes some of the \nautonomous systems (e.g., on care robots), \nchallenges of having a one-size-fits-all \n2019.\napproach to embedding human values in \n•  A. Rizzo and L. L. Swisher, “Comparing the  autonomous systems.  \nStewart–Sprinthall Management Survey and   \nthe Defining Issues Test-2 as Measures of   \nMoral Reasoning in Public Administration.”   \nJournal of Public Administration Research    \nand Theory, vol. 14, pp. 335–348, 2004.   \n \n•  Describes two assessment instruments \n \nof moral reasoning (including norm \n \nmaintenance) based on Kohlberg’s theory  \n \nof moral development. \n \n \n \n \n \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 177', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nEmbedding Values into Autonomous and Intelligent Systems\nSection 2—Implementing Norms in  \nAutonomous and Intelligent Systems\nOnce the norms relevant to A/IS’ role in a specific \ncommunity have been identified, including \nIssue 1: Many approaches \ntheir properties and priority structure, we must \nto norm implementation are \nlink these norms to the functionalities of the \ncurrently available, and it is  \nunderlying computational system. We discuss \nthree issues that arise in this process of norm  not yet settled which ones  \nimplementation. First, computational approaches  are most suitable.\nto enable a system to represent, learn, and \nexecute norms are only slowly emerging. \nBackground\nHowever, the diversity of approaches may soon \nlead to substantial advances. Second, for A/IS  The prospect of developing A/IS that are sensitive \nthat operate in human communities, there is a  to human norms and factor them into morally or \nparticular need for transparency—ranging from  legally significant decisions has intrigued science \nthe technical process of implementation to the  fiction writers, philosophers, and computer \nethical decisions that A/IS will make in human- scientists alike. Modest efforts to realize this \nmachine interactions, which will require a high  worthy goal in limited or bounded contexts are \nlevel of explainability. Third, failures of normative  already underway. This emerging field of research \nreasoning can be considered inevitable and  appears under many names, including: machine \nmitigation strategies should therefore be put in  morality, machine ethics, moral machines, value \nplace to handle such failures when they occur.  alignment, computational ethics, artificial morality, \nsafe AI, and friendly AI.\nAs a general guideline, we recommend that, \nthrough the entire process of implementation of  There are a number of different implementation \nnorms, designers should consider various forms  routes for implementing ethics into autonomous \nand metrics of evaluation, and they should define  and intelligent systems. Following Wallach and \nand incorporate central criteria for assessing the  Allen (2008)14, we might begin to categorize \nA/IS’ norm conformity, e.g., human-machine  these as either: \nagreement on moral decisions, verifiability of \nA. Top-down approaches, where the system, \nA/IS decisions, or justified trust. In this way, \ne.g., a software agent, has some symbolic \nimplementation already prepares for the critical \nrepresentation of its activity, and so can \nthird phase of evaluation (discussed in Section 3).\nidentify specific states, plans, or actions \nas ethical or unethical with respect to \nparticular ethical requirements (Dennis, \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 178', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nEmbedding Values into Autonomous and Intelligent Systems\nFisher, Slavkovik, Webster 201615; Pereira  progresses, engineers will explore new ways to \nand Saptawijaya 201616; Rötzer, 201617;  improve these capabilities.\nScheutz, Malle, and Briggs 201518); or\nEach of the first two options has obvious \nB. Bottom-up approaches, where the system,  limitations, such as option A’s inability to learn \ne.g., a learning component, builds up,  and adapt and option B’s unconstrained learning \nthrough experience of what is to be  behavior. A third option tries to address these \nconsidered ethical and unethical in certain  limitations:\nsituations, an implicit notion of ethical \nC. Hybrid approaches, combining (A) and (B).\nbehavior (Anderson and Anderson 201419; \nRiedl and Harrison 201620).\nFor example, the selection of action might be \ncarried out by a subsymbolic system, but this \nRelevant examples of these two are: (A) symbolic \naction must be checked by a symbolic “gateway” \nagents that have explicit representations of plans, \nagent before being invoked. This is a typical \nactions, goals, etc.; and (B) machine learning \napproach for “Ethical Governors” (Arkin, 200822; \nsystems that train subsymbolic mechanisms with \nWinfield, Blum, and Liu 201423) or “Guardians” \nacceptable ethical behavior. For more detailed \n(Etzioni 201624) that monitor, restrict, and even \ndiscussion, see Charisi et al. 201721.\nadapt certain unacceptable behaviors proposed \nMany of the existing experimental approaches  by the system (see Issue 3). Alternatively, action \nto building moral machines are top-down,  selection in light of norms could be done in \nin the sense that norms, rules, principles, or  a verifiable logical format, while many of the \nprocedures are used by the system to evaluate  norms constraining those actions can be learned \nthe acceptability of differing courses of action,  through bottom-up learning mechanisms (Arnold, \nor as moral standards or goals to be realized.  Kasenberg, and Scheutz 201725).\nIncreasingly, however, A/IS will encounter \nThese three architectures do not cover all \nsituations that initially programmed norms do not \npossible techniques for implementing norms \nclearly address, requiring algorithmic procedures \ninto A/IS. For example, some contributors to the \nto select the better of two or more novel courses \nmulti-agent systems literature have integrated \nof action. Recent breakthroughs in machine \nnorms into their agent specifications (Andrighetto \nlearning and perception enable researchers to \net al. 201326), and even though these agents live \nexplore bottom-up approaches in which the  \nin societal simulations and are too underspecified \nA/IS learn about their context and about human \nto be translated into individual A/IS such as \nnorms, similar to the manner in which a child \nrobots, the emerging work can inform cognitive \nslowly learns which forms of behavior are safe \narchitectures of such A/IS that fully integrate \nand acceptable. Of course, unlike current  \nnorms. Of course, none of these experimental \nA/IS, children can feel pain and pleasure, and \nsystems should be deployed outside of the \nempathize with others. Still, A/IS can learn to \nlaboratory before testing or before certain criteria \ndetect and take into account others’ pain and \nare met, which we outline in the remainder of \npleasure, thus at least achieving some of the \nthis section and in Section 3.\npositive effects of empathy. As research on A/IS \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 179', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nEmbedding Values into Autonomous and Intelligent Systems\nRecommendation •  L. Dennis, M. Fisher, M. Slavkovik, and M. \nWebster, “Formal Verification of Ethical Choices \nIn light of the multiple possible approaches \nin Autonomous Systems.” Robotics and \nto computationally implement norms, diverse \nAutonomous Systems, vol. 77, pp. 1–14, 2016.\nresearch efforts should be pursued, especially \n•  A. Etzioni and O. Etzioni, “Designing AI \ncollaborative research between scientists from \nSystems That Obey Our Laws and Values.” \ndifferent schools of thought and different \nCommunications of the ACM, vol. 59, no. 9, \ndisciplines.\npp. 29–31, Sept. 2016.\nFurther Resources  •  L. M. Pereira and A. Saptawijaya, Programming \nMachine Ethics. Cham, Switzerland: Springer \n•  M. Anderson, and S. L. Anderson, “GenEth: \nInternational, 2016.\nA General Ethical Dilemma Analyzer,” \n•  M. O. Riedl and B. Harrison. “Using Stories to \nProceedings of the Twenty-Eighth AAAI \nTeach Human Values to Artificial Agents.” AAAI \nConference on Artificial Intelligence, Québec \nWorkshops 2016. Phoenix, Arizona, February \nCity, Québec, Canada, July 27 –31, 2014, pp. \n12–13, 2016.\n253–261, Palo Alto, CA, The AAAI Press, 2014.\n•  F. Rötzer, ed. Programmierte Ethik: Brauchen \n•  G. Andrighetto, G. Governatori, P. Noriega, \nRoboter Regeln oder Moral? Hannover, \nand L. W. N. van der Torre, eds. Normative \nGermany: Heise Medien, 2016.\nMulti-Agent Systems. Saarbrücken/Wadern, \n•  M. Scheutz, B. F. Malle, and G. Briggs. \nGermany: Dagstuhl Publishing, 2013.\n“Towards Morally Sensitive Action Selection for \n•  R. Arkin, “Governing Lethal Behavior:  Autonomous Social Robots.” Proceedings of \nEmbedding Ethics in a Hybrid Deliberative/ the 24th International Symposium on Robot \nReactive Robot Architecture.” Proceedings  and Human Interactive Communication,  \nof the 2008 3rd ACM/IEEE International  RO-MAN 2015 (2015): 492–497.\nConference on Human-Robot Interaction  •  U. Sommer, Werte: Warum Man Sie Braucht, \n(HRI), Amsterdam, Netherlands, March 12 -15,  Obwohl es Sie Nicht Gibt. [Values. Why we \n2008, IEEE, pp. 121–128, 2008. need them even though they don’t exist.] \nStuttgart, Germany: J. B. Metzler, 2016.\n•  T. Arnold, D. Kasenberg, and M. Scheutz. \n“Value Alignment or Misalignment—What Will  •  I. Sommerville, Software Engineering. Harlow, \nKeep Systems Accountable?” The Workshops  U.K.: Pearson Studium, 2001. \nof the Thirty-First AAAI Conference on Artificial  •  W. Wallach and C. Allen. Moral Machines: \nIntelligence: Technical Reports, WS-17-02: AI,  Teaching Robots Right from Wrong. New York: \nEthics, and Society, pp. 81–88. Palo Alto, CA:  Oxford University Press, 2008.\nThe AAAI Press, 2017.\n•  F. T. Winfield, C. Blum, and W. Liu. “Towards an \n•  V. Charisi, L. Dennis, M. Fisher, et al. “Towards  Ethical Robot: Internal Models, Consequences \nand Ethical Action Selection” in Advances in \nMoral Autonomous Systems,” 2017.\nAutonomous Robotics Systems, Lecture Notes \n•  A. Conn, “How Do We Align Artificial \nin Computer Science Volume, M. Mistry, A. \nIntelligence with Human Values?” Future  \nLeonardis, Witkowski, and C. Melhuish, eds. \nof Life Institute, Feb. 3, 2017. pp. 85–96. Springer, 2014.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 180', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nEmbedding Values into Autonomous and Intelligent Systems\nrange of transparency by reference to four ways \nin which systems can be transparent—traceability, \nIssue 2: The need \nverifiability, honest design, and intelligibility—and \nfor transparency from \napply these considerations to the implementation \nimplementation to deployment of norms in A/IS.\nTransparency as traceability—Most relevant for \nBackground\nthe topic of implementation is the transparency \nWhen A/IS become part of social communities  of the software engineering process during \nand behave according to the norms of their  implementation (Cleland-Huang, Gotel, and \ncommunities, people will want to understand the  Zisman201232). It allows for the originally \nA/IS decisions and actions, just as they want to  identified norms (Section 1, Issue 1) to be \nunderstand each other’s decisions and actions.  traced through to the final system. This allows \nThis is particularly true for morally significant  technical inspection of which norms have been \nactions or omissions: an ethical reasoning system  implemented, for which contexts, and how \nshould be able to explain its own reasoning  norm conflicts are resolved, e.g., priority weights \nto a user on request. Thus, transparency, or  given to different norms. Transparency in the \n“explainability”, of A/IS is paramount (Chaudhuri  implementation process may also reveal biases \n201727; Wachter, Mittelstadt, and Floridi 201728),  that were inadvertently built into systems, such as \nand it will allow a community to understand,  racism and sexism, in search engine algorithms \npredict, and modify the A/IS (see Section 1,  (Noble 201333). (See Section 3, Issue 2.) Such \nIssue 2; for a nuanced discussion see Selbst and  traceability in turn calibrates a community’s \nBarocas29). Moreover, as the norms embedded  trust about whether A/IS are conforming to the \nin A/IS are continuously updated and refined  norms and values relevant in their use contexts \n(see Section 1, Issue 2), transparency allows for  (Fleischmann and Wallace 200534).\nappropriate trust to be developed (Grodzinsky, \nMiller, and Wolf 201130), and, where necessary,  Transparency as verifiability—Transparency \nallows the community to modify a system’s  concerning how normative reasoning is \nnorms, reasoning, and behavior. approached in the implementation is important \nas we wish to verify that the normative decisions \nTransparency can occur at multiple levels, e.g.,  the system makes match the required norms and \nordinary language or coder verification, and for  values. Explicit and exact representations of these \nmultiple stakeholders, e.g., user, engineer, and  normative decisions can then provide the basis \nattorney. (See IEEE P7001™, IEEE Standards  for a range of strong mathematical techniques, \nProject for Transparency of Autonomous  such as formal verification (Fisher, Dennis, and \nSystems). It should be noted that transparency  Webster 201335). Even if a system cannot explain \nto all parties may not always be advisable,  every single reasoning step in understandable \nsuch as in the case of security programs that  human terms, a log of ethical reasoning should \nprevent a system from being hacked (Kroll et  be available for inspection of later evaluation \nal. 201631). Here we briefly illustrate the broad  purposes (Hind et al. 201836).\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 181', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nEmbedding Values into Autonomous and Intelligent Systems\nTransparency as honest design—German  to “an explanation of the [algorithmic] decision \ndesigner Dieter Rams coined the term “honest  reached after such assessment and to challenge \ndesign” to refer to design that “does not make a  the decision”. (See boyd [sic] 201642, for a critical \nproduct more innovative, powerful or valuable  discussion of this regulation.)\nthan it really is” (Vitsoe 201837; see also Donelli \nRecommendation\n201538; Jong 201739). Honest design of A/IS \nis one aspect of their transparency, because it  A/IS, especially those with embedded norms, \nallows the user to “see through” the outward  must have a high level of transparency, shown \nappearance and accurately infer the A/IS’ actual  as traceability in the implementation process, \ncapacities. At times, however, the physical  mathematical verifiability of their reasoning, \nappearance of a system does not accurately  honesty in appearance-based signals,  \nrepresent what the system is capable of  and intelligibility of the systems’ operation  \ndoing—e.g., the agent displays signs of a certain  and decisions.\nhuman-like emotion but its internal state does \nFurther Resources \nnot represent that human emotion. Humans are \nquick to make strong inferences from outward  •  d. boyd, “Transparency ≠ Accountability.”  \nappearances of human-likeness to the mental  Data & Society: Points, November 29, 2016.\nand social capacities the A/IS might have. \n•  A. Chaudhuri, “ Philosophical Dimensions \nDemands for transparency in design therefore put \nof Information and Ethics in the Internet \na responsibility on the designer to “not attempt \nof Things (IoT) Technology,”The \nto manipulate the consumer with promises that \nEDP Audit, Control, and Security \ncannot be kept” (Vitsoe 201840).\nNewsletter, vol. 56, no. 4, pp. 7-18, DOI: \nTransparency as intelligibility—As mentioned  10.1080/07366981.2017.1380474, 2017.\nabove, humans will want to understand the  \n•  J. Cleland-Huang, O. Gotel, and A. Zisman, \nA/IS’ decisions and actions, especially the morally \neds. Software and Systems Traceability. \nsignificant ones. A clear requirement for an ethical \nLondon: Springer, 2012. doi:10.1007/978- \nA/IS is that the system be able to explain its own \n1-4471-2239-5\nreasoning to a user, when asked—or, ideally, also \nwhen suspecting the user’s confusion, and the  •  G. Donelli, “Good design is honest.” (blog). \nsystem should do so at a level of ordinary human  March 13, 2015. Accessed Oct 22, 2018. \nreasoning, not with incomprehensible technical  https://blog.astropad.com/good-design-is-\ndetail (Tintarev and Kutlak 201441). Furthermore,  honest/ \nwhen the system cannot explain some of its \nactions, technicians or designers should be  •  M. Fisher, L. A. Dennis, and M. P. Webster. \navailable to make those actions intelligible. Along  “Verifying Autonomous Systems.” \nthese lines, the European Union’s General Data  Communications of the ACM, vol. 56,  \nProtection Regulation (GDPR), in effect since  no. 9, pp. 84–93, 2013. \nMay 2018, states that, for automated decisions \nbased on personal data, individuals have a right \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 182', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nEmbedding Values into Autonomous and Intelligent Systems\n•  K. R. Fleischmann and W. A. Wallace. “A  •  D. Selbst and S. Barocas, “The Intuitive Appeal \nCovenant with Transparency: Opening the  of Explainable Machines,” 87 Fordham Law \nBlack Box of Models.” Communications of the  Review 1085, Available at SSRN: https://\nACM, vol. 48, no. 5, pp. 93–97, 2005. ssrn.com/abstract=3126971 or http://dx.doi.\norg/10.2139/ssrn.3126971, Feb. 19, 2018.\n•  F. S. Grodzinsky, K. W. Miller, and M. J. Wolf. \n“Developing Artificial Agents Worthy of Trust:  •  N. Tintarev and R. Kutlak. “Demo: Making \nWould You Buy a Used Car from This Artificial  Plans Scrutable with Argumentation and \nAgent?” Ethics and Information Technology,  Natural Language Generation.” Proceedings \nvol. 13, pp. 17–27, 2011. of the Companion Publication of the 19th \nInternational Conference on Intelligent User \n•  M. Hind, et al. “Increasing Trust in AI Services \nInterfaces, pp. 29–32, 2014.\nthrough Supplier’s Declarations of Conformity.” \nArXiv E-Prints, Aug. 2018. [Online] Available:  •  Vitsoe. “The Power of Good Design.” Vitsoe, \nhttps://arxiv.org/abs/1808.07261. [Accessed  2018. Retrieved Oct 22, 2018 from https://\nOctober 28, 2018]. www.vitsoe.com/us/about/good-design.\n•  C. W. De Jong, ed., Dieter Rams: Ten  •  S.Wachter, B. Mittelstadt, and L. Floridi, \nPrinciples for Good Design. New York, NY:  “Transparent, Explainable, and Accountable AI \nPrestel Publishing, 2017. for Robotics.” Science Robotics, vol. 2, no. 6, \neaan6080. doi:10.1126/scirobotics. aan6080, \n•  J. A. Kroll, J. Huey, S. Barocas et al. \n2017.\n“Accountable Algorithms.” University of \nPennsylvania Law Review 165 2017.\n \n•  S. U. Noble, “Google Search: Hyper-Visibility as \n \na Means of Rendering Black Women and Girls   \nInvisible.” InVisible Culture 19, 2013.  \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 183', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nEmbedding Values into Autonomous and Intelligent Systems\nnorms. This approach tries to anticipate potential \npoints of failure, e.g., norm violations, and, \nIssue 3: Failures will occur.\nwhere possible, develops some ways to reduce \nor remove the effects of failures. Successful \nBackground\nbehavior, and occasional failures, can then \nOperational failures and, in particular, violations  iteratively improve predictions and  \nof a system’s embedded community norms,  mitigation attempts.\nare unavoidable, both during system testing and \nThird, because not all risks and failures are \nduring deployment. Not only are implementations \npredictable (Brundage et al 201844; Vanderelst \nnever perfect, but A/IS with embedded norms \nand Winfield 201845), especially in complex \nwill update or expand their norms over time (see \nhuman-machine interactions in social contexts, \nSection 1, Issue 2) and interactions in the social \nadditional mitigation mechanisms must be made \nworld are particularly complex and uncertain. \navailable. Designers are strongly encouraged to \nThus, prevention and mitigation strategies must \naugment the architectures of their systems with \nbe adopted, and we sample four possible ones.\ncomponents that handle unanticipated norm \nFirst, anticipating the process of evaluation during  violations with a fail-safe, such as the symbolic \nthe implementation phase requires defining  “gateway” agents discussed in Section 2, Issue \ncriteria and metrics for such evaluation, which in  1. Designers should identify a number of strict \nturn better allows the detection and mitigation of  laws, that is, task- and community-specific norms \nfailures. Metrics will include: that should never be violated, and the fail-\nsafe components should continuously monitor \n•  Technical variables, such as traceability and \noperations against possible violations of these \nverifiability,\nlaws. In case of violations, the higher-order \ngateway agent should take appropriate actions, \n•  User-level variables such as reliability, \nsuch as safely disabling the system’s operation, \nunderstandable explanations, and \nor greatly limiting its scope of operation, until \nresponsiveness to feedback, and \nthe source of failure is identified. The fail-\n•  Community-level variables such as justified  safe components need to be understandable, \ntrust (see Issue 2) and the collective belief  extremely reliable, and protected against security \nthat A/IS are generally creating social benefits  breaches, which can be achieved, for example, \nrather than, for example, technological  by validating them carefully and not letting them \nunemployment. adapt their parameters during execution.\nSecond, a systematic risk analysis and  Fourth, once failures have occurred, responsible \nmanagement approach can be useful (Oetzel and  entities, e.g., corporate, government, science, and \nSpiekermann 201443) for an application to privacy  engineering, shall create a publicly accessible \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 184', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nEmbedding Values into Autonomous and Intelligent Systems\ndatabase with undesired outcomes caused  Further Resources \nby specific A/IS systems. The database would \n•  M. Brundage, S. Avin, J. Clark, H. Toner, P. \ninclude descriptions of the problem, background \nEckersley, B. Garfunkel, A. Dafoe, P. Scharre, T. \ninformation on how the problem was detected, \nZeitzo, et al. "" “The Malicious Use of Artificial \nwhich context it occurred in, and how it was \nIntelligence: Forecasting, Prevention, and \naddressed.\nMitigation,” CoRR abs/1802.07228 [cs.AI]. \nIn summary, we offer the following  2018. https://arxiv.org/abs/1802.07228 \nrecommendation.\n•  M. C. Oetzel and S. Spiekermann, “A \nSystematic Methodology for Privacy Impact \nRecommendation\nAssessments: A Design Science Approach.” \nBecause designers and developers cannot  European Journal of Information Systems, vol. \nanticipate all possible operating conditions and  23, pp. 126–150, 2014. https://link.springer.\npotential failures of A/IS, multiple strategies to  com/article/10.1057/ejis.2013.18\nmitigate the chance and magnitude of harm  \nmust be in place.  •  D. Vanderelst and A.F. Winfield, 2018 “The \n  Dark Side of Ethical Robots,” In Proc. The \n  First AAAI/ACM Conf. on Artificial Intelligence, \n  Ethics and Society, New Orleans, LA, Feb. 1 -3, \n  2018.\n \n \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 185', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nEmbedding Values into Autonomous and Intelligent Systems\nSection 3—Evaluating the  \nImplementation of A/IS\nThe success of implementing appropriate norms  the user expects the system to do, verify that \nin A/IS must be rigorously evaluated. This  the system really does this, and validate that the \nevaluation process must be anticipated during  specification actually matches the criteria. Many \ndesign and incorporated into the implementation  different evaluation techniques are available in \nprocess and continue throughout the life cycle  the field of software engineering (Sommerville \nof the system’s deployment. Assessment before  201547), ranging from formal mathematical proof, \nfull-scale deployment would best take place in  through rigorous empirical testing against criteria \nsystematic test beds that allow human users— of normatively correct behavior, to informal \nfrom the defined community and representing all  analysis of user interactions and responses to the \ndemographic groups—to engage safely with the  machine’s norm awareness and compliance. All \nA/IS in intended tasks. Multiple disciplines and  these approaches can, in principle, be applied \nmethods should contribute to developing and  to the full range of A/IS including robots (Fisher, \nconducting such evaluations. Dennis, and Webster 201348). More general \nprinciples from system quality management may \nEvaluation criteria must capture, among others, \nalso be integrated into the evaluation process, \nthe quality of human-machine interactions, \nsuch as the Plan-Do-Check-Act (PDCA) cycle that \nhuman approval and appreciation of the A/IS, \nunderlies standards like ISO 9001 (International \nappropriate trust in the A/IS, adaptability of the \nOrganization for Standardization 201549). \nA/IS to human users, and benefits to human \nwell-being in the presence or under the influence  Evaluation may be done by first parties, e.g., \nof the A/IS. A range of normative aspects to be  designers, manufacturers, and users, as well \nconsidered can be found in British Standard BS  as third parties, e.g., regulators, independent \n8611:2016 on Robot Ethics (British Standards  testing agencies, and certification bodies. In \nInstitution 201646). These are important general  either case, the results of evaluations should \nevaluation criteria, but they do not yet fully  be made available to all parties, with strong \ncapture evaluation of a system that has   encouragement to resolve discovered system \n“norm capacities”.  limitations and resolve potential discrepancies \namong multiple evaluations.\nTo evaluate a system’s norm-conforming \nbehavior, one must describe—and ideally,  As a general guideline, we recommend that \nformally specify—criterion behaviors that reflect  evaluation of A/IS implementations must be \nthe previously identified norms, describe what  anticipated during a system’s design, incorporated \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 186', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nEmbedding Values into Autonomous and Intelligent Systems\ninto the implementation process, and continue \nthroughout the system’s deployment (cf. ITIL \nIssue 1: Not all norms of a target \nprinciples, BMC 201650). Evaluation must include \ncommunity apply equally to \nmultiple methods, be made available to all \nparties—from designers and users to regulators,  human and artificial agents\nand should include procedures to resolve   \nconflicting evaluation results. Specific issues  Background\nthat need to be addressed in this process are \nAn intuitive criterion for evaluations of norms \ndiscussed next.\nembedded in A/IS would be that the A/IS norms \nshould mirror the community’s norms—that is, \nFurther Resources \nthe A/IS should be disposed to behave the same \n•  British Standards Institution. BS8611:2016, \nway that people expect each other to behave. \n“Robots and Robotic Devices. Guide to the  \nHowever, for a given community and a given  \nEthical Design and Application of Robots and \nA/IS use context, A/IS and humans are unlikely \nRobotic Systems,” 2016.\nto have identical sets of norms. People will have \nsome unique expectations for humans than they \n•  BMC Software. ITIL: The Beginner’s Guide to \ndo not for machines, e.g., norms governing the \nProcesses & Best Practices. http://www.bmc.\nregulation of negative emotions, assuming that \ncom/guides/itil-introduction.html,  \nmachines do not have such emotions. People \nDec. 6, 2016.\nmay in some cases have unique expectations \n•  M. Fisher, L. A. Dennis, and M. P. Webster.  of A/IS that they do not have for humans, e.g., \n“Verifying Autonomous Systems.”  a robot worker, but not a human worker, is \nCommunications of the ACM, vol. 56, no. 9,  expected to work without regular breaks.\npp. 84–93, 2013.\nRecommendation\n•  International Organization for Standardization \nThe norm identification process must document \n(2015). ISO 9001:2015, Quality management \nthe similarities and differences between the \nsystems —Requirements. Retrieved July \nnorms that humans apply to other humans \n12, 2018 from https://www.iso.org/\nand the norms they apply to A/IS. Norm \nstandard/62085.html. \nimplementations should be evaluated  \nspecifically against the norms that the  \n•  I. Sommerville, Software Engineering. 10th \ncommunity expects the A/IS to follow. \ned. Harlow, U.K.: Pearson Studium, 2015.  \n \n \n \n \n \n \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 187', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nEmbedding Values into Autonomous and Intelligent Systems\nassumptions. Therefore, unanticipated or \nundetected biases should be further reduced \nIssue 2: A/IS can have biases \nby including members of diverse social groups \nthat disadvantage specific groups\nin both the planning and evaluation of A/IS \nand integrating community outreach into the \nBackground evaluation process, e.g., DO-IT program and RRI \nframework. Behavioral scientists and members \nEven when reflecting the full system of \nof the target populations will be particularly \ncommunity norms that was identified, A/IS may \nvaluable when devising criterion tasks for \nshow operation biases that disadvantage specific \nsystem evaluation and assessing the success of \ngroups in the community or instill biases in users \nevaluating the A/IS performance on those tasks. \nby reinforcing group stereotypes. A system’s \nSuch tasks would assess, for example, whether \nbias can emerge in perception. For example, a \nthe A/IS apply norms in discriminatory ways to \npassport application AI rejected an Asian man’s \ndifferent races, ethnicities, genders, ages, body \nphoto because it insisted his eyes were closed \nshapes, or to people who use wheelchairs  \n(Griffiths 201651). Bias can emerge in information \nor prosthetics, and so on.\nprocessing. For instance, speech recognition \nsystems are notoriously less accurate for female \nRecommendation\nspeakers than for male speakers (Tatman \nEvaluation of A/IS must carefully assess potential \n201652). System bias can affect decisions, such \nbiases in the systems’ performance that \nas a criminal risk assessment device which \ndisadvantage specific social and demographic \noverpredicts recidivism by African Americans \ngroups. The evaluation process should integrate \n(Angwin et al. 201653). The system’s bias can \nmembers of potentially disadvantaged groups in \npresent itself even in its own appearance and \nefforts to diagnose and correct such biases.\npresentation: the vast majority of humanoid \nrobots have white “skin” color and use female \nFurther Resources\nvoices (Riek and Howard 201454).\n•  J. Angwin, J. Larson, S. Mattu, and L. Kirchner, \nThe norm identification process detailed in \n“Machine Bias: There’s Software Used Across \nSection 1 is intended to minimize individual \nthe Country to Predict Future Criminals. And \ndesigners’ biases because the community norms \nIt’s Biased Against Blacks.” ProPublica,  \nare assessed empirically. The identification \nMay 23, 2016.\nprocess also seeks to incorporate norms against \nprejudice and discrimination. However, biases  •  J. Griffiths, “New Zealand Passport Robot  \nmay still emerge from imperfections in the norm  Thinks This Asian Man’s Eyes Are Closed.” \nidentification process itself, from unrepresentative  CNN.com, December 9, 2016. \ntraining sets for machine learning systems, and   \nfrom programmers’ and designers’ unconscious \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 188', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nEmbedding Values into Autonomous and Intelligent Systems\n•  L. D. Riek and D. Howard,. “A Code of Ethics   approach (see Section 2, Issue 1). This approach \nfor the Human-Robot Interaction Profession.”  requires access to the decision-making process \nProceedings of We Robot, April 4, 2014. and the reasons for each decision (Fisher, Dennis, \nand Webster 201355). A simpler alternative, \n•  R. Tatman, “Google’s Speech Recognition Has \nsometimes suitable even for machine learning \na Gender Bias.” Making Noise and Hearing \nsystems, is to test the A/IS against a set of \nThings, July 12, 2016.\nscenarios and assess how well they matches \ntheir normative requirements, e.g., acting in \naccordance with relevant norms and recognizing \nother agents’ norm violations. A “red team” may \nalso devise scenarios that try to get the A/IS  \nIssue 3: Challenges to evaluation \nto break norms so that its vulnerabilities can  \nby third parties\nbe revealed.\nBackground These different evaluation techniques can be \nassigned different levels of “strength”: strong \nA/IS should have sufficient transparency to \nones demonstrate the exhaustive set of the  \nallow evaluation by third parties, including \nA/IS’ allowable behaviors for a range of criterion \nregulators, consumer advocates, ethicists, \nscenarios; weaker ones sample from criterion \npost-accident investigators, or society at large. \nscenarios and illustrate the systems’ behavior for \nHowever, transparency can be severely limited \nthat subsample. In the latter case, confidence in \nin some systems, especially in those that rely \nthe A/IS’ ability to meet normative requirements \non machine learning algorithms trained on large \nis more limited. An evaluation’s concluding \ndata sets. The data sets may not be accessible \njudgment must therefore acknowledge the \nto evaluators; the algorithms may be proprietary \nstrength of the verification technique used,  \ninformation or mathematically so complex that \nand the expressed confidence in the evaluation —\nthey defy common-sense explanation; and even \nand in the A/IS themselves—must be qualified  \nfellow software experts may be unable to verify \nby this level of strength.\nreliability and efficacy of the final system because \nthe system’s specifications are opaque. Transparency is only a necessary requirement \nfor a more important long-term goal: having \nFor less inscrutable systems, numerous \nsystems be accountable to their users and \ntechniques are available to evaluate the \ncommunity members. However, this goal raises \nimplementation of the A/IS’ norm conformity. \nmany questions such as to whom the A/IS are \nOn one side there is formal verification, which \naccountable, who has the right to correct the \nprovides a mathematical proof that the A/IS will \nsystems, and which kind of A/IS should be \nalways match specific normative and ethical \nsubject to accountability requirements.\nrequirements, typically devised in a top-down \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 189', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nEmbedding Values into Autonomous and Intelligent Systems\nRecommendation From Human Morality to Artificial Morality” \n(Dagstuhl Seminar 16222). Dagstuhl Reports \nTo maximize effective evaluation by third parties, \n6, no. 5, pp. 114–137, 2016. \ne.g., regulators and accident investigators, A/IS \nshould be designed, specified, and documented  •  K. R. Fleischmann, Information and Human \nso as to permit the use of strong verification and  Values. San Rafael, CA: Morgan and Claypool, \nvalidation techniques for assessing the system’s  2014. \nsafety and norm compliance, in order to achieve \naccountability to the relevant communities. •  G. Governatori and A. Rotolo. “How Do \nAgents Comply with Norms? ” in Normative \nMulti-Agent Systems, G. Boella, P. Noriega, \nFurther Resources\nG. Pigozzi, and H. Verhagen, eds., Dagstuhl \n•  M. Fisher, L. A. Dennis, and M. P. Webster.  Seminar Proceedings. Dagstuhl, Germany: \n“Verifying Autonomous Systems.”  Schloss Dagstuhl—Leibniz- Zentrum für \nCommunications of the ACM, vol. 56, pp.   Informatik, 2009.\n84–93, 2013. \n•  B. Higgins, “New York City Task Force \n•  K. Abney, G. A. Bekey, and P. Lin. Robot Ethics:  to Consider Algorithmic Harm.” Artificial \nThe Ethical and Social Implications of Robotics.  Intelligence Technology and the Law Blog, \nCambridge, MA: The MIT Press, 2011. Feb. 7, 2018. [Online]. Available: http://\naitechnologylaw.com/2018/02/new-york-city-\n•  M. Anderson and S. L. Anderson, eds.  \ntask-force-algorithmic-harm/. [Accessed Nov. \nMachine Ethics. New York: Cambridge \n1, 2018].\nUniversity Press, 2011.\n•  S. L. Jarvenpaa, N. Tractinsky, and L. Saarinen. \n•  M. Boden, J. Bryson, et al. “Principles of \n“Consumer Trust in an Internet Store: A Cross-\nRobotics: Regulating Robots in the Real \nCultural Validation” Journal of Computer-\nWorld.” Connection Science 29, no. 2, pp. \nMediated Communication, vol. 5, no. 2, pp. \n124–129, 2017. \n1–37, 1999. \n•  M. Coeckelbergh, “Can We Trust Robots?” \n•  E. H. Leet and W. A. Wallace. “Society’s Role \nEthics and Information Technology, vol.14, pp. \nand the Ethics of Modeling,” in Ethics in \n53–60, 2012.\nModeling, W. A. Wallace, ed., Tarrytown, NY: \nElsevier, 1994, pp. 242– 245. \n•  L. A. Dennis, M. Fisher, N. Lincoln, A. Lisitsa, \nand S. M. Veres, “Practical Verification of \n•  M. A. Mahmoud, M. S. Ahmad, M. Z. M. Yusoff, \nDecision-Making in Agent-Based Autonomous \nand A. Mustapha. “A Review of Norms and \nSystems.” Automated Software Engineering, \nNormative Multiagent Systems,” The Scientific \nvol. 23, no. 3, pp. 305–359, 2016.\nWorld Journal, vol. 2014, Article ID 684587, \n2014. \n•  M. Fisher, C. List, M. Slavkovik, and A. F. \nT. Winfield. “Engineering Moral Agents—\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 190', ""The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nEmbedding Values into Autonomous and Intelligent Systems\nThanks to the Contributors \nWe wish to acknowledge all of the people  •  Malo Bourgon – COO, Machine Intelligence \nwho contributed to this chapter. Research Institute\n \n•  Richard S. Bowyer – Adjunct Senior Lecturer \nThe Embedding Values into  \nand Research Fellow, College of Science and \nAutonomous Intelligent  \nEngineering, Centre for Maritime Engineering, \nSystems Committee\nControl and Imaging (cmeci), Flinders \n•  AJung Moon (Founding Chair) – Director of \nUniversity, South Australia\nOpen Roboethics Institute \n•  Stephen Cave – Executive Director of \n•  Bertram F. Malle (Co-Chair) – Professor, \nthe Leverhulme Centre for the Future of \nDepartment of Cognitive, Linguistic, and \nIntelligence, University of Cambridge\nPsychological Sciences, Co-Director of the \nHumanity-Centered Robotics Initiative, Brown  •  Raja Chatila – CNRS-Sorbonne Institute \nUniversity of Intelligent Systems and Robotics, Paris, \nFrance; Member of the French Commission on \n•  Francesca Rossi (Co-Chair) – Full Professor, \nthe Ethics of Digital Sciences and Technologies \ncomputer science at the University of Padova, \nCERNA; Past President of IEEE Robotics and \nItaly, currently at the IBM Research Center at \nAutomation Society\nYorktown Heights, NY\n•  Mark Coeckelbergh – Professor, Philosophy \n•  Stefano Albrecht – Postdoctoral Fellow in \nof Media and Technology, the University of \nthe Department of Computer Science at The \nVienna\nUniversity of Texas at Austin\n•  Louise Dennis – Lecturer, Autonomy and \n•  Bijilash Babu – Senior Manager, Ernst and \nVerification Laboratory, University of Liverpool\nYoung, EY Global Delivery Services India LLP\n•  Laurence Devillers – Professor of Computer \n•  Jan Carlo Barca – Senior Lecturer in \nSciences, University Paris Sorbonne, LIMSI-\nSoftware Engineering and Internet of Things \nCNRS 'Affective and social dimensions in \n(IoT), School of Info Technology, Deakin \nspoken interactions'; member of the French \nUniversity, Australia\nCommission on the Ethics of Research in \nDigital Sciences and Technologies (CERNA)\n•  Catherine Berger – IEEE Standards Senior \nProgram Manager, IEEE \n•  Virginia Dignum – Associate Professor, \nFaculty of Technology Policy and Management, \nTU Delft\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 191"", 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nEmbedding Values into Autonomous and Intelligent Systems\n•  Ebru Dogan – Research Engineer, VEDECOM •  Sven Koenig – Professor, Computer Science \nDepartment, University of Southern California\n•  Takashi Egawa – Cloud Infrastructure \nLaboratory, NEC Corporation, Tokyo •  Brenda Leong – Senior Counsel, Director of \nOperations, The Future of Privacy Forum\n•  Vanessa Evers – Professor, Human-Machine \nInteraction, and Science Director, DesignLab,  •  Alan Mackworth – Professor of Computer \nUniversity of Twente Science, University of British Columbia; \nFormer President, AAAI; Co-author of “Artificial \n•  Michael Fisher – Professor of Computer \nIntelligence: Foundations of Computational \nScience, University of Liverpool, and Director \nAgents”.\nof the UK Network on the Verification and \n•  Pablo Noriega – Scientist, Artificial \nValidation of Autonomous Systems, vavas.org\nIntelligence Research Institute of the Spanish \n•  Ken Fleischmann – Associate Professor in  National Research Council (IIIA-CSIC), \nthe School of Information at The University of  Barcelona.\nTexas at Austin\n•  Rajendran Parthiban – Professor, School \nof Engineering, Monash University, Bandar \n•  Edith Pulido Herrera – Bioengineering \nSunway, Malaysia \ngroup, Antonio Nariño University, Bogotá, \nColombia •  Heather M. Patterson – Senior Research \nScientist, Anticipatory Computing Lab,  \n•  Ryan Integlia – assistant professor, Electrical \nIntel Corp.\nand Computer Engineering, Florida Polytechnic \n•  Edson Prestes – Professor, Institute of \nUniversity; Co-Founder of the em[POWER] \nInformatics, Federal University of Rio Grande \nEnergy Group\ndo Sul (UFRGS), Brazil; Head, Phi Robotics \n•  Catholijn Jonker – Full professor of  Research Group, UFRGS; CNPq Fellow. \nInteractive Intelligence at the Faculty of \n•  Laurel Riek – Associate Professor, Computer \nElectrical Engineering, Mathematics and \nScience and Engineering, University of \nComputer Science of the Delft University of \nCalifornia San Diego\nTechnology. Part-time full professor at Leiden \n•  Leanne Seeto – Co-Founder and Strategy \nInstitute of Advanced Computer Science of the \nand Operations Precision Autonomy\nLeiden University\n•  Sarah Spiekermann – Chair of the Institute \n•  Sara Jordan – Assistant Professor of Public \nfor Information Systems & Society at Vienna \nAdministration in the Center for Public \nUniversity of Economics and Business; Author \nAdministration & Policy at Virginia Tech\nof the textbook “Ethical IT-Innovation”, the \npopular book “Digitale Ethik—Ein Wertesystem \n•  Jong-Wook Kim – Professor, AI.Robotics Lab, \nfür das 21. Jahrhundert” and Blogger on  \nDepartment of Electronic Engineering, Dong-A \n”The Ethical Machine” \nUniversity, Busan, Korea\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 192', ""The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nEmbedding Values into Autonomous and Intelligent Systems\n•  John P. Sullins – Professor of Philosophy,  •  Nell Watson – CFBCS, FICS, FIAP, FIKE, \nChair of the Center for Ethics Law and Society  FRSA, FRSS, FLS Co-Founder and Chairman, \n(CELS), Sonoma State University  EthicsNet, AI & Robotics Faculty Singularity \nUniversity, Foresight Machine Ethics Fellow\n•  Jaan Tallinn – Founding engineer of Skype \nand Kazaa; co-founder of the Future of Life  •  Karolina Zawieska – Postdoctoral Research \nInstitute  Fellow in Ethics and Cultural Learning of \nRobotics at DeMontfort University, UK and \n•  Mike Van der Loos – Associate Prof., \nResearcher at Industrial Research Institute for \nDept. of Mechanical Engineering, Director \nAutomation and Measurements PIAP, Poland\nof Robotics for Rehabilitation, Exercise and \nAssessment in Collaborative Healthcare  For a full listing of all IEEE Global Initiative \n(RREACH) Lab, and Associate Director of  Members, visit standards.ieee.org/content/dam/\nCARIS Lab, University of British Columbia ieee-standards/standards/web/documents/other/\nec_bios.pdf. \n•  Wendell Wallach – Consultant, ethicist, \nand scholar, Yale University's Interdisciplinary  For information on disclaimers associated with \nCenter for Bioethics EAD1e, see How the Document Was Prepared.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 193"", 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nEmbedding Values into Autonomous and Intelligent Systems\nEndnotes\n1  S. Hitlin and J. A. Piliavin. “Values: Reviving a  8  I. van de Poel, “An Ethical Framework for Evaluat-\nDormant Concept.” Annual Review of Sociology 30  ing Experimental Technology”, Science and Engi-\n(2004): 359–393. neering Ethics, 22, no. 3 (2016): 667-686.  \n2  B. F. Malle, and S. Dickert. “Values,” The Encyclo- 9  I. Misra, C. L. Zitnick, M. Mitchell, and R. Girshick, \npedia of Social Psychology, edited by R. F. Baumeis- (2016). Seeing through the human reporting bias: \nter and K. D. Vohs. Thousand Oaks, CA: Sage, 2007. Visual Classifiers from Noisy Human-Centric Labels. \nIn Proceedings of the 2016 IEEE Conference on \n3  M. J. Rohan, “A Rose by Any Name? The Values \nComputer Vision and Pattern Recognition (CVPR) \nConstruct.” Personality and Social Psychology Re-\n(pp. 2930–2939). doi:10.1109/CVPR.2016.320\nview 4 (2000): 255–277.\n10  A. Mack, (Ed.). (2018). Changing social norms. \n4  A. U. Sommer, Werte: Warum Man Sie Braucht, \nSocial Research: An International Quarterly, 85(1, \nObwohl es Sie Nicht Gibt. [Values. Why We Need \nSpecial Issue), 1–271. \nThem Even Though They Don’t Exist.] Stuttgart, \nGermany: J. B. Metzler, 2016. 11  B. Green and L. Hu. “The Myth in the Method-\nology: Towards a Recontextualization of Fairness \n5  B. F. Malle, M. Scheutz, and J. L. Austerweil. “Net-\nin ML.” Paper presented at the Debates workshop \nworks of Social and Moral Norms in Human and \nat the 35th International Conference on Machine \nRobot Agents,” in A World with Robots: International \nLearning, Stockholm, Sweden 2018. \nConference on Robot Ethics: ICRE 2015, edited by \nM. I. Aldinhas Ferreira, J. Silva Sequeira, M. O. Tokhi,  12  J. Van den Hoven, “Engineering and the Problem \nE. E. Kadar, and G. S. Virk, 3–17. Cham, Switzerland:  of Moral Overload.” Science and Engineering Ethics \nSpringer International Publishing, 2017. 18, no. 1 (2012): 143–155. \n6  J. Vázquez-Salceda, H. Aldewereld, and F. Dig- 13  C. Andre and M. Velasquez. “The Common \nnum. “Implementing Norms in Multiagent Systems,”  Good.” Issues in Ethics 5, no. 1 (1992). \nin Multiagent System Technologies. MATES 2004, \n14  W. Wallach and C. Allen. Moral Machines: Teach-\nedited by G. Lindemann, Denzinger, I. J. Timm, and \ning Robots Right from Wrong. New York: Oxford \nR. Unland. (Lecture Notes in Computer Science, \nUniversity Press, 2008. \nvol. 3187.) Berlin: Springer, 2004.\n15  L. Dennis, M. Fisher, M. Slavkovik, and M. \n7  A. Mack, (Ed.). “Changing social norms.” Social \nWebster. “Formal Verification of Ethical Choices in \nResearch: An International Quarterly, 85, no.1 \nAutonomous Systems.” Robotics and Autonomous \n(2018): 1–271. \nSystems 77 (2016): 1–14. \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 194', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nEmbedding Values into Autonomous and Intelligent Systems\n16  L. M. Pereira and A. Saptawijaya. Programming  24  A. Etzioni, “Designing AI Systems That Obey Our \nMachine Ethics. Cham, Switzerland: Springer Inter- Laws and Values.” Communications of the ACM 59, \nnational, 2016.  no. 9 (2016): 29–31. \n17  F. Rötzer, ed. Programmierte Ethik: Brauchen  25  T. Arnold, D. Kasenberg, and M. Scheutz. “Value \nRoboter Regeln oder Moral? Hannover, Germany:  Alignment or Misalignment—What Will Keep Sys-\nHeise Medien, 2016.  tems Accountable?” The Workshops of the Thir-\nty-First AAAI Conference on Artificial Intelligence: \n18  M. Scheutz, B. F. Malle, and G. Briggs. “Towards \nTechnical Reports, WS-17-02: AI, Ethics, and Society, \nMorally Sensitive Action Selection for Autonomous \n81–88. Palo Alto, CA: The AAAI Press, 2017. \nSocial Robots.” Proceedings of the 24th Interna-\ntional Symposium on Robot and Human Interactive  26  G. Andrighetto, G. Governatori, P. Noriega, and \nCommunication, RO-MAN 2015 (2015): 492–497.  L. W. N. van der Torre, eds. Normative Multi-Agent \nSystems. Saarbrücken/Wadern, Germany: Dagstuhl \n19  M. Anderson and S. L. Anderson. “GenEth: A \nPublishing, 2013.\nGeneral Ethical Dilemma Analyzer.” Proceedings \nof the Twenty-Eighth AAAI Conference on Artificial  27  A. Chaudhuri, (2017) Philosophical Dimen-\nIntelligence (2014): 253–261.  sions of Information and Ethics in the Internet of \nThings (IoT) Technology. The EDP Audit, Con-\n20  M. O. Riedl and B. Harrison. “Using Stories to \ntrol, and Security Newsletter, 56:4, 7-18, DOI: \nTeach Human Values to Artificial Agents.” Proceed-\n10.1080/07366981.2017.1380474\nings of the 2nd International Workshop on AI, Ethics \nand Society, Phoenix, Arizona, 2016.  28  S.Wachter, B. Mittelstadt, and L. Floridi, “Trans-\nparent, Explainable, and Accountable AI for Robot-\n21  V. Charisi, L. Dennis, M. Fisher et al. “Towards \nics.” Science Robotics 2, no. 6 (2017): eaan6080. \nMoral Autonomous Systems,” 2017. \ndoi:10.1126/scirobotics. aan6080\n22  R. Arkin, “Governing Lethal Behavior: Embedding \n29  A. D. Selbst and S. Barocas, The Intuitive Ap-\nEthics in a Hybrid Deliberative/Reactive Robot Ar-\npeal of Explainable Machines (February 19, 2018). \nchitecture.” Proceedings of the 2008 3rd ACM/IEEE \nFordham Law Review. Available at SSRN: https://\nInternational Conference on Human-Robot Interac-\nssrn.com/abstract=3126971 or http://dx.doi.\ntion (2008): 121–128. \norg/10.2139/ssrn.3126971\n23  A. F. T. Winfield, C. Blum, and W. Liu. “Towards an \n30  F. S. Grodzinsky, K. W. Miller, and M. J. Wolf. “De-\nEthical Robot: Internal Models, Consequences and \nveloping Artificial Agents Worthy of Trust: Would You \nEthical Action Selection” in Advances in Autono-\nBuy a Used Car from This Artificial Agent?” Ethics \nmous Robotics Systems, Lecture Notes in Computer \nand Information Technology 13, (2011): 17–27.  \nScience Volume, edited by M. Mistry, A. Leonardis, \n \nWitkowski, and C. Melhuish, 85–96. Springer, 2014. \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 195', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nEmbedding Values into Autonomous and Intelligent Systems\n31  J. A. Kroll, J. Huey, S. Barocas et al. “Accountable  41  N. Tintarev and R. Kutlak. “Demo: Making Plans \nAlgorithms.” University of Pennsylvania Law Review  Scrutable with Argumentation and Natural Language \n165 (2017).  Generation.” Proceedings of the Companion \nPublication of the 19th International Conference on \n32  J. Cleland-Huang, O. Gotel, and A. Zisman, \nIntelligent User Interfaces (2014): 29–32. \neds. Software and Systems Traceability. London: \nSpringer, 2012. doi:10.1007/978- 1-4471-2239-5 42  d. boyd, “Transparency ≠ Accountability.” Data & \nSociety: Points, November 29, 2016.\n33  S. U. Noble, “Google Search: Hyper-Visibility \nas a Means of Rendering Black Women and Girls  43  C. Oetzel and S. Spiekermann, “A Systematic \nInvisible.” InVisible Culture 19 (2013).  Methodology for Privacy Impact Assessments: A \nDesign Science Approach.” European Journal of \n34  K. R. Fleischmann and W. A. Wallace.  \nInformation Systems 23, (2014): 126–150. https://\n“A Covenant with Transparency: Opening the  \nlink.springer.com/article/10.1057/ejis.2013.18\nBlack Box of Models.” Communications of the  \nACM 48, no. 5 (2005): 93–97.  44  M. Brundage, S. Avin, J. Clark, H. Toner, P. \nEckersley, B. Garfunkel, A. Dafoe, P. Scharre, T. \n35  M. Fisher, L. A. Dennis, and M. P. Webster. \nZeitzo, et al. 2018. The Malicious Use of Artificial \n“Verifying Autonomous Systems.” Communications \nIntelligence: Forecasting, Prevention, and Mitigation. \nof the ACM 56, no. 9 (2013): 84–93. \nCoRR abs/1802.07228 (2018). https://arxiv.org/\nabs/1802.07228M. \n36  M. Hind, et al. “Increasing Trust in AI Services \nthrough Supplier’s Declarations of Conformity.” ArXiv \n45  D. Vanderelst and A.F. Winfield, 2018 The Dark \nE-Prints, Aug. 2018. Retrieved October 28, 2018 \nSide of Ethical Robots. In Proc. AAAI/ACM Conf. \nfrom https://arxiv.org/abs/1808.07261. \non Artificial Intelligence, Ethics and Society, New \nOrleans.\n37  Vitsoe. “The Power of Good Design.” Vitsoe, \n2018. Retrieved Oct 22, 2018 from https://www.\n46  British Standards Institution. BS8611:2016, \nvitsoe.com/us/about/good-design. \n“Robots and Robotic Devices. Guide to the Ethical \nDesign and Application of Robots and Robotic \n38  G. Donelli, (2015, March 13). Good design is \nSystems,” 2016. \nhonest (Blogpost). Retrieved Oct 22, 2018 from \nhttps://blog.astropad.com/good-design-is-honest/ \n47  I. Sommerville, Software Engineering (10th \nedition). Harlow, U.K.: Pearson Studium, 2015. \n39  C. de Jong Ed., “Ten principles for good design: \nDieter Rams.” New York, NY: Prestel Publishing, \n48  M. Fisher, L. A. Dennis, and M. P. Webster. \n2017.\n“Verifying Autonomous Systems.” Communications \nof the ACM 56, no. 9 (2013): 84–93. \n40  Ibid.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 196', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nEmbedding Values into Autonomous and Intelligent Systems\n49  International Organization for Standardization \n(2015). ISO 9001:2015, Quality management \nsystems—Requirements. Retrieved July 12, 2018 \nfrom https://www.iso.org/standard/62085.html. \n50  BMC Software. ITIL: The Beginner’s Guide to \nProcesses & Best Practices. 6 Dec. 2016, http://\nwww.bmc.com/guides/itil-introduction.html. \n51  J. Griffiths, “New Zealand Passport Robot Thinks \nThis Asian Man’s Eyes Are -Closed.” CNN.com, \nDecember 9, 2016. \n52  R. Tatman, “Google’s Speech Recognition Has \na Gender Bias.” Making Noise and Hearing Things, \nJuly 12, 2016. \n53  J. Angwin, J. Larson, S. Mattu, L. Kirchner. \n“Machine Bias: There’s Software Used Across the \nCountry to Predict Future Criminals. And It’s Biased \nAgainst Blacks.” ProPublica, May 23, 2016. \n54  L. D. Riek and D. Howard. “A Code of Ethics \nfor the Human-Robot Interaction Profession.” \nProceedings of We Robot, April 4, 2014.\n55  M. Fisher, L. A. Dennis, and M. P. Webster. \n“Verifying Autonomous Systems.” Communications \nof the ACM 56 (2013): 84–93.\n \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 197', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nPolicy\nIntroduction\nAutonomous and intelligent systems (A/IS) are a part of our society. The use of these \npowerful technologies promotes a range of social benefits. They may spur development \nacross economies and society through numerous applications, including in commerce, \nfinance, employment, health care, agriculture, education, transportation, politics, privacy, \npublic safety, national security, civil liberties, and human rights. To encourage the \ndevelopment of socially beneficial applications of A/IS, and to protect the public from \nadverse consequences of A/IS, intended or otherwise, effective policies and government \nregulations are needed.\nEffective A/IS policies serve the public interest in several important respects. A/IS policies \nand regulations, at both the national level and as developed by professional organizations \nand governing institutions, protect and promote safety, privacy, human rights, and \ncybersecurity, as well as enhance the public’s understanding of the potential impacts of  \nA/IS on society. Without policies designed with these considerations in mind, there may \nbe critical technology failures, loss of life, and high-profile social controversies. Such events \ncould engender policies that unnecessarily hinder innovation, or regulations that do not \neffectively advance public interest and protect human rights.\nWe believe that effective A/IS policies should embody a rights-based approach1 that \naddresses five issues:\n1. Ensure that A/IS support, promote, and enable internationally  \nrecognized legal norms. \nEstablish policies for A/IS using the internationally recognized legal framework for human \nrights standards that is directed at accounting for the impact of technology on individuals. \n \n \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 198', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nPolicy\n2. Develop government expertise in A/IS. \nFacilitate skill development, technical and otherwise, to further boost the ability of policy \nmakers, regulators, and elected officials to make informed proposals and decisions about \nthe various facets of these new technologies.\n3. Ensure governance and ethics are core components in A/IS research, \ndevelopment, acquisition, and use. \nRequire support for A/IS research and development (R&D) efforts with a focus on the \nethical impact of A/IS. To benefit from these new technologies while also ensuring they \nmeet societal needs and values, governments should be actively involved in supporting \nrelevant R&D efforts.\n4. Create policies for A/IS to ensure public safety and responsible A/IS design. \nGovernments must ensure consistent and locally adaptable policies and regulations \nfor A/IS. Effective regulation should address transparency, explainability, predictability, \nbias, and accountability for A/IS algorithms, as well as risk management, privacy, data \nprotection measures, safety, and security considerations. Certification of systems \ninvolving A/IS is a key technical, societal, and industrial issue.\n5. Educate the public on the ethical and societal impacts of A/IS. \nIndustry, academia, the media, and governments must establish strategies for informing \nand engaging the public on benefits and challenges posed by A/IS. Communicating \naccurately both the positive potential of A/IS and the areas that require caution and \nfurther development is critical to effective decision-making environments.\nAs A/IS comprise a greater part of our daily lives, managing the associated risks and rewards \nbecomes increasingly important. Technology leaders and policy makers have much to \ncontribute to the debate on how to build trust, promote safety and reliability, and integrate \nethical and legal considerations into the design of A/IS technologies. This chapter provides  \na principled foundation for these discussions.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 199', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nPolicy\nThe Ruggie principles have been widely \nreferenced and endorsed by corporations and \nIssue 1: Ensure that A/IS \nhave led to the adoption of several corporate \nsupport, promote, and enable \nsocial responsibility (CSR) policies in various \ninternationally recognized  companies. With broadened support, the Ruggie \nlegal norms principles will strengthen the role of businesses \nin protecting and promoting human rights and \nensuring that the most crucial human values and \nBackground\nlegal standards of human rights are respected by \nA/IS technologies have the potential to impact \nA/IS technologists.\ninternationally recognized economic, social, \ncultural, and political rights through unintended   \noutcomes and outright design decisions. Important  Recommendations\nexamples of this issue have occurred with certain \nNational policies and business regulations for \nunmanned aircraft systems (Bowcott 2013), use of \nA/IS should be founded on a rights-based \nA/IS in predictive policing (Shapiro 2017), banking \napproach. The Ruggie principles provide the \n(Garcia 2017), judicial sentencing (Osoba and \ninternationally recognized legal framework for \nWelser 2017), and job hunting and hiring practices \nhuman rights standards that accounts for the \n(Datta, Tschantz, and Datta 2014). Even service \nimpact of technology on individuals while also \ndelivery of goods (Ingold and Soper 2016) can \naddressing inequalities, discriminatory practices, \nimpact human rights by automating discrimination \nand the unjust distribution of resources. \n(Eubanks 2018) and inhibiting the right of \nassembly, freedom of expression, and access to  These six considerations for a rights-based \ninformation. To ensure A/IS are used as a force for  approach to A/IS flow from the recommendation \nsocial benefit, nations must develop policies that  above:\nsafeguard human rights.\n•  Responsibility: Identify the right holders and \nA/IS regulation, development, and deployment  the duty bearers and ensure that duty bearers \nshould, therefore, be based on international  have an obligation to fulfill all human rights.\nhuman rights standards and standards of \n•  Accountability: Oblige states, as duty bearers, \ninternational humanitarian laws. When put into \nto behave responsibly, to seek to represent \npractice, both states and private actors will \nthe greater public interest, and to be open to \nconsider their responsibilities to protect and \npublic scrutiny of their A/IS policies.\nrespect internationally recognized political, social, \neconomic, and cultural rights. Similarly, business  •  Participation: Encourage and support a high \nactors will consider their obligations to respect  degree of participation of duty bearers, right \ninternational human rights, as described in the  holders, and other interested parties. \nUnited Nations Guiding Principles on Business   \nand Human Rights (OHCHR 2011), also known  \nas the Ruggie principles.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 200', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nPolicy\n•  Nondiscrimination: Underlie the practice of  •  O. A. Osoba, and W. Welser IV, “An Intelligence \nA/IS with principles of nondiscrimination,  in Our Image: The Risks of Bias and Errors in \nequality, and inclusiveness. Particular attention  Artificial Intelligence,” (Research Report 1744). \nmust be given to vulnerable groups, to  Santa Monica, CA: RAND Corporation, 2017.\nbe determined locally, such as minorities, \n•  A. Datta, M. C. Tschantz, and A. Datta. \nindigenous peoples, or persons with \n“Automated Experiments on Ad Privacy \ndisabilities.\nSettings: A Tale of Opacity, Choice, and \n•  Empowerment: Empower right holders to  Discrimination,” arXiv:1408.6491 [Cs] , 2014.\nclaim and exercise their rights.\n•  D. Ingold, and S. Soper, “Amazon Doesn’t \n•  Corporate responsibility: Ensure that  Consider the Race of Its Customers. Should \ncompanies’ developments of A/IS comply  It?” Bloomberg, April 21, 2016.\nwith the rights-based approach. Companies \n•  United Nations. Office of the High \nmust not willingly provide A/IS to actors that \nCommissioner of Human Rights. Guiding \nwill use them in ways that lead to human \nPrinciples on Business and Human Rights: \nrights violations.\nImplementing the United Nations “Protect, \nRespect and Remedy” Framework. United \nFurther Resources\nNations Office of the High Commissioner of \n•  Human rights-based approaches have been  Human Rights. New York and Geneva: UN, \napplied to development, education and  2011.\nreproductive health. See the UN Practitioners’ \n•  “Mapping Regulatory Proposals for Artificial \nPortal on Human Rights Based Programming.\nIntelligence in Europe.” Access Now, \n•  O. Bowcott, “Drone Strikes by US May Violate  November 2018.\nInternational Law, Says UN,” The Guardian, \n•  V. Eubanks, Automating Inequality. How High-\nOctober 18, 2013.\nTech Tools Profile, Police, and Punish the Poor. \n•  A. Shapiro, “Reform Predictive Policing,”  St. Martin’s Press, January 2018.\nNature News, vol. 541, no. 7638, pp. 458–\n460, Jan. 25, 2017.  \n \n•  M. Garcia, “How to Keep Your AI from Turning \n \nInto a Racist Monster,” Wired, April 21, 2017. \n \n   \n   \n   \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 201', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nPolicy\norganizations3 that operate at the intersection \nof technology policy, technical engineering, \nIssue 2: Develop government \nand advocacy. This will enhance the technical \nexpertise in A/IS\nknowledge of policy makers, strengthen ties \nbetween political and technical communities, \nBackground and contribute to the formulation of effective \nA/IS policy.\nThere is a consensus among private sector and \nacademic stakeholders that effectively governing \n•  Expertise can also be developed through \nA/IS and related technologies requires a level of \ncross-border sharing of best practices around \ntechnical expertise that governments currently \nA/IS legislation, consumer protection, \ndo not possess. Effective governance requires \nworkforce transformation, and economic \nexperts who understand and can analyze the \ndisplacement stemming from A/IS-based \ninteractions between A/IS technologies, policy \nautomation. This can be done through \nobjectives, and overall societal values. Sufficient \ngovernmental cooperation, knowledge \ndepth and breadth of technical expertise will \nexchanges, and by building A/IS components \nhelp ensure policies and regulations successfully \ninto venues and efforts surrounding existing \nsupport innovation, adhere to national principles, \nregulation, e.g., the General Data Protection \nand protect public safety.\nRegulation (GDPR).\nEffective governance also requires an A/IS \n•  Because A/IS involve rapidly evolving \nworkforce that has adequate training in ethics \ntechnologies, both workforce training in  \nand access to other resources on human rights \nA/IS areas and long-term science, technology, \nstandards and obligations, along with guidance  \nengineering, and math (STEM) educational \non how to apply them in practice.\nstrategies, along with ethics courses, are \nneeded beginning in primary school and \nRecommendations\nextending into university or vocational courses. \nPolicy makers should support the development  These strategies will foster A/IS expertise \nof expertise required to create a public policy,  in the next generation of many groups, e.g., \nlegal, and regulatory environment that allows  supervisors of critical systems, scientists,  \ninnovation to flourish while protecting the public  and policy makers.\nand gaining public trust.2 Example strategies \n \ninclude the following:\n \n•  Expertise can be furthered through technical   \nfellowships, or rotation schemes, where   \ntechnologists spend an extended time in   \npolitical offices, or policy makers work with   \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 202', ""The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nPolicy\nFurther Resources technologies such as intelligent robots and self-\ndriving cars that will revolutionize automobile \n•  J. Holdren, and M. Smith, “Preparing for the \ntransportation and logistics systems and reduce \nFuture of Artificial Intelligence.” Washington, \ntraffic fatalities. A/IS can improve quality of life \nDC: Executive Office of the President, National \nthrough smart cities and decision support in \nScience and Technology Council, 2016.\nhealth care, social services, criminal justice, and \n•  P. Stone, R. Brooks, E. Brynjolfsson, R.  the environment. To ensure such a positive effect \nCalo, O. Etzioni, G. Hager, J. Hirschberg, S.  on individuals, societies, and businesses, nations \nKalyanakrishnan, E. Kamar, S. Kraus, K. Leyton- must increase A/IS R&D investments, with \nBrown, D. Parkes, W. Press, A. Saxenian, J.  particular focus on the ethical development and \nShah, M. Tambe, and A. Teller. “'Artificial  deployment of A/IS.\nIntelligence and Life in 2030': One Hundred \nInternational collaboration involving governments, \nYear Study on Artificial Intelligence.” (Report \nprivate industry, and non-governmental \nof the 2015-2016 Study Panel). Stanford, CA: \norganizations (NGOs) would promote the \nStanford University, 2016.\ndevelopment of standards, data sharing, and \n•  “Japan Industrial Policy Spotlights AI, Foreign  norms that guide ethically aligned A/IS R&D.\nLabor.” Nikkei Asian Review, May 20, 2016.\nRecommendations\n•  Y.H. Weng, “A European Perspective on Robot \nLaw: Interview with Mady Delvaux-Stehres.”  Develop national and international standards for \nRobohub, July 15, 2016. A/IS to enable efficient and effective public and \nprivate sector investments. Important aspects \nfor international standards include measures of \nsocietal benefits derived from A/IS, the use of \nIssue 3: Ensure governance and  ethical considerations in A/IS investments, and \nethics are core components in  risks increased or decreased by A/IS. Nations \nshould consider their own ethical principles and \nA/IS research, development, \ndevelop a framework for ethics that each country \nacquisition, and use.\ncould use to reflect local systems of values and \nlaws. This will encourage actors to think both \nBackground\nlocally and globally regarding ethics. Therefore, \nGreater national investment in ethical A/IS  we recommend governments to:\nresearch and development would stimulate \n•  Establish priorities for funding A/IS research \nthe economy, create high-value jobs, improve \nthat identify approaches and challenges  \ngovernmental services to society, and encourage \nfor A/IS governance. This research will  \ninternational innovation and collaboration (U.S. \nidentify models for national and global  \nOSTP report on the Future of AI 2016). A/IS have \nA/IS governance and assess their benefits  \nthe potential to improve our societies through \nand adequacy to address A/IS societal needs.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 203"", 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nPolicy\n•  Encourage the participation of a diverse set  •  The Networking and Information Technology \nof stakeholders in the standards development  Research and Development Program, \nprocess. Standards should address A/IS  “Supplement to the President’s Budget, \nissues such as fairness, security, transparency,  FY2017.” NITRD National Coordination Office, \nunderstandability, privacy, and societal impacts  April 2016.\nof A/IS. A global framework for identification \n•  S. B. Furber, F. Galluppi, S. Temple, and L. A. \nand sharing of these and other issues should \nPlana, “The SpiNNaker Project.” Proceedings \nbe developed. Standards should incorporate \nof the IEEE, vol. 102, no. 5, pp. 652–665, \nindependent mechanisms to properly vet, \n2014.\ncertify, audit, and assign accountability for  \nthe A/IS applications.\n•  H. Markram, “The Human Brain Project,” \nScientific American, vol. 306, no. 2, pp. \n•  Encourage and establish national and \n50–55, June 2012.\ninternational research groups that provide \nincentives for A/IS research that is publicly \n•  L. Yuan, “China Gears Up in Artificial-\nbeneficial but may not be commercially viable.\nIntelligence Race.” Wall Street Journal,  \nAugust 24, 2016.\nFurther Resources\n•  E. T. Kim, “How an Old Hacking Law Hampers \nthe Fight Against Online Discrimination.” The \nIssue 4: Create policies for  \nNew Yorker, October 1, 2016.\nA/IS to ensure public safety  \n•  National Research Council. “Developments \nand responsible A/IS design\nin Artificial Intelligence, Funding a Revolution: \nGovernment Support for Computing \nBackground\nResearch.” Washington, DC: The National \nAcademies Press, 1999. Effective governance encourages innovation and \ncooperation, helps synchronize policies globally, \n•  N. Chen, L. Christensen, K. Gallagher, R. Mate,  and reduces barriers to trade. Governments must \nand G. Rafert, “Global Economic Impacts  ensure consistent and appropriate policies and \nAssociated with Artificial Intelligence.”   regulations for A/IS that address transparency, \nAnalysis Group, February 25, 2016.  explainability, predictability, and accountability \n  of A/IS algorithms, risk management,4 data \n  protection, safety, and certification of A/IS.\n \n  Appropriate regulatory responses are context-\n  dependent and should be developed through an \napproach that is based on human rights5 and has \nhuman well-being as a key goal.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 204', ""The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nPolicy\nRecommendations Further Resources\nNations should develop and harmonize their \n•  P. Stone, R. Brooks, E. Brynjolfsson, R. \npolicies and regulations for A/IS using a process \nCalo, O. Etzioni, G. Hager, J. Hirschberg, S. \nthat is based on informed input from a range of \nKalyanakrishnan, E. Kamar, S. Kraus, K. Leyton-\nexpert stakeholders, including academia, industry, \nBrown, D. Parkes, W. Press, A. Saxenian, J. \nNGOs, and government officials, that addresses \nShah, M. Tambe, and A. Teller. “'Artificial \nquestions related to the governance and safe \nIntelligence and Life in 2030': One Hundred \ndeployment of A/IS. We recommend:\nYear Study on Artificial Intelligence.” (Report \nof the 2015-2016 Study Panel). Stanford, CA: \n•  Policy makers should consider similar \nStanford University, 2016.\nwork from around the world. Due to \nthe transnational nature of A/IS, globally \n•  R. Calo, “The Case for a Federal Robotics \nsynchronized policies can benefit public safety, \nCommission,” The Brookings Institution, 2014.\ntechnological innovation, and access to A/IS.\n•  O. Groth, and Mark Nitzberg, Solomon’s Code: \n•  Policies should foster the development of \nHumanity in a World of Thinking Machines \neconomies able to absorb A/IS. Additional \n(chapter 8 on governance), New York: \nfocus is needed to address the effect of  \nPegasus Books, 2018.\nA/IS on employment and income and how  \nto ameliorate certain societal conditions.   •  A. Mannes, “Institutional Options for Robot \nNew models of public-private partnerships  Governance,” 1–40, in We Robot 2016, Miami, \nshould be studied. FL, April 1–2, 2016.\n•  Policies for A/IS should remain founded on a  •  G. E. Marchant, K. W. Abbott, and B. Allenby, \nrights-based approach. Innovative Governance Models for Emerging \nTechnologies. Cheltenham, U.K.: Edward Elgar \n•  Policy makers should be prepared to address \nPublishing, 2014.\nissues that will arise when innovative and new \npractices enabled by A/IS are not consistent  •  Y. H. Weng, Y. Sugahara, K. Hashimoto, and \nwith current law. In A/IS, where there is often  A. Takanishi. “Intersection of ‘Tokku’ Special \na different system developer, integrator,  Zone, Robots, and the Law: A Case Study \nuser, and ultimate customer, application of  on Legal Impacts to Humanoid Robots,” \ntraditional legal concepts of agency, strict  International Journal of Social Robotics 7, no. \nliability, and parental liability will require legal  5, pp. 841–857, 2015. \nresearch and deliberation. Challenges from    \nA/IS that must be considered include   \nincreasing complexity of and interactions   \nbetween systems, and the potential for   \nreduced predictability due to the nature    \nof machine learning systems.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 205"", 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nPolicy\nRecommendations\nIssue 5: Educate the public on  Establish an international multi-stakeholder \nthe ethical and societal impacts  forum, to include commercial, governmental, and \nother civil society groups, to determine the best \nof A/IS\npractices for using and developing A/IS. Codify \nthe deliberations into international norms and \nBackground\nstandards. Many industries—in particular, system \nIt is imperative for industry, academia, and  industries (automotive, air and space, defense, \ngovernment to communicate accurately to the  energy, medical systems, manufacturing)—will be \npublic both the positive and negative potential  changed by the growing use of A/IS. Therefore, \nof A/IS and the areas that require caution.6  we recommend governments to:\nStrategies for informing and engaging the public \n•  Increase funding for interdisciplinary research \non A/IS benefits and challenges are critical to \nand communication on topics ranging from \ncreating an environment conducive to effective \nbasic research on intelligence to principles \ndecision-making.\nof ethics, safety, privacy, fairness, liability, \nEducating users of A/IS will help influence the  and trustworthiness of A/IS. Societal aspects \nnature of A/IS development. Educating policy  should be addressed both at an academic \nmakers and regulators on the technical and legal  level and through the engagement of \naspects of A/IS will help enable the creation  business, civil society, public authorities, and \nof well-defined policies that promote human  policy makers.\nrights, safety, and economic benefits. Educating \n•  Empower and enable independent journalists \ncorporations, researchers, and developers of  \nand media outlets to report on A/IS by \nA/IS on the benefits and risks to individuals and \nproviding access to technical expertise.\nsocieties will enhance the creation of A/IS that \nbetter serve human well-being.7\n•  Conduct educational outreach to inform \nthe public on A/IS research, development, \nAnother key requirement is that A/IS are \napplications, risks and rewards, along with \nsufficiently transparent regarding implicit and \nthe policies, regulations, and testing that are \nexplicit values and algorithmic processes. This is \ndesigned to safeguard human rights and \nnecessary for the public understanding of A/IS \npublic safety. \naccountability, predictions, decisions, biases,  \n \nand mistakes.\n \n   \n \n \n \n \n \n \n \n   \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 206', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nPolicy\nDevelop a broad range of A/IS educational  •  C. Garvie, A. Bedoya, and J. Frankle. “The \nprograms. Undergraduate, professional degree,  Perpetual Line-Up: Unregulated Police \nadvanced degree, and executive education  Face Recognition in America.” Washington, \nprograms should offer instruction that ensures  DC: Georgetown Law, Center on Privacy & \nlawyers, legislators, and A/IS workers are well  Technology, 2016.\ninformed about issues arising from A/IS, including \n•  M. Chui, and J. Manyika, “Automation, \nthe need for measurable standards of A/IS \nJobs, and the Future of Work.” Seattle, WA: \nperformance, effects, and ethics, and the need to \nMcKinsey Global Institute, 2014.\nmature the still nascent capabilities to measure \nthese elements of A/IS.\n•  R. C. Arkin, “Ethics and Autonomous Systems: \nPerils and Promises [Point of View].” \nFurther Resources\nProceedings of the IEEE 104, no. 10, pp. \n•  Networking and Information Technology \n1779–1781, Sept. 19, 2016.\nResearch and Development (NITRD) Program, \n“The National Artificial Intelligence Research  •  European Commission, Eurobarometer Survey \nand Development Strategic Plan,” Washington,  on Autonomous Systems (DG Connect, June \nDC: Office of Science and Technology   2015), looks at Europeans’ attitudes toward \nPolicy, 2016. robots, driverless vehicles, and autonomous \ndrones. The survey shows that those who \n•  J. Saunders, P. Hunt, and J. S. Hollywood, \nhave more experience with robots (at home, \n“Predictions Put into Practice: A Quasi-\nat work or elsewhere) are more positive \nExperimental Evaluation of Chicago’s Predictive \ntoward their use.\nPolicing Pilot,” Journal of Experimental \nCriminology, vol. 12, no. 347, pp. 347–371,   \n2016. [Online] Available: doi:10.1007/s11292-  \n019272-0. [Accessed Nov. 10, 2018].  \n \n•  B. Edelman and M. Luca, “Digital \n \nDiscrimination: The Case of Airbnb.com.” \n \nHarvard Business School Working Paper  \n \n14-054, Jan. 28, 2014.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 207', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nPolicy\nThanks to the Contributors \nWe wish to acknowledge all of the people who  The Privacy Engineer’s Manifesto: Getting from \ncontributed to this chapter. Policy to Code to QA to Value\nThe Policy Committee •  Eileen Donahoe – Executive Director of \nStanford Global Digital Policy Incubator\n•  Kay Firth-Butterfield (Founding Co-Chair) \n– Project Head, AI and Machine Learning \n•  Danit Gal – Project Assistant Professor, Keio \nat the World Economic Forum. Founding \nUniversity; Chair, IEEE Standard P7009 on the \nAdvocate of AI-Global; Senior Fellow and \nFail-Safe Design of Autonomous and Semi-\nDistinguished Scholar, Robert S. Strauss Center \nAutonomous Systems\nfor International Security and Law, University \nof Texas, Austin; Co-Founder, Consortium for  •  Olaf J. Groth – Professor of Strategy, \nLaw and Ethics of Artificial Intelligence and  Innovation, Economics & Program Director \nRobotics, University of Texas, Austin; Partner,  for Disruption Futures, HULT International \nCognitive Finance Group, London, U.K. Business School; Visiting Scholar, UC Berkeley \nBRIE/CITRIS; CEO, Cambrian.ai\n•  Dr. Peter S. Brooks (Co-Chair) – Institute for \nDefense Analyses •  Philip Hall – (Founding Co-Chair) Co-\nFounder & CEO, RelmaTech; Member (and \n•  Mina Hanna (Co-Chair) – Chair IEEE-USA \nImmediate Past Chair), IEEE-USA Committee \nArtificial Intelligence and Autonomous Systems \non Transportation & Aerospace Policy \nPolicy Committee, Vice Chair IEEE-USA \n(CTAP); and Member, IEEE Society on Social \nResearch and Development Policy Committee, \nImplications of Technology\nMember of the Editorial Board of IEEE \nComputer Magazine •  John C. Havens – Executive Director, The \nIEEE Global Initiative on Ethics of Autonomous \n•  Chloe Autio – Government & Policy Group, \nand Intelligent Systems; Executive Director, \nIntel Corporation\nThe Council on Extended Intelligence; Author, \nHeartificial Intelligence: Embracing Our \n•  Stan Byers – Frontier Markets Specialist\nHumanity to Maximize Machines \n•  Corinne Cath-Speth – PhD student at \n•  Cyrus Hodes – Senior Advisor, AI Office, \nOxford Internet Institute, The University of \nUAE Prime Minister’s Office; Co-Founded \nOxford, Doctoral student at the Alan Turing \nat Harvard Kennedy School the AI Initiative; \nInstitute, Digital Consultant at ARTICLE 19\nMember, AI Expert Group at the OECD; \n•  Michelle Finneran Dennedy – Vice  Member, Global Council on Extended \nPresident, Chief Privacy Officer, Cisco; Author,  Intelligence.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 208', ""The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nPolicy\n•  Chihyung Jeon – Assistant Professor,  •  Evangelos Simoudis – Co-Founder and \nGraduate School of Science and Technology  Managing Director, Synapse Partners. Author, \nPolicy (STP), Korea Advanced Institute of  The Big Data Opportunity in our Driverless \nScience and Technology (KAIST) Future\n•  Anja Kaspersen – Former Head of  •  Brian W. Tang – Founder and Managing \nInternational Security, World Economic Forum  Director, Asia Capital Markets Institute (ACMI); \nand head of strategic engagement and new  Founding executive director, LITE Lab@HKU at \ntechnologies at the international committee of  Hong Kong University Faculty of Law\nRed Cross (ICRC)\n•  Martin Tisné – Managing Director, Luminate\n•  Nicolas Miailhe – Co-Founder & President, \n•  Sarah Villeneuve – Policy Analyst; \nThe Future Society; Member, AI Expert Group \nMember, IEEE P7010: Well-being Metric for \nat the OECD; Member, Global Council on \nAutonomous and Intelligent Systems\nExtended Intelligence; Senior Visiting Research \n•  Adrian Weller – Senior Research Fellow, \nFellow, Program on Science Technology and \nUniversity of Cambridge; Programme Director \nSociety at Harvard Kennedy School. Lecturer, \nfor AI, The Alan Turing Institute\nParis School of International Affairs (Sciences \nPo); Visiting Professor, IE School of Global and  •  Yueh-Hsuan Weng – Assistant Professor, \nPublic Affairs. Frontier Research Institute for Interdisciplinary \nSciences (FRIS), Tohoku University; Fellow, \n•  Simon Mueller – Executive Director, The AI \nTransatlantic Technology Law Forum (TTLF), \nInitiative; Vice President, The Future Society\nStanford Law School\n•  Carolyn Nguyen – Director, Microsoft's \n•  Darrell M. West – Vice President and \nTechnology Policy Group, responsible for \nDirector, Governance Studies | Founding \npolicy initiatives related to data governance \nDirector, Center for Technology Innovation | \nand personal data\nThe Douglas Dillon Chair, Brookings Institution\n•  Mark J. Nitzberg – Executive Director, Center \n•  Andreas Wolkenstein – Researcher on \nfor Human-Compatible Artificial Intelligence \nneurotechnologies, AI, and political philosophy \nat UC Berkeley; co-author, Solomon’s Code: \nat LMU Munich (Germany)\nHumanity in a World of Thinking Machines\nFor a full listing of all IEEE Global Initiative \n•  Daniel Schiff – PhD Student, Georgia \nMembers, visit standards.ieee.org/content/dam/\nInstitute of Technology; Chair, Sub-Group \nieee-standards/standards/web/documents/other/\nfor Autonomous and Intelligent Systems \nec_bios.pdf. \nImplementation, IEEE P7010: Well-being \nMetric for Autonomous and Intelligent Systems \nFor information on disclaimers associated with \n \nEAD1e, see How the Document Was Prepared.\n \n \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 209"", 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nPolicy\nEndnotes\n1  This approach is rooted in internationally  5  Human rights–based approaches have been \nrecognized economic, social, cultural, and political  applied to development, education, and \nrights. reproductive health. See the UN Practitioners’ \nPortal on Human Rights Based Programming.\n2  This recommendation concurs with the multiple \nrecommendations of the United States National  6  “(AI100),” Stanford University., August 2016.\nScience and Technology Council, One Hundred \n7  Private sector initiatives are already emerging, \nYear Study of Artificial Intelligence, Japan’s Cabinet \nsuch as the Partnership on AI; the AI for Good \nOffice Council, European Parliament’s Committee \nFoundation; and the Ethics and Governance \non Legal Affairs, and others.\nof Artificial Intelligence Initiative, launched by \n3  For example, American Civil Liberties Union,  Harvard’s Berkman Klein Center for Internet & \nArticle 19, the Center for Democracy &  Society and the MIT Media Lab.\nTechnology, Canada.AI, or Privacy International. \nUnited Nations committees may also be useful  \nin fostering knowledge exchanges.\n4  This includes consideration regarding application \nof the precautionary principle, as used in \nenvironmental and health policy-making, where \nthe possibility of widespread harm is high and \nextensive scientific knowledge or understanding \non the matter is lacking.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 210', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLLaaww\nThe law affects and is affected by the development and deployment of autonomous  \nand intelligent systems (A/IS) in contemporary life. Science, technological development, \nlaw, public policy, and ethics are not independent fields of activity that occasionally overlap. \nInstead, they are disciplines that are fundamentally tied to each other and collectively \ninteract in the creation of a social order. \nAccordingly, in studying A/IS and the law, we focus not only on how the law responds  \nto the technological innovation represented by A/IS, but also on how the law guides \nand sets the conditions for that innovation. This interactive process is complex, and its \ndesired outcomes can rest on particular legal and cultural traditions. While acknowledging \nthis complexity and uncertainty, as well as the acute risk that A/IS may intentionally or \nunintentionally be misused or abused, we seek to identify principles that will steer this \ninteractive process in a manner that leads to the improvement, prosperity, and well-being  \nof everyone.\nThe fact that the law has a unique role to play in achieving this outcome is observed  \nby Sheila Jasanoff, a preeminent scholar of science and technology studies: \nPart of the answer is to recognize that science and technology—for all their power to create, \npreserve, and destroy—are not the only engines of innovation in the world. Other social \ninstitutions also innovate, and they may play an invaluable part in realigning the aims  \nof science and technology with those of culturally disparate human societies. Foremost \namong these is the law.1\nThe law can play its part in ensuring that A/IS, in both design and operation, are aligned \nwith principles of ethics and human well-being.2\nComprehensive coverage of all issues within our scope of study is not feasible in a single \nchapter of Ethically Aligned Design (EAD). Accordingly, aggregate coverage will expand  \nas issues not yet studied are selected for treatment in future versions of EAD.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 211', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\nEAD, First Edition includes commentary about how the law should respond to a number  \nof specific ethical and legal challenges raised by the development and deployment of  \nA/IS in contemporary life. It also focuses on the impact of A/IS on the practice of law  \nitself. More specifically, we study both the potential benefits and the potential risks  \nresulting from the incorporation of A/IS into a society’s legal system—specifically, in law \nmaking, civil justice, criminal justice, and law enforcement. Considering the results of  \nthose inquiries, we endeavor to identify norms for the adoption of A/IS in a legal system \nthat will enable the realization of the benefits while mitigating the risks.3\nIn this chapter of EAD, we include the following:\nSection 1: Norms for the Trustworthy Adoption of A/IS in Legal Systems.  \nThis section addresses issues raised by the potential adoption of A/IS in legal systems \nfor the purpose of performing, or assisting in performing, tasks traditionally carried out by \nhumans with specialized legal training or expertise. The section begins with the question \nof how A/IS, if properly incorporated into a legal system, can improve the functions of that \nlegal system and thus enhance its ability to contribute to human well-being. The section \nthen discusses challenges to the safe and effective incorporation of A/IS into a legal system \nand identifies the chief challenge as an absence of informed trust. The remainder of \nthe section examines how societies can fill the trust gap by enacting policies and promoting \npractices that advance publicly accessible standards of effectiveness, competence, \naccountability, and transparency. \nSection 2: Legal Status of A/IS.  \nThis section addresses issues raised by the legal status of A/IS, including the potential \nassignment of certain legal rights and obligations to such systems. The section provides \nbackground on the issue and outlines some of the potential advantages and disadvantages \nof assigning some form of legal personhood to A/IS. Based on these considerations, the \nsection concludes that extending legal personhood to A/IS is not appropriate at this time. \nIt then considers alternatives and outlines certain future conditions that might warrant \nreconsideration of the section’s central recommendation. \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 212', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\nSection 1: Norms for the Trustworthy \nAdoption of A/IS in Legal Systems4\n“It’s a day that is here.”   for purposes other than those for which the \nJohn G. Roberts , Chief Justice of the Supreme  systems have been validated and vetted \nCourt of the United States, when asked in 2017  for legal use. In addition to actual harm to \nwhether he could foresee a day when intelligent  individuals, the result will be distrust, not only \nmachines would assist with courtroom fact- of the effectiveness of A/IS, but also of the \nfinding or judicial decision-making.5  fairness and effectiveness of the legal system \nitself. \nA/IS hold the potential to improve the functioning \n•  Uninformed avoidance of adoption poses \nof a legal system and, thereby, to contribute to \nthe risk that a lack of understanding of what  \nhuman well-being. That potential will be realized, \nis required for the safe and effective operation \nhowever, only if both the use of A/IS and the \nof A/IS will result in blanket distrust of all \navoidance of their use are grounded in solid \nforms and applications of A/IS, even those \ninformation about the capabilities and limitations \nthat are, when properly applied, safe and \nof A/IS, the competencies and conditions \neffective. The result will be a failure to realize \nrequired for their safe and effective operation \nthe significant improvements in the legal \n(including data requirements), and the lines along \nsystem that A/IS can offer and a continuation \nwhich responsibility for the outcomes generated \nof systems that are, even with the best \nby A/IS can be assigned. Absent that information, \nof safeguards, still subject to human bias, \nsociety risks both uninformed adoption of  \ninconsistency, and error.6\nA/IS and uninformed avoidance of adoption \nof A/IS, risks that are particularly acute when  \nIn this section, we consider how society can \nA/IS are applied in an integral component of the \naddress these risks by developing norms for the \nsocial order, such as the law.\nadoption of A/IS in legal systems. The specific \nissues discussed follow. The first and second \n•  Uninformed adoption poses the risk that \nissues reflect the potential benefits of, and \nA/IS will be applied to inform or replace the \nchallenges to, trustworthy adoption of A/IS in \njudgments of legal actors (legislators, judges, \nthe world’s legal systems. The remaining issues \nlawyers, law enforcement officers, and jurors) \ndiscuss four principles,7 which, if adhered to, will \nwithout controls to ensure their safe and \nenable trustworthy adoption.8 9\neffective operation. They may even be used \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 213', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\n•  Issue 1: Well-being, Legal Systems, and \nA/IS—How can A/IS improve the functioning \nIssue 1: Well-Being, Legal \nof a legal system and, thereby, enhance \nSystems, and A/IS\nhuman well-being?\nHow can A/IS improve the \n•  Issue 2: Impediments to Informed \nTrust—What are the challenges to adopting  functioning of a legal system  \nA/IS in legal systems and how can those  and, thereby, enhance human \nimpediments be overcome?\nwell-being?\n•  Issue 3: Effectiveness—How can the \ncollection and disclosure of evidence of \neffectiveness of A/IS foster informed trust \nBackground\nin the suitability of A/IS for adoption in legal \nsystems? An effective legal system contributes  \nto human well-being. The law is an integral \n•  Issue 4: Competence—How can \ncomponent of social order; the nature of a legal \nspecification of the knowledge and skills \nsystem informs, in fundamental ways, the nature \nrequired of the human operator(s) of A/IS \nof a society, its potential for economic growth \nfoster informed trust in the suitability of A/IS \nand technological innovation, and its capacity  \nfor adoption in legal systems? \nfor advancing the well-being of its members. \n•  Issue 5: Accountability—How can the ability \nto apportion responsibility for the outcome   If the law is a constitutive element of social \nof the application of A/IS foster informed trust  order, it is not surprising that it also plays a key \nin the suitability of A/IS for adoption in legal  role in setting the conditions for well-being and \nsystems? economic growth. In part, this flows from the \nfact that a well-functioning legal system is an \n•  Issue 6: Transparency—How can sharing \nelement of good governance. Good governance \ninformation that explains how A/IS reach given \nand a well-functioning legal system can help \ndecisions or outcomes foster informed trust \nsociety and its members flourish, as measured \nin the suitability of A/IS for adoption in legal \nby indicators of both economic prosperity10 \nsystems? \nand human well-being.11 The attributes of good \ngovernance can be defined in several ways. \nGood governance can mean democracy; the \nobservance of norms of human rights enshrined \nin conventions such as the Universal Declaration \nof Human Rights12 and the Convention of \nthe Rights of the Child;13 and constitutional \nconstraints on government power. It can also \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 214', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\nmean bureaucratic competence, law and order,  other hand, may foster monopolistic markets  \nproperty rights, and contract enforcement.  and decrease competition, resulting in a \ndecreased pace of technological innovation, \nThe United Nations (UN) defines the rule of  \nfewer gains in productivity, and slower  \nlaw as:\neconomic growth.17\na principle of governance in which all persons, \nWhile economic growth is a valuable benefit  \ninstitutions and entities, public and private, \nof a well-designed and well-functioning legal \nincluding the State itself, are accountable to \nsystem, it is not the only benefit. Such a system \nlaws that are publicly promulgated, equally \ncan bring benefits to society and its members \nenforced and independently adjudicated. . \nthat, beyond economic prosperity, extend to \n. . It requires, as well, measures to ensure \nmental and physical well-being. Specific benefits \nadherence to the principles of supremacy of \ninclude the protection and advancement of \nlaw, equality before the law, accountability \nan individual’s dignity,18 human rights,19 liberty, \nto the law, fairness in the application of the \nstability, security, equality of treatment under  \nlaw, separation of powers, participation in \nthe law, and ability to provide for the future.20\ndecision-making, legal certainty, avoidance \nof arbitrariness and procedural and legal  In fact, recent thinking on the relationship \ntransparency.14  between law and economic development \nhas come to hold that a well-functioning legal \nOrderly systems of legal rules and institutions \nsystem is not simply a means to development \ngenerally correlate positively with economic \nbut is development, insofar as such a system \nprosperity, social stability, and human well-\nis a constitutive element of a social order that \nbeing, including the protection of childhood.15 \nprotects and advances human dignity, rights, \nStudies from the World Bank suggest that legal \nand well-being. As this position has been \nreforms can lead to increased foreign investment, \ncharacterized by David Kennedy:\nhigher incomes, and greater wealth.16 Wealth, in \nturn, can enable policies that support improved  … the focal point for development policy was \neducation, health, environmental protection,  increasingly provided less by economics than \nequal opportunity, and, in democratic societies,  from ideas about the nature of the good state \ngreater individual freedom. themselves provided by literatures of political \nscience, political economy, ethics, social theory, \nLaw, moreover, can contribute to prosperity not \nand law. In particular, “human rights” and the \nonly through its functional attributes, but also \n“rule of law”21 became substantive definitions \nthrough its substantive content. Patent laws, \nof development. One should promote human \nfor example, if well-designed, can encourage \nrights not to facilitate development—but \ntechnological innovation, leading to increases \nas development. The rule of law was not a \nin productivity and the economic growth that \ndevelopment tool—it was itself a development \nfollows. Poorly designed patent laws, on the  \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 215', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\nobjective. Increasingly, law—understood  •  Accurate: achieve accurate results, \nas a combination of human rights, courts,  minimizing both false positives (persons \nproperty rights, formalization of entitlements,  unjustly or incorrectly targeted, investigated, \nprosecution of corruption, and public order— or sentenced for crimes) and false negatives \ncame to define development.22 (persons incorrectly not targeted, investigated, \nor sentenced for crimes);\nWhile this shift from considering law as a means \n•  Adaptable: have the flexibility to adapt  \nto an end to considering law as an end in itself \nto changes in societal circumstances.\nhas been criticized on the grounds that it takes \nthe focus off the difficult political choices that   A/IS have the potential to alter the overall \nare inherent in any development policy,23 it  functioning of a legal system. A/IS, applied \nremains true that a well-functioning legal system  responsibly and appropriately, could improve  \nis essential to the realization of a social order   the legislative process, enhance access to justice, \nthat protects and advances human dignity, rights,  accelerate judicial decision-making, provide \nand well-being. transparent and readily accessible information  \non why and how decisions were reached, reduce \nA/IS can contribute to the proper \nbias, support uniformity in judicial outcomes, help \nfunctioning of a legal system. A properly \nsociety identify (and potentially correct) judicial \nfunctioning legal system, one that is conducive to \nerrors, and improve public confidence in the legal \nboth economic prosperity and human well-being, \nsystem. By way of example:\nwill have a number of attributes. It should be:\n•  A/IS can make legislation and regulation more \n•  Speedy: enable quick resolution of civil   effective and adaptable. For lawmaking,  \nand criminal cases; A/IS could help legislators analyze data to  \ncraft more finely tuned, responsive, evidence-\n•  Fair: produce results that are just and \nbased laws and regulations. This could, \nproportionate to circumstance;24\npotentially, offer self-correcting suggestions \n•  Free from undesirable bias: operate \nto legislators (and to the general public) to \nwithout prejudice;\nhelp inform dialogue on how to meet defined \n•  Consistent: arrive at outcomes in a  public policy objectives. \nprincipled, consistent, and nonarbitrary \n•  A/IS can make the practice of law more \nmanner;\neffective and efficient. For example,  \n•  Transparent: be open to appropriate public  A/IS can enhance the speed, accuracy, and \nexamination and oversight;25 accessibility of the process of fact-finding in \nlegal proceedings. When used appropriately \n•  Accessible: be equally open to all citizens \nin legal fact-finding, particularly in jurisdictions \nand residents in resolving disputes;\nthat allow extensive discovery or disclosure, \n•  Effective: achieve the ends intended by  \nA/IS already make litigation and investigations \nits laws and rules without negative collateral \nmore accessible by analyzing vast data \nconsequences;26 \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 216', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\ncollections faster, more efficiently, and  rates by identifying individuals who are less \npotentially more effectively27 than document  likely to commit crimes if released.\nanalysis conducted solely by human attorneys. \n•  A/IS can help to ensure that the tools, \nBy making fact-finding in an era of big data \nprocedures, and resources of the legal system \nprogressively easier, faster, and cheaper, A/IS \nare more transparent and accessible \nmay facilitate access to justice for parties that \nto citizens. For the ordinary citizen, A/IS \notherwise may find using the legal system  \ncan democratize access to legal expertise, \nto resolve disputes cost-prohibitive. A/IS can \nespecially in smaller matters, where they \nalso help ensure that justice is rendered based \nmay provide effective, prompt, and low-cost \non better accounting of the facts, thus serving \ninitial guidance to an aggrieved party; for \nthe central purpose of any legal system.\nexample, in landlord-tenant, product purchase, \n•  In both civil and criminal proceedings,   employment, or other contractual contexts \nA/IS can be used to improve the accuracy,  where the individual often tends to find \nfairness, and consistency of decisions  access to legal information and legal advice \nrendered during proceedings. A/IS could   prohibitive, or where asymmetry of resources \nserve as an auditing function for both the  between the parties renders recourse to  \ncivil and criminal justice systems, helping  the legal system inequitable.30\nto identify and correct judicial and law \nA/IS have the potential to improve how a legal \nenforcement errors.28\nsystem functions in fundamental ways. As is \n•  A/IS can increase the speed, accuracy, \nthe case with all powerful tools, there are some \nfairness, freedom from bias, and general \nrisks. A/IS should not be adopted in a legal \neffectiveness with which law enforcement \nsystem without due care and scrutiny; \nresources are deployed to combat crime.  \nthey should be adopted after a society’s careful \nA/IS could be used to reduce or prevent \nreflection and proper examination of evidence \ncrime, respond more quickly to crimes in \nthat their deployment and operation can be \nprogress, and improve collaboration among \ntrusted to advance human dignity, rights, and \ndifferent law enforcement agencies.29\nwell-being (see Issues 2–6).\n•  A/IS can help ensure that determinations \nabout the arrest, detention, and incarceration  Recommendations31\nof individuals suspected of, or convicted of, \n1.  Policymakers should, in the interest  \nviolations of the law are fair, free from bias, \nof improving the function of their legal \nconsistent, and accurate. Automated risk \nsystems and bringing about improvements  \nassessment tools have the potential to address \nto human well-being, explore, through  \nissues of systemic racial bias in sentencing, \na broad consultative dialogue with all \nparole, and bail determination while also \nstakeholders, how A/IS can be adopted for \nsafely reducing incarceration and recidivism \nuse in their legal systems. They should do  \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 217', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\nso, however, only in accordance with norms  in The New Law and Economic Development: \nfor adoption that mitigate the risks attendant  A Critical Appraisal, D. M. Trubek and A. \non such adoption (see Issues 2–6 in   Santos, eds., Cambridge: Cambridge University \nthis section). Press, 2006, pp. 95-173. \n2. Governments, non-governmental  •  “Artificial Intelligence,” National Institute  \norganizations, and professional associations  of Standards and Technology.\nshould support educational initiatives \n•  K. Schwab, “The Global Competitiveness \ndesigned to create greater awareness among \nReport: 2018,” The World Economic  \nall stakeholders of the potential benefits  \nForum, 2018. \nand risks of adopting A/IS in the legal system, \n•  A. Sen, Development as Freedom. New York, \nand of the ways of mitigating such risks.  \nNY: Alfred A. Knopf, 1999.\nA particular focus of these initiatives should \nbe the ordinary citizen who interacts with the  •  United Nations General Assembly, Universal \nlegal system as a victim or criminal defendant. Declaration of Human Rights, Dec. 10, 1948.\n•  UNICEF, Convention on the Rights of the Child, \nFurther Resources Nov. 4, 2014. \n•  A. Brunetti, G. Kisunko, and B. Weder,  •  United Nations Office of the High \n“Credibility of Rules and Economic Growth:  Commissioner: Human Rights, The Vienna \nEvidence from a Worldwide Survey of the  Declaration and Programme of Action,  \nPrivate Sector,” The World Bank Economic  June 25, 1993.\nReview, vol. 12, no. 3, pp. 353-384,  \n•  World Bank, World Development Report 2017: \nSep. 1998. \nGovernance and the Law, Jan. 2017. \n•  S. Jasanoff, “Governing Innovation: The Social \n•  World Justice Project, Rule of Law Index,  \nContract and the Democratic Imagination,” \nJune 2018.\nSeminar, vol. 597, pp. 16-25, May 2009.\n \n•  D. Kennedy, “The ‘Rule of Law,’ Political \nChoices and Development Common Sense,” \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 218', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\n•  the loss of privacy and dignity; \nIssue 2: Impediments   •  and the erosion of democratic institutions.32 \nto Informed Trust\nBy way of example:\nWhat are the challenges to \n•  Currently, A/IS used in justice systems are \nadopting A/IS in legal systems \nnot subject to uniform rules and norms and \nand how can those impediments  are often adopted piecemeal at the local \nbe overcome? or regional level, thereby creating a highly \nvariable landscape of tools and adoption \npractices. Critics argue that, far from improving \nfact-finding in civil and criminal matters or \nBackground\neliminating bias in law enforcement, these \nAlthough the benefits to be gained by adopting  tools have unproven accuracy, are error-prone, \nA/IS in legal systems are potentially numerous  and may serve to entrench existing social \n(see the discussion of Issue 1), there are also  inequalities. These tools’ potential must be \nsignificant risks that must be addressed in order  weighed against their pitfalls. These include \nfor the A/IS to be adopted in a manner that will  unclear efficacy; incompetent operation; and \nrealize those benefits. The risks sometimes mirror  potential impairment of a legal system’s ability \nexpected benefits:  to adhere to principles of socioeconomic, \nracial, or religious equality, government \n•  the potential for opaque decision-making; \ntransparency, and individual due process,  \n•  the intentional or unintentional biases and  to render justice in an informed, consistent, \nabuses of power;  and fair manner.\n•  the emergence of nontraditional bad actors;  •  In the case of State v. Loomis, an important \n•  the perpetuation of inequality;  but not widely known case, the Wisconsin \nSupreme Court held that a trial court’s use \n•  the depletion of public trust in a legal system; \nof an algorithmic risk assessment tool in \n•  the lack of human capital active in judicial  sentencing did not violate the defendant’s \nsystems to manage and operate A/IS;  due process rights, despite the fact that the \n•  the sacrifice of the spirit of the law in order to  methodology used to obtain the automated \nachieve the expediency that the letter of the  assessment was not disclosed to either the \nlaw allows;  court or the defendant.33 A man received  \na lengthy sentence based in part on what an \n•  the unanticipated consequences of the \nopaque algorithm thought of him. While the \nsurrender of human agency to nonethical \ncourt considered many factors, and sought \nagents; \nto balance competing societal values, this \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 219', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\nis just one case in a growing set of cases  Such risks must be addressed in order to ensure \nillustrating how criminal justice systems are  sustainable management and public oversight \nbeing impacted by proprietary claims of trade  of what will foreseeably become an increasingly \nsecrets, opaque operation of A/IS, a lack of  automated justice system.35 The view expressed \nevidence of the effectiveness of A/IS, and a  by the Organisation for Economic Co-operation \nlack of norms for the adoption of A/IS in the  and Development (OECD) in the domain of \nextended legal system. digital security that “robust strategies to [manage \nrisk] are essential to establish the trust needed \n•  More generally, humans tend to be subject  \nfor economic and social activities to fully benefit \nto the cognitive bias known as “anchoring”, \nfrom digital innovation”36 applies equally to the \nwhich can be described as the excessive \nadoption of A/IS in the world’s legal systems.\nreliance on an initial piece of information. \nThis may lead to the progressive, unwitting, \nInformed trust. If we are to realize the benefits \nand detrimental reliance of judges and legal \nof A/IS, we must trust that they are safe and \npractitioners on assessments produced by  \neffective. People board airplanes, take medicine, \nA/IS. This risk is compounded by the fact that \nand allow their children on amusement park  \nA/IS are (and shall remain in the foreseeable \nrides because they trust that the tools, methods, \nfuture) nonethical agents, incapable of \nand people powering those technologies meet \nempathy, and thus at risk of being unable  \ncertain safety and effectiveness standards that \nto produce decisions aligned with not just the \nreduce the risks to an acceptable level given  \nletter of the law, but also the spirit of the law \nthe objectives and benefits to be achieved. \nand reasonable regard for the circumstances \nThis need for trust is especially important in the \nof each defendant.\ncase of A/IS used in a legal system. The “black \n•  The required technical and scientific  box” nature and lack of trust in A/IS deployed \nknowledge to procure, deploy, and effectively  in the service of a legal system could quickly \noperate A/IS, as well as that required to  translate into a lack of trust in the legal system \nmeasure the ability of A/IS to achieve a  itself. This, in turn, may lead to an undermining \ngiven purpose without adverse collateral  of the social order. Therefore, if we are to \nconsequences, represent significant hurdles  improve the functioning of our legal systems \nto the beneficial long-term adoption of A/IS  through the adoption of A/IS, we must enact \nin a legal system. This is especially the case  policies and promote practices that allow \nwhen—as is the case presently—actors in the  those technologies to be adopted on the \ncivil and criminal justice systems and in law  basis of informed trust. Informed trust rests \nenforcement may lack the requisite specialized  on a reasoned evaluation of clear and accurate \ntechnological or scientific expertise.34  information about the effectiveness of A/IS  \nand the competence of their operators.37 \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 220', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\nTo formulate policies and standards of practice  the skills and knowledge required for their \nintended to foster informed trust, it is helpful,  effective operation and if their operators \nfirst, to identify principles applicable over   adhere to those competency requirements.\nthe entire supply chain for the delivery of  \n•  Accountability: A/IS should be adopted \nA/IS-enabled decisions and guidance, including \nin a legal system only if all those engaged \ndesign, development, procurement, deployment, \nin their design, development, procurement, \noperation, and validation of effectiveness, that,  \ndeployment, operation, and validation of \nif adhered to, will foster trust. Once those general \neffectiveness maintain clear and transparent \nprinciples have been identified, specific policies \nlines of responsibility for their outcomes and \nand standards of practice can be formulated \nare open to inquiries as may be appropriate. \nthat encourage adherence to the principles \n•  Transparency: A/IS should be adopted in  \nin every aspect of a legal system, including \na legal system only if the stakeholders in the \nlawmaking, civil and criminal justice, and law \nresults of A/IS have access to pertinent and \nenforcement. Such principles, if they are to serve \nappropriate information about their design, \ntheir intended purpose of informing effective \ndevelopment, procurement, deployment, \npolicies and practices, must meet certain design \noperation, and validation of effectiveness.\ncriteria. Specifically, the principles should be \n(a) individually necessary and collectively \nIn the remainder of Section 1, we elaborate  \nsufficient, (b) globally applicable but \non each of these principles. Before turning to  \nculturally flexible, and (c) capable of being \na specific discussion of each, we add two further \noperationalized in applicable functions of \nconsiderations that should be kept in mind when \nthe legal system. A set of principles that meets \napplying them collectively.\nthese criteria will provide an effective framework \nfor the development of policies and practices that  Differences in emphasis. While all four of \nfoster trust, while leaving considerable flexibility  the aforementioned principles will contribute \nin the specific policies and standards of practice  to the fostering of trust, each principle will not \nthat a society chooses to implement in furthering  contribute equally in every circumstance. For \nadherence to the principles.  example, in many applications of A/IS, a well-\nestablished measure of effectiveness, obtained \nA set of four principles that we believe meets the \nby proven and accepted methods, may go a \ndesign criteria just described are the following:\nconsiderable way to creating conditions for trust \nin the given application. In such a case, the other \n•  Effectiveness: Adoption of A/IS in a legal \nprinciples may add to trust, but they may not be \nsystem should be based on sound empirical \nnecessary to establish trust. Or, to take another \nevidence that they are fit for their intended \nexample, in some applications the role of the \npurpose.\nhuman operator may be minimal, while in other \n•  Competence: A/IS should be adopted in \napplications there will be extensive scope for \na legal system only if their creators specify \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 221', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\nhuman agency where competence has a greater  law (such as lawyers, judges, and law \nrole to play. In finding the right emphasis and  enforcement officers), when engaging with \nbalance among the four principles, policymakers  or relying on providers of A/IS technology \nand practitioners will have to consider the specific  or services, should require, at a minimum, \ncircumstances of A/IS. that those providers adhere to, and be \nable to demonstrate adherence to, the \nFlexibility in implementation. It should be \nprinciples of effectiveness, competence, \nnoted that we have addressed the four principles \naccountability, and transparency as described \nabove at a rather high level and have not offered \nin this chapter. Likewise, those professionals, \nspecific prescriptions of how adherence to the \nwhen operating A/IS themselves, should \nprinciples should be implemented. This is by \nadhere to, and be able to demonstrate \ndesign. Although adherence to all four principles \nadherence to, the principles of effectiveness, \nis important, it is also important that, at the \ncompetence, accountability, and transparency. \noperational level, flexibility be allowed for the \nDemonstrations of adherence to the \nselection and implementation of policies and \nrequirements should be publicly accessible.\npractices that (a) are in harmony with a given \n3. Regulators should permit insurers to issue \nsociety’s traditions, norms, and values;  \nprofessional liability and other insurance \n(b) conform with the laws and regulations \npolicies that consider whether the insured \noperative in a given jurisdiction; and (c) are \n(either a provider or operator of A/IS in  \nconsistent with the ethical obligations of legal \na legal system) adheres to the principles  \npractitioners.\nof effectiveness, competence, accountability, \nand transparency (as they are articulated  \nRecommendations\nin this chapter).\n1.  Governments should set procurement and \ncontracting requirements that encourage \nFurther Resources\nparties seeking to use A/IS in the conduct \n•  “Criminal Law—Sentencing Guidelines—\nof business with or for the government, \nWisconsin Supreme Court Requires Warning \nparticularly with or for the court system and \nBefore Use of Algorithmic Risk Assessments  \nlaw enforcement agencies, to adhere to the \nin Sentencing—State v. Loomis, 881 N.W.2d \nprinciples of effectiveness, competence, \n749 (Wis. 2016),” Harvard Law Review,  \naccountability, and transparency as described \nvol. 130, no. 5, pp. 1530-1537, 2017.  \nin this chapter. This can be achieved through \nlegislation or administrative regulation.   •  K. Freeman, “Algorithmic Injustice: How the \nAll government efforts in this regard should   Wisconsin Supreme Court Failed to Protect \nbe transparent and open to public scrutiny. Due Process Rights in State v. Loomis,”  \nNorth Carolina Journal of Law and Technology, \n2. Professionals engaged in the practice, \nvol. 18, no. 5, pp. 75-76, 2016. \ninterpretation, and enforcement of the \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 222', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\n•  “Managing Digital Security and Privacy Risk:  consumers of the evidence are. We then identify \nBackground Report for Ministerial Panel 3.2,”  the specific features of the practice of measuring \nOrganisation for Economic Co-operation and  effectiveness that will enable it to contribute to \nDevelopment (OECD) Directorate for Science,  informed trust in A/IS as applied in a  \nTechnology, and Innovation: Committee   legal system.\non Digital Economy Policy, June 1, 2016. \n•  State v Loomis, 881 N.W.2d 749 (Wis. 2016),  What constitutes evidence  \ncert. denied (2017).  of effectiveness?\n•  “Global Governance of AI Roundtable:  What we are measuring. In gathering  \nSummary Report 2018,” World Government  evidence of effectiveness, we are seeking  \nSummit, 2018.  to gather empirical data that will tell us whether  \na given technology or its application will serve  \nas an effective solution to the problem it is \nintended to address. Serving as an effective \nsolution means more than meeting narrow \nIssue 3: Effectiveness\nspecifications or requirements; it means that  \nHow can the collection and  the A/IS are capable of addressing their \ntarget problems in the real world, which, \ndisclosure of evidence of \nin the case of A/IS applied in a legal system, \neffectiveness of A/IS foster \nare problems in the making, administration, \ninformed trust in the suitability \nadjudication, or enforcement of the law.  \nfor adoption in legal systems? It also means remaining practically feasible once \ncollateral concerns and potential unintended \nconsequences are taken into account.39 To take \na non-A/IS example, under the definition of \nBackground\neffectiveness we are considering, for an herbicide \nAn essential component of trust in a technology  to be considered effective, it must be shown  \nis trust that it works and meets the purpose for  not only to kill the target weeds, but also to  \nwhich it is intended. We now turn to a discussion  do so without causing harm to nontarget plants, \nof the role that evidence of effectiveness, chiefly  to the person applying the agent, and to the \nin the form of the results of a measurement  environment in general.\nexercise, can play in fostering informed trust \nin A/IS as applied in legal systems.38 We begin  Under the definition above, assessing the \nwith a general characterization of what we  effectiveness of A/IS in accomplishing the  \nmean by evidence of effectiveness: what we  target task (narrowly defined) is not sufficient;  \nare measuring, how we are measuring, what  it may also be necessary to assess the extent  \nform our results take, and who the intended  to which the A/IS are aligned with applicable \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 223', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\nlaws, regulations, and standards,40 and whether  •  A single-system validation exercise \n(and to what extent) they impinge on values   measures and reports on the effectiveness \nsuch as privacy, fairness, or freedom from bias.41   of a single system on a given task. In such \nWhether such collateral concerns are salient will   an exercise, the system to be validated will \ndepend on the nature of the A/IS and on the  typically have already carried out the target \nparticular circumstances in which they are to be   task on a given data set. The purpose of the \napplied.42 However, it is only from such a complete   validation is to provide empirical evidence \nview of the impact of A/IS that a balanced  of how successful the system has been \njudgment can be made of the appropriateness   in carrying out the task on that data set. \nof their adoption.43  Measurements are obtained by independent \nsampling and review of the data to which \nAlthough the scope of an evaluation of \nthe system was applied. Once obtained, \neffectiveness is broader than a narrowly focused \nthose metrics serve to corroborate or refute \nverification that a specific requirement is met, \nthe hypothesis that the system operated as \nit has its limits. There are measures of aspects \nintended in the instance under consideration. \nof A/IS that one might find useful but that are \nAn example of validation as applied to \noutside the scope of effectiveness. For example, \nlegal fact-finding would be a test of the \ngiven frequently expressed concerns that  \neffectiveness of A/IS that had been used  \nA/IS will one day cross the limits of their intended \nto retrieve material relevant (as defined by  \npurpose and overwhelm their creators and users, \nthe humans deploying the system) to a given \none might seek to define and obtain general \nlegal inquiry from a collection of emails.\nmeasures of the autonomy of a system or of a \n•  A multi-system (or benchmarking) \nsystem’s capacity for artificial general intelligence \nevaluation involves conducting a \n(AGI). Although such measures could be useful—\ncomparative study of the effectiveness of \nassuming they could be defined—they are \nseveral systems designed to meet the same \nbeyond the scope of evaluations of effectiveness. \nobjective. Typically, in such a study, a test \nEffectiveness is always tied to a target purpose, \ndata set is identified, a task to be performed \neven if it includes consideration of the collateral \nis defined (ideally, a task that models the \neffects of the manner of meeting that purpose. \nreal-world objectives and conditions for which \nWhat we are measuring is therefore a general  the systems under evaluation have been \n“fitness for purpose”.  designed45), the systems to be evaluated are \nused to carry out the task, and the success \nHow we measure. Evidence of effectiveness \nof each system in carrying out the task is \nis typically gathered in one of two types of \nmeasured and reported. An example of  \nexercises:44 \nthis sort of evaluation applied to a specific \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 224', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\nreal-world challenge in the justice system is  have reached a consensus, requiring further \nthe series of evaluations of the effectiveness  study. In the case of A/IS, given both their \nof information retrieval systems in civil  accelerating development and the fact that \ndiscovery, including A/IS, conducted as part  they are often applied to tasks for which the \nof the US National Institute of Standards and  effectiveness of their human counterparts  \nTechnology (NIST) Text REtrieval Conference  is seldom precisely gauged, we are often still  \n(TREC) Legal Track initiative.46 at the stage of defining metrics. An example  \nof an application of A/IS for which there is \nThe measurements obtained by both types of \na general consensus around measures of \nevaluation exercises are valuable. The results of \neffectiveness is legal electronic discovery,50  \na single-system validation exercise are typically \nwhere there is a working consensus around \nmore specific, answering the question of whether \nthe use of the evaluation metrics referred to \na system was effective in a specific instance.  \nas “recall” and “precision”.51 Conversely, in the \nThe results of a multi-system evaluation are \ncase of A/IS applied in support of sentencing \ntypically more generic, answering the question  \ndecisions, a consensus on the operative \nof whether a system can be effective in  \neffectiveness metrics does not yet exist.52\nreal-world circumstances. Both questions are \nimportant, hence both types of evaluations   The consumers of the results. In defining \nare valuable.47 metrics, it is important to keep in mind the \nconsumers of the results of an evaluation  \nThe form of results. The results of an \nof effectiveness. Broadly speaking, it is \nevaluation typically take the form of a number— \nhelpful to distinguish between two categories \na quantitative gauge of effectiveness. This can \nof stakeholders who will be interested in \nbe, for example, the decreased likelihood of \nmeasurements of effectiveness: \ndeveloping a given medical condition; safety \nratings for automobiles; recall measures for  •  Experts are the researchers, designers, \nretrieving responsive documents; and so on.  operators, and advanced users with \nCertainly, qualitative considerations are not  appropriate scientific or professional \n(and should not) be ignored; they often provide  credentials who have a technical \ncontext crucial to interpreting the quantitative  understanding of the way in which a system \nresults.48 Nevertheless, at the heart of the results  works and are well-versed in evaluation \nof an evaluation exercise is a number, a metric  methods and the results they generate. \nthat serves as a telling indicator of effectiveness.49\n•  Nonexperts are the legislators, judges, \nlawyers, prosecutors, litigants, communities, \nIn some cases, the research community engaged \nvictims, defendants, and system advocates \nin developing any new system will have reached \nwhose work or legal outcomes may, even  \nconsensus on salient effectiveness metrics. In \nif only indirectly, be affected by the results  \nother cases, the research community may not \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 225', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\nof a given system. These individuals, however,  metrics are not sufficiently fine-grained  \nmay not have a technical understanding of the  to give immediately actionable information; \nway in which a system operates. Furthermore,  for consumers, however, the metrics, insofar \nthey may have little experience in conducting  as they are accurate, empower them to make \nscientific evaluations and interpreting their  better-informed buying decisions.\nresults. \nFor the purpose of fostering informed trust  \nEffectiveness metrics must meet the needs   in A/IS adopted in the legal system, the most \nof both expert and nonexpert consumers. important goal is to establish a clear measure \nof effectiveness that can be understood by \n•  With respect to experts, the purpose of an \nnonexperts. However, significant obstacles \neffectiveness metric is to advance both long-\nto achieving this goal include (a) developer \nterm research and more immediate product \nincentives that prioritize research and \ndevelopment, maintenance, and oversight.  \ndevelopment, along with the metrics that support \nTo achieve that purpose, it is appropriate  \nsuch efforts, and (b) market forces that inhibit,  \nto define a fine-grained metric that may \nor do not encourage, consumer-facing metrics. \nnot be within the grasp of the nonexpert. \nFor those reasons, it is important that the \nResearchers and developers will be acting  \nselection and definition of the operative metrics \non the information provided by such a metric, \ndraw on input not only from the A/IS creators  \nso it should be tailored to their needs. \nbut from other stakeholders as well; only under \n•  With respect to nonexperts, including  these conditions will a consensus form around \nthe general public, the purpose of an  the meaningfulness of the metrics.\neffectiveness metric is to advance informed \ntrust, meaning trust that is based on sound \nWhat measurement practices foster \nevidence that the A/IS have met, or will  \ninformed trust?\nmeet, their intended objectives, taking into \nBy equipping both experts and nonexperts  \naccount both the immediate purpose and  \nwith accurate information regarding the \nthe contextual purpose of preserving and \ncapabilities and limitations of a given system, \nfostering important values such as human \nmeasurements of effectiveness can provide \nrights, dignity, and well-being. For this \nsociety with information needed to adopt and \npurpose, it will be necessary to define a \napply A/IS in a thoughtful, carefully considered, \nmetric that can serve as a readily understood \nbeneficial manner.53\nsummary measure of effectiveness. This \nmetric must provide a simple, direct answer  \nIn order for the practice of measuring effectiveness  \nto the question of how effective a given \nto realize its full potential for fostering trust and \nsystem is. Automobile safety ratings are an \nmitigating the risks of uninformed adoption  \nexample of this sort of metric. For automobile \nand uninformed avoidance of adoption, it must \ndesigners and engineers, the summary \nhave certain features:\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 226', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\n•  Meaningful metrics: As noted above, an  •  Implementation: Measurement practices \nessential element of a measurement practice  must be both practically feasible and actually \nis a metric that provides an accurate and  implemented, i.e., widely adopted by \nreadily understood gauge of effectiveness.   practitioners56.\nThe metric should provide clear and actionable \n•  Transparency. Measurement methods  \ninformation as to the extent to which a \nand results must be open to scrutiny by \ngiven application has, or has not, met its \nexperts and the general public.57 Without  \nobjective so that potential users of the results \nsuch scrutiny, the measurements will not  \nof the application can respond accordingly. \nbe trusted and will be incapable of fulfilling \nFor example, in legal discovery, both recall \ntheir intended purpose.58\nand precision have done this well and have \ncontributed to the acceptance of the use   In seeking to advance informed trust in  \nof A/IS for this purpose.54 A/IS, policymakers should formulate policies \nand promote standards that encourage sound \n•  Sound methods: Measures of effectiveness \nmeasurement practices, especially those that \nmust be obtained by scientifically sound \nincorporate the key features.\nmethods. If, for example, measures are \nobtained by sampling, those sample-based \nAdditional note. While in all circumstances \nestimates must be the result of sound \nall four principles discussed in this chapter \nstatistical procedures that hold up to  \n(Effectiveness, Competence, Accountability, \nobjective scrutiny.\nTransparency) will have something to contribute \n•  Valid data: Data on which evaluations of  to the fostering of informed trust, it is not the \neffectiveness are conducted should accurately  case that in every circumstance all four principles \nrepresent the actual data to which the given  will contribute equally to the fostering of trust. \nA/IS would be applied and should be vetted  In some circumstances, a well-established \nfor potential bias. Any data sets used for  measure of effectiveness, obtained by proven \nbenchmarking or testing should be collected,  and accepted methods, may go a considerable \nmaintained, and used in accordance with  way, on its own, in fostering trust in a given \nprinciples for the protection of individual  application—or distrust, if that is what the \nprivacy and agency.55 measurements indicate. In such circumstances, \nthe challenges presented by the other principles, \n•  Awareness and consensus: Measurement \ne.g., the challenge of adhering to the principle \npractices must not only be technically sound \nof transparency while respecting intellectual \nin terms of metrics, methods, and data, but \nproperty considerations, may become of \nthey must also be widely understood and \nsecondary importance.\naccepted as evidence of effectiveness.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 227', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\nIllustration—Effectiveness The studies provided empirical evidence  \nthat some systems could achieve high scores  \nThe search for factual evidence in large document \n(80%) according to both metrics.63 In a seminal \ncollections in US civil or criminal proceedings \nfollow-up study, Maura R. Grossman and Gordon \nhas traditionally involved page-by-page manual \nV. Cormack found that two automated systems \nreview by attorneys. Starting in the 1990s, the \ndid, in fact, “conclusively” outperform human \nproliferation of electronic data, such as email, \nreviewers.64 Drawing on the results of that study, \nrendered manual review prohibitively costly  \nMagistrate Judge Andrew Peck, in an opinion  \nand time-consuming. By 2008, A/IS designed  \nwith far-reaching consequences, gave court \nto substantially automate review of electronic \napproval for the use of A/IS to conduct legal \ndata (a task known as “e-discovery”) were \ndiscovery.65 \navailable. Yet, adoption remained limited. Chief \namong the obstacles to adoption was a concern  The story of the TREC Legal Track’s role in \nabout the effectiveness, and hence defensibility  facilitating the adoption of A/IS for legal fact-\nin court, of A/IS in e-discovery. Simply put,  finding contains a few lessons:\npractitioners and courts needed a  \nsound answer to a simple question:   •  Metrics: By focusing on recall and precision, \n“Does it work?” the TREC studies quantified the effectiveness \nof the systems evaluated in a way that  \nStarting in 2006, the US NIST 59 conducted  legal practitioners could readily understand. \nstudies to assess that question.60 The studies \n•  Benchmarks: The TREC studies filled an \nfocused on, among others, two sound statistical \nimportant gap: independent, scientifically \nmetrics, both expressed as easy-to-understand \nsound evaluations of the effectiveness of  \npercentages:61,62 \nA/IS applied to the real-world challenge  \n•  Recall, which is a gauge of the extent   of legal e-discovery. \nto which all the relevant documents were  •  Collaboration: The founders of the TREC \nretrieved. For example, if there were 1,000  studies and the most successful participants \nrelevant documents to be found in the  came from both scientific and legal \ncollection, and the review process identified  backgrounds, demonstrating the importance \n700 of them, then it achieved 70% recall. of multidisciplinary collaboration.\n•  Precision, which is a gauge of the extent  \nThe TREC studies are a shining example of how \nto which the documents identified as  \nthe truth-seeking protocols of science can be \nrelevant by a process were actually relevant. \nused to advance the truth-seeking protocols  \nFor example, if for every two relevant \nof the law. They can serve as a conceptual  \ndocuments the system captured, it also \nbasis for future benchmarking efforts, as well as \ncaptured a nonrelevant one (i.e., a false \nthe development of standards and certification \npositive), then it achieved 67% precision.\nprograms to support informed trust when  \n  it comes to effectiveness of A/IS deployed  \nin legal systems.66\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 228', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\nRecommendations the procedures and results of the testing  \nin clear language that is understandable  \n1.  Governments should fund and support the \nto both experts and nonexperts, and should \nestablishment of ongoing benchmarking \ndo so without disclosing intellectual property. \nexercises designed to provide valid, publicly \nFurther, the descriptions should be open  \naccessible measurements of the effectiveness \nto examination by all stakeholders, including, \nof A/IS deployed, or potentially deployed, \nwhen appropriate, the general public.\nin the legal system. That support could take \na number of forms, ranging from direct  4. Researchers engaged in the study and \nsponsorship and oversight—for example, by  development of A/IS for use in the legal \nnonregulatory measurement laboratories such  system should seek to define meaningful \nas the US NIST—to indirect support by the  metrics that gauge the effectiveness of the \nrecognition of the results of a credible third- systems they study. In selecting and defining \nparty benchmarking exercise for the purposes  metrics, researchers should seek input  \nof meeting procurement and contracting  from all stakeholders in the outcome of the \nrequirements. All government efforts in this  given application of A/IS in the legal system. \nregard should be transparent and open to  The metrics should be readily understandable \npublic scrutiny. by experts and nonexperts alike.\n2. Governments should facilitate the creation  5. Governments and industry associations  \nof data sets that can be used for purposes  should undertake educational efforts to  \nof evaluating the effectiveness of A/IS as  inform both those engaged in the operation  \napplied in the legal system. In assisting in the  of A/IS deployed in the legal system and \ncreation of such data sets, governments and  those affected by the results of their operation \nadministrative agencies will have to take into  of the salient measures of effectiveness and \nconsideration potentially competing societal  what they can indicate about the capabilities \nvalues, such as the protection of personal  and limitations of the A/IS in question.\ndata, and arrive at solutions that maintain  6. Creators of A/IS for use in the legal system \nthose values while enabling the creation of  should ensure that the effectiveness metrics \nusable, real-world data sets. All government  defined by the research community are readily \nefforts in this regard should be transparent  obtainable and accessible to all stakeholders, \nand open to public scrutiny. including, when appropriate, the general \n3. Creators of A/IS to be applied to legal  public. Creators should provide guidance on \nmatters should pursue valid measures of  how to interpret and respond to the metrics \nthe effectiveness of their systems, whether  generated by the system.\nthrough participation in benchmarking  7.  Operators of A/IS applied to a legal \nexercises or through conducting single-system  task should follow the guidance on the \nvalidation exercises. Creators should describe  measurement of effectiveness provided for \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 229', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\nthe A/IS being used. This includes guidance  •  C. Garvie, A. M. Bedoya, and J. Frankle, “The \nabout which metrics to obtain, how and when  Perpetual Line-Up: Unregulated Police Face \nto obtain them, how to respond to given  Recognition in America,” Georgetown Law, \nresults, when it may be appropriate to follow  Center on Privacy & Technology, Oct. 2016. \nalternative methods of gauging effectiveness, \n•  M. R. Grossman and G. V. Cormack, \nand so on.\n“Technology-Assisted Review in E-Discovery \n8. In interpreting and responding to  Can Be More Effective and More Efficient \nmeasurements of the effectiveness of   Than Exhaustive Manual Review,” Richmond \nA/IS applied to legal problems or questions,  Journal of Law and Technology, vol. 17,  \nallowance should be made by those  no. 3, 2011.\ninterpreting the results for variation in the \n•  B. Hedin, D. Brassil, and A. Jones, “On the \nspecific objectives and circumstances of \nPlace of Measurement in E-Discovery,” in \na given deployment of A/IS. Quantitative \nPerspectives on Predictive Coding and Other \nresults should be supplemented by qualitative \nAdvanced Search Methods for the Legal \nevaluation of the practical significance  \nPractitioner, J. R. Baron, R. C. Losey, and \nof a given outcome and whether it indicates  \nM. D. Berman, Eds. Chicago: American Bar \na need for remediation. This evaluation should \nAssociation, 2016.\nbe done by an individual with the technical \n•  J. A. Kroll, “The fallacy of inscrutability,” \nexpertise and pragmatic experience needed  \nPhilosophical Transactions of the Royal Society \nto make a sound judgment. \nA: Mathematical, Physical, and Engineering \n9. Industry associations or other organizations \nSciences, vol. 376, no. 2133, Oct. 2018. \nshould collaborate on developing standards for \n•  D. W. Oard, J. R. Baron, B. Hedin, D. Lewis, \nmeasuring and reporting on the effectiveness \nand S. Tomlinson, “Evaluation of Information \nof A/IS. These standards should be developed \nRetrieval for E-Discovery,” Artificial Intelligence \nwith input from both the scientific and legal \nand Law, vol. 18, no. 4, pp. 347-386,  \ncommunities.\nAug. 2010. \n10. Recommendation 1 under Issue 2,  \n•  The Sedona Conference, “The Sedona \nwith respect to effectiveness.\nConference Commentary on Achieving  \n11. Recommendation 2 under Issue 2,  \nQuality in the E-Discovery Process,”  \nwith respect to effectiveness.\nThe Sedona Conference Journal, vol. 15,  \npp. 265-304, 2014.\nFurther Resources\n•  M. T. Stevenson, “Assessing Risk Assessment \n•  Da Silva Moore v. Publicis Groupe, 2012 WL  in Action,” Minnesota Law Review, vol. 103, \n607412 (S.D.N.Y. Feb. 24, 2012). June 2018. \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 230', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\n•  “Global Governance of AI Roundtable:  operating room or cockpit. This informed trust in \nSummary Report 2018,” World Government  operator competence is what gives us confidence \nSummit, 2018. that surgery or air travel will result in the desired \noutcome. No such standards of operator \n•  High-Level Expert Group on Artificial \ncompetence currently exist with respect to A/IS \nIntelligence, “DRAFT Ethics Guidelines  \napplied in legal systems, where the life, liberty, \nfor Trustworthy AI: Working Document for \nand rights of citizens can be at stake. That absence \nStakeholders’ Consultation,” The European \nof standards hinders the trustworthy adoption  \nCommission. Brussels, Belgium:  \nof A/IS in the legal domain.\nDec. 18, 2018.\nThe human operator is an integral \ncomponent of A/IS\nAlmost all current applications of A/IS in legal \nIssue 4: Competence\nsystems, like those in most other fields, require \nHow can specification of the  human mediation and likely will continue to do  \nknowledge and skills required   so for the near future. This human mediation, \npost design and post development, will take  \nof the human operator(s) of  \na number of forms, including decisions about  \nA/IS foster informed in the \n(a) whether or not to use A/IS for a given \nsuitability of A/IS for adoption  \npurpose,67 (b) the data used to train the systems, \nin legal systems? (c) settings for system parameters to be used \nin generating results, (d) methods of validating \nresults, (e) interpretation and application of \nthe results, and so on. Because these systems’ \nBackground\noutcomes are a function of all their components, \nAn essential component of informed trust in \nincluding the human operator(s), their \na technological system, especially one that \neffectiveness, and by extension trustworthiness, \nmay affect us in profound ways, is confidence \nwill depend on their human operator(s). \nin the competence of the operator(s) of the \ntechnology. We trust surgeons or pilots with  Despite this, there are few standards that specify \nour lives because we have confidence that they  how humans should mediate applications of  \nhave the knowledge, skills, and experience to  A/IS in legal systems, or what knowledge qualifies  \napply the tools and methods needed to carry out  a person to apply A/IS and interpret their results.68  \ntheir tasks effectively. We have that confidence  This reality is especially troubling for the instances \nbecause we know that these operators have met  in which the life, rights, or liberty of humans are \nrigorous professional and scientific accreditation  at stake. Today, while professional codes of ethics \nstandards before being allowed to step into the  for lawyers are beginning to include among their \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 231', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\nrequirements an awareness and understanding  well-being that would follow from that disorder. \nof technologies with legal application,69 the  By way of illustration:\noperators of A/IS in legal systems are essentially \n•  A city council might misallocate funds for \ndeemed to be capable of determining their \npolicing across city neighborhoods because \nown competence: lawyers or IT professionals \nit relies on the output of an algorithm that \noperating in civil discovery, correctional officers \ndirects attention to neighborhoods based  \nusing risk assessment algorithms, and law \non arrest rates rather than actual crime rates.71 \nenforcement agencies engaging in predictive \npolicing or using automated surveillance  •  In civil justice, A/IS applied in a search of \ntechnologies. All are mostly able to use A/IS  documents to uncover relevant facts may \nwithout demonstrating that they understand  fail to do so because an operator without \nthe operation of the system they are using or  sufficient competence in statistics may \nthat they have any particular set of consensus  materially overestimate the accuracy of \ncompetencies.70  the system, thus ceasing vital fact-finding \nactivities.72 \nThe lack of competency requirements or \n•  In the money bail system, reliance on  \nstandards undermines the establishment of \nA/IS to reduce bias may instead perpetuate  \ninformed trust in the use of A/IS in legal systems. \nit. For example, if a judge does not understand \nIf courts, legal practitioners, law enforcement \nwhether an algorithm makes sufficient \nagencies, and the general public are to rely on the \ncontextual distinctions between gradations  \nresults of A/IS when applied to tasks traditionally \nof offenses,73 that judge would not able  \ncarried out by legal professionals, they must \nto probe the output of the A/IS and make  \nhave grounds for believing that those operating \na well-informed use of it.\nA/IS will possess the requisite knowledge and \nskill to understand the conditions and methods  •  In the criminal justice system, an operator \nfor operating the systems effectively, including  using A/IS in sentencing decision-support \nevaluating the data on which the A/IS trained,  may fail to identify bias, or to assess the risk \nthe data to which they are applied, the results  of bias, in the results generated by the A/IS,74 \nthey produce, and the methods and results  unfairly depriving a citizen of his or her liberty \nof measuring the effectiveness the systems.  or prematurely granting an offender’s release, \nApplied incompetently, A/IS could produce the  increasing the risk of recidivism.\nopposite intended effect. Instead of improving \nMore generally, without the confidence that A/IS \na legal system—and bringing about the gains in \noperators will apply the technology as intended \nwell-being that follow from such improvements—\nand supervise it appropriately, the general public \nthey may undermine both the fairness and \nwill harbor fear, uncertainty, and doubt about  \neffectiveness of a legal system and trust in its \nthe use of A/IS in legal systems and potentially \nfairness and effectiveness, creating conditions  \nabout the legal systems themselves.\nfor social disorder and the deterioration of human \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 232', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\nFostering informed trust in the  through professional schools, such as law \ncompetence of human operators school; through institutions providing ongoing \nprofessional training, such as, for federal judges \nIf negative outcomes such as those just described \nin the United States, the Federal Judicial Center; \nare to be avoided, it will be necessary to \nthrough professional and industry associations, \ninclude among norms for the adoption \nsuch as the American Bar Association; or through \nof A/IS in a legal system a provision for \nresources accessible by the general public.76 \nbuilding informed trust in the operators \nMaking sure such training is available and \nof A/IS. Building trust will require articulating \naccessible will be essential to ensuring that the \nstandards and best practices for two groups \nresources needed for the competent operation  \nof agents involved in the deployment of A/IS: \nof A/IS are widely and equitably distributed.77\ncreators and operators. \nIt will take a combined effort of both creators  \nOn the one hand, those engaged in the design, \nand operators to ensure both that A/IS designed \ndevelopment, and marketing of A/IS must \nfor use in legal systems are properly applied \ncommit to specifying the knowledge, skills, \nand that those with a stake in the effective \nand conditions required for the safe, ethical, \nfunctioning of legal systems—including legal \nand effective deployment and operation of the \nprofessionals, of course, but also decision \nsystems.75 On the other hand, those engaged  \nsubjects, victims of crime, communities, and \nin actually operating the systems, including \nthe general public—will have informed trust, \nboth legal professionals and experts acting in \nor, for that matter, informed distrust (if that is \nthe service of legal professionals, must commit \nwhat a competence assessment finds) in the \nto adhering to these requirements in a manner \ncompetence of the operators of A/IS as applied \nconsistent with other operative legal, ethical,  \nto legal problems and questions.78\nand professional requirements. The precise \nnature of the competency requirements will  \nIllustration—Competence\nvary with the nature and purpose of the A/IS  \nand what is at stake in their effective operation.  Included among the offerings of Amazon \nThe requirements for the operation of A/IS  Web Services is an image and video analysis \ndesigned to assist in the creation of contracts,   service known as Amazon Rekognition.79 The \nfor example, might be less stringent than those  service is designed to enable the recognition \nfor the operation of A/IS designed to assess   of text, objects, people, and actions in images \nflight risk, which could affect the liberty of  and videos. The technology also enables the \nindividual citizens.  search and comparison of faces, a feature with \npotential law enforcement and national security \nA corollary of these provisions is that education \napplications, such as comparing faces identified \nand training in the requisite skills should be \nin video taken by a security camera with those \navailable and accessible to those who would \nin a database of jail booking photos. Attracted by \noperate A/IS, whether that training is provided \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 233', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\nthe latter feature, police departments in Oregon  Amazon recommends utilizing a similarity \nand Florida have undertaken pilots of Rekognition  threshold value of 99% or above to reduce \nas a tool in their law enforcement efforts.80 accidental misidentification.84 Amazon also noted \nthat, in all law enforcement use cases, it would \nIn 2018, the American Civil Liberties Union \nbe expected that the results of the technology \n(ACLU), a frequent critic of the use of facial \nwould be reviewed by a human before any  \nrecognition technologies by law enforcement \nactual police action would be undertaken.\nagencies,81 conducted a test of Rekognition.  \nThe test consisted of first constructing a database  The story of the ACLU’s testing of Rekognition \nof 25,000 booking photos (“mugshots”) then  and Amazon’s response to the test highlights \ncomparing publicly available photos of all then- the importance of specifying and adhering \ncurrent members of the US Congress against  to guidelines for competent use.85 Had a law \nthe images in the database. The test found that  enforcement agency used the technology in \nRekognition incorrectly matched the faces of 28  the way it was used in the ACLU test, it would, \nmembers of Congress with faces of individuals  in most legitimate use cases, be guilty of \nwho had been arrested for a crime.82 The ACLU  incompetent use. At the same time, Amazon  \nargues that the high number of false positives  is not free of blame insofar as it did not specify \ngenerated by the technology shows that police  prominently and clearly the competency \nuse of facial recognition technologies generally  guidelines for effective use of the technology  \n(and of Rekognition in particular) poses a risk   in support of law enforcement efforts, as well \nto the privacy and liberty of law-abiding  as the risks that might be incurred if those \ncitizens. The ACLU has used the results of its  guidelines are not followed. Competent use86 \ntest of Rekognition to support its proposal that  follows both from the A/IS creator’s specification \nCongress enact a moratorium on the use of facial  of well-grounded87 competency guidelines  \nrecognition technologies by law enforcement  and from the A/IS operator’s adherence to  \nagencies until stronger safeguards against   those guidelines.88\ntheir misuse, and potential abuse, can be put  \nin place.83 Recommendations\nIn response to the ACLU report, Amazon noted  1.  Creators of A/IS for application in legal \nthat the ACLU researchers, in conducting their  systems should provide clear and accessible \nstudy, had applied the technology utilizing   guidance for the knowledge, skills, and \na similarity threshold (a gauge of the likelihood   experience required of the human operators \nof a true match) of 80%, a threshold that casts   of the A/IS if the systems are to achieve \na fairly wide net for potential matches (and hence  expected levels of effectiveness. Included  \ngenerates a high number of false positives).   in that guidance should be a delineation  \nFor applications in which there are greater costs  of the risks involved if those requirements \nassociated with false positives (e.g., policing),  are not met. Such guidance should be \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 234', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\ndocumented in a form that is accessible   certain conditions, requiring, as appropriate, \nand understandable by both experts and the  acknowledgment of receipt; limiting access \ngeneral public. to A/IS functionality based on the operator’s \nlevel of expertise; enabling system shut-down \n2. Creators and developers of A/IS for application \nin potentially high-risk conditions; and more. \nin legal systems should create written policies \nThese safeguards should be flexible and \nthat govern how the A/IS should be operated. \ngoverned by context-sensitive policies  \nIn creating these policies, creators and \nset by competent personnel of the entity  \ndevelopers should draw on input from the \n(e.g., the judiciary), utilizing the A/IS to \nlegal professionals who will be using the A/IS \naddress a legal problem.\nthey are creating. The policies should include: \n4. Governments should provide that any \n  •  the specification of the real-world \nindividual whose legal outcome is affected \napplications for the A/IS; \nby the application of A/IS should be notified \n  •  the preconditions for their effective use; \nof the role played by A/IS in that outcome. \n  •  the training and skills that are required   Further, the affected party should have \nfor operators of the systems;  recourse to appeal to the judgment of  \na competent human being. \n  •  the procedures for gauging the  \neffectiveness of the A/IS;  5. Professionals engaged in the creation,  \npractice, interpretation, and enforcement \n  •  the considerations to take into account  \nof the law, such as lawyers, judges, and \nin interpreting the results of the A/IS; \nlaw enforcement officers, should recognize \n  •  the outcomes that can be expected by  \nthe specialized scientific and professional \nboth operators and other affected parties \nexpertise required for the ethical and effective \nwhen the A/IS are operated properly; and\napplication of A/IS to their professional \n  •  the specific risks that follow from   duties. The professional associations to \nimproper use.  which such legal practitioners belong, such \nas the American Bar Association, should, \nThe policies should also specify circumstances  \nthrough both educational programs and \nin which it might be necessary for the operator  \nprofessional codes of ethics, seek to ensure \nto override the A/IS. All such policies should  \nthat their members are well informed about \nbe publicly accessible.\nthe scientific and technical competency \n3. Creators and developers of A/IS to be applied  requirements for the effective and trustworthy \nin legal systems should integrate safeguards  application of A/IS to the law.89\nagainst the incompetent operation of their  6. The operators of A/IS applied in legal  \nsystems. Safeguards could include issuing  systems—whether the operator is a specialist \nnotifications and warnings to operators in  in A/IS or a legal professional—should \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 235', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\nunderstand the competencies required for \nthe effective performance of their roles and \nIssue 5: Accountability\nshould either acquire those competencies or \nidentify individuals with those competencies  How can the ability to apportion \nwho can support them in the performance  responsibility for the outcome  \nof their roles. The operator does not need to \nof the application of A/IS  \nbe an expert in all the pertinent domains but \nfoster informed trust in the \nshould have access to individuals with the \nsuitability of A/IS for adoption  \nrequisite expertise.\nin legal systems?\n7.  Recommendation 1 under Issue 2,  \nwith respect to competence.\n8. Recommendation 2 under Issue 2,  \nBackground\nwith respect to competence.\nApportioning responsibility. An essential \nFurther Resources component of informed trust in a technological \nsystem is confidence that it is possible, if the \n•  C. Garvie, A. M. Bedoya, and J. Frankle, “The \nneed arises, to apportion responsibility among \nPerpetual Line-Up: Unregulated Police Face \nthe human agents engaged along the path of  \nRecognition in America,” Georgetown Law, \nits creation and application: from design through \nCenter on Privacy & Technology, Oct. 2016. \nto development, procurement, deployment,90 \n•  International Organization for Standardization,  operation, and, finally, validation of effectiveness. \nISO/IEC 27050-3: Information technology— Unless there are mechanisms to hold the agents \nSecurity techniques—Electronic discovery—  engaged in these steps accountable, it will be \nPart 3: Code of practice for electronic  difficult or impossible to assess responsibility  \ndiscovery, Geneva, 2017. for the outcome of the system under any \n•  J. A. Kroll, “The fallacy of inscrutability,”  framework, whether a formal legal framework  \nPhilosophical Transactions of the Royal Society  or a less formal normative framework. A model  \nA: Mathematical, Physical, and Engineering  of A/IS creation and use that does not have  \nSciences, vol. 376, no. 2133, Oct. 2018.  such mechanisms will also lack important forms \nof deterrence against poorly thought-out design, \n•  A. G. Ferguson, “Policing Predictive Policing,” \ncasual adoption, and inappropriate use of A/IS. \nWashington University Law Review, vol. 94, \nno. 5 2017. Simply put, a system that produces outcomes  \n•  “Global Governance of AI Roundtable:  for which no one is responsible cannot be \nSummary Report 2018,” World Government  trusted. Those engaged in creating, procuring, \nSummit, 2018.  deploying, and operating such a system will \nlack the discipline engendered by the clear \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 236', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\nassignment of responsibility. Meanwhile, those  machine learning algorithms that adjust the \naffected by the results of the system’s operation  model as the algorithm encounters new data.91\nwill find their questions around a given result \nThis opacity of the systems makes it challenging \ninadequately answered, and errors generated \nto trace cause to effect,92 which, in turn, makes  \nby the system will go uncorrected. In the case \nit difficult or even impossible, to draw lines  \nof A/IS applied in a legal system, where an \nof responsibility.\nindividual’s basic human rights may be at issue, \nthese questions and errors are of fundamental \nThe diffuseness challenge stems from the fact \nimportance. In such circumstances, the only \nthat even the most seemingly straightforward \noptions are either blind trust or blind distrust. \nA/IS can be complex, with a wide range of \nNeither of those options is satisfactory, especially \nagents—systems designers, engineers, data \nin the case of a technological system applied  \nanalysts, quality control specialists, operators, \nin a domain as fundamental to the social order  \nand others—involved in design, development, \nas the law.\nand deployment. Moreover, some of these \nagents may not even have been engaged in \nChallenges to accountability  the development of the A/IS in question; they \nmay have, for example, developed open-source \nIn the case of A/IS, whether applied in a \ncomponents that were intended for an entirely \nlegal system or another domain, maintaining \ndifferent purpose but that were subsequently \naccountability can be a particularly steep \nincorporated into the A/IS. This diffuseness \nchallenge. This challenge to accountability  \nof responsibility poses a challenge to the \nis because of both the perceived “black box” \nmaintenance of accountability.93 As Matthew \nnature of A/IS and the diffusion of responsibility \nScherer, a frequent writer and speaker on topics \nit brings.\nat the intersection of law and A/IS, observes:\nThe perception of A/IS as a black box stems from \nThe sheer number of individuals and firms that \nthe opacity that is an inevitable characteristic of \nmay participate in the design, modification, and \na system that is a complex nexus of algorithms, \nincorporation of an AI system’s components will \ncomputer code, and input data. As observed by \nmake it difficult to identify the most responsible \nJoshua New and Daniel Castro of the Information \nparty or parties. Some components may have \nTechnology and Innovation Foundation:\nbeen designed years before the AI project had \nThe most common criticism of algorithmic  even been conceived, and the components’ \ndecision-making is that it is a “black box” of  designers may never have envisioned, much \nextraordinarily complex underlying decision  less intended, that their designs would be \nmodels involving millions of data points and  incorporated into any AI system, still less the \nthousands of lines of code. Moreover, the model  specific AI system that caused harm. In such \ncan change over time, particularly when using  circumstances, it may seem unfair to assign \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 237', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\nblame to the designer of a component whose  another, who is responsible for the resulting \nwork was far-removed in both time and  encroachment on the civil rights of the person \ngeographic location from the completion and  erroneously targeted? Is it the A/IS designer, \noperation of the AI system.94 the individual who selected the data on  \nwhich to train the algorithm, the individual \nExamples include the following:\nwho chose how the effectiveness of the  \nA/IS would be measured,96 the experts who \n•  When a judge’s ruling includes a long prison \nprovided training to the officer, or the officer \nsentence, based in part on a flawed A/IS-\nhimself or herself?\nenabled process that erroneously deemed \na particular person to be at high risk of \nAs a result of the challenges presented by  \nrecidivism, who is responsible for the error?  \nthe opacity and diffuseness of responsibility  \nIs it the A/IS designer, the person who chose  \nin A/IS, the present-day answer to the question, \nthe data or weighed the inputs, the prosecution \n“Who is accountable?” is, in far too many \nteam who developed and delivered the risk \ninstances, “It’s hard to say.” This is a response \nprofile to the court, or the judge who did not \nthat, in practice, means “no one” or, equally \nhave the competence to ask the appropriate \nunhelpful, “everyone”. Such failure to maintain \nquestions that would have enabled a clearer  \naccountability will undermine efforts to bring \nunderstanding of the limitations of the system?  \nA/IS (and all their potential benefits) into legal \nOr is responsibility somehow distributed \nsystems based on informed trust.\namong these various agents?95 \n•  When a lawyer engaged in civil or criminal \nMaintaining accountability and  \ndiscovery believes, erroneously, that all \ntrust in A/IS\nthe relevant information was found when \nAlthough maintaining accountability in complex \nusing A/IS in a data-intensive matter, who is \nsystems can be a challenge, it is one that must \nresponsible for the failure to gather important \nbe met in order to engender informed trust in \nfacts? The A/IS designer who typically would \nthe use of A/IS in the legal domain. “Blaming \nhave had no ability to foretell the specific \nthe algorithm” is not a substitute for taking on \ncircumstances of a given matter, the legal \nthe challenge of maintaining transparent lines \nor IT professional who operated the A/IS or \nof responsibility and establishing norms of \nerroneously measured its effectiveness, or \naccountability.97 This is true even if we allow \nthe lawyer who made a representation to his \nthat, given the complexity of the systems in \nor her client, to the court, or to investigatory \nquestion, some number of “systems accidents” \nagencies? \nis inevitable.98 Informed trust in a system does \n•  When a law enforcement officer, relying  \nnot require a belief that zero errors will occur; \non A/IS, erroneously identifies an individual \nhowever, it does require a belief that there are \nas being more likely to commit a crime than \nmechanisms in place for addressing errors when \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 238', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\nthey do occur. Accountability is an essential  The goal of clarifying lines of responsibility in  \ncomponent of those mechanisms. the operation of A/IS is to implement a governing \nmodel that specifies who is responsible for \nIn meeting the challenge, it should be  \nwhat, and who has recourse to which corrective \nrecognized that there are existing norms and \nactions, i.e., a trustworthy model that ensures \ncontrols that have a role to play in ensuring \nthat it will admit actionable answers should \nthat accountability is maintained. For example, \nquestions of accountability arise. Arriving at  \ncontractual arrangements between the A/IS \nan effective model will require the participation \nprovider and a party acquiring and applying  \nof those engaged in the creation and operation  \na system may help to specify who is (and is \nof A/IS, those affected by the results of their  \nnot) to be held liable in the event the system \nuse, and those with the expertise to understand \nproduces undesirable results. Professional \nhow such a model would be used in a given  \ncodes of ethics may also go some way toward \nlegal system. For example:\nspecifying the extent to which lawyers, for \nexample, are responsible for the results generated   •  Individuals responsible for the design of  \nby the technologies they use, whether they  A/IS will have to maintain a transparent record \noperate them directly or retain someone else   of the sources of the various components of \nto do so. Judicial systems may have procedures  their systems, including identification of which \nfor assessing responsibility when a citizen’s   components were developed in-house and \nrights are improperly infringed. As illustrated   which were acquired from outside sources, \nby the cases described above, however, existing  whether open source or acquired from \nnorms and controls, while helpful, are insufficient  another firm.\nin themselves to meet the specific challenge \n•  Individuals responsible for the design of A/IS \nrepresented by the opacity and diffuseness of  \nwill have to specify the roles, responsibilities, \nA/IS. To meet the challenge further steps must \nand potential subsequent liabilities of those \nbe taken.99\nwho will be engaged in the operation of the \nsystems they create. \nThe first step is ensuring that all those engaged \nin the creation, procurement, deployment,  •  Individuals responsible for the operation of \noperation, and testing of A/IS recognize that,   a system will have to understand their roles, \nif accountability is not maintained, these systems  responsibilities, potential liabilities, and will \nwill not be trusted. In the interest of maintaining  have to maintain documentation of their \naccountability, these stakeholders should take  adherence to requirements. \nsteps to clarify lines of responsibility throughout \n•  Individuals affected by the results of the \nthis continuum, and make those lines of \noperation of A/IS, e.g., a defendant in a \nresponsibility, when appropriate, accessible  \ncriminal proceeding, will have to be given \nto meaningful inquiry and audit.\naccess to information about the roles and \nresponsibilities of those involved in relevant \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 239', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\naspects of the creation, operation, and  such review boards would, in the interest \nvalidation of the effectiveness of the A/IS  of legitimacy, have to include participation \naffecting them.100 from various citizens’ groups, such as those \nrepresenting defendants in the criminal system  \n•  Individuals with legal and political training \nas well as those representing victims of crime.102\n(e.g., jurists, regulators, as well as legal and \npolitical scholars) will have to ensure that any \nThe goal of opening lines of responsibility \nmodel that is created will provide information \nto meaningful inquiry is to ensure that an \nthat is in fact actionable within the operative \ninvestigation into the use of A/IS will be able \nlegal system.\nto isolate responsibility for errors (or potential \nerrors) generated by the systems and their \nA governing model of accountability that reflects \noperation.103 This means that all those engaged \nthe interests of all these stakeholders will be \nin the design, development, procurement, \nmore effective both at deterring irresponsible \ndeployment, operation, and validation of the \ndesign or use of A/IS before it happens and  \neffectiveness of A/IS, as well as the organizations \nat apportioning responsibility for an undesirable \nthat employ them, must in good faith be willing \noutcome when it does happen.101\nto participate in an audit, whether the audit  \nPulling together the input from the various  is a formal legal investigation or a less formal \nstakeholders will likely not take place without  inquiry. They must also be willing to create and \nsome amount of institutional initiative.  preserve documentation of key procedures, \nOrganizations that employ A/IS for accomplishing  decisions, certifications,104 and tests made  \nlegal tasks—private firms, regulatory agencies,  in the course of developing and deploying  \nlaw enforcement agencies, judicial institutions— the A/IS.105\nshould therefore develop and implement policies \nThe combination of a governing model of \nthat will advance the goal of clarifying lines of \naccountability and an openness to meaningful \nresponsibility. Such policies could take the form \naudit will allow the maintenance of accountability, \nof, for example, designating an official specifically \neven in complex deployments of A/IS in the \ncharged with oversight of the organization’s \nservice of a legal system.\nprocurement, deployment, and evaluation of A/IS \nas well as the organization’s efforts to educate  \nAdditional note 1. The principle of \npeople both inside and outside the organization \naccountability is closely linked with each of the \non its use of A/IS. Such policies might also \nother principles intended to foster informed \ninclude the establishment of a review board \ntrust in A/IS: effectiveness, competence, and \nto assess the organization’s use of A/IS and \ntransparency. With respect to effectiveness, \nto ensure that lines of responsibility for the \nevidence of attaining key metrics and benchmarks  \noutcomes of its use are maintained. In the case \nto confirm that A/IS are functioning as intended \nof agencies, such as police departments, whose \nmay put questions of where, among creators, \nuse of A/IS could impact the general public,  \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 240', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\nowners, and operators, responsibility for  or may commit a new crime or a new violent \nthe outcome of a system lies on a sound  act. While math has played a role in these \nempirical footing. With respect to competence,  determinations since at least the 1920s,110 a new \noperator credentialing and specified system  interest in accountability and transparency has \nhandoffs enable a clear chain of responsibility  brought novel legal challenges to these tools.\nin the deployment of A/IS.106 With respect to \nIn 2013, Eric Loomis was arrested for a drive-by \ntransparency, providing a view into the general \nshooting in La Crosse, Wisconsin. No one was \ndesign and methods of A/IS, or even a specific \nhit, but Loomis faced prison time. Loomis denied \nexplanation for a given outcome, can help  \ninvolvement in the shooting, but waived his right \nto advance accountability. \nto trial and entered a guilty plea to two of the \nAdditional note 2. Closely related to  less severe offenses with which he was charged: \naccountability is the trust that follows from  attempting to flee a traffic officer and operating \nknowing that a human expert is guiding the A/IS  a motor vehicle without the owner’s consent. \nand is capable of overriding them, if necessary.  The judge sentenced him to six years in prison, \nSubjecting humans to automated decisions  saying he was “high risk”. The judge based this \nnot only raises legal and ethical concerns, both  conclusion, in part, on the risk assessment score \nfrom a data protection107 and fundamental rights  given by Compas, a secret and privately held \nperspective,108 but also will likely be viewed with  algorithmic tool used routinely by the Wisconsin \ndistrust if the human component, which can  Department of Corrections.\nintroduce circumstantial flexibility in the interest \nOn appeal, Loomis made three major arguments, \nof realizing an ethically superior outcome, is \ntwo focused on accountability.111 First, the tool’s \nmissing. In addition to ensuring technical safety \nproprietary nature—the underlying code was \nand reliability of A/IS used in the course of \nnot made available to the defense—made it \ndecision-making processes, the legal system \nimpossible to test its scientific validity. Second, \nshould also, where appropriate, provide for the \nthe tool inappropriately considered gender in \npossibility of an appeal for review by a human \nmaking its determination.\njudge. Careful attention must be paid to the \ndesign of corresponding appeal procedures.109 \nA unanimous Wisconsin Supreme Court ruled \nagainst Loomis on both arguments.\nIllustration—Accountability\nThe court reasoned that knowing the inputs and \nOver the last two decades, criminal justice \noutput of the tool, and having access to validating \nagencies have increasingly embraced predictive \nstudies of the tool’s accuracy, were sufficient  \ntools to assist in the determination for bail, \nto prevent infringement of Loomis’ due process.112 \nsentencing, and parole. A mix of companies, \nRegarding the use of gender—a protected class  \ngovernment agencies, nonprofits, and universities \nin the United States—the court said he did \nhave built and promoted tools that provide  \nnot show that there was a reliance on gender \na likelihood that someone may fail to appear \nin making the output or sentencing decision. \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 241', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\nWithout the ability to interrogate the tool and  remain unsettled. Adopters and operators  \nknow how gender is used, the court created   of A/IS should nevertheless understand to \na paradox with its opinion. what extent they could, potentially, be held \nliable for an undesirable outcome.\nThe Loomis decision represents the challenges \n3. When negotiating contracts for the provision \nthat judges have balancing accountability of \nof A/IS products and services for use in the \n“black boxed” A/IS and trade secret protections.113 \nlegal system, providers and buyers of A/IS \nOther decisions have sided against accountability \nshould include contractual terms specifying \nof other risk assessments,114 probabilistic DNA \nclear lines of responsibility for the outcomes \nanalysis tools,115 and government remote \nof the systems being acquired.\nhacking investigation software.116 Siding with \naccountability, a federal judge found that the  4. Creators and operators of A/IS applied in \nunderlying code of a probability software used   a legal system, and the organizations that \nin DNA comparisons was admissible and relevant  employ them, should be amenable to  \nto a pretrial hearing where the admissibility   internal oversight mechanisms and inquiries \nof expert testimony is challenged.117 (or audits) that have the objective of allocating \nresponsibility for the outcomes generated \nThese issues will continue to be litigated as A/IS \nby the A/IS. In the case of A/IS adopted and \ntools continue to proliferate in judicial systems.  \ndeployed by organizations that have direct \nTo that end, as the Loomis court notes, “The \npublic interaction (e.g., a law enforcement \njustice system must keep up with the research \nagency), oversight and inquiry could also \nand continuously assess the use of these tools.”\nbe conducted by external review boards. \nBeing prepared for such inquiries means \nRecommendations maintaining clear documentation of all salient \nprocedures followed, decisions made, and \n1.  Creators of A/IS to be applied in a legal \ntests conducted in the course of developing \nsystem should articulate and document well-\nand applying the A/IS.\ndefined lines of responsibility, among all those \nwho would be engaged in the development  5. Organizations engaged in the development \nand operation of the A/IS, for the outcome   and operation of A/IS for legal tasks should  \nof the A/IS. consider mechanisms that will create individual  \nand collective incentives for ensuring both  \n2. Those engaged in the adoption and operation \nthat the outcomes of the A/IS adhere to ethical  \nof A/IS to be applied in a legal system should \nstandards and that accountability for those \nunderstand their specific responsibilities \noutcomes is maintained, e.g., mechanisms \nfor the outcome of the A/IS as well as their \nto ensure that speed and efficiency are \npotential liability should the A/IS produce \nnot rewarded at the expense of a loss of \nan outcome other than that intended. In the \naccountability.\ncase of A/IS, many questions of legal liability \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 242', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\n6. Those conducting inquiries to determine  Pennsylvania Law Review, vol. 165, pp. 633-\nresponsibility for the outcomes of A/IS  705. Feb. 2017. \napplied in a legal system should take into \n•  J. New and D. Castro, “How Policymakers  \nconsideration all human agents involved \nCan Foster Algorithmic Accountability,” \nin the design, development, procurement, \nInformation Technology and Innovation \ndeployment, operation, and validation of \nFoundation, May 21, 2018.\neffectiveness of the A/IS and should assign \n•  M. U. Scherer, “Regulating Artificial Intelligence \nresponsibility accordingly.\nSystems: Risks, Challenges, Competencies, \n7.  Recommendation 1 under Issue 2,  \nand Strategies,” Harvard Journal of Law & \nwith respect to accountability.\nTechnology, vol. 29. no. 2, pp. 369-373, 2016. \n8. Recommendation 2 under Issue 2,  \n•  J. Tashea, “Calculating Crime: Attorneys are \nwith respect to accountability.\nChallenging the Use of Algorithms to Help \nDetermine Bail, Sentencing and Parole,”  \nFurther Resources ABA Journal, March 2017. \n•  N. Diakopoulos, S. Friedler, M. Arenas, \nS. Barocas, M. Hay, B. Howe, H. V. \nJagadish, K. Unsworth, A. Sahuguet, S. \nVenkatasubramanian, C. Wilson, C. Yu, and \nIssue 6: Transparency\nB. Zevenbergen, “Principles for Accountable \nAlgorithms and a Social Impact Statement   How can sharing information  \nfor Algorithms,” FAT/ML. that explains how A/IS reached \n•  F. Doshi-Velez, M. Kortz, R. Budish, C. Bavitz,  given decisions or outcomes \nS. J. Gershman, D. O’Brien, S. Shieber,  foster informed trust in the \nJ. Waldo, D. Weinberger, and A. Wood, \nsuitability of A/IS for adoption  \n“Accountability of AI Under the Law: The Role \nin legal systems?\nof Explanation,” Berkman Center Research \nPublication Forthcoming; Harvard Public Law \nWorking Paper, no. 18-07, Nov. 3, 2017.\nBackground\n•  European Commission for the Efficiency  \nof Justice. European Ethical Charter on the  Access to meaningful information.  \nUse of Artificial Intelligence in Judicial Systems   An essential component of informed trust in \nand their Environment. Strasbourg, 2018. a technological system is confidence that the \ninformation required for a human to understand \n•  J. A. Kroll, J. Huey, S. Barocas, E. W. Felten, J. \nwhy the system behaves a certain way in a \nR. Reidenberg, D. G. Robinson, and H. Yu,  \nspecific circumstance (or would behave in  \n“Accountable Algorithms,” University of \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 243', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\na hypothetical circumstance) will be accessible.  For A/IS used in a legal system to achieve their \nWithout transparency, there is no basis for trusting  intended purposes, all those with a stake in the \nthat a given decision or outcome of the system  effective functioning of the legal system must \ncan be explained, replicated, or, if necessary,  have a well-grounded trust that the A/IS can \ncorrected.118 Without transparency, there is no  meet these requirements. This trust can be \nbasis for informed trust that the system can be  fostered by transparency.\noperated in a way that achieves its ends reliably \nand consistently or that the system will not be  The elements of transparency\nused in a way that impinges on human rights.  \nTransparency of A/IS in legal matters requires \nIn the case of A/IS applied in a legal system,  \ndisclosing information about the design and \nsuch a lack of trust could undermine the \noperation of the A/IS to various stakeholders.  \ncredibility of the legal system itself.\nIn implementing the principle, however, we \nmust, in the interest of both feasibility and \nTransparency and trust\neffectiveness, be more precise both about \nTransparency, by prioritizing access to information  the categories of stakeholders to whom the \nabout the operation and effectiveness of A/IS,  information will be disclosed, and about the \nserves the purpose of fostering informed trust  categories of information that will be disclosed  \nin the systems. More specifically, transparency  to those stakeholders. \nfosters trust that:\nRelevant stakeholders in a legal system include \n•  the operation of A/IS and the results they  those who:\nproduce are explainable;\n•  operate A/IS for the purpose of carrying  \n•  the operation and results of A/IS are fair;119\nout tasks in civil justice, criminal justice, and \n•  the operation and results of A/IS are unbiased; law enforcement, such as a law enforcement \nofficer who uses facial recognition tools  \n•  the A/IS meet normative standards for \nto identify potential suspects;\noperation and results; \n•  rely on the results of A/IS to make important \n•  the A/IS are effective; \ndecisions, such as a judge who draws  \n•  the results of A/IS are replicable;120 and \non the results of an algorithmic assessment  \n•  those engaged in the design, development,  of recidivism risk in deciding on a sentence;\nprocurement, deployment, operation, and \n•  are directly affected by the use of A/IS— \nvalidation of the effectiveness of A/IS can \na “decision subject”, such as a defendant  \nbe held accountable, where appropriate, for \nin a criminal proceeding whose bail terms  \nnegative outcomes, and that corrective or \nare influenced by an algorithmic assessment \npunitive action can be taken when warranted.\nof flight risk;\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 244', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\n•  are indirectly affected by the results of A/IS,  On the other hand, an inquiry, undertaken  \nsuch as the members of a community   by a designer or operator, into ways to improve \nthat receives more or less police attention  system performance may benefit from access  \nbecause of the results of predictive policing  to information about the formal models on  \ntechnology; and  which the system relies.121\n•  have an interest in the effective functioning  \nThese distinctions also matter because there  \nof the legal system, such as judges, lawyers, \nmay be circumstances in which it would be \nand the general public.\ndesirable to limit access to a given type of \ninformation to certain stakeholders. For example, \nDifferent types of relevant information can be \nthere may be circumstances in which one would \ngrouped into high-level categories. As illustrated \nwant to identify an agent to serve as a public \nbelow, a taxonomy of such high-level categories \ninterest steward. For auditing purposes, this \nmay, for example, distinguish between:\nindividual would have access to certain types  \n•  nontechnical procedural information   of sensitive information unavailable to others. \nregarding the employment and development  Such restrictions on information access are \nof a given application of A/IS; necessary if the transparency principle is not  \nto impinge on other societal values and goals, \n•  information regarding data involved in the \nsuch as security, privacy, and appropriate \ndevelopment, training, and operation of  \nprotection of intellectual property.122\nthe system;\n•  information concerning a system’s  The salience of the question, “Who is given \neffectiveness/performance; access to what information?” is illustrated by \nSentiment Meter, a technology developed by \n•  information about the formal models that  \nElucd, a GovTech company that provides cities \nthe system relies on; and\nwith near real-time understanding of how citizens \n•  information that serves to explain a system’s \nfeel about their government, in conjunction with \ngeneral logic or specific outputs. \nthe New York Police Department, to assist the \nNYPD in gauging citizens’ views regarding police \nThese more granular distinctions matter because \nactivity in their communities.123 One of the stated \ndifferent sorts of inquiries will require different \ngoals of the program is to build public trust in \nsorts of information, and it is important to match \nthe police department. In the interest of trust, \nthe information provided to the actual needs  \nshould “the public” have access to all potentially \nof the inquiry. For example, an inquiry into  \nrelevant information, including how the system \na predictive policing system that misdirected \nwas designed and developed, what the input \npolice resources may not be much advanced by \ndata are, who operates the system and what their \ninformation about the formal models on which \nqualifications are, how the system’s effectiveness \nthe system relied, but it may well be advanced  \nwas tested, and why the public was not brought \nby an explanation for the specific outcome.  \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 245', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\ninto the process of construction? If the answer   Transparency in practice\nis that the general public should not have access  \nAs just noted, although transparency can foster \nto all this information, then who should? How do  \ninformed trust in A/IS applied in a legal system, \nwe define “the public?” Is it the whole community  \nits practical implementation requires \nrepresented in its elected officials? Or should \ncareful thought. Requiring public access to \ncertain communities have greater access, for \nall information pertaining to the operation and \nexample, those most affected by controversial \nresults of A/IS is neither necessary nor feasible. \npolice practices such as stop, question, and frisk? \nWhat is required is a careful consideration  \nSuch questions must be answered if the program \nof who needs access to what information for  \nis to achieve its stated goals.\nthe specific purpose of building informed trust. \nThe following table is an example of a tool that \nmight be used to match type of information  \nto type of information consumer for the purpose \nof fostering trust.124\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 246', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\nTypes of information that should be considered in determining  Stakeholders whose interest in access to different types  \ntransparency demands in relation to a given A/IS of information should be considered in determining the  \ntransparency demands in relation to a given application of A/IS\nHigh-level  Specific type of information (examples) Operators Decision-  Public interest  General \ncategory Disclosure of... subjects steward public\nthe fact that a given context involves  \nProcedural aspects  N/A ? ? ?\nthe employment of A/IS\nregarding A/IS \nemployment \nhow the employment of the system  \nand development ? ? ? ?\nwas authorized\nwho developed the system ? ? ? ?\n...\nthe origins of training data and data  \nData involved  ? ? ? ?\ninvolved in the operation of the system\nin A/IS \ndevelopment \nthe kinds of quality checks that data  \nand operation ? ? ? ?\nwas subject to and their results\nhow data labels are defined and to  \n? ? ? ?\nwhat extent data involves proxy variables\nrelevant data sets themselves ? ? ? ?\n...\nthe kinds of effectiveness/performance \nEffectiveness/  ? ? ? ?\nmeasurement that have occurred\nperformance\nmeasurement results ? ? ? ?\nany independent auditing or certification ? ? ? ?\n...\nModel  the input variables involved ? ? ? ?\nspecification\nthe variable(s) that the model optimizes for ? ? ? ?\ntthe complete model (complete formal \n? ? ? ?\nrepresentation, source code, etc.)\n...\nExplanation information concerning the system’s  \n? ? ? ?\ngeneral logic or functioning\ninformation concerning the determinants  \n? ? ? ?\nof a particular output125\n...\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 247', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\nWhen it comes to deciding whether a specific  actual effectiveness of a drug. Clinical trials \ntype of information should be made available  provide that insight. In a legal system, an \nand, if so, which types of stakeholders should  excessive focus on transparency-related \nhave access to it, there are various considerations,  information-gathering and assessment may \nfor example:  overwhelm courts, legal practitioners, and \nlaw enforcement agencies. Meanwhile, other \n•  The release of certain types of information \nfactors, such as measurement of effectiveness \nmay conflict with data privacy concerns, \nor operator competence, coupled with \ncommercial or public policy interests—such \ninformation on training data, may often suffice \nas the promotion of innovation through \nto ensure that there is a well-informed basis \nappropriate intellectual property protections—\nfor trusting A/IS in a given circumstance.129\nand security interests, e.g., concerns about \ngaming and adversarial attacks. At the same  Given these competing considerations, arriving  \ntime, such competing interests should not  at a balance that is optimal for the functioning of \nbe permitted to be used, without specific  a legal system and that has legitimacy in the eyes \njustification, as a blanket cover for not adhering   of the public will require an inclusive dialogue, \nto due process, transparency, or accountability  bringing together the perspectives of those with \nstandards. The tension between these  an immediate stake in the proper functioning  \ninterests is particularly acute in the case of  of a given technology, including those engaged \nA/IS applied in a legal system, where the  in the design, development, procurement, \ndignity, security, and liberty of individuals are  deployment, operation, and validation of \nat stake.126 effectiveness of the technology, as well as \nthose directly affected by the results of the \n•  There is tension between the specific goal  \ntechnology; the perspectives of communities that \nof explainability, which may argue for limits on \nmay be indirectly impacted by the technology; \nsystem complexity, and system performance, \nand the perspectives of those with specialized \nwhich may be served by greater complexity,  \nexpertise in ethics, government, and the law, \nto the detriment of explainability.127 \nsuch as jurists, regulators, and scholars. How the \n•  One must carefully consider the question \ncompeting considerations should be balanced \nthat is being asked in an inquiry into A/IS and \nwill also vary from one circumstance to another. \nwhat information transparency can actually \nRather than aiming for universal transparency \nproduce to answer that question. Disclosure \nstandards that would be applicable to all uses \nof A/IS algorithms or training data is, itself, \nof A/IS within a legal system, transparency \ninsufficient to enable an auditor to determine \nstandards should allow for circumstance-\nwhether the system was effective in a specific \ndependent flexibility, in the context of the four \ncircumstance.128 By analogy, transparency \nconstitutive components of trust discussed in  \ninto drug manufacturing processes does \nthis section.\nnot, itself, provide information about the \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 248', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\nAdditional note 1. The goals of transparency,  Illustration—Transparency\ne.g., answering a question as to why A/IS \nIn 2004, the city of Memphis, Tennessee,  \nreached a given decision, may, in some cases, \nwas experiencing an increase in crime rates  \nbe better served by modes of explanation that \nthat exceeded the national average. In response, \ndo not involve examining an algorithm’s terms \nin 2005, the city piloted a predictive policing \nor opening the “black box”. A counterfactual \nprogram known as Blue CRUSH (Crime  \nexplanation taking the form of, for example,  \nReduction Utilizing Statistical History).131  \n“You were denied a loan because your annual \nBlue CRUSH, developed in conjunction with the \nincome was £30,000; if your income had been \nUniversity of Memphis,132 utilizes IBM’s SPSS \n£45,000, you would have been offered a loan,” \npredictive analytics software to identify “hot \nmay provide more insight sooner than the \nspots”: locations and times in which a given type \ndisclosure of an algorithm.130 \nof crime has a greater than average likelihood \nof occurring. The system generates its results \nAdditional note 2. The transparency principle \nthrough the analysis of a range of both historical \nintersects with other principles focused on \ndata (type of crime, location, time of day, day  \nfostering trust. More specifically, we note  \nof week, characteristics of victim, etc.) and live \nthe following:\ndata provided by units on patrol. Equipped with \n•  Transparency and effectiveness.  the predictive crime map generated by the \nInformation about the measurement   system, the Memphis Police Department can \nof effectiveness can foster trust only if it is  allocate resources dynamically to preempt or \ndisclosed, i.e., only if there is transparency  interrupt the target criminal activity. The precise \npertaining to the procedures and results   response the department takes will vary with \nof a measurement exercise.  circumstance: deployment of a visible patrol \n•  Transparency and competence.  car, deployment of an unmarked observer car, \nTransparency is essential in ensuring that  increasing vehicle stops in the area, undercover \nthe competencies required by the human  infiltration of the location, and so on.\noperators of A/IS are known and met.  \nThe pilot program of Blue CRUSH focused on \nAt the same time, questions addressed by \ngang-related gun violence, which had been on the  \ntransparency extend beyond competence, \nrise in Memphis prior to the pilot. The program \nwhile the questions addressed by  \nshowed an improvement, relative to incumbent \ncompetence extend beyond those answered \nmethods, in the interdiction of such violence. \nby transparency.\nBased on the success of the pilot, the scope \n•  Transparency and accountability.  of program was expanded, in 2007, for use \nTransparency is essential in determining  throughout the city. By 2013, the policing  \naccountability, but transparency serves  efforts enabled by Blue CRUSH had helped  \npurposes beyond accountability, while  to reduce overall crime in the city by over 30% \naccountability seeks to answer questions not  and violent crime by 20%.133 The program  \naddressed directly by transparency. \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 249', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\nalso enabled a dramatic increase in the rate   watch meetings to inform the public about \nat which crimes were solved: for cases handled  the technology and how it would be used in \nby the department’s Felony Assault Unit, the  policing their communities.137 Without that level \npercentage of cases solved increased from  of transparency, it is doubtful that Blue CRUSH \n16% to nearly 70%.134 And the program was  would have had the public support needed  \ncost effective: an analysis by Nucleus Research  for its successful deployment.\nfound that the program, when compared to the \nHolding community meetings is an important \nresources required to achieve the same results  \nstep in building trust in a predictive policing \nby traditional means, realized an annual benefit  \nprogram. As such programs become more widely \nof approximately $7.2 million at a cost of just \nimplemented, however, and become more \nunder $400,000.135\nwidely studied, trust may require more than \nThe story of the deployment of Blue CRUSH   town-hall meetings. Research into the programs \nin the metropolitan Memphis area is not just  has raised serious concerns about the ways in \nabout the technology; it is equally about the  which they are implemented and their potential \npolice personnel utilizing the technology and  for perpetuating or even exacerbating historical \nabout the communities in which the technology  bias.138 Addressing these concerns will require \nwas deployed. As noted by former Memphis  more sophisticated and intrusive oversight than \nPolice Department Director Larry Godwin:   can be realized through community meetings.\n“You can have all the technology in the world  \nIncluded among the questions that must be \nbut you’ve got to have leadership, you’ve got  \naddressed are the following.\nto have accountability, you’ve got to have boots \non the streets for it to succeed.”136 Crucial to \n•  In identifying hot spots, does the program \nthe program’s success was public support. Blue \nrely primarily on arrest rates, which reflect \nCRUSH represents a variety of predictive policing \n(potentially biased) police activity, or does  \ntechnology that limits itself to identifying the \nit rely on actual crime rates?\n“where”, the “when”, and the “what” of criminal \n•  What are the specific criteria for identifying  \nactivity; it does not attempt to identify the  \na hot spot and are those criteria free of bias?139\n“who”, and therefore avoids a number of the \nprivacy questions raised by technologies that   •  How accessible are the input data used to \ndo attempt to identify individual perpetrators.   identify hot spots? Are they open to analysis \nThe technology will still, however, prompt  by an independent expert?\nresponses by the police that could include more \n•  What mechanisms for oversight, review, and \nintrusive police activity in identified hot spots.  \nremediation of the program have been put \nThe public must be willing to accept that activity, \nin place? Such oversight should have access \nand that acceptance is won by transparency. \nto the data used to train the system, the \nTo that end, Godwin and Janikowski held \nmodels used to identify hot spots, tests of the \nmore than 200 community and neighborhood \n250 This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License.', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\neffectiveness of the system, and steps taken  develop are sensitive both to the distinctions \nto remediate errors (such as bias) when they  among the types of information that might \nare uncovered. be disclosed and to the distinctions among \ncategories of individuals who may seek \nAs the public becomes more aware of the \ninformation about the design, operation,  \npotential negative impact140 of predictive policing \nand results of a given system. \nprograms, law enforcement agencies hoping  \n3. Policymakers developing frameworks for \nto build trust in such programs will have to  \nrealizing transparency in A/IS to be adopted \nput in place transparency mechanisms that  \nin a legal system should consider the role \ngo beyond town-hall meetings and that enable  \nof appropriate protection for intellectual \na sophisticated response to such questions.\nproperty, but should not allow those concerns \nto be used as a shield to prevent duly limited \nRecommendations\ndisclosure of information needed to ascertain \n1.  Governments and professional associations  whether A/IS meet acceptable standards  \nshould facilitate dialogue among  of effectiveness, fairness, and safety.  \nstakeholders—those engaged in the design,  In developing such frameworks, policymakers \ndevelopment, procurement, deployment,  should make allowance that the level of \noperation, and validation of effectiveness  disclosure warranted will be, to some extent, \nof the technology; those who may be  dependent on what is at stake in a given \nimmediately affected by the results of the  circumstance.\ntechnology; those who may be indirectly \n4. Policymakers developing frameworks for \naffected by the results of the technology, \nrealizing transparency in A/IS to be adopted \nincluding the general public; and those with \nin a legal system should consider the option \nspecialized expertise in ethics, politics, and the \nof creating a role for a specially designated \nlaw—on the question of achieving a balance \n“public interest steward”, or “trusted third \nbetween transparency and other priorities, \nparty”, who would be given access to sensitive \ne.g., security, privacy, appropriate property \ninformation not accessible to others. Such  \nrights, efficient and uniform response by \na public interest steward would be charged \nthe legal system, and more. In developing \nwith assessing the information to answer the \nframeworks for achieving such balance, \npublic interest questions at hand but would be \npolicymakers and professional associations \nunder obligation not to disclose the specifics \nshould make allowance for circumstantial \nof the information accessed in arriving at \nvariation in how competing interests may be \nthose answers.\nreconciled.\n5. Designers of A/IS should design their \n2. Policymakers developing frameworks for \nsystems with a view to meeting transparency \nrealizing transparency in A/IS applied to legal \nrequirements, i.e., so as to enable some \ntasks should require that any frameworks they \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 251', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\ncategories of information about the system  9. Governments should provide whistleblower \nand its performance to be disclosed while  protections to individuals who volunteer  \nenabling other categories, such as intellectual  to offer information in situations where  \nproperty, to be protected. A/IS are not designed as claimed or operated \nas intended, or when their results are not \n6. When negotiating contracts for the provision  \ninterpreted correctly. For example, if a law \nof A/IS products and services for use in the \nenforcement agency is using facial recognition \nlegal system, providers and buyers of A/IS \ntechnology for a purpose that is illegal or \nshould include contractual terms specifying \nunethical, or in a manner other than that in \nwhat categories of information will be \nwhich it is intended to be used, an individual \naccessible to what categories of individuals \nreporting that misuse should be given \nwho may seek information about the design, \nprotection against reprisal. All government \noperation, and results of the A/IS.\nefforts in this regard should be transparent \n7.  In developing frameworks for realizing \nand open to public scrutiny.\ntransparency in A/IS to be adopted in a  \n10. Recommendation 1 under Issue 2, with \nlegal system, policymakers should recognize \nrespect to transparency.\nthat the information provided by other types \nof inquiries, e.g., examination of evidence  11. Recommendation 2 under Issue 2, with \nof effectiveness or of operator competence,  respect to transparency.\nmay in certain circumstances provide a more \nefficient means to informed trust in the  Further Resources\neffectiveness, fairness, and safety of the A/IS  \n•  J. A. Kroll, J. Huey, S. Barocas, E. W. Felten, \nin question.\nJ. R. Reidenberg, D. G. Robinson, and H. \n8. Governments should, where appropriate,  \nYu, “Accountable Algorithms,” University of \nwork together with A/IS developers, as well as \nPennsylvania Law Review, vol. 165, Feb. 2017. \nother stakeholders in the effective functioning \n•  J. A. Kroll, “The fallacy of inscrutability,” \nof the legal system, to facilitate the creation  \nPhilosophical Transactions of the Royal Society \nof error-sharing mechanisms to enable  \nA: Mathematical, Physical, and Engineering \nthe more effective identification, isolation,  \nSciences, vol. 376, no. 2133, Oct. 2018. \nand correction of flaws in broadly deployed  \nA/IS in their legal systems, such as a  •  W. L. Perry, B. McInnis, C. C. Price, S. C. Smith, \nsystematic facial recognition error in policing  and J. S. Hollywood, “Predictive Policing:  \napplications or in risk assessment algorithms.  The Role of Crime Forecasting in Law \nIn developing such mechanisms, the question  Enforcement Operations,” The RAND \nof precisely what information gets shared  Corporation, 2013.\nwith precisely which groups may vary from \n•  A. D. Selbst and S. Barocas, “The Intuitive \napplication to application. All government \nAppeal of Explainable Machines,” Fordham \nefforts in this regard should be transparent \nLaw Review, vol. 87, no. 3, 2018.\nand open to public scrutiny.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 252', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\n•  S. Wachter, B. Mittelstadt, and L. Floridi,   •  R. Wexler, “Life, Liberty, and Trade Secrets: \n“Why a Right to Explanation of Automated  Intellectual Property in the Criminal Justice \nDecision-Making Does Not Exist in the General  System,” Stanford Law Review, vol. 70, no. 5, \nData Protection Regulation,” International   pp. 1342-1429, 2017.\nData Privacy Law, vol. 7, no. 2, pp. 76-99,  \nJune 2017.\n•  S. Wachter, B. Mittelstadt, and C. Russell, \n“Counterfactual Explanations Without Opening \nthe Black Box: Automated Decisions and the \nGDPR,” Harvard Journal of Law & Technology, \nvol. 31, no. 2, 2018.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 253', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\nSection 2: Legal Status of A/IS\nThere has been much discussion about how  beings in terms of their autonomy, ability to \nto legally regulate A/IS-related technologies  perform intellectual tasks, and, in the case of \nand the appropriate legal treatment of systems  some robots, their physical appearance. As some \nthat deploy these technologies. Already, some  types of A/IS begin to display characteristics \nlawmakers are wrestling with the issue of what  resembling those of human actors, some \nstatus to apply to A/IS. Legal “personhood”— governmental entities and private commentators \napplied to humans and certain types of human  have concluded that it is time to examine how \norganizations—is one possible option for framing  legal regimes should categorize and treat various \nsuch legal treatment, but granting that status  types of A/IS, often with an eye toward according \nto A/IS applications raises issues in multiple  A/IS a legal status beyond that of mere property. \ndomains of human interaction. These entities have posited questions such as \nwhether the law should treat such systems as \nlegal persons.141 \nWhile legal personhood is a multifaceted concept, \nIssue the essential feature of “full” legal personhood  \nis the ability to participate autonomously within \nWhat type of legal status  \nthe legal system by having the right to sue \n(or other legal analytical \nand the capacity to be sued in court.142 This \nframework) is appropriate   allows legal “persons” to enter legally binding \nfor A/IS given (i) the legal issues   agreements, take independent action to enforce \ntheir own rights, and be held responsible for \nraised by deployment of such \nviolations of the rights of others.\ntechnologies, and (ii) the desire \nto maximize the benefits of  Conferring such status on A/IS seems initially \nremarkable until consideration is given to the \nA/IS and minimize negative \nlong-standing legal personhood status granted  \nexternalities?\nto corporations, governmental entities, and  \nthe like—none of which are themselves human. \nUnlike these familiar legal entities, however,  \nBackground A/IS are not composed of—or necessarily \ncontrolled by—human beings. Recognizing A/IS  \nThe convergence of A/IS and robotics \nas independent legal entities could therefore \ntechnologies has led to the development of \nlead to abuses of that status, possibly by A/IS \nsystems and devices resembling those of human \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 254', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\nand certainly by the humans and legal entities  these systems. Clarification along these lines \nwho create or operate them, just as human  will encourage more certain development and \nshareholders and agents have abused the  deployment of A/IS and will help clarify lines \ncorporate form.143 A/IS personhood is a   of legal responsibility and liability when A/IS \nsignificant departure from the legal traditions   cause harm. One of the problems of exploiting \nof both common law and civil law.144  the existing status of legal personhood is that \ninternational treaties may bind multiple countries \nCurrent legal frameworks provide a number  \nto follow the lead of a single legislature, as in \nof categories of legal status, other than full legal \nthe EU, making it impossible for a single country \npersonhood, that could be used as analogues  \nto experiment with the legal and economic \nfor the legal treatment of A/IS and how to \nconsequences of such a strategy.\nallocate legal responsibility for harm caused  \nby A/IS. At one extreme, legal systems could   Recognizing A/IS as independent legal \ntreat A/IS as mere products, tools, or other form  persons would limit or eliminate some human \nof personal or intellectual property, and therefore  responsibility for subsequent decisions made \nsubject to the applicable regimes of property  by such A/IS. For example, under a theory of \nlaw. Such treatment would have the benefit of  intervening causation, a hammer manufacturer \nsimplifying allocation of responsibility for harm.   is not held responsible when a burglar uses \nIt would, however, not account for the fact that  a hammer to break the window of a house. \nA/IS, unlike other forms of property, may be  However, if similar “relief” from responsibility was \ncapable of making legally significant decisions  available to the designers, developers, and users \nautonomously. In addition, if A/IS are to be  of A/IS, it will potentially reduce their incentives \ntreated as a form of property, governments  to ensure the safety of A/IS they design and \nand courts would have to establish rules  use. In this example, legal issues that are applied \nregarding ownership, possession, and use by  in similar chain of causation settings—such as \nthird parties. Other legal analogues may include  foreseeability, complicity, reasonable care, strict \nthe treatment of pets, livestock, wild animals,  liability for unreasonably dangerous goods, and \nchildren, prisoners, and the legal principles of  other precedential notions—will factor into the \nagency, guardianship, and powers of attorney.145  design process. Different jurisdictions may reach \nOr perhaps A/IS are something entirely without  different conclusions about the nature of such \nprecedent, raising the question of whether one   causation chains, inviting future creative legal \nor more types of A/IS might be assigned a hybrid,  planners to consider how and where to pursue \nintermediate, or novel type of legal status? design, development, and deployment of future \nA/IS in order to receive the most beneficial  \nClarifying the legal status of A/IS in one or \nlegal treatment. \nmore jurisdictions is essential in removing the \nuncertainty associated with the obligations and  The legal status of A/IS thus intertwines with \nexpectations for organization and operation of  broader legal questions regarding how to ensure \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 255', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\naccountability and assign and allocate liability  Therefore, even absent the consideration of \nwhen A/IS cause harm. The question of legal  any negative ramifications from personhood \npersonhood for A/IS, in particular, also interacts  status, it would be unwise to accord such \nwith broader ethical and practical questions   status to A/IS at this time.\non the extent to which A/IS should be treated \n2. In determining what legal status, including \nas moral agents independent from their human \ngranting A/IS legal rights short of full legal \ndesigners and operators, whether recognition  \npersonhood, to accord to A/IS, government \nof A/IS personhood would enhance or detract \nand industry stakeholders alike should:  \nfrom the purposes for which humans created  \n(1) identify the types of decisions and \nthe A/IS in the first place, and whether A/IS  \noperations that should never be delegated \npersonhood facilitates of debilitates the \nto A/IS; and (2) determine what rules and \nwidespread benefits of A/IS.\nstandards will most effectively ensure human \ncontrol over those decisions.\nSome assert that because A/IS are at a very \nearly stage of development, it is premature to  3. Governments and courts should review \nchoose a particular legal status or presumption  various potential legal models—including \nin the many forms and settings in which those  agency, animal law, and the other analogues \nsystems are and will be deployed. However,  discussed in this section—and assess  \nthoughtfully establishing a legal status early in the  whether they could serve as a proper basis  \ndevelopment could also provide crucial guidance  for assigning and apportioning legal rights  \nto researchers, programmers, and developers.  and responsibilities with respect to the \nThis uncertainty about legal status, coupled with   deployment and use of A/IS.\nthe fact that multiple legal jurisdictions are already  \n4. In addition, governments should scrutinize \ndeploying A/IS—and each of them, as a sovereign \nexisting laws—especially those governing \nentity, can regulate A/IS as it sees fit—suggests \nbusiness organizations—for mechanisms  \nthat there are multiple general frameworks that \nthat could allow A/IS to have legal autonomy. \ncan and should be considered when assessing \nIf ambiguities or loopholes create a legal \nthe legal status of A/IS.\nmethod for recognizing A/IS personhood, the \ngovernment should review and, if appropriate, \nRecommendations amend the pertinent laws.\n1.  While conferring full legal personhood on   5. Manufacturers and operators should learn \nA/IS might bring some economic benefits,   how each jurisdiction would categorize a \nthe technology has not yet developed to   given autonomous and/or intelligent system \nthe point where it would be legally or morally  and how each jurisdiction would treat harm \nappropriate to generally accord A/IS the rights  caused by the system. Manufacturers and \nand responsibilities inherent in the legal  operators should be required to comply \ndefinition of personhood as it is now defined.  with the applicable laws of all jurisdictions in \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 256', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\nwhich that system could operate. In addition,  regulators, representatives of civil society, \nmanufacturers and operators should be  industry actors, and other stakeholders to \naware of standards of performance and  ensure that the interest of humanity—and \nmeasurement promulgated by standards  not the interests of the autonomous systems \ndevelopment organizations and agencies. themselves—remains the guiding principle.\n6. Stakeholders should be attentive to \nfuture developments that could warrant  Further Resources\nreconsideration of the legal status of A/IS. \n•  S. Bayern. “The Implications of Modern \nFor example, if A/IS were developed that \nBusiness-Entity Law for the Regulation of \ndisplayed self-awareness and consciousness, \nAutonomous Systems.” Stanford Technology \nit may be appropriate to revisit the issue \nLaw Review 19, no. 1, pp. 93-112, 2015.\nof whether they deserve a legal status on \n•  S. Bayern, et al., “Company Law and \npar with humans. Likewise, if legal systems \nAutonomous Systems: A Blueprint for Lawyers, \nunderwent radical changes such that human \nEntrepreneurs, and Regulators.” Hastings \nrights and dignity no longer represented the \nScience and Technology Law Journal, vol. 9, \nprimary guiding principle, the concept of full \nno. 2, pp. 135-162, 2017.\npersonhood for artificial entities may not \nrepresent the radical departure it might today.  •  D. Bhattacharyya. “Being, River: The Law,  \nIf the development of A/IS were to go in the  the Person and the Unthinkable.” Humanities \nopposite direction, and mechanisms were  and Social Sciences Online, April 26, 2017.\nintroduced allowing humans to control and \n•  B. A. Garner, Black’s Law Dictionary, 10th \npredict the actions of A/IS easily and reliably, \nEdition, Thomas West, 2014.\nthen the dangers of A/IS personhood would \n•  J. Bryson, et al., “Of, for, and by the people: \nnot be any greater than for well-established \nthe legal lacuna of synthetic persons,” Artificial \nlegal entities, such as corporations.\nIntelligence Law 25, pp. 273-91, 2017.\n7.  In considering whether to accord or expand \n•  D. J. Calverley, “Android Science and Animal \nlegal protections, rights, and responsibilities \nRights, Does an Analogy Exist? ” Connection \nto A/IS, governments should exercise \nScience 18, no. 4, pp. 403-417, 2006.\nutmost caution. Before according full legal \npersonhood or a comparable legal status   •  D. J. Calverley, “Imagining a Non-Biological \non A/IS, governments and courts should  Machine as a Legal Person.” AI & Society 22, \ncarefully consider whether doing so might   pp. 403-417, 2008.\nlimit how widely spread the benefits of A/IS  •  R. Chatila, “Inclusion of Humanoid Robots in \nare or could be, as well as whether doing   Human Society: Ethical Issues,” in Springer \nso would harm human dignity and uniqueness  Humanoid Robotics: A Reference, A. Goswami \nof human identity. Governments and decision- and P. Vadakkepat, Eds., Springer 2018.\nmakers at every level must work closely with \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 257', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\n•  European Parliament Resolution of 16 February   •  M. U. Scherer, “Is Legal Personhood for AI \n2017 (2015/2103(INL) with recommendations  Already Possible Under Current United States \nto the Commission on Civil Law Rules on  Laws?” Law and AI, May 14, 2017.\nRobotics, 2017.\n•  L. B. Solum. “Legal Personhood for Artificial \n•  L. M. LoPucki, “Algorithmic Entities”,   Intelligences.” North Carolina Law Review 70, \n95 Washington University Law Review 887,  no. 4, pp. 1231–1287, 1992.\n2018.\n•  J. F. Weaver. Robots Are People Too: How  \n•  J. S. Nelson, “Paper Dragon Thieves.”  Siri, Google Car, and Artificial Intelligence  \nGeorgetown Law Journal 105, pp. 871-941,  Will Force Us to Change Our Laws. Santa \n2017. Barbara, CA: Praeger, 2013.\n•  M. U. Scherer, “Of Wild Beasts and Digital  •  L. Zyga. “Incident of drunk man kicking \nAnalogues: The Legal Status of Autonomous  humanoid robot raises legal questions,” \nSystems.” Nevada Law Journal 19, forthcoming  Techxplore, October 2, 2015. \n2018.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 258', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\nThanks to the Contributors \nWe wish to acknowledge all of the people who  • Chandramauli Chaudhuri – Senior Data\ncontributed to this chapter. Scientist; Fractal Analytics\n• Danielle Keats Citron – Lois K. Macht\nThe Law Committee Research Professor & Professor of Law,\nUniversity of Maryland Carey School of Law\n• John Casey (Co-Chair) – Attorney-at-Law, \nCorporate, Wilson Sonsini Goodrich & • Fernando Delgado – PhD Student,\nRosati, P.C. Information Science, Cornell University.\n• Nicolas Economou (Co-Chair) – Chief  • Deven Desai – Associate Professor of Law\nExecutive Officer, H5; Chair, Science, Law and  and Ethics, Georgia Institute of Technology,\nSociety Initiative at The Future Society; Chair,  Scheller College of Business\nLaw Committee, Global Governance\n• Julien Durand – International Technology\nof AI Roundtable; Member, Council on \nLawyer; Executive Director Compliance\nExtended Intelligence\n& Ethics, Amgen Biotechnology\n• Aden Allen – Senior Associate, Patent \n• Todd Elmer, JD – Member of the Board of\nLitigation, Wilson Sonsini Goodrich &\nDirectors, National Science and Technology\nRosati, P.C.\nMedals Foundation\n• Miles Brundage – Research Scientist\n• Kay Firth-Butterfield – Project Head, AI\n(Policy), OpenAI; Research Associate, Future of \nand Machine Learning at the World Economic\nHumanity Institute, University of Oxford; PhD \nForum. Founding Advocate of AI-Global;\ncandidate, Human and Social Dimensions of \nSenior Fellow and Distinguished Scholar,\nScience and Technology, Arizona State \nRobert S. Strauss Center for International\nUniversity\nSecurity and Law, University of Texas, Austin;\n• Thomas Burri – Assistant Professor Co-Founder, Consortium for Law and Ethics\nof International Law and European Law,  of Artificial Intelligence and Robotics, University\nUniversity of St. Gallen (HSG), Switzerland of Texas, Austin; Partner, Cognitive Finance\n• Ryan Calo – Assistant Professor of Law, the  Group, London, U.K.\nSchool of Law at the University of Washington\n• Tom D. Grant – Fellow, Wolfson College;\n• Clemens Canel – Referendar (Trainee  Senior Associate of the Lauterpacht\nLawyer) at Hanseatisches Oberlandesgericht,  Centre for International Law, University\ngraduate of the University of Texas School of Cambridge, U.K.\nof Law and Bucerius Law School\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 259', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\n•  Cordel Green – Attorney-at-Law; Executive  •  Mark Lyon – Attorney-at-Law, Chair, Artificial \nDirector, Broadcasting Commission—Jamaica Intelligence and Autonomous Systems Practice \nGroup at Gibson, Dunn & Crutcher LLP\n•  Maura R. Grossman – Research Professor, \nDavid R. Cheriton School of Computer  •  Gary Marchant – Regents’ Professor of Law, \nScience, University of Waterloo; Adjunct  Lincoln Professor of Emerging Technologies, \nProfessor, Osgoode Hall Law School,   Law and Ethics, Arizona State University \nYork University\n•  Nicolas Miailhe – Co-Founder & \n•  Bruce Hedin – Principal Scientist, H5 President, The Future Society; Member,  \nAI Expert Group at the OECD; Member,  \n•  Daniel Hinkle – Senior State Affairs Counsel \nGlobal Council on Extended Intelligence; \nfor the American Association for Justice\nSenior Visiting Research Fellow, Program  \n•  Derek Jinks – Marrs McLean Professor in \non Science Technology and Society at  \nLaw, University of Texas Law School; Director, \nHarvard Kennedy School. Lecturer, Paris \nConsortium on Law and Ethics of Artificial \nSchool of International Affairs (Sciences Po); \nIntelligence and Robotics (CLEAR), Robert  \nVisiting Professor, IE School of Global and \nS. Strauss Center for International Security  \nPublic Affairs \nand Law, University of Texas. \n•  Paul Moseley – Master’s student, Electrical \n•  Nicolas Jupillat – Adjunct Professor, \nEngineering, Southern Methodist University; \nUniversity of Detroit Mercy School of Law\ngraduate of the University of Texas School  \n•  Marwan Kawadri – Analyst, Founders  of Law\nIntelligence; Research Associate,  \n•  Florian Ostmann – Policy Fellow, The Alan \nThe Future Society. \nTuring Institute\n•  Mauricio K. Kimura – Lawyer; PhD  \n•  Pedro Pavón – Assistant General Counsel, \nstudent, Faculty of Law, University of Waikato, \nGlobal Data Protection, Honeywell\nNew Zealand; LLM from George Washington \n•  Josephine Png – AI Policy Researcher and \nUniversity, Washington DC, USA; Bachelor  \nDeputy Project Manager, The Future Society; \nof Laws from Sao Bernardo do Campo  \nbudding barrister; and BA Chinese and Law, \nSchool of Law, Brazil\nSchool of Oriental and African Studies\n•  Irene Kitsara – Lawyer; IP Information \n•  Matthew Scherer – Attorney at Littler \nOfficer, Access to Information and Knowledge \nMendelson, P.C., and legal scholar based in \nDivision, World Intellectual Property \nPortland, Oregon, USA; Editor, LawAndAI.com\nOrganization, Switzerland\n•  Bardo Schettini Gherardini –  \n•  Timothy Lau, J.D., Sc.D. – Research \nIndependent Legal Advisor on standardization, \nAssociate, Federal Judicial Center \nAI and robotics\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 260', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\n•  Jason Tashea – Founder, Justice Codes   •  Julius Weitzdörfer – Affiliated Lecturer, \nand adjunct law professor at Georgetown   Faculty of Law, University of Cambridge; \nLaw Center Research Associate, Centre for the Study  \nof Existential Risk, University of Cambridge\n•  Yan Tougas – Global Ethics & Compliance \nOfficer, United Technologies Corporation;  •  Yueh-Hsuan Weng – Assistant \nAdjunct Professor, Law & Ethics, University   Professor, Frontier Research Institute for \nof Connecticut School of Business; Fellow,  Interdisciplinary Sciences (FRIS), Tohoku \nEthics & Compliance Initiative; Kallman  University; Fellow, Transatlantic Technology \nExecutive Fellow, Bentley University Hoffman  Law Forum (TTLF), Stanford Law School\nCenter for Business Ethics \n•  Andrew Woods – Associate Professor  \n•  Sandra Wachter – Lawyer and Research  of Law, University of Arizona\nFellow in Data Ethics, AI and Robotics,  \nFor a full listing of all IEEE Global Initiative \nOxford Internet Institute, University of Oxford\nMembers, visit standards.ieee.org/content/dam/\n•  Axel Walz – Lawyer; Senior Research Fellow \nieee-standards/standards/web/documents/\nat the Max Planck Institute for Innovation \nother/ec_bios.pdf. \nand Competition, Germany. (Member until \nOctober 31, 2018) For information on disclaimers associated with \nEAD1e, see How the Document Was Prepared.\n•  John Frank Weaver – Lawyer, McLane \nMiddleton, P.A; Columnist for and Member \nof Board of Editors of Journal of Robotics, \nArtificial Intelligence & Law; Contributing Writer  \nfor Slate; Author, Robots Are People Too \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 261', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\nThe Law Committee of the IEEE Global Initiative  •  Gallia Daor, Policy Analyst, OECD.\non Ethics of Autonomous and Intelligent Systems \n•  Lydia de la Torre, Privacy Law Fellow, Santa \nwould like to thank the following individuals for \nClara University.\ntaking the time to offer valuable feedback and \n•  Isabela Ferrari, Federal Judge, Federal Court, \nsuggestions on Section 1 of the Law Chapter, \nRio de Janeiro, Brazil.\n“Norms for the Trustworthy Adoption of A/IS \nin Legal Systems”. Each of these contributors  •  Albert Fox Cahn, Founder and Executive \noffered comments in an individual capacity, not  Director, Surveillance Technology Oversight \nin the name of the organization for which they  Project; former Legal Director, CAIR-NY.\nwork. The final version of the Section does not \n•  Paul W. Grimm, United States District Judge, \nnecessarily incorporate all comments or reflect \nUnited States District Court for the District of \nthe views of each contributor.\nMaryland.\n•  Rediet Abebe, PhD Candidate, Department  •  Gillian Hadfield, Professor of Law and \nof Computer Science, Cornell University;  Professor of Strategic Management, University \ncofounder, Mechanism Design for Social  of Toronto; Member, World Economic Forum \nGood; cofounder, Black in AI. Future Council for Agile Governance.  \n•  Ifeoma Ajunwa, Assistant Professor, Labor &  •  Sheila Jasanoff, Pforzheimer Professor of \nEmployment Law, Cornell Industrial and Labor  Science and Technology Studies, Harvard \nRelations School; faculty Associate at Harvard  Kennedy School of Government.\nLaw, Berkman Klein Center.\n•  Baroness Beeban Kidron, OBE, Member, \n•  Jason R. Baron, of counsel, Drinker Biddle;  United Kingdom House of Lords.\nco-chair, Information Governance Initiative; \n•  Eva Kaili, Member, European Parliament; \nformer Director of Litigation, United States \nChair, European Parliament Science and \nNational Archives and Records Administration.\nTechnology Options Assessment body (STOA).\n•  Irakli Beridze, Head, Centre for Artificial \n•  Mantalena Kaili, cofounder, European Law \nIntelligence and Robotics, United Nations \nObservatory on New Technologies.\n(UNICRI).\n•  Jon Kleinberg, Tisch University Professor, \n•  Juan Carlos Botero, Law Professor, Pontificia \nDepartments of Computer Science and \nUniversidad Javeriana, Bogota; former \nInformation Science, Cornell University; \nExecutive Director, World Justice Project. \nmember of the National Academy of Sciences, \n•  Anne Carblanc, Principal Administrator,  the National Academy of Engineering, and the \nInformation, Communications and Consumer  American Academy of Arts and Sciences.\nPolicy (ICCP) Division, Directorate for \n•  Shuang Lu Frost, Teaching Fellow, PhD \nScience, Technology and Industry, OECD; \ncandidate, Department of Anthropology, \nformer criminal investigations judge (juge \nHarvard University.\nd’instruction), Tribunal of Paris.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 262', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\n•  Arthur R. Miller CBE, University Professor,  •  George Socha, Esq., Consulting Managing \nNew York University; former Bruce Bromley  Director, BDO USA; co-founder, Electronic \nProfessor of Law, Harvard Law School. Discovery Reference Model (EDRM) and \nInformation Governance Reference Model \n•  Manuel Muñiz, Dean and Rafael del Pino \n(IGRM).\nProfessor of Practice of Global Leadership, IE \nSchool of Global and Public Affairs, Madrid;  •  Lee Tiedrich, Partner, IP/Technology \nSenior Associate, Belfer Center, Harvard  Transactions, and Co-Chair, Artificial \nUniversity. Intelligence Initiative, Covington & Burling LLP.\n•  Erik Navarro Wolkart, Federal Judge,  •  Darrell M. West, VP, Governance Studies, \nFederal Court, Rio de Janeiro, Brazil. Director, Center for Technology Innovation, \nDouglas Dillon Chair in Governance Studies, \n•  Aileen Nielsen, chair, Science and Law \nBrookings Institution.\nCommittee, New York City Bar Association.\n•  Bendert Zevenbergen, Research Fellow, \n•  Michael Philips, Assistant General Counsel, \nCenter for Information Technology Policy, \nMicrosoft.\nPrinceton University; Researcher, Oxford \n•  Dinah PoKempner, General Counsel, Human \nInternet Institute.\nRights Watch.\n•  Jiyu Zhang, Associate Professor and \n•  Irina Raicu, Director, Internet Ethics Program, \nExecutive Director of the Law and Technology \nMarkkula Center for Applied Ethics, Santa Clara \nInstitute, Renmin University of China School of \nUniversity.\nLaw.\n•  David Robinson, Visiting Scientist, AI Policy \n•  Peter Zimroth, Director, New York University \nand Practice Initiative, Cornell University; \nCenter on Civil Justice; retired partner, Arnold \nAdjunct Professor of Law, Georgetown \n& Porter; former Assistant US Attorney, \nUniversity Law Center; Managing Director (on \nSouthern District of New York.\nleave), Upturn.\n•  Alanna Rutherford, Vice President, Global \nLitigation & Competition, Visa.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 263', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\nEndnotes\n1  See S. Jasanoff, “Governing Innovation: The  and professional competencies in computer \nSocial Contract and the Democratic Imagination,”  science, linguistics, data science, statistics, and \nSeminar, vol. 597, pp. 16-25, May 2009. related technical fields. Lawyers, judges, and \nlaw enforcement officers increasingly draw on \n2  As articulated in EAD General Principles 1 \nthese fields, directly or indirectly, as A/IS are \n(Human Rights), 2 (Well-Being), and 3 (Data \nprogressively adopted in the legal system. This \nAgency). See also EAD Chapter, “Classical Ethics \ndocument does not seek to offer legal advice \nin A/IS,” In applying A/IS in pursuit of these \nto lawyers, courts, or law enforcement agencies \ngoals, tradeoffs are inevitable. Some applications \non how to practice their professions or enforce \nof predictive policing, for example, may reduce \nthe law in their jurisdictions around the globe. \ncrime, and so enhance well-being, but may do \nInstead, it seeks to help ensure that A/IS and \nso at the cost of impinging on a right to privacy \ntheir operators in a given legal system can be \nor weakening protections against unwarranted \ntrusted by lawyers, courts, and law enforcement \nsearch and seizure. How these tradeoffs are \nagencies, and civil society at large, to perform \nnegotiated may vary with cultural and legal \neffectively and safely. Such effective and safe \ntraditions.\noperation of A/IS holds the potential of producing \nsubstantial benefits for the legal system, while \n3  Risks and benefits, and their perception, \nprotecting all of its participants from the ethical, \nare neither always well-defined at the outset \nprofessional, and business risks, or personal \nnor static over time. Social expectations and \njeopardy, that may result from the intentional, \neven ideas of lawfulness constantly evolve. For \nunintentional, uninformed, or incompetent \nexample, if younger generations, accustomed \nprocurement and operation of artificial \nto the use of social networking technologies, \nintelligence. \nhave lower expectations of privacy than older \ngenerations, should this be deemed to be a \n5  See Rensselaer Polytechnic Institute,  \nbenefit to society, a risk, or neither? \n“A Conversation with Chief Justice John G. \nRoberts, Jr.,” April 11, 2017. YouTube video, 40:12. \n4  Regarding the nature of the guidance provided \nApril 12, 2017. [Online]. Available: https://www.\nin this section: Artificial intelligence, like many \nyoutube.com/watch?v=TuZEKlRgDEg.\nother domains relied on by the legal realm  \n(e.g., medical and accounting forensics, ballistics, \n6  “Uninformed avoidance of adoption” can  \nor economic analysis), is a scientific discipline \nbe one of two types: (a) avoidance of adoption \ndistinct from the law. Its effective and safe design \nwhen the information needed to enable sound \nand operation have underpinnings in academic \ndecisions is available but is not taken into \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 264', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\nconsideration, and (b) avoidance of adoption  enable enforcement (through private contracts, \nwhen the information needed to enable sound  professional codes of practice, or legislation). \ndecisions is simply not available. Unlike the former   Finally, those enforceable standards render \ntype of avoidance, the latter type is a prudent  the performance of some activities sufficiently \nand well-reasoned avoidance of adoption  reliable and predictable to enable trustworthy \nand, pending better information, is the course  operation at the scale of society. Where these \nrecommended by a number experts and  elements (rulemaking, enforcement, scalable \nnonexperts. operation) are present, new institutions are born.\n7  For purposes of this chapter, we have made  9  For a discussion of the definition of A/IS, \nthe deliberate choice to focus on these four  see the Terminology Update in the Executive \nprinciples without taking a prior position on  Summary of EAD. The principles outlined in this \nwhere the deployment of A/IS may or may not  section as constitutive of “informed trust” do  \nbe acceptable in legal systems. Where these  not depend on a precise, consensus definition \nprinciples cannot be adequately operationalized,  of A/IS and are, in fact, designed to be enable \nit would follow that the deployment of A/IS in   successful operationalization under a broad  \na legal system cannot be trusted. Where A/IS   range of definitions.\ncan be evidenced to meet desired thresholds \n10  Such as Gross Domestic Product (GDP),  \nfor each duly operationalized principle, it would \nGross National Income (GNI) per capita, the  \nfollow that their deployment can be trusted.  \nWEF Global Competitiveness Index, and others.\nSuch information is intended to facilitate, not \npreempt, the indispensable public policy dialogue \n11  Such as life expectancy, infant mortality rate, \non the extent to which A/IS should be relied \nand literacy rate, as well as composite indices \nupon to meet the specific needs of the legal \nsuch as the Human Development Index, the \nsystems of societies around the world.\nInequality-Adjusted Human Development Index, \nthe OECD Framework for Measuring Well-being \n8  It is beyond the scope of this chapter \nand Progress, and others. For more on measures \nto discuss the process through which such \nof well-being, see the EAD chapter on “Well-being”.\nadherence may become institutionalized in \nthe complex legal, technological, political, \n12  See United Nation General Assembly, \nand cultural dynamics in which sociotechnical \nUniversal Declaration of Human Rights, Dec. 10, \ninnovation occurs. It is worth noting, however, \n1948, available: http://www.un.org/en/universal-\nthat this process typically involves four steps. \ndeclaration-human-rights/index.html; see also \nFirst, a wide range of market and culture-\nUnited Nations Office of the High Commissioner: \ndriven practices emerge. Second, a set of best \nHuman Rights, The Vienna Declaration and \npractices arises, reflecting a group’s willingness \nProgramme of Action, June 25, 1993, available: \nto adopt certain rules. Third, some of these best \nhttps://www.ohchr.org/en/professionalinterest/\npractices are formulated into standards, which \npages/vienna.aspx.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 265', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\n13  See UNICEF, Convention on the Rights of  18  A component of human dignity is privacy, and \nthe Child, Nov. 4, 2014, available: https://www. a component of privacy is protection and control \nunicef.org/crc/index_30160.html. of one’s data; in this regard, frameworks such \nas the EU’s General Data Protection Regulation \n14  See United Nations Security Council,  \n(GDPR) and the Council of Europe’s “Guidelines \n“The Rule of Law and Transitional Justice in \non the protection of individuals with regard to  \nConflict and Post-conflict Societies: Report of the \nthe processing of personal data in a world of  \nSecretary General,” Report S/2004/616 (2004).\nBig Data” have a role to play in setting standards \nfor how legal systems can protect data privacy. \n15  See The World Economic Forum, The Global \nSee also EAD General Principle 3 (Data Agency).\nCompetitiveness Report: 2018, ed. K. Schwab \n(2018), pp. 12ff.\n19  Frameworks such as the Universal Declaration \nof Human Rights and the Vienna Declaration  \n16  See A. Brunetti, G. Kisunko, and B. Weder, \nand Programme of Action (VDPA) have a role  \n“Credibility of Rules and Economic Growth: \nto play in articulating human-rights standards  \nEvidence from a Worldwide Survey of the Private \nto which legal systems should adhere. See also \nSector,” The World Bank Economic Review, \nEAD General Principle 1 (Human Rights).\nvol. 12, no. 3, pp. 353–384, 1998. Available: \nhttps://doi.org/10.1093/wber/12.3.353; see also \n20  For more on the importance of measures \nWorld Bank, World Development Report 2017: \nof well-being beyond GDP, see EAD General \nGovernance and the Law, Jan. 2017. Available: \nPrinciple 2 (Well-being).\ndoi.org/10.1596/978-1-4648-0950-7.\n21  For a conceptual framework enabling the \n17  The question of intellectual property law in  \ncountry-by-country assessment of the Rule of \nan era of rapidly advancing technology (both  \nLaw, see World Justice Project, Rule of Law Index. \nA/IS and other technologies) is a complex and \n2018. url: https://worldjusticeproject.org/sites/\noften contentious one involving legal, economic, \ndefault/files/documents/WJP-ROLI-2018-June-\nand ethical considerations. We have not yet \nOnline-Edition_0.pdf. \nstudied the question in sufficient depth to reach  \na consensus on the issues raised. We may  22  See D. Kennedy, “The ‘Rule of Law,’ Political \nexamine the issues in depth in a future version  Choices and Development Common Sense,”  \nof EAD. For a forum in which such issues are  in The New Law and Economic Development:  \ndiscussed, see the Berkeley-Stanford Advanced  A Critical Appraisal, D. M. Trubek and A. Santos,  \nPatent Law Institute. See also The World  Ed. Cambridge: Cambridge University Press,  \nEconomic Forum, “Artificial Intelligence Collides  2006, pp. 156-157; see also A. Sen, Development  \nwith Patent Law.” April 2018. Available: http:// as Freedom. New York: Alfred A. Knopf, 1999.\nwww3.weforum.org/docs/WEF_48540_WP_End_\nof_Innovation_Protecting_Patent_Law.pdf.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 266', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\n23  See Kennedy (2006): pp. 168-169. “The idea  27  Studies conducted by the US National Institute \nthat building ‘the rule of law’ might itself be a  of Standards and Technology (NIST) between \ndevelopment strategy encourages the hope that  2006 and 2011, known as the US NIST Text \nchoosing law in general could substitute for all  REtrieval Conference (TREC) Legal Track, suggest \nthe perplexing political and economic choices  that some A/IS-enabled processes, if operated \nthat have been at the center of development  by trained experts in the relevant scientific \npolicy making for half a century. The politics of  fields, can be more effective (or accurate) than \nallocation is submerged. Although a legal regime  human attorneys in correctly identifying case-\noffers an arena to contest those choices, it cannot  relevant information in large data sets. NIST has \nsubstitute for them.”  a long-standing reputation for cultivating trust in \ntechnology by participating in the development \n24  Fairness (as well as bias) can be defined \nof standards and metrics that strengthen \nin more than one way. For purposes of this \nmeasurement science and make technology \nchapter, a commitment is not made to any one \nmore secure, usable, interoperable, and reliable. \ndefinition—and indeed, it may not be either \nThis work is critical in the A/IS space to ensure \ndesirable or feasible to arrive at a single definition \npublic trust of rapidly evolving technologies so \nthat would be applied in all circumstances. \nthat we can benefit from all that this field has  \nThe trust principles proposed in the chapter \nto promise.\n(Effectiveness, Competence, Accountability, and \nTransparency) are defined such that they will  28  In describing the potential A/IS have for  \nprovide information that will allow the testing of  aiding in the auditing of decisions made in \nan application of A/IS against any fairness criteria. the civil and criminal justice systems, we are \nenvisioning them acting as aids to a competent \n25  The confidentiality of jury deliberations, certain \nhuman auditor (see Issue 4) in the context of \nsensitive cases, and personal data are some  \ninternal or judicial review.\nof the considerations that influence the extent \nof appropriate public examination and oversight  29  Of course, the use of A/IS in improving the \nmechanisms. effectiveness of law enforcement may raise \nconcerns about other aspects of well-being,  \n26  The avoidance of negative consequences  \nsuch as privacy and the rise of the surveillance \nis important to note in relation to effectiveness. \nstate, cf. Minority Report (2002). If A/IS are to  \nThe law can be used for malevolent or intensely \nbe used for law enforcement, steps must be \ndisputed purposes (for example, the quashing  \ntaken to ensure that they are used, and that \nof dissent or mass incarceration). The instruments  \ncitizens trust that they will be used, in ways that \nof the law, including A/IS, can render the \nare conducive to ethical law enforcement and \nadvancement of such purposes more effective  \nindividual well-being (see Issue 2).\nto the detriment of democratic values, human \nrights, and human well-being.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 267', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\n30  A/IS may also provide assistance in carrying  vol. 18, no. 5, pp. 75-76, 2016. Available: https://\nout legal tasks associated with larger transactions,  scholarship.law.unc.edu/ncjolt/vol18/iss5/3/. \nsuch as evaluating contracts for risk in connection \n34  An example of an initiative that seeks to bridge \nwith a M&A transaction or reporting exposure  \nthe gap between technical and legal expertise  \nto regulators.\nis the Artificial Intelligence Legal Challenge, held \n31  The recommendations provided in this chapter  at Ryerson University and sponsored by Canada’s \n(both under this issue and under the other issues  Ministry of the Attorney General: http://www.\ndiscussed in the chapter) are intended to give  legalinnovationzone.ca/press_release/ryersons-\ngeneral guidance as to how those with a stake in  legal-innovation-zone-announces-winners-of-ai-\nthe just and effective operation of a legal system  legal-challenge/. \ncan develop norms for the trustworthy adoption \n35  And, in addressing the challenges, \nof A/IS in the legal system. The specific ways in \nconsideration must be given to existing modes  \nwhich the recommendations are operationalized \nof proposing and approving innovation in the \nwill vary from society to society and from \nlegal system. Trust in A/IS will be undermined  \njurisdiction to jurisdiction.\nif they are viewed as not having been vetted via \n32  See “Global Governance of AI Roundtable:  established processes.\nSummary Report 2018,” World Government \n36  For an overview of risk and risk management, \nSummit, 2018: p. 32. Available: https://www.\nsee Working Party on Security and Privacy in \nworldgovernmentsummit.org/api/publications/\nthe Digital Economy, Background Report for \ndocument?id=ff6c88c5-e97c-6578-b2f8-\nMinisterial Panel 3.2, Directorate for Science, \nff0000a7ddb6. (The February 2018 Dubai Global \nTechnology and Innovation, Committee on Digital \nGovernance of AI Roundtable brought together \nEconomy Policy, Managing Digital Security and \nninety leading thinkers on AI governance.)\nPrivacy Risk, OECD, June 1, 2016; see p. 5. \n33  See State v Loomis, 881 N.W.2d 749 (Wis. \n37  It is worth emphasizing the “informed” \n2016), cert. denied (2017); see also “Criminal \nqualifier we attach to trust here. Far from \nLaw—Sentencing Guidelines—Wisconsin Supreme \nadvocating for a “blind trust” in A/IS, we argue \nCourt Requires Warning Before Use of Algorithmic \nthat A/IS should be adopted only when we have \nRisk Assessments in Sentencing—State v. Loomis, \nsound evidence of their effectiveness, when we \n881 N.W.2d 749 (Wis. 2016),” Harvard Law \ncan be confident of the competence of their \nReview, vol. 130, no. 5, pp. 1535-1536, 2017. \noperators, when we have assurances that these \nAvailable: http://harvardlawreview.org/wp-\nsystems allow for the attribution of responsibility \ncontent/uploads/2017/03/1530-1537_online.\nfor outcomes (both positive and negative), and \npdf; see also K. Freeman, “Algorithmic Injustice: \nwhen we have clear views into their operation. \nHow the Wisconsin Supreme Court Failed to \nWithout those conditions, we would argue that  \nProtect Due Process Rights in State v. Loomis,” \nA/IS should not be adopted in the legal system.\nNorth Carolina Journal of Law and Technology, \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 268', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\n38  The importance of testing the effectiveness  system and facilitating access to justice. As part  \nof advanced technologies, including A/IS, in  of interactive technology solutions, appropriate \nthe legal system (and beyond) is not new: it  data standards may help connect the ordinary \nwas highlighted by Judge Paul W. Grimm in an  citizen to the appropriate resources and \nimportant early ruling on legal fact-finding, Victor  information for his or her legal needs. For a \nStanley v. Creative Pipe, Inc., 250 F.R.D. 251, 257  discussion of open data standards in the context \n(D. Md. 2008), followed, among others, by the  of the US court system, see D. Colarusso and  \ninfluential research and educational institute The  E. J. Rickard, “Speaking the Same Language:  \nSedona Conference as well as the International  Data Standards and Disruptive Technologies in \nOrganization for Standardization (ISO). See An  the Administration of Justice,” Suffolk University \nOpen Letter to Law Firms and Companies in  Law Review, vol. L387, 2017.\nthe Legal Tech Sector, The Sedona Conference \n41  For measurement of bias in facial recognition \n(2009), and Commentary on Achieving Quality  \nsoftware, see C. Garvie, A. M. Bedoya, and J. \nin the E-Discovery Process (2013): 7; ISO \nFrankle, “The Perpetual Line-Up: Unregulated \nstandard on electronic discovery (ISO/IEC 27050-\nPolice Face Recognition in America,” Georgetown \n3:2017): 19. Most recently, in the summary \nLaw, Center on Privacy & Technology, Oct. 2016. \nreport of the Global Governance of AI Roundtable \nAvailable: https://www.perpetuallineup.org/.\nat the 2018 World Government Summit, Omar \nbin Sultan Al Olama, Minister of State for Artificial  \n42  The inclusion of such collateral effects in \nIntelligence of the UAE, highlighted the importance  \nassessing effectiveness is an important element \nof “empirical information” in assessing the \nin overcoming the apparent “black box” or \nsuitability of A/IS.\ninscrutable nature of A/IS. See, for example,  \nJ. A. Kroll, “The fallacy of inscrutability,” \n39  In the terminology of software development, \nPhilosophical Transactions of the Royal Society  \nverification is a demonstration that a given \nA: Mathematical, Physical, and Engineering \napplication meets a narrowly defined requirement;  \nSciences, vol. 376, no. 2133, Oct. 2018.  \nvalidation is a demonstration that the application \nAvailable: doi.org/0.1098/rsta.2018.0084.  \nanswers its real-world use case. When we speak \nThe study addresses, among other questions, \nof gathering evidence of the effectiveness of  \n“how measurement of a system beyond \nA/IS, we are speaking of validation.\nunderstanding of its internals and its design  \n40  Standards may include compliance with  can help to defeat inscrutability.”\ndefined professional competence or other ethical \n43  The question of the salience of collateral \nrequirements, but also other types of standards, \nimpact will vary with the specific application  \nsuch as data standards. Data standards may \nof A/IS. For example, false positives in document \nserve as “a digital lingua franca” with the potential \nreview related to fact-finding will generally not \nof both supporting broad-based technological \nraise acute ethical issues, but false positives \ninnovation (including A/IS innovation) in a legal \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 269', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\nin predictive policing or sentencing will. In  47  When a complex system can be broken down \nthese latter domains, complex and sometimes  into separate component systems, it may be \nunsettled issues of fairness arise, particularly  appropriate to assess either the effectiveness \nwhen social norms of fairness change regionally  of each component, or that of the end-to-\nand over time (sometimes rapidly). Any A/IS that  end application as a whole (including human \nwas designed to replicate some notion of fairness  operators), depending on the specific question  \nwould need to demonstrate its effectiveness,  to be answered.\nfirst, at replicating prevailing notions of fairness \n48  Qualitative considerations may also help \nthat have legitimacy in society, and second, \ncounter attempts to “game the system”  \nat responding to evolutions in such notions of \n(i.e., attempts to use bad-faith methods to \nfairness. In the current state of A/IS, in which no \nmeet a specific numerical target); see B. Hedin, \nsystem has been able to demonstrate consistent \nD. Brassil, and A. Jones, “On the Place of \neffectiveness in either of the above regards,  \nMeasurement in E-Discovery,” in Perspectives  \nit is essential that great discretion be exercised  \non Predictive Coding and Other Advanced  \nin considering any reliance on A/IS in domains \nSearch Methods for the Legal Practitioner,  \nsuch as sentencing and predictive policing.\ned. J. R. Baron, R. C. Losey, and M. D. Berman. \n44  These exercises go by various names  Chicago: American Bar Association, 2016,  \nin the literature: effectiveness evaluations,  p. 415f.\nbenchmarking exercises, validation studies, \n49  Even in fact-finding, accurate extraction of \nand so on. See, for example, the definition of \nfacts does not eliminate the need for reasoned \nvalidation study in AINOW’s 2018 Algorithmic \njudgment as to the significance of the facts in \nAccountability Toolkit (https://ainowinstitute.org/\nthe context of specific circumstances and cultural \naap-toolkit.pdf), p. 29. For our purposes, what \nconsiderations. Used properly, A/IS will advance \nmatters is that the exercise be one that collects, \nthe spirit of the law, not just the letter of the law.\nin a scientifically sound manner, evidence of  \nhow “fit for purpose” any given A/IS are.\n50  Electronic discovery is the task of searching \nthrough large collections of electronically stored \n45  This feature of evaluation design is important, \ninformation (ESI) for material relevant to civil \nas only tasks that accurately reflect real-world \nand criminal litigation and investigations. Among \nconditions and objectives (which may include  \napplications of A/IS to legal tasks and questions, \nthe avoidance of unintended consequences, such \nthe application to legal discovery is probably the \nas racial bias) will provide compelling guidance  \nmost “mature,” as measured against the criteria  \nas to the suitability of an application for adoption \nof having been tested, assessed and approved  \nin the real world.\nby courts, and adopted fairly widely across \n46  For TREC generally, see: https://trec.nist.gov/.  various jurisdictions.\nFor the TREC Legal Track specifically, see: https://\ntrec-legal.umiacs.umd.edu/.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 270', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\n51  While there is general consensus about  (e.g., flight risk assessment technologies); \nthe importance of these metrics in gauging  see Stevenson, “Assessing Risk Assessment”. \neffectiveness in legal discovery, there is not a  Unsubstantiated claims are an appropriate source  \nconsensus around the precise values for those  of an informed distrust in A/IS. Such well-founded \nmetrics that must be met for a discovery effort  distrust can be addressed only with truly \nto be acceptable. That is a good thing, as the  meaningful and sound measures that provide \nprecise value that should be attained, and  accurate information regarding the capabilities \ndemonstrated to have been attained, in any given  and limitations of a given system.\nmatter will be dependent on, and proportional  \n54  See the discussion under “Illustration—\nto, the specific facts and circumstances of  \nEffectiveness” in this chapter.\nthat matter.\n55  For more on principles for data protection,  \n52  Different domains of application of A/IS to \nsee the EAD chapter “Personal Data and \nlegal matters will vary not only with regard to the \nIndividual Agency”.\navailability of consensus metrics of effectiveness, \nbut also with regard to conditions that affect the \n56  The importance of validation by practitioners \nchallenge of measuring effectiveness: availability \nis reflected in The European Commission’s  \nof data, impact of social bias, and sensitivity  \nHigh-Level Expert Group on Artificial Intelligence \nto privacy concerns all affect how difficult it may \nDraft Ethics Guidelines for Trustworthy AI: \nbe to arrive at consensus protocols for gauging \n“Testing and validation of the system should \neffectiveness. In the case of defining  \nthus occur as early as possible and be iterative, \nan effectiveness metric for A/IS used in support \nensuring the system behaves as intended \nof sentencing decisions, one challenge is that, \nthroughout its entire life cycle and especially  \nwhile it is easy to find when an individual who \nafter deployment.” (Emphasis added.) See \nhas been released commits a crime (or is \nHigh-Level Expert Group on Artificial Intelligence, \nconvicted of committing a crime), it is difficult to \n“DRAFT Ethics Guidelines for Trustworthy \nassess when an individual who was not released \nAI: Working Document for Stakeholders’ \nwould have committed a crime. For a discussion \nConsultation,” The European Commission. \nof the challenges in measuring the effectiveness \nBrussels, Belgium: Dec. 18, 2018. \nof tools designed to assess flight risk, see M. T. \nStevenson, “Assessing Risk Assessment in Action.”  57  That scrutiny need not extend to IP or  \nMinnesota Law Review, vol. 103, 2018. Available:  other protected information (e.g., attorney work \ndoi.org/10.2139/ssrn.3016088.  product). Validation methods and results are a \nmatter of numbers and procedures for obtaining \n53  Sound measurement may also serve as an \nthe numbers, and their disclosure would not \neffective antidote to the unsubstantiated claims \nimpinge on safeguards against the disclosure  \nsometimes made regarding the effectiveness \nof legitimately protected information.\nof certain applications of A/IS to legal matters \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 271', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\n58  A recent matter from the US legal system  59  https://www.nist.gov/.\nillustrates how a failure to disclose the results of \n60  TREC Legal Track (2006-2011): https://trec-\na validation exercise can limit the exercise’s ability \nlegal.umiacs.umd.edu/. \nto achieve its intended purpose. In Winfield v. \nCity of New York (Opinion & Order. 15-CV-05236 \n61  The statistical evidence in question here  \n[LTS] [KHP]. SDNY 2017), a party had utilized \nis statistical evidence of the effectiveness of \nthe A/IS-enabled system to conduct a review \nA/IS applied to the task of discovery; it is not \nof documents for relevance to the matter being \nstatistical evidence of facts actually at issue in \nlitigated. When the accuracy and completeness  \nlitigation. Courts may have different rules for \nof the results of that review were challenged \nthe admissibility of the two kinds of statistical \nby the requesting party, the producing party \nevidence (and there will be jurisdictional \ndisclosed that it had, in fact, conducted \ndifferences on these questions).\nvalidation of its results. Rather than requiring \nthat the producing party simply disclose the  62  It is important to underscore that, whereas \nresults of the validation to the requesting party,  developers and operators of A/IS should \nthe judge overseeing the dispute chose to  be able to derive sound measurements of \nreview the results herself in camera, without  effectiveness, the courts should determine what \nproviding access to the requesting party.  level of effectiveness—what score—should be \nAlthough the judge then said that the evidence  demonstrated to have been achieved, based on \nshe was provided supported the accuracy and  the facts and circumstances of a given matter. \ncompleteness of the review, the requesting party  In some instances, the cost (in terms of sample \ncould not itself examine either the evidence or  sizes, resources required to review the samples, \nthe methods whereby it was obtained, and so  and so on) of demonstrating the achievement  \ncould not gain confidence in the results. That  of a high score will be disproportionate to the \nconfidence comes only from examining the  stakes of a given matter. In others, for example, \nmetrics and the procedures followed in obtaining  a major securities fraud claim that potentially \nthem. Moreover, the results of a validation  affects thousands of citizens, a court might \nexercise, which are usually simple numbers that  justifiably demand a demonstration of the \nreflect sampling procedures, can be disclosed  achievement of a very high score, irrespective  \nwithout revealing the content of any documents,  of cost. Demonstrations of the effectiveness  \nany proprietary tools or methods, or any attorney  of A/IS (and of their operators) are instruments  \nwork product. If the purpose of conducting a  in support of, not in substitution of, judicial \nvalidation exercise is to gather evidence of the  decision-making. \neffectiveness of a process, in the event that the \n63  See, for example, B. Hedin, S. Tomlinson, J. R. \nprocess is challenged, keeping that evidence \nBaron, and D. W. Oard, “Overview of the TREC \nhidden from those who would challenge the \n2009 Legal Track,” in NIST Special Publication: \nprocess limits the ability of the validation exercise \nSP 500-278, The Eighteenth Text REtrieval \nto achieve its intended purpose.\nConference (TREC 2009) Proceedings (2009).\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 272', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\n64  See M. R. Grossman and G. V. Cormack,  67  The competence principle is intended  \n“Technology-Assisted Review in E-Discovery  to apply to the post design operation of A/IS.  \nCan Be More Effective and More Efficient Than  Of course, that does not mean that designers  \nExhaustive Manual Review,” Richmond Journal  and developers of A/IS are free of responsibility \nof Law and Technology, vol. 17, no. 3, 2011.  for their systems’ outcomes. As discussed  \nAvailable: http://jolt.richmond.edu/jolt-archive/ in the background to this issue, it is incumbent  \nv17i3/article11.pdf. Note that the two systems  on designers and developers to assess the risks \nthat conclusively demonstrated “better than  associated with the operation of their systems \nhuman” performance took methodologically  and to specify the operator competencies \ndistinct approaches, but they shared the  needed to mitigate those risks. For more on \ncharacteristic of having been designed, operated,  the question of designer incompetence or \nand measured for accuracy by scientifically  negligence, see the discussion of “software \ntrained experts. malpractice” in Kroll (2018). \n65  Da Silva Moore v. Publicis Groupe, 2012  68  The ISO standard on e-discovery, ISO/IEC \nWL 607412 (S.D.N.Y. Feb. 24, 2012). See also  27050-3, does recognize the importance  \nA. Peck, “Search, Forward,” Legaltech News.  of expertise in applying advanced technologies \nOct. 1, 2011. Available: https://www.law.com/ in a search for documents responsive to a legal \nlegaltechnews/almID/1202516530534Search- inquiry; see ISO/IEC 27050-3: Information \nForward/. technology — Security techniques — Electronic \ndiscovery — Part 3: Code of practice for electronic \n66  The fact that NIST has as important \ndiscovery, Geneva (2017), pp. 19-20.\nrole to play in developing standards for the \nmeasurement of the safety and security of A/IS  69  See, for example, ABA Model Rule 1, \nwas recognized in a recent (September, 2018)  comment 8: “To maintain the requisite \nreport from the U.S. House of Representatives:  knowledge and skill, a lawyer should keep abreast \n“At minimum, a widely agreed upon standard   of changes in the law and its practice, including \nfor measuring the safety and security of AI  the benefits and risks associated with relevant \nproducts and applications should precede any  technology, engage in continuing study and \nnew regulations. ... The National Institute of  education and comply with all continuing legal \nStandards and Technology (NIST) is situated   education requirements to which the lawyer is \nto be a key player in developing standards.”   subject.” Available: https://www.americanbar.org/\n(Will Hurd and Robin Kelly, “Rise of the Machines:  groups/professional_responsibility/publications/\nArtificial Intelligence and its Growing Impact on  model_rules_of_professional_conduct/rule_1_1_\nU.S. Policy,” U.S. House of Representatives— competence/comment_on_rule_1_1/. See also, \nCommittee on Oversight and Government  The State Bar of California Standing Committee \nReform—Subcommittee on Information  on Professional Responsibility and Conduct, \nTechnology, September, 2018). Formal Opinion No. 2015-193. Available:  \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 273', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\nhttps://www.calbar.ca.gov/Portals/0/documents/ See In re: Biomet M2a Magnum Hip Implant \nethics/Opinions/CAL%202015-193%20%5B11- Prods. Liab. Litig.No. 3:12-MD-2391 (N.D. Ind. \n0004%5D%20(06-30-15)%20-%20FINAL.pdf. April 18, 2013).\n70  In the deliberations of the Law Committee   73  For example, a prior violent conviction may \nof the 2018 Global Governance of AI Roundtable,  be weighted equally, whether the violent act \nthe question of the competencies needed   was a shove or a knife attack. See Human Rights \n“in order to effectively operate and measure  Watch. “Q & A: Profile Based Risk Assessment \nthe efficacy of AI systems in legal functions that  for US Pretrial Incarceration, Release Decisions,” \naffect the rights and liberty of citizens” was cited  June 1, 2018. Available: https://www.hrw.\nas one of the considerations that “appear to be  org/news/2018/06/01/q-profile-based-risk-\nmost overlooked in the current public dialogue.”  assessment-us-pretrial-incarceration-release-\nSee “Global Governance of AI Roundtable:  decisions.\nSummary Report 2018,” World Government \n74  Bias can be introduced in a number of ways: \nSummit, 2018: p. 7. Available: https://www.\nvia the features taken into consideration by the \nworldgovernmentsummit.org/api/publications/\nalgorithm, via the nature and composition of \ndocument?id=ff6c88c5-e97c-6578-b2f8-\nthe training data, via the design of the validation \nff0000a7ddb6. \nprotocol, and so on. A competent operator will be \n71  See A. G. Ferguson, “Policing Predictive  alert to and assess such potential sources of bias.\nPolicing,” Washington University Law Review, vol. \n75  Among the conditions may be, for example, \n94, no. 5, 2017: 1109, 1172. Available: https://\nthat the results of the system are to be used  \nopenscholarship.wustl.edu/law_lawreview/vol94/\nonly to provide guidance to the human decision \niss5/5/. \nmaker (e.g., judge) and should not be taken as,  \n72  In addition, a lack of competence in  in themselves, dispositive.\ninterpreting the results of a statistical exercise can \n76  Given that the effective functioning of a  \n(and often does) result in an incorrect conclusion \nlegal system is a matter of interest to the whole \n(on the part of a party to a dispute or of a judge \nof society, it is important that all members of a \nseeking to resolve a dispute). For example, in \nsociety be provided with access to the resources \nIn re: Biomet, a judge addressing a discovery \nneeded to understand when and how A/IS  \ndispute interpreted the statistical data provided \nare applied in support of the functioning of  \nby the producing party as indicating that the \na legal system.\nproducing party’s retrieval process had left behind \n“a comparatively modest number” of responsive \n77  Among the topics covered by such training \ndocuments, when the statistical evidence \nshould be the potential for “automation bias” \nshowed, in fact, that a substantial number of \nand ways to mitigate it. See L. J. Skitka, K. \nresponsive documents had been left behind.  \nMosier, and M. D. Burdick, “Does automation \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 274', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\nbias decision-making?” International Journal  82  It is also the case that, among the false \nof Human-Computer Studies, vol. 51, no. 5,  positives, nonwhite members of Congress were \npp. 991-1006, 1999. Available: https://doi. overrepresented relative to their proportion in \norg/10.1006/ijhc.1999.0252; L. J. Skitka, K.  Congress as a whole, perhaps indicating that the \nMosier, and M. D. Burdick, “Accountability and  accuracy of the technology is, to some degree, \nautomation bias,” International Journal of Human- race-dependent. Without knowing more about \nComputer Studies, vol. 52, no. 4, pp. 701-717,  the composition of the mugshot database, \n2000. Available: https://doi.org/10.1006/ however, we cannot assess the significance  \nijhc.1999.0349. of this result.\n78  Some government agencies are working  83  See J. Snow, “Amazon’s Face Recognition \ntoward creating a more effective partnership  Falsely Matched 28 Members of Congress with \nbetween the skills found in technology start- Mugshots.” ACLU—Free Future, July 26, 2018. \nups and the skills required of legal practitioners.  Available: https://www.aclu.org/blog/privacy-\nSee Legal Innovation Zone. “Ryerson’s Legal  technology/surveillance-technologies/amazons-\nInnovation Zone Announces Winners of AI Legal  face-recognition-falsely-matched-28. See also R. \nChallenge,” March 26, 2018. Available: http:// Brandom, “Amazon’s facial recognition matched \nwww.legalinnovationzone.ca/press_release/ 28 members of Congress to criminal mugshots.” \nryersons-legal-innovation-zone-announces- The Verge, July 26, 2018. Available: https://www.\nwinners-of-ai-legal-challenge/. theverge.com/2018/7/26/17615634/amazon-\nrekognition-aclu-mug-shot-congress-facial-\n79  See Amazon. “Amazon Rekognition.” https://\nrecognition. \naws.amazon.com/rekognition/ (2018).\n84  See “Amazon Rekognition Developer Guide.” \n80  See E. Dwoskin, “Amazon is selling facial \nAmazon, p. 131, 2018. Available: https://docs.aws.\nrecognition to law enforcement—for a fistful \namazon.com/rekognition/latest/dg/rekognition-dg.\nof dollars.” Washington Post, May 22, 2018. \npdf. Also see K. Tenbarge, “Amazon Responds \nAvailable: https://www.washingtonpost.com/\nto ACLU’s Highly Critical Report of Rekognition \nnews/the-switch/wp/2018/05/22/amazon-is-\nTech,” Inverse, July 26, 2018. Available: https://\nselling-facial-recognition-to-law-enforcement-\nwww.yahoo.com/news/amazon-responds-aclu-\nfor-a-fistful-of-dollars/?noredirect=on&utm_\napos-highly-160000264.html.\nterm=.07d9ca13ab77. \n85  The story also highlights the question of \n81  See, for example, J. Stanley, “FBI and Industry \naccountability, illustrating how the principles \nFailing to Provide Needed Protections for Face \ndiscussed in this report intersect with and \nRecognition.” ACLU—Free Future, June 15, 2016. \ncomplement each other.\nAvailable: https://www.aclu.org/blog/privacy-\ntechnology/surveillance-technologies/fbi-and-\nindustry-failing-provide-needed.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 275', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\n86  Of course, competent use does not preclude  the administration of justice; in the case of A/IS \nuse for bad ends (e.g., government surveillance  used by a party to legal proceedings, this could \nthat impinges on human rights). The principle  be the party’s counsel.\nof competence is one principle in a set that, \n91  J. New and D. Castro, “How Policymakers  \ncollectively, is designed to ensure the ethical \nCan Foster Algorithmic Accountability.”Information \napplication of A/IS. See the EAD chapter  \nTechnology & Innovation Foundation,  \n“General Principles”.\np. 5, 2018. Available: https://www.itif.org/\n87  Developing “well grounded” guidelines will  publications/2018/05/21/how-policymakers-can-\ntypically require that the creators of A/IS gather  foster-algorithmic-accountability.\ninput from both those operating the technology \n92  Included among possible “causes” for \nand those affected by the technology’s operation. \nan effect are not only the decision-making \n88  The use of facial recognition technologies   pathways of algorithms but also, importantly, \nby security and law enforcement agencies raises  the decisions made by humans involved in the \nissues that extend beyond the question of  design, development, procurement, deployment, \noperator competence. For further discussion of  operation, and validation of effectiveness  \nsuch issues, see C. Garvie, A. M. Bedoya, and  of A/IS.\nJ. Frankle, “The Perpetual Line-Up: Unregulated \n93  The challenge, moreover, is one not only of \nPolice Face Recognition in America,” Georgetown \nassigning responsibility, but of assigning levels \nLaw, Center on Privacy & Technology, October 18, \nof responsibility (a task that could benefit from \n2016, Available: https://www.perpetuallineup.org/.\na neutral model that could consider how much \n89  As noted above, some professional  interaction and influence each stakeholder has  \norganizations, such as the ABA, have begun to  in every decision).\nrecognize in their codes of ethics the importance \n94  Scherer (2016): 372. In addition to \nof technological competence, although the \ndiffuseness, Scherer identifies discreetness, \nguidance does not yet address A/IS specifically.\ndiscreteness, and opacity as features of the \n90  Including those engaged in the procurement  design and development of A/IS that make \nand deployment of a system means that those  apportioning responsibility for their outcomes  \nacquiring and authorizing the use of a system  a challenge for regulators and courts.\ncan share in the responsibility for its results. For \n95  In answering these questions, it will be \nexample, in the case of A/IS deployed in the \nimportant to keep in mind the distinction \nservice of the courts, this could be the judiciary; \nbetween responsibility (a factual question) and \nin the case of A/IS deployed in the service \nultimate accountability (a normative question). In \nof law enforcement, this could be the agency \nthe case of the example under discussion, there \nresponsible for the enforcement of the law and \nmay be multiple individuals who have  \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 276', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\nsome practical responsibility for the sentence  Paper No. 18-07. Available: https://ssrn.com/\ngiven, but the normative framework may place  abstract=3064761 or http://dx.doi.org/10.2139/\nultimate accountability on the judge. Before  ssrn.3064761.\nnormative accountability can be assigned, \n100 Also, gaining access to that information should \nhowever, pragmatic responsibilities must be \nnot be unduly burdensome.\nclarified and understood. Hence the focus,  \nin this section, on clarifying lines of responsibility \n101 Those developing a model for accountability \nso that ultimate accountability can be determined.\nfor A/IS may find helpful guidance in considering \nmodels of accountability used in other domains \n96  If effectiveness is measured against statistics \n(e.g., data protection).\nthat themselves may represent human bias  \n(e.g., arrest rates), then the effectiveness \n102 For a discussion of how such policies might \nmeasures may just reflect and reinforce that bias.\nbe implemented in accordance with protocols \nfor information governance, see J. R. Baron and \n97  “‘The algorithm did it’ is not an acceptable \nK. E. Armstrong, “The Algorithm in the C-Suite: \nexcuse if algorithmic systems make mistakes  \nApplying Lessons Learned and Information \nor have undesired consequences, including from \nGovernance Best Practices to Achieve Greater \nmachine-learning processes.” See “Principles \nPost-GDPR Algorithmic Accountability,” in  \nfor Accountable Algorithms and a Social Impact \nThe GDPR Challenge: Privacy, Technology, and \nStatement for Algorithms.” FAT/ML Resources. \nCompliance In An Age of Accelerating Change,  \nwww.fatml.org/resources/principles-for-\nA. Taal, Ed. Boca Raton, FL: CRC Press, \naccountable-algorithms.\nforthcoming.\n98  See Langewiesche, W. 1998. “The Lessons of \n103 These inquiries can be supported by \nValuJet 592”. Atlantic Monthly. 281: 81-97; S. D. \ntechnological tools that may provide information \nSagan. Limits of Safety: Organizations, Accidents, \nessential to answering questions of accountability \nand Nuclear Weapons. Princeton University \nbut that do not require full transparency into \nPress, 1995.\nunderlying computer code and may avoid the \n99  For a discussion of the role of explanation   necessity of an intrusive audit; see Kroll et al. \nin maintaining accountability for the results   (2017). Among the tools identified by Kroll \nof A/IS and of the question of whether the  and his colleagues are: software verification, \nstandards for explanation should be different for  cryptographic commitments, zero-knowledge \nA/IS than they are for humans, see F. Doshi-Velez,  proofs, and fair random choices. While the  \nM. Kortz, R. Budish, C. Bavitz, S. J. Gershman,  use of such tools may avoid the limitations of \nD. O’Brien, S. Shieber, J. Waldo, D. Weinberger,  solutions such as transparency and audit, they do \nand A. Wood, Accountability of AI Under the  require that creators of A/IS design their systems \nLaw: The Role of Explanation (November 3,  so that they will be compatible with  \n2017). Berkman Center Research Publication  the application of such tests.\nForthcoming; Harvard Public Law Working \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 277', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\n104 Certifications may include, for example,  and continue not to be necessarily bound by it in \nprofessional certifications of competence, but  the light of the specific features of that particular \nalso certifications of compliance of processes  case,” and, with regard to decision subjects, \nwith standards. An example of a certification  that he or she must “be clearly informed of any \nprogram specifically addressing A/IS is The Ethics  prior processing of a case by artificial intelligence \nCertification Program for Autonomous and  before or during a judicial process and have \nIntelligent Systems (ECPAIS), https://standards. the right to object, so that his/her case can be \nieee.org/industry-connections/ecpais.html. heard directly by a court.” See CEPEJ, European \nEthical Charter on the Use of Artificial Intelligence \n105 This means that A/IS used in legal systems \nin Judicial Systems and their Environment \nwill have to be defensible in courts. The margin \n(Strasbourg, 2018), p. 10.\nof error will have to be low or the use of A/IS  \nwill not be permitted. 110 J. Tashea, Calculating Crime: Attorneys are \nChallenging the Use of Algorithms to Help \n106 It is also the case that evidence produced  \nDetermine Bail, Sentencing and Parole, ABA \nby A/IS will be subject to chain-of-custody rules, \nJournal (March 2017).\nas are other types of forensic evidence, to ensure \nintegrity, confidentiality, and authenticity. 111 Loomis v. Wisconsin, 68 WI. (2016). \n107 See for instance Art. 22(1) Regulation (EU)  112 Id. at pp. 46-66.\n2016/679.\n113 R. Wexler, Life, Liberty, and Trade Secrets: \n108 Human dignity, as a core value protected  Intellectual Property in the Criminal Justice \nby the United Nations Universal Declaration of  System, Stanford Law Review, 2018.\nHuman Rights, requires us to fully respect the \n114 Malenchik v. State, 928 N.E.2d 564, 574  \npersonality of each human being and prohibits \n(Ind. 2010).\ntheir objectification.\n115 People v. Chubbs CA2/4, B258569 (Cal. Ct. \n109 This concern is reflected in Principle 5 of \nApp. 2015).\nthe European Ethical Charter on the Use of \nArtificial Intelligence in Judicial Systems and their \n116 U.S. v. Ocasio, No. 3:11-cr-02728-KC, slip op. \nEnvironment, recently published by the Council of \nat 1-2, 11-12 (W.D. Tex. May 28, 2013). \nEurope’s European Commission for the Efficiency \nof Justice (CEPEJ). Principle 5 (“Principle ‘Under  117 U.S. v. Johnson, No. 1:15-cr-00565-VEC, order \nUser Control’: preclude a prescriptive approach  (S.D.N.Y., June 7, 2016).\nand ensure that users are informed actors and in \n118 Indeed, without transparency, there may, \ncontrol of the choices made”) states, with regard \nin some circumstances, be no means for even \nto professionals in the justice system that they \nknowing whether an error that needs to be \nshould “at any moment, be able to review judicial \ncorrected was committed. In the case of A/IS \ndecisions and the data used to produce a result \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 278', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\napplied in a legal system, an “error” can mean  123 See A. Baker, “Updated N.Y.P.D. Anti-Crime \nreal harm to the dignity, liberty, and life of an  System to Ask: ‘How We Doing?’” New York \nindividual. Times, May 8, 2017, https://www.nytimes.\ncom/2017/05/08/nyregion/nypd-compstat-\n119 Fairness (as well as bias) can be defined \ncrime-mapping.html; S. Weichselbaum, “How \nin more than one way. For purposes of this \na ‘Sentiment Meter’ Helps Cops Understand \ndiscussion, a commitment is not made to any \nTheir Precincts,” Wired, July 16, 2018. Available: \none definition—and indeed, it may not be either \nhttps://www.wired.com/story/elucd-sentiment-\ndesirable or feasible to arrive at a single definition \nmeter-helps-cops-understand-precincts/.\nthat would be applied in all circumstances.  \nFor purposes of this discussion, the key point  124 This table is a preliminary draft and is meant \nis that transparency will be essential in building  only to illustrate a useful tool for facilitating \ninformed trust in the fairness of a system,  reasoning about who should have access to what \nregardless of the specific definition of fairness  information. Other categories of stakeholder and \nthat is operative. other categories of information (e.g., the identity \nand nature of the designer/manufacturer of the \n120 To the extent permitted by the normal \nA/IS, the identity and nature of the investors \noperation of the A/IS: allowing for, for example, \nbacking a particular system or company) could  \nvariation in the human inputs to a system \nbe added as needed.\nthat may not be eliminated in any attempt at \nreplication.  125 For discussions of these two dimensions of \nexplanation, see S. Wachter, et al. (2017). “Why \n121 With regard to information explaining how  \na Right to Explanation of Automated Decision-\na system arrived at a given output, GDPR makes \nMaking Does Not Exist in the General Data \nprovision for a decision subject’s right to an \nProtection Regulation”; A. Selbst, and S. Barocas, \nexplanation of algorithmic decisions affecting \nThe Intuitive Appeal of Explainable Machines.\nhim or her: automated processing of personal \ndata “should be subject to suitable safeguards,  126 Wexler, Rebecca. 2018. “Life, Liberty, and \nwhich should include specific information to  Trade Secrets: Intellectual Property in the \nthe data subject and the right to obtain human  Criminal Justice System”. Stanford Law Review. \nintervention, to express his or her point of view,  70 (5): 1342-1429; Tashea, Jason. “Federal judge \nto obtain an explanation of the decision reached  releases DNA software source code that was \nafter such assessment and to challenge the  used by New York City’s crime lab.” ABA Journal \ndecision.” GDPR, Recital 71. (2017). http://www.abajournal.com/news/\narticle/federal_judge_releases_dna_software_\n122 Even among sensitive data, some data may  \nsource_code.\nbe more sensitive than others. See I. Ajunwa, \n“Genetic Testing Meets Big Data: Tort and Contract   127 Or, if two approaches are found to be, for  \nLaw Issues,” 75 Ohio St. L. J. 1225 (2014).  practical purposes, equally effective, the simpler,  \nAvailable: https://ssrn.com/abstract=2460891. more easily explained approach may be preferred.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 279', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\n128 For a discussion of the limits of transparency  131 W. L. Perry, B. McInnis, C. C. Price, S. C. Smith, \nand of alternative modes of gaining actionable  and J. S. Hollywood, “Predictive Policing: The \nanswers to questions of verification and  Role of Crime Forecasting in Law Enforcement \naccountability, see J.A. Kroll, J. Huey, S. Barocas,  Operations,” The RAND Corporation,  \nE.W. Felten, J.R. Reidenberg, D.G. Robinson, H.  pp. 67-69, 2013.\nYu, “Accountable Algorithms” (March 2, 2016). \n132 Support from the University of Memphis  \nUniversity of Pennsylvania Law Review, Vol. \nwas led by Richard Janikowski, founding Director \n165, 2017 Forthcoming; Fordham Law Legal \nof the Center for Community Criminology and \nStudies Research Paper No. 2765268. Available \nResearch (School of Urban Affairs and Public \nat SSRN: https://ssrn.com/abstract=2765268. \nPolicy, the University of Memphis) and the \nSee also J.A. Kroll, The fallacy of inscrutability, \nShared Urban Data System (The University  \nPhil. Trans. R. Soc. A 376: 20180084. http://\nof Memphis).\ndx.doi.org/10.1098/rsta.2018.0084 (Note p. \n9: “While transparency is often taken to mean \n133 E. Figg, “The Legacy of Blue CRUSH,” High \nthe disclosure of source code or data, possibly \nGround, March 19, 2014.\nto a trusted entity such as a regulator, this is \nneither necessary nor sufficient for improving  134 Figg, “Legacy.”\nunderstanding of a system, and it does not \n135 Nucleus Research, ROI Case Study: IBM \ncapture the full meaning of transparency.”)\nSPSS—Memphis Police Department, Boston, \n129 In particular with respect to due process, the  Mass., Document K31, June 2010. Perry et al., \ncurrent dialogue on the use of A/IS centers on  Predictive Policing, 69.\nthe tension between the need for transparency \n136 Figg, “Legacy.”\nand the need for the protection of intellectual \nproperty rights. Adhering to the principle of  137 Figg, “Legacy.”\nEffectiveness as articulated in this work can \nsubstantially help in defusing this tension.   138 See: AI Now, Algorithmic Accountability  \nReliable empirical evidence of the effectiveness  Policy Toolkit, p. 12, Oct. 2018. Available:  \nof A/IS in meeting specific real-world objectives  https://ainowinstitute.org/aap-toolkit.pdf;  \nmay foster informed trust in such A/IS, without  D. Robinson and L. Koepke, Stuck in a Pattern: \ndisclosure of proprietary or trade secret  Early evidence on “predictive policing” and civil \ninformation. rights, Upturn, Aug. 2016. Available: https://\nwww.upturn.org/reports/2016/stuck-in-a-\n130 S. Wachter, B. Mittelstadt, and C. Russell,  pattern/; S. Brayne, “Big Data Surveillance: \n“Counterfactual Explanations without Opening  The Case of Policing,” American Sociological \nthe Black Box: Automated Decisions and the  Review, 2016. Available: https://journals.\nGDPR,” SSRN Electronic Journal, p. 5, 2017 for  sagepub.com/doi/10.1177/0003122417725865; \nthe example cited. A. G. Ferguson, “Policing Predictive Policing,” \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 280', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nLaw\nWashington University Law Review, vol. 94,  141 See, for example: J. Tashea, “Estonia \nno. 5, 2017. Available: https://openscholarship. considering new legal status for artificial \nwustl.edu/law_lawreview/vol94/iss5/5/; K.  intelligence,” ABA Journal, Oct. 20, 2017, and \nLum and W. Isaac, “To predict and serve?”  European Parliament Resolution of Feb. 16, 2017.\nSignificance 2016. Available: https://rss.\n142 See Legal Entity, Person, in B. Bryan A. Garner, \nonlinelibrary.wiley.com/doi/epdf/10.1111/j.1740-\nBlack’s Law Dictionary, 10th Edition. Thomas \n9713.2016.00960.x; B. J. Jefferson, “Predictable \nWest, 2014. \nPolicing: Predictive Crime Mapping and \nGeographies of Policing and Race,” Annals of the \n143  J. S. Nelson, “Paper Dragon Thieves.” \nAmerican Association of Geographers, vol. 108, \nGeorgetown Law Journal 105 (2017): 871-941.\nno. 1, pp. 1-16, 2018. Available: https://doi.org/1\n0.1080/24694452.2017.1293500. 144 M. U. Scherer, “Of Wild Beasts and Digital \nAnalogues: The Legal Status of Autonomous \n139 For a discussion of the criteria that may define \nSystems.” Nevada Law Journal 19, forthcoming \na “high-crime area,” and so potentially license \n2018.\nmore intrusive policing, see A. G. Ferguson and \nD. Bernache, “The ‘High-Crime Area’ Question:  145 See M. U. Scherer, “Of Wild Beasts and Digital \nRequiring Verifiable and Quantifiable Evidence  Analogues: The Legal Status of Autonomous \nfor Fourth Amendment Reasonable Suspicion  Systems.” Nevada Law Journal 19, forthcoming \nAnalysis,” American University Law Review,   2018; J. F. Weaver. Robots Are People Too: How \nvol. 57, pp. 1587-1644. Siri, Google Car, and Artificial Intelligence Will \nForce Us to Change Our Laws. Santa Barbara, CA: \n140 While A/IS, if misapplied, may perpetuate  \nPraeger, 2013; L. B. Solum. “Legal Personhood  \nbias, it holds at least the potential, if applied  \nfor Artificial Intelligences.” North Carolina Law \nwith appropriate controls, to reduce bias. For  \nReview 70, no. 4 (1992): 1231–1287.\na study of how an impersonal technology such \nas a red light camera may reduce bias, see \nR. J. Eger, C. K. Fortner, and C. P. Slade, “The \nPolicy of Enforcement: Red Light Cameras and \nRacial Profiling,” Police Quarterly, pp. 1-17, 2015. \nAvailable: http://hdl.handle.net/10945/46909. \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 281', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nAbout Ethically Aligned Design\nThe Mission and Results of  \nThe IEEE Global Initiative on Ethics  \nof Autonomous and Intelligent Systems\nTo ensure every stakeholder involved in the design and development \nof autonomous and intelligent systems is educated, trained,  \nand empowered to prioritize ethical considerations so that these \ntechnologies are advanced for the benefit of humanity.\nTo advance toward this goal, The IEEE Global Initiative on Ethics of Autonomous and \nIntelligent Systems brought together more than a thousand participants from six continents \nwho are thought leaders from academia, industry, civil society, policy, and government in \nthe related technical and humanistic disciplines to identify and find consensus on timely \nissues surrounding autonomous and intelligent systems.\nBy “stakeholder” we mean anyone involved in the research, design, manufacture,  \nor messaging around intelligent and autonomous systems—including universities, \norganizations, governments, and corporations—all of which are making these technologies  \na reality for society.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 282', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nAbout Ethically Aligned Design\nFrom Principles to Practice–  \nResults from our Work to Date\nIn addition to the creation of Ethically Aligned  •  The creation of an A/IS \nDesign, The IEEE Global Initiative, independently   Ethics Glossary  \nor through the IEEE Standards Association, has  The Glossary features more than two hundred \ndirectly inspired the following works: pages of terms that help to define the \ncontext of A/IS ethics for multiple stakeholder \n•  The launch of the IEEE P7000™ \ngroups, specifically: engineers, policy makers, \nseries of approved  \nphilosophers, standards developers, and \nstandardization projects \n  computational disciplines experts. It is \nThis is the first series of standards in the  currently in its second iteration and has  \nhistory of the IEEE Standards Association that  also been informed by the IEEE P7000™ \nexplicitly focuses on societal and ethical issues  standards working groups. \n \nassociated with a certain field of technology\nDownload the Glossary at:  \nMore information can be found at: \nstandards.ieee.org/content/dam/ieee-\nethicsinaction.ieee.org\nstandards/standards/web/documents/other/\nead1e_glossary.pdf\n•  Artificial Intelligence  \nand Ethics in Design   •  The launch of OCEANIS  \n \nThe IEEE Standards Association, inspired \nThese ten courses are designed for global \nby the work of The IEEE Global Initiative, \nprofessionals, as well as their managers, \nhas contributed significantly to the \nworking in engineering, IT, computer science, \nestablishment of The Open Community \nbig data, artificial intelligence, and related \nfor Ethics in Autonomous and Intelligent \nfields across all industries who require up-to- Systems (OCEANIS). It is a global forum for \ndate information on the latest technologies.  discussion, debate, and collaboration for \nThe courses explicitly mirror content from  organizations interested in the development \nEthically Aligned Design, and feature  and use of standards to further the creation of \nnumerous experts as instructors who helped  autonomous and intelligent systems. OCEANIS \ncreate Ethically Aligned Design.   members are working together to enhance \n \nthe understanding of the role of standards \nMore information can be found at: \nin facilitating innovation, while addressing \ninnovationatwork.ieee.org/courses/ problems that expand beyond technical \nartificial-intelligence-and-ethics-in-design  solutions to addressing ethics and values.  \n \n \nMore information can be found at:  \n \nethicsstandards.org \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 283', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nAbout Ethically Aligned Design\n•  T  he launch of ECPAIS  •   The launch of EADUC  \nThe Ethics Certification Program for  The Ethically Aligned Design University \nAutonomous and Intelligent Systems (ECPAIS)  Consortium (EADUC) is being established \nhas the goal to create specifications for  with the aim to reach every engineer at \ncertification and marking processes that  the beginning of their studies to help them \nadvance transparency, accountability, and  prioritize values-driven, applied ethical \nreduction in algorithmic bias in autonomous  principles at the core of their work. Working \nand intelligent systems. ECPAIS intends to  in conjunction with philosophers, designers, \noffer a process and define a series of marks  social scientists, academics, data scientists, \nby which organizations can seek certifications  and the corporate and policy communities, \nfor their processes around the A/IS products,  EADUC also has the goal that Ethically Aligned \nsystems, and services they provide.   Design will be used in teaching at all levels of \n \neducation globally as the new vision for design \nMore information can be found at:  \nin the algorithmic age.\nstandards.ieee.org/industry-connections/\necpais.html •   The launch of “AI Commons”  \nThe work of The IEEE Global Initiative has \n•   The launch of CXI  \ndelivered key ideas and inspiration that \nThe Council on Extended Intelligence \nare rapidly evolving toward establishing a \n(CXI) was directly inspired by the work of \nglobal collaborative platform around A/IS. \nThe IEEE Global Initiative and the work \nThe mission of AI Commons is to gather a \nof The MIT Media Lab around “Extended \ntrue ecosystem to democratize access to \nIntelligence”. CXI was launched jointly by \nAI capabilities and thus to allow anyone, \nthe IEEE Standards Association and The MIT \nanywhere to benefit from the possibilities \nMedia Lab. CXI’s mission is to proliferate the \nthat AI can provide. In addition, the group \nideals of responsible participant design, data \nwill be working to connect problem owners \nagency, and metrics of economic prosperity, \nwith the community of solvers, to collectively \nprioritizing people and the planet over profit \ncreate solutions with AI. The ultimate goal is to \nand productivity. Membership includes \nimplement a framework for participation and \nthought leaders from the EU Parliament and \ncooperation to make using and benefiting  \nCommission, the UK House of Lords, the \nfrom AI available to all.   \n \nOECD, the United Nations, local and national \nMore information can be found at:  \nadministrations, and renowned experts  \nwww.aicommons.com\nin economics, data science, and multiple  \nother disciplines from around the world.  \n \nMore information can be found at:  \nglobalcxi.org\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 284', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nAbout Ethically Aligned Design\nIEEE P7000™ Approved  \nStandardization Projects         \nThe IEEE P7000™ series of standards projects  •  IEEE P7001™ - IEEE Standards  \nunder development represents a unique  Project for Transparency of \naddition to the collection of over 1,900 global  Autonomous Systems \n \nIEEE standards and projects. Whereas more \nInspired by the General Principles \ntraditional standards have a focus on technology \nCommittee, and supported by IEEE  \ninteroperability, functionality, safety, and trade \nVehicular Technology Society  \nfacilitation, the IEEE P7000 series addresses \nstandards.ieee.org/project/7001.html\nspecific issues at the intersection of technological \nand ethical considerations. Like its technical \n•  IEEE P7002™ - IEEE Standards  \nstandards counterparts, the IEEE P7000 series \nProject for Data Privacy Process  \nempowers innovation across borders and    \nenables societal benefit.  Inspired by The Personal Data and Individual \nAgency Control Committee, and supported by \nFor more information or to join any working \nIEEE Computer Society  \ngroup, please see the links below. Committees \nstandards.ieee.org/project/7002.html\nthat authored Ethically Aligned Design, as well \nas other committees within IEEE, that created \n•  IEEE P7003™ - IEEE Standards Project \nspecific working groups are listed below  \nfor Algorithmic Bias Considerations \neach project.   \nSupported by IEEE Computer Society  \nstandards.ieee.org/project/7003.html\n•  IEEE P7000™ - IEEE Standards Project \nModel Process for Addressing Ethical \nConcerns During System Design  •  IEEE P7004™ - IEEE Standards  \n \nProject for Child and Student  \nInspired by Methodologies to Guide Ethical \nData Governance \n \nResearch and Design Committee, and  \nInspired by The Personal Data and Individual \nsupported by IEEE Computer Society  \nstandards.ieee.org/project/7000.html  Agency Control Committee, and supported  \n  by IEEE Computer Society  \n  standards.ieee.org/project/7004.html \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 285', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nAbout Ethically Aligned Design\n•  IEEE P7005™ - IEEE Standards Project  •  IEEE P7010™ - IEEE Standards Project \nfor Employer Data Governance  for Well-being Metric for Autonomous \n \nand Intelligent Systems \nInspired by The Personal Data and Individual   \nAgency Control Committee, and supported   Inspired by the Well-being Committee,  \nby IEEE Computer Society   and supported by IEEE Systems, Man  \nstandards.ieee.org/project/7005.html and Cybernetics Society  \nstandards.ieee.org/project/7010.html\n•  IEEE P7006™ - IEEE Standards  \nProject for Personal Data AI Agent  •  IEEE P7011™ - IEEE Standards Project \nWorking Group  for the Process of Identifying and \n \nRating the Trustworthiness of News \nInspired by The Personal Data and Individual \nSources \nAgency Control Committee, and supported    \nby IEEE Computer Society   Supported by IEEE Society on Social \nstandards.ieee.org/project/7006.html Implications of Technology  \nstandards.ieee.org/project/7011.html\n•  IEEE P7007™ - IEEE Standards  \nProject for Ontological Standard  •  IEEE P7012™ - IEEE Standards Project \nfor Ethically Driven Robotics and  for Machine Readable Personal \nAutomation Systems  Privacy Terms \n   \nSupported by IEEE Robotics   Supported by IEEE Society on Social \nand Automation Society   Implications of Technology  \nstandards.ieee.org/project/7007.html standards.ieee.org/project/7012.html\n•  IEEE P7008™ - IEEE Standards Project  •  IEEE P7013™ - IEEE Standards \nfor Ethically Driven Nudging for Robotic,  Project for Inclusion and Application \nIntelligent and Autonomous Systems  Standards for Automated Facial \n \nAnalysis Technology \nInspired by the Affective Computing   \nCommittee, and supported by IEEE Robotics  Supported by IEEE Society on Social \nand Automation Society   Implications of Technology  \nstandards.ieee.org/project/7008.html standards.ieee.org/project/7013.html\n•  IEEE P7009™ - IEEE Standards Project \nfor Fail-Safe Design of Autonomous  \nand Semi-Autonomous Systems   \n \nSupported by IEEE Reliability Society  \nstandards.ieee.org/project/7009.html\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 286', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nAbout Ethically Aligned Design\nWho We Are Standards Association with the status of an \nOperating Unit of The Institute of Electrical and \nElectronics Engineers, Incorporated (IEEE), the \nworld’s largest technical professional organization \nAbout IEEE dedicated to advancing technology for the benefit \nof humanity with over 420,000 members in \nIEEE is the largest technical professional \nmore than 160 countries. \norganization dedicated to advancing technology   \nfor the benefit of humanity, with over 420,000  To learn more, visit The IEEE Global Initiative \nmembers in more than 160 countries. Through  website:  \nits highly cited publications, conferences,  standards.ieee.org/industry-connections/ec/\ntechnology standards, and professional and  autonomous-systems.html\neducational activities, IEEE is the trusted voice  \nThe IEEE Global Initiative provides the opportunity \nin a wide variety of areas ranging from aerospace \nto bring together multiple voices in the related \nsystems, computers, and telecommunications \ntechnological and scientific communities to identify \nto biomedical engineering, electric power, and \nand find consensus on timely issues. \nconsumer electronics.  \n \nTo learn more, visit the IEEE website:   Names of experts involved in the various \nwww.ieee.org committees of The IEEE Global Initiative can be \nfound at: standards.ieee.org/content/dam/\nAbout the IEEE Standards Association ieee-standards/standards/web/documents/\nother/ec_bios.pdf \nThe IEEE Standards Association (IEEE-SA), a \nglobally recognized standards-setting body within  IEEE makes all versions of Ethically Aligned \nIEEE, develops consensus standards through an  Design available under the Creative Commons \nopen process that engages industry and brings  Attribution-Non-Commercial 4.0 United States \ntogether a broad stakeholder community. IEEE  License. Subject to the terms of that license, \nstandards set specifications and best practices  organizations or individuals can adopt aspects \nbased on current scientific and technological  of this work at their discretion at any time. It \nknowledge. The IEEE-SA has a portfolio of over  is also expected that Ethically Aligned Design \n1,900 active standards and over 650 standards  content and subject matter will be selected for \nunder development.   submission into formal IEEE processes, including \n \nstandards development and education purposes. \nFor more information, visit the IEEE-SA \nwebsite: standards.ieee.org \nThe IEEE Global Initiative and Ethically Aligned \nDesign contribute, together with other \nAbout The IEEE Global Initiative\nefforts within IEEE, such as IEEE TechEthics™, \nThe IEEE Global Initiative on Ethics of  (techethics.ieee.org), to a broader effort at IEEE \nAutonomous and Intelligent Systems (The IEEE   to foster open, broad, and inclusive conversation \nGlobal Initiative) is a program of the IEEE  about ethics in technology.\n \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 287', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nAbout Ethically Aligned Design\nOur Process\nTo ensure the greatest cultural relevance and  This process included matters of “internal \nintellectual rigor possible in our work, The IEEE  consistency” across the various chapters of \nGlobal Initiative has sought for and received  EAD1e and also more specific or broader criteria, \nglobal feedback for versions 1 and 2 (after  such as maturity of the specific chapters and \nhundreds of experts created first drafts) to   consistency with respect to policy statements \ninform this Ethically Aligned Design, First   of IEEE. The review also considered the need \nEdition (EAD1e). for IEEE to maintain a neutral―and thus credible― \nposition in areas and processes where it is likely \nWe released Ethically Aligned Design, Version \nthat IEEE may become active in the future.\n1 (EADv1) as a Request for Input in December \nof 2016 and received over two hundred  Beyond these formal procedures, the Board of \npages of in-depth feedback about the draft. We  Governors of IEEE Standards Association has \nsubsequently released Ethically Aligned Design,  endorsed the work of the IEEE Global Initiative \nVersion 2 (EADv2) in December 2017 and  and offers it for consideration by governments, \nreceived over three hundred pages of in-depth  businesses, and the public at large with the \nfeedback about the draft. This feedback included  following resolution:\nfurther insights about the eight original sections \nWhereas the IEEE Global Initiative on Ethics \nfrom EADv1, along with unique/new input for the \nof Autonomous and Intelligent Systems is an \nfive new sections included in EADv2.\nauthorized activity within the IEEE Standards \nBoth versions included “candidate  Association Industry Connections program \nrecommendations” instead of direct  created with the stated mission:\n“recommendations”, because our communities \nTo ensure every stakeholder involved in the \nhad been engaged in debate and weighing \ndesign and development of autonomous and \nvarious options. \nintelligent systems is educated, trained, and \nThis process was taken to the next level with  empowered to prioritize ethical considerations \nEthically Aligned Design, First Edition (EAD1e),  so that these technologies are advanced for the \nusing EADv1 and EADv2 as its initial foundation.  benefit of humanity;\nAlthough we expect future editions of Ethically \nWhereas versions 1 and 2 of Ethically Aligned \nAligned Design, a vetting process has taken \nDesign: A Vision for Prioritizing Human Well-\nplace within the global community that gave rise \nbeing with Autonomous and Intelligent Systems \nto this seminal work. Therefore, we can now \n(A/IS) were developed as calls for comment and \nspeak of “recommendations” without any further \ncandidate recommendations by several hundred \nrestriction, and EAD1e also includes a set of \nprofessionals including engineers, scientists, \npolicy recommendations. \nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 288', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nAbout Ethically Aligned Design\nethicists, sociologists, economists, and many  3. endorses and offers Ethically Aligned Design:  \nothers from six continents; A Vision for Prioritizing Human Well-being  \nwith Autonomous and Intelligent Systems  \nWhereas the recommendations contained in \n(A/IS), First Edition to businesses, governments \nEthically Aligned Design: A Vision for Prioritizing \nand the public at large for consideration  \nHuman Well-being with Autonomous and \nand guidance in the ethical development  \nIntelligent Systems (A/IS), First Edition are \nof autonomous and intelligent systems.\nthe result of the consideration of hundreds of \ncomments submitted by professionals and the   \npublic at large on versions 1 and 2; Terminology Update\nFor Ethically Aligned Design, we prefer not to \nWhereas through an extensive, global, and \nuse—as far as possible—the vague term “AI” \nopen collaborative process, more than a \nand use instead the term, autonomous and \nthousand experts of The IEEE Global Initiative \nintelligent systems (or A/IS). Even so, it is \nhave developed and are in the process of final \ninherently difficult to define “intelligence” and \nediting and publishing Ethically Aligned Design: \n“autonomy”. One could, however, limit the scope \nA Vision for Prioritizing Human Well-being with \nfor practical purposes to computational systems \nAutonomous and Intelligent Systems (A/IS),  \nusing algorithms and data to address complex \nFirst Edition; now, therefore, be it \nproblems and situations, including the capability \n \nof improving their performance based on \nResolved, that the IEEE Standards \nevaluating previous decisions, and say that such \nAssociation Board of Governors:\nsystems could be considered as “intelligent”. \n1.  expresses its appreciation to the leadership \nSuch systems could be regarded also as \nand members of the IEEE Global Initiative on \n“autonomous” in a given domain as long as \nEthics of Autonomous and Intelligent Systems \nthey are capable of accomplishing their tasks \nfor the creation of Ethically Aligned Design: A \ndespite environment changes within the given \nVision for Prioritizing Human Well-being with \ndomain. This terminology is applied throughout \nAutonomous and Intelligent Systems (A/IS), \nEthically Aligned Design, First Edition to ensure \nFirst Edition; and\nthe broadest possible application of ethical \nconsiderations in the design of the addressed \n2. supports and commends the collaborative \ntechnologies and systems. \nprocess used by The IEEE Global Initiative \nto achieve extraordinary consensus in such \ncomplex and vast matters in less than three \nyears; and\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 289', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nAbout Ethically Aligned Design\nHow the Document Was Prepared\nThis document was developed by The IEEE Global Initiative on Ethics of Autonomous and Intelligent \nSystems, which is an authorized Industry Connections activity within the IEEE Standards Association,  \na Major Organizational Unit of IEEE.  \nIt was prepared using an open, collaborative, and consensus building approach, following the processes \nof the Industry Connections framework program of the IEEE Standards Association (standards.ieee.\norg/industry-connections). This process does not necessarily incorporate all comments or reflect the \nviews of every contributor listed in the Acknowledgements above or after each chapter of this work. \nThe views and opinions expressed in this collaborative work are those of the authors and do not \nnecessarily reflect the official policy or position of their respective institutions or of the Institute of \nElectrical and Electronics Engineers (IEEE). This work is published under the auspices of The IEEE \nGlobal Initiative on Ethics of Autonomous and Intelligent Systems for the purposes of furthering public \nunderstanding of the importance of addressing ethical considerations in the design of autonomous  \nand intelligent systems.\nIn no event shall IEEE or IEEE-SA Industry Connections Activity Members be liable for any errors, \nomissions or damage, direct or otherwise, however caused, arising in any way out of the use of  \nor application of any recommendation contained in this publication. \nThe Board of Governors of the IEEE Standards Association, its highest governing body, commends the \nconsensus-building process used in developing Ethically Aligned Design, First Edition, and offers the \nwork for consideration and guidance in the ethical development of autonomous and intelligent systems.\nHow to Cite Ethically Aligned Design\nPlease cite Ethically Aligned Design, First Edition in the following manner:\nThe IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. Ethically Aligned Design:  \nA Vision for Prioritizing Human Well-being with Autonomous and Intelligent Systems, First Edition. \nIEEE, 2019. https://standards.ieee.org/content/ieee-standards/en/industry-connections/ec/\nautonomous-systems.html\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 290', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems\nKey References\nKey References\nKey reference documents listed in Ethically Aligned Design, First Edition: \n•  Appendix 1 - The State of Well-being Metrics (An Introduction)  \nbit.ly/ead1e-appendix1 \n(Referenced in Well-being Section)\n•  Appendix 2 - The Happiness Screening Tool for Business Product Decisions   \nbit.ly/ead1e-appendix2 \n(Referenced in Well-being Section) \n•  Appendix 3 - Additional Resources: Standards Development Models and Frameworks  \nbit.ly/ead1e-appendix3 \n(Referenced in Well-being Section)\n•  Glossary \nbit.ly/ead1e-glossary\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. 291', 'The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems  \n(“The IEEE Global Initiative”) is a program of The Institute of Electrical and \nElectronics Engineers, Incorporated (“IEEE”), the world’s largest technical \nprofessional organization dedicated to advancing technology for the benefit  \nof humanity with over 420,000 members in more than 160 countries. \nThe IEEE Global Initiative and Ethically Aligned Design contribute to  \nbroader efforts at IEEE about ethics in technology.  \n©2019 IEEE\n082-03-19']"
F551030,10 September 2020,Solène Gastal,Wirtschaftsverband,Fédération Française de l'Assurance,mittel (50 bis 249 Beschäftigte),-,Frankreich,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"FFA welcomes EC’s initiative to encourage the ethical and responsible development of artificial intelligence (AI) in the EU. AI offers economic, societal and competitive advantages to European businesses and citizens. The appropriate ethical and legal framework, based on European values, in line with the Charter of Fundamental Rights, will allow for the deployment of improved AI. A European approach is necessary to limit the fragmentation of the internal market, ensure fair competition and protect European citizens and businesses from unreliable AI.
Calculating premiums, evaluating claims, provisioning funds using algorithms are the very essence of insurer's activities and AI can significantly improve operational efficiency and customer experience. Insurers fully acknowledge the necessity to protect fundamental rights as well as the safety of users and third parties. In that regard, a human-centric technology is needed. Insurers are more and more able to consider customers’ needs and expectations to offer the most adapted products through differentiation, without leading to discriminatory or anti-privacy practice, and further sectoral initiatives are in development. 
In addition, as outlined in the High Level Expert Group on AI’s report, aspects related to innovation are governed by extensive European legislation (GDPR, anti-discrimination rules, etc.). Likewise, insurance is a heavily regulated sector (Solvency II Directive, Insurance Distribution Directive, etc.). A soft-law approach to facilitate and spur industry-led intervention (Option 1) would be the most relevant approach to encourage a single set of AI principles while enforcing existing rules.
Another alternative is the establishment of a voluntary labelling scheme (Option 2). For insurers, it would allow AI applications to meet requirements, thus, creating an upward competition. However, to maintain the necessary level playing field, a solution could be to encourage labelling by providing a clear, simple and inexpensive model through a European reference system (see for example the French National Institute of Intellectual Property (INPI) recognition mechanism). This would prevent a fragmentation of labelling and of the internal market.
In case the EC is heading towards an EU legislative instrument establishing mandatory requirements for AI applications (Option 3), it should adopt a risk-based approach. The legislative instrument should be limited to specific category of AI applications raising high-risk for users and third-party so not generate disproportionate burdens hindering innovation. The identification of high risks should follow the conditions set out in the White Paper on AI.
Further debates with all stakeholders would then be needed to elaborate a clear and realistic definition of high-risk. AI can produce different effects and having a risk assessment scale for autonomous system should consider the different types of risks. Once defined, it is essential to avoid over-regulation of many sectors and hinder their development.  As stated by the EC, a double criterion should be applied: First, activities that could raise risks could be identified by the EC allowing for less fragmentation of the market. Second, competent sectoral authorities could then assess the high-risk potential of specific AI applications, which are not harmful in themselves and have an impact only based on their use. Only a sectoral authority would be able to understand the realities of a market and the specific context of AI applications.
Insurance should not be considered as a high-risk sector. Insurers assist their policyholders at every stages of their lives and on their daily routine. Certain insurers’ activities may be related to sectors that could raise risks (e.g. transport or health) and, the related AI applications should be assessed. It would however not be pertinent to individualize the insurance sector and to categorize the variety of its activities as a high-risk sector."
-,-,-,-,-,-,-,-,-,"['September 2020 \n \nDevelopers Alliance’s response to the Inception Impact Assessment for a legal \nact of the European Parliament and the Council laying down requirements for \nArtificial Intelligence \n \n \n \nDevelopers  Alliance  welcomes  the  opportunity  to  further  provide  feedback  on  the \nEuropean Commission’s proposals on requirements for AI.  \nThe Inception Impact Assessment (IIA) presents several regulatory options and the \nunderlying policy objectives, based on the approach initiated by the White Paper on AI. \nOur response emphasizes the need to avoid excessive regulatory burden and to adopt a \nfit-for-purpose approach aimed at promoting AI development in the EU. \nWe commend the EC’s engagement to avoid “a fragmentation of the Digital Single \nMarket into potentially divergent frameworks preventing the free circulation of goods \nand services containing AI”.  \n \nOn the problems the initiative aims to tackle \nWe suggest extreme caution in extending liability to immaterial harm. This could prove \nextremely difficult to assess and quantify and therefore pose legal uncertainties. \nCitizens are interacting with AI embedded in products and services. Applicable \nlegislation, be it general frameworks or sectoral legislation, is already covering risks to \nfundamental rights and freedoms (e.g. principle of accountability under the GDPR). \nAppropriate guidance on practical issues and soft law initiatives would ensure better \nenforcement. \nWith reference to the potential harms, we disagree with the perspective presented by \nthe IIA. In the case of AI systems used for decision making and the enforcement of EU \nlaw meant to protect fundamental rights, the approach should not focus on the \ncharacteristics of certain AI systems. Appropriate safeguards in the administrative \nprocess itself are more important instead, in order to mitigate biased and discriminatory \n1', 'outcomes, in the same vein as they should be applied for human only decision-making. \nIt is reasonable to expect that complete explanations on how the outputs of some AI \nsystems are provided might be impossible (e.g. deep learning). This becomes obvious \nwhen we consider that human decision-making processes aren\'t fully known, and that \nAI targets the same problems. One should avoid setting a higher standard on AI \n\u200b\nthan human decision-making.  \nThe alleged new safety risks that AI may generate for users and third parties doesn’t \nneed to be “explicitly tackled clearly by the product safety legislation”. General \nrequirements, such as not to put users’ life and health in danger, provide a sufficient \nlegal base. Stand-alone software is usually put on the market as a specific product and \nthe legal guarantees against harm are already covered by existing relevant specific \nlegislation or civil liability.  \nWe welcome EC’s commitment to ensure coherence and complementarity between the \nlegal act setting out requirements for AI and the proposals on the revision of the \nframeworks for product liability and general product safety.  \n \n On the proposed policy options \nWe agree with the overall objective and the specific aims. European innovators and \nentrepreneurs should benefit from a coherent, harmonized framework, allowing them to \ndevelop and deploy AI products and services across the Single Market and beyond.  \nIt  is  essential  to  avoid  excessive  regulatory  burdens.  Therefore,  a  regulatory \nintervention should  only adapt the existing legal requirements and cover the gaps, \nwhere necessary. We are looking forward to the impact assessment which “will identify \nand  quantify  regulatory  costs-benefits  savings,  burden  reduction  and  simplification \npotential”. \nBaseline (no EU policy change, Option ""0""): \nAs indicated in the roadmap, ‘EU legislation on the protection of fundamental rights and \nconsumer protection as well as on product safety and liability remains relevant and \napplicable to a large number of emerging AI applications’. There are many areas where \nnew legislation is not justified.   \nAlternative options to the baseline scenario: \nOption 1: EU “soft law” (non-legislative) \nIndustry  initiatives  for  trustworthy  AI  reflect,  as  mentioned  by  the  IIA,  “growing \nconsensus around the importance of aspects such as privacy, accountability, safety and \nsecurity,  transparency  and  explainability,  fairness  and  non-discrimination,  human \ncontrol,  professional  responsibility”.  The  Ethics  guidelines  and  the  assessment  list \ndeveloped by the AI HLEG could provide a good complementary tool, once agreed. This \nwill ensure a coherent industry-led approach that would meet the proposed objectives \n2', 'in a more efficient way, avoiding unnecessary burdens. This will allow developers to \nfurther innovate while still promoting caution. \nOption 2: EU legislative instrument setting up a voluntary labelling scheme \nWe reiterate our reservations towards this proposal expressed in the feedback to the \nWhite Paper on AI. The proposed labelling system will create pressure on startups and \nscale-ups.  Even  if  voluntary,  the  proposed  system  will  add  additional  costs  and \nadministrative  burdens  that  only  larger  businesses  can  afford.  SME  access  to  the \nmarket  could  be  hampered.  As  mentioned  above,  developers  already benefit from \nself-regulatory efforts  that provide best practices in different software  development \necosystems. However, it is still premature to think that AI standardization could offer \nsolutions at scale for the market.  \nOption  3:  EU  legislative  instrument  establishing  mandatory requirements for all  or \ncertain types of AI  \nThe  first  sub-option  (political  decision  on  the  use  of  biometric  systems)  must  be \nconsidered in the context of what unique role AI would play versus human surveillance \nand identification. The regulation of facial recognition is not legally distinct from human \nsurveillance, and regulation in this area needs to be principle-based, and not technology \nbased. Above all, the use of AI in fingerprint or biometric access control (on phones or \nin secure facilities) should not be impeded through overbroad definitions. \nThe second sub-option, proposing a scope limited to certain AI applications that might \n\u200b\npose “high-risk”,  based on the two  criteria set out in the White Paper (sector and \nspecific use/impact on rights or safety) could represent  a suitable approach  in  the \ncontext of the adaptation of the current applicable legislation. We underline that the \n“high-risk”  formula  should  be  applied  case-by-case,  according  to  the  proposed \ncumulative criteria (selected sectors and use cases). Legal clarity is key.   \nThe third sub-option, of a broad scope, covering all AI applications, is disproportionate \n\u200b\nand unjustified. The only  feasible approach  is the adaptation of existing legislation. \nSuch a horizontal legal act would be a source of legal uncertainty affecting all industry \nsectors, with a chilling effect on AI development in the EU.  \nOption 4: combination of any of the options above taking into account the different \n\u200b \u200b\nlevels of risk that could be generated by a particular AI application. \nThis option is not feasible. AI systems are embedded in products and services subject \nalready  to  existing  legal  requirements,  as  the  IIA  also  states.  Furthermore,  AI \ntechnologies can have dual-use, or new forms of use can be discovered after they are \nplaced on the market. Other new, innovative solutions can always be discovered. This \nrequires that the applicable legislation be future-proof and technology-neutral. \n  \n \n3', 'Regulatory experimentation promoting innovation \nWe  strongly  suggest  to  consider  regulatory  sandboxes  as  a  flexible  and  agile \n\u200b \u200b\napproach to pursue the policy objectives, a suitable alternative to regulation.  \nThe Coordinated Plan on Artificial Intelligence stated that “bringing state-of-the-art AI \napplications  to  the  market  requires  experimenting  and  testing  in  real-world \nenvironments”1.  The  Competitiveness  Council  called  on  to “explore  the  use, where \nappropriate, of regulatory sandboxes for the regulations that may have impact on the \nfunctioning of the Single Market in a digital environment, in particular for the projects \nrelated  to  the  use  of  Artificial  Intelligence”2. Member States like Finland, Malta or \nLuxembourg  consider  this  approach  in  their  national  AI  strategies3.  The  European \nParliament also welcomed “the use of regulatory sandboxes to introduce, in cooperation \nwith  regulators,  innovative  new  ideas,  allowing  safeguards  to  be  built  into  the \ntechnology  from  the  start,  thus  facilitating  and encouraging its market entry” and \nhighlighted “the need to introduce AI-specific regulatory sandboxes to test the safe and \neffective use of AI technologies in a real-world environment”4. \nDespite all the political statements mentioned above, the White Paper on AI lacks any \nreference to regulatory sandboxes.  \nAs a relevant good practice example, the Norwegian Data Protection Authority set out a \nregulatory sandbox for AI5, which “will help to increase knowledge about and provide \ninsight into new innovative solutions, as well as make it easier to identify potential risks \nat an early stage”. \n \nOn the ex-ante conformity assessment \nCompulsory ex-ante conformity assessments will only set up a layer of redundant and \noverlapping obligations. The process of adjusting the current legislation should follow \nthe  objective  of  addressing  high  risks  and  identifying  where  amendments  or  new \nspecific provisions should be added, as lex specialis.  \nWhere justified (e.g. when the use of certain AI applications might pose “high-risks”, \nclearly  defined  by  the  legal  act),  we  recommend  an  ex-ante  risk  self-assessment \n(similar to the data protection impact assessments under GDPR), complemented by \nex-post enforcement. We strongly recommend avoiding disproportionate administrative \nburdens.  \nWe agree that existing ex-ante and ex-post enforcement structures would need to be \ncompetent and fully equipped to fulfil their mandate where AI tools are used. \n1 https://ec.europa.eu/digital-single-market/en/news/coordinated-plan-artificial-intelligence \n\u200b\n2 https://www.consilium.europa.eu/media/39510/st09743-en19.pdf \n\u200b\n3 Finland AI Strategy Report | Knowledge for policy; Artificial Intelligence: a strategic vision for Luxembourg; A \n\u200b \u200b \u200b \u200b \u200b\nStrategy and Vision for Artificial Intelligence in Malta 2030 _The_Ultimate_AI_Launchpad_vFinal.pdf \n4 https://www.europarl.europa.eu/doceo/document/TA-8-2019-0081_EN.html \n\u200b\n5 https://iapp.org/news/a/norwegian-dpa-creating-regulatory-sandbox-for-ai/ \n\u200b\n4', 'On a definition of AI \nThe legal definition should not be over-inclusive but should seek legal clarity, in order to \nserve  regulatory  purposes.  The  definition  should  describe  those  elements that are \nspecifically relevant for the scope of the legal act. \n \nEU regulation within the international context \nWe emphasize the importance of coordination with the current efforts in defining ethical \nstandards for AI at the international level.  \nThe  intrinsic  nature  of  software  development  and  the  digital  economy  transcends \ngeographical and political borders. European AI developers should not be forced to work \nin  isolation  while  advancement  continues  beyond  our  borders.  Innovative  startups \nshould not be limited within the EU borders by obstructive standards and administrative \nburdens.  European  consumers  should  not  be  deprived  of  the  potential  benefits of \naccessing innovative products. \n   \n5']"
F551021,10 September 2020,Cornelia Kutterer,Unternehmen/Unternehmensverband,Microsoft Corporation,groß (250 oder mehr Beschäftigte),0801162959-21,Vereinigten Staaten,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"Microsoft appreciates the opportunity to respond to the European Commission’s Inception Impact Assessment (“IIA”) on its proposal for a legal act setting out requirements for artificial intelligence (“AI”). We support the Commission’s goal of ensuring that AI evolves in a manner that respects EU values and fundamental rights. An appropriate regulatory framework for AI could advance the responsible development and use of AI both within the EU and internationally. Given the EU’s deep integration into the global economy, any legislative framework for AI also should be interoperable to the greatest extent possible with efforts in other jurisdictions and forums to promote trustworthy AI.

In our response to the Commission’s February 2020 AI White Paper (“AI White Paper Response,” uploaded separately), we welcomed the Commission’s proposal to adopt an incremental, risk-based approach to AI regulation. We noted that such an approach—one that imposes mandatory requirements on a discrete and clearly defined set of high-risk AI scenarios—will help protect people and society against AI use cases that raise significant risks of harm, while giving Europe’s AI sector the flexibility it needs to grow and mature. 

We continue to support that approach. Given the many nuances raised by the various scenarios in which AI can be deployed, we believe the Commission can most effectively achieve its goals through a mix of binding requirements for scenarios that pose the greatest risk, combined with soft-law approaches (e.g., codes of conduct, self-regulatory mechanisms) for lower-risk scenarios that incentivize the use of governance mechanisms and tools for promoting trustworthy AI. Microsoft addresses these issues primarily from the perspective of a developer and supplier of AI solutions that customers across the public and private sectors use for an almost unlimited range of purposes. These various uses may raise very different risks. In light of this reality, we urge the Commission give careful thought to which obligations are most appropriately placed on developers of multi-purpose AI solutions, and which are most appropriately placed on deployers.

Microsoft supports Option #4 of the IIA—namely, a combination of Options 1 through 3 that considers the different levels and types of risks that different AI scenarios may raise. This combination of measures is vital to ensure that the EU’s overall regulatory approach to AI is meaningful, balanced, and effective. It bears repeating at the outset that there is no “one-size-fits-all” approach to AI regulation. Many AI systems pose low or even no risks to individuals or society (e.g., AI systems that optimize storage of items in a warehouse or fix typing errors); these systems do not require new or mandatory regulation. And where meaningful risks do exist, they will often be different and context-specific, and thus require different mitigation measures, including regulation.

Please find attached a more detailed response.
","['Response of Microsoft Corporation to the European Commission’s \nInception Impact Assessment on Artificial Intelligence Legislation \n(10 September 2020) \nMicrosoft appreciates the opportunity to respond to the European  Commission’s \nInception  Impact  Assessment  (“IIA”)  on  its  proposal  for  a  legal  act  setting  out \nrequirements for artificial intelligence (“AI”). We support the Commission’s goal of \nensuring that AI evolves in a manner that respects EU values and fundamental rights. \nAn  appropriate  regulatory  framework  for  AI  could  advance  the  responsible \ndevelopment and use of AI both within the EU and internationally. Given the EU’s deep \nintegration into the global economy, any legislative framework for AI also should be \ninteroperable to the greatest extent possible with efforts in other jurisdictions and \nforums to promote trustworthy AI. \nIn our response to the Commission’s February 2020 AI White Paper (“AI White Paper \nResponse,” uploaded separately), we welcomed the Commission’s proposal to adopt \nan  incremental,  risk-based  approach  to  AI  regulation.  We  noted  that  such  an \napproach—one  that  imposes  mandatory  requirements  on  a  discrete  and  clearly \ndefined set of high-risk AI scenarios—will help protect people and society against AI \nuse cases that raise significant risks of harm, while giving Europe’s AI sector the \nflexibility it needs to grow and mature.  \nWe continue to support that approach. Given the many nuances raised by the various \nscenarios in which AI can be deployed, we believe the Commission can most effectively \nachieve its goals through a mix of binding requirements for scenarios that pose the \ngreatest  risk,  combined  with  soft-law  approaches  (e.g.,  codes  of  conduct,  self-\nregulatory mechanisms) for lower-risk scenarios that incentivize the use of governance \nmechanisms and tools for promoting trustworthy AI. Microsoft addresses these issues \nprimarily  from  the  perspective  of  a  developer  and  supplier  of  AI  solutions  that \ncustomers across the public and private sectors use for an almost unlimited range of \npurposes. These various uses may raise very different risks. In light of this reality, we \nurge the Commission give careful thought to which obligations are most appropriately \nplaced on developers of multi-purpose AI solutions, and which are most appropriately \nplaced on deployers. \nMicrosoft supports Option #4 of the IIA—namely, a combination of Options 1 \nthrough  3  that  considers  the  different  levels  and  types  of  risks  that  different  AI \nscenarios may raise. This combination of measures is vital to ensure that the EU’s overall \nregulatory approach to AI is meaningful, balanced, and effective. It bears repeating at \n1', 'the outset that there is no “one-size-fits-all” approach to AI regulation. Many AI \nsystems pose low or even no risks to individuals or society (e.g., AI systems that \noptimize storage of items in a warehouse or fix typing errors); these systems do not \nrequire new or mandatory regulation. And where meaningful risks do exist, they will \noften be different and context-specific, and thus require different mitigation measures, \nincluding regulation. \nHere are our views on the Options: \nOption #1: EU “soft law” (non-legislative) approach to facilitate and spur \nindustry-led intervention (no EU legislative instrument). \nMicrosoft believes that industry has responsibility to develop and deploy AI that is \ntrustworthy. We are investing heavily in AI governance standards and tools to put our \nethical principles into practice across our engineering and sales teams and to help \nensure that they  guide our actions on a consistent basis.  \nBased on this experience, we believe developers and deployers of AI applications \nshould  adopt appropriate governance standards and procedures to support efforts to \noperationalize trustworthy AI across their organizations, and to use technologies, tools, \nand systems to help them identify and mitigate relevant risks of all types and levels. \nThis standard could include, for example, the following requirements:  \n•  Envision the full range of harms that any new AI system might impose on \nindividuals  and  society  both  at  the  outset  and  throughout  the  product \ndevelopment lifecycle, and take appropriate mitigation steps; \n•  Provide appropriate training for those involved in the design, development, \ntesting, and marketing of the AI system, and to assign specific individuals or \ngroups within the company with responsibility for overseeing implementation \nand compliance; \n•  Adopt  transparency  obligations  with  customers,  users,  and  other  affected \nstakeholders to inform them about risks inherent in the use of the relevant AI \nsystem as well as the systems capabilities and limitations; and  \n•  Adopt an escalation process through which employees and others can raise \nconcerns and seek guidance on the company’s compliance with its policies.  \nDeployers  might  have  similar  governance  obligations,  including  for  example  a \nrequirement to take necessary steps to avoid deployments of the AI system that could \npose certain risks of harm. In many cases, these steps may be specific to the particular \nuse scenario and/or sector at issue (we discuss these points further in our AI White \nPaper  response).  Similarly,  public  authorities  should  be  encouraged  to  develop \nguidance on how best to achieve responsible governance when AI is used. \n2', 'By leveraging private-sector-led efforts and related domain-specific expertise, the \nCommission’s soft-law options could lead to the development of industry- or sector-\nspecific codes of conduct  that effectively target the unique risks that arise in specific \nscenarios and sectors (e.g., education, health care, autonomous transportation). As \nthese codes mature, they could also foster convergence on common principles and \nbest practices applicable to other areas as well. \nThis said, and while we continue to believe that such private-sector-led efforts must \nplay a central role in promoting trustworthy AI and that the Commission should \nencourage the adoption of such efforts through various soft-law approaches, we also \nbelieve that these approaches alone will not always be sufficient to address the unique \nrisks to fundamental rights and safety that might arise in certain scenarios (as we \ndescribe in more detail below). Thus, we do not believe that the “no EU legislative \ninstrument” approach of Option #1 by itself would be appropriate. \n•  Option #2: EU legislative instrument setting up a voluntary labelling scheme. \nSimilar to our view on the soft law approach of Option #1, we see value in co-regulatory \nefforts to promote trustworthy AI, which might include efforts such as the voluntary \nlabelling scheme described in Option #2. A co-regulatory approach may be particularly \nappropriate  in cases where regulatory frameworks exist that require interpretation or \nadaptation in the light of AI, as well as in cases where AI usage could cause risk, but \nthat risk does not rise to the level of “high risk.” Option #2, therefore, should leverage \nexisting laws and regulatory frameworks to the largest possible extent. \nAs noted in our AI White Paper response, labelling could prove beneficial for deployers \nwhen comparing and choosing among competing AI solutions or those affected by the \ndevelopment of an AI system. Such a scheme could signal to the market that a given \nAI system meets a defined set of requirements, such as the implementation of an \norganizational AI standard designed to ensure risk is identified and mitigated. That \nsaid, we think certain AI scenarios may raise such uniquely high risks that they merit \nthe imposition of mandatory requirements. Thus, while we believe that a co-regulatory \napproach could be a useful component of a broader AI regulatory strategy, we do not \nthink that Option #2 on its own would be sufficient to address all risks posed by AI in \nall scenarios.    \nTo the extent a version of Option #2 is included in the final legislative package, we \nwould  encourage  the  Commission  to  consider  a  broader  range  of  co-regulatory \nmechanisms beyond just labelling. Companies operating in the EU have significant \nexperience in helping develop, and complying with, industry codes of conduct, formal \nand  informal  standards,  and  related  self-certification  schemes.  Some  of  these \nmechanisms involve labeling, others do not. Developers and deployers of AI should be \n3', 'encouraged to leverage that experience in developing similar voluntary compliance \nmechanisms to promote trustworthy AI.  \nAny such mechanism might  include objectives such being transparent about  the \nlimitations of a given system or the key data elements used to train the system, or \nadopting internal processes to promote accountability. It should also allow for regular, \ntransparent, and independent monitoring and evaluation. One source of inspiration for \nrelevant objectives could be the Assessment List for Trustworthy AI developed by the \nHigh  Level  Expert  Group  on  AI  (although  not  all  the  components  will  be  either \nappropriate or necessary for all AI scenarios, given the many different settings in which \nAI can be used). We would also suggest drawing from the experience of companies \nthat are already implementing trustworthy AI governance models. \nExperience has shown that both self- and co-regulatory instruments, implemented in \naccordance with the different legal traditions of the Member States, can play an \nimportant  role  in  delivering  a  high  level  of  protection  (for  instance,  under  the \nAudiovisual  Media  Services  Directive,  and  more  recently  under  the  General  Data \nProtection Regulation). Measures aimed at achieving general public-interest objectives \nin AI applications are more effective if they are taken with the active support of the \ndevelopers and deployers themselves. Building on voluntary measures, the proposed \nlegislative package could include several co- or self-regulatory mechanisms tailored to \nspecific scenarios and use cases, modelled on the Principles for Better Self- and Co-\nregulation, while creating a legislative backstop if industry fails to meet the objectives.  \n•  Option #3: EU legislative instrument establishing mandatory requirements for \nall or certain types of AI applications (see sub-options below). \nMicrosoft agrees that certain AI deployment scenarios may raise such unique and \nsignificant  risks  that  their  use  should  be  subject  to  mandatory  requirements. \nAccordingly, we believe that Option #3 should be part of the Commission’s legislative \npackage (along with soft law options, for the reasons set out above). Given the almost \ninfinite scenarios and domains in which AI can be deployed, it will be important for the \nmandatory elements of the legislative package to be carefully targeted at those AI \ndeployments that raise the greatest risk of harm, and where existing EU and Member \nState laws do not already provide sufficient protections.  \nOn this point, Option #3 of the IIA sets out three sub-options for the scope of any \nmandatory regulatory requirements—namely, that they should apply to:  \n(1) “a  specific  category  of  AI  applications  only,  notably  remote  biometric \nidentification systems (e.g. facial recognition)”;  \n(2) “high-risk” applications as that term was defined in the AI White Paper; or  \n(3) all AI applications.  \n4', 'Given the nascent nature of AI development and deployment, Microsoft believes that, \nwithin this Option, the Commission should look to sub-options 1 and 2.  One benefit \nto  a  focus  on  specific  categories  of  AI  applications  (such  as  remote  biometric \nidentification) is that learnings from addressing these categories could inform future \nefforts to regulate other high-risk AI scenarios. Candidates for such future regulation \ncould include AI applications that raise a meaningful risk of physical harm, such as AI-\ndriven  vehicles.  We  also  believe  that  the  legislative  framework  should  include  a \nmechanism that enables the EU, through new legislative instruments that build on the \nframework, to extend appropriate mandatory requirements into new areas based on \nthe learnings generated from self or co-regulatory regimes, or from the enforcement \nof existing laws or new laws applying to specific systems. Because the harms will \nnecessarily differ across different use cases, any future regulation must be tailored to \nthe unique risks raised by the AI scenario at issue. \nWith regard to sub-option 1, Microsoft has been outspoken on the need to legislate \nFRT when used by public authorities, also in Europe despite the EU’s comprehensive \nlegislation on data protection. The use of FRT by public-sector authorities for remote \nidentification, for example in public spaces, is widely recognized to raise unique and \npotentially serious risks to fundamental rights and other vital interests. We believe the \nCommission could significantly advance the discussion on trustworthy AI both within \nand outside of the EU by adopting a comprehensive legislative framework on such \nuses.  \nWithin Europe, some Member States have begun to consider what an appropriate \nregulatory framework for police use of FRT might include. Courts are also beginning to \ndefine the permissible scope of use of FRT by public authorities. UK courts, for example, \nrecently held that police use of FRT for remote identification must be authorized by law \nand subject to robust safeguards, in a case involving the South Wales Police. Regulation \nin this area by individual Member States  could be usefully harmonized through EU-\nwide quality standards, such as  on testing and transparency requirements, robust \nnecessity and proportionality standards, and strong safeguards for the protection of \nfundamental rights.  \nLawmakers in jurisdictions outside of Europe are similarly beginning to recognize the \nneed for legislation in this space. The State of Washington in the United States, for \nexample, recently adopted a law that regulates all public-sector uses of AI. Many of the \nrequirements set out in the law effectively impose requirements not only public sector \nentities, but also on the suppliers of these AI technologies. Microsoft supported the \nWashington State legislation, which includes a number of important safeguards. \nAn EU-level legislative instrument could provide the legal basis necessary for police use \nof FRT, on the condition that such use is strictly necessary and proportionate, and \n5', 'subject  to  specific  safeguards.  These  safeguards  might  include,  for  example, \nrequirements that use of FRT for remote identification is allowed only in relation to \nmost serious crimes or imminent risks to life, and in specific locations for limited times; \nthat police may not use FRT to monitor individuals in the exercise of their fundamental \nrights (e.g., participating in a demonstration); that police develop robust data use and \nmanagement policies, and conduct impact assessments, before any deployments; that \nsystems are deployed in open and transparent ways; and that humans be responsible \nfor any decisions made on the basis of FRT. EU legislation could also impose obligations \non developers of FRT intended for such uses, such as requirements to make  available \nan application programming interface (API) or other technical capability to enable \nlegitimate, independent and reasonable testing for unfair bias. \n•   Option # 4: combination of any of the options above taking into account the \ndifferent levels of risk that could be generated by a particular AI application. \nAs indicated above, Microsoft supports Option #4—specifically, a combination of \nOption #1 (soft-law measures), Option #2 (a voluntary labelling scheme or similar \napproaches), and Option #3 with sub-options 1/2 (mandatory requirements, including \nfor  public-sector  use  of  FRT  in  public  spaces).  To  ensure  that  the  Commission’s \nlegislative package can adequately address the many different contexts in which AI can \nbe deployed, and the very different  levels and types of risks that these scenarios may \npose, we believe that this combination of options is both appropriate and necessary – \ndependent on the risk level. \n \n*  *  *  *  * \nOnce again, Microsoft welcomes this opportunity to respond to the Commission’s IIA. \nAny questions about this response should be directed to Cornelia Kutterer, Senior \nDirector European Government Affairs (cokutter@microsoft.com). \n \n6']"
-,-,-,-,-,-,-,-,-,"['PARC-000237-2020\n \nSubject: Proposal for a legal act of the European Parliament and the Council laying\ndown requirements for Artificial Intelligence\n \nApplicant: European Commission - CNECT.A.2\n \n \nI. General comments:\n \n1. First of all, we welcome the opportunity to comment on this\ninitial impact assessment document referring to the legislative initiative of the\nEuropean Parliament and of the Council laying down requirements for\nArtificial Intelligence (AI).\n \nAs well mentioned in this document, AI is a technology that can contribute\nrelevant to a diversity of economic and social benefits across\nindustrial and social sectors, providing important competitive advantages for\ncompanies and enhance the most varied social and environmental benefits.\n \n2. Among others, AI brings new challenges in terms of civil liability,\nnamely in areas such as product safety legislation, which can\nwill be further developed to provide a greater degree of legal certainty.\n \nIt also raises a significant number of ethical and legal issues, since only doing\nmake sense to bet on the development and adoption of AI if it is safe and legal,\nrespecting the fundamental rights of citizens. Indeed, some of their\nuses and applications can cause physical and moral damage, particularly in terms of\nrespects the health, safety and physical integrity of people, as well as causing\nviolation of privacy or restrictions on freedom of expression, for example.\n \n \nII. Specialty comments:\n \n1. According to a survey carried out by consumer organizations in nine\nEU countries1 and published this month, consumers believe that intelligence\nartificial intelligence (AI) can bring benefits. However, when asked to assess the value\naggregate of a series of specific services that already today use AI technology - for\n                                                 \n1 This survey was carried out simultaneously by consumer organizations from nine EU countries\n(Test Achats/Test Aankoop - Belgium; Forbrugerrådet Tænk - Denmark; UFC-Que Choisir - France;\nvzbv - Germany; Altroconsumption - Italy; Federacja Konsummentow - Poland; Deco Protest - Portugal;\nOCU - Spain and Sveriges Konsumenter - Sweden) throughout November and December 2019.\n1', ""example, home virtual assistants or advertisements on e-commerce sites -\nmost respondents are not convinced.\n \nConsumers also expressed a number of concerns, such as the issue that the\nAI could lead to an increase in breach and abuse of your personal data. At\ntrue when it comes to particularly intrusive technologies such as\nvoice recognition, many consumers - 68% in Germany and 71% in Belgium -\nlittle trust in protecting their privacy.\n \nA large number of respondents said they do not think the current legislation is\nadequate to effectively regulate AI-based activities. only a fifth\nof respondents said current rules protect them from the potential harm that AI\nrepresents. EU plans for an AI law are therefore very much in line\nwith people's expectations.\n \n2. This survey also showed that consumers trust the potential of\nAI, but wish to remain in control. More than two-thirds of respondents\nexpressed that users should have the right to say “no” to taking\nautomated decision.\n \n3. Other survey responses indicate that:\n\uf0b7 Knowledge about AI is still relatively low, with 21% of\nconsumers saying they have never heard of or have no idea of \u200b\u200btheir\npresence.\n\uf0b7 Consumers expect services based on machine calculations\nhelp predict traffic accidents (91%), health (87%) or problems\nfinancial (81%).\n\uf0b7 Consumers report experiencing “bad service”, with 41% of\ninterviewees, for example, reported a bad experience with regard to\ninformation provided for loan proposals based on\nautomated decisions.\n \n4. AI-powered products and services (from voice assistants to chat bots)\nare increasingly present in the lives of consumers. Still, 43% of the\nrespondents feel ill-informed about AI or have never heard of it. AND\nwhile consumers believe that AI can bring benefits, they also\nclaim that such benefits have not yet occurred. Many also raise serious\nconcerns, with up to five in ten saying that AI will lead to discrimination\nunfair.\n \ntwo"", 'We find it worrying that most consumers do not trust their\nprivacy is protected when using AI tools such as smart watches\nor voice assistants. Furthermore, consumers report being concerned about the\nrisk that companies and governments could deploy AI to manipulate their decisions and\nthat AI leads to unfair discrimination. EU lawmakers need to take these\nconcerns seriously and ensure that consumers are protected and can\nrely on this technology, being protected from risks such as discrimination or\nmanipulation.\n \n \n3']"
F551019,10 September 2020,Ann Becker,Wirtschaftsverband,Interactive Software Federation of Europe,sehr klein (1 bis 9 Beschäftigte),20586492362-11,Belgien,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"ISFE represents the video games industry in Europe and is based in Brussels, Belgium. ISFE welcomes the opportunity to submit comments to the European Commission Inception Impact Assessment regarding the proposed policy options and instruments for Artificial Intelligence. Please find attached our comments. ","['Comments to the European Commission Inception impact Assessment regarding \nthe proposed policy options for a legal act of the European Parliament and the \nCouncil laying down requirements for Artificial Intelligence. \n \n10 September 2020  \nTransparency Register Identification Number: 20586492362-11 \n \nIntroduction  \n \n1.  ISFE welcomes the opportunity to submit comments to the European Commission Inception \nImpact  Assessment  regarding  the  proposed  policy  options  and  instruments  for  Artificial \nIntelligence. In June,  ISFE responded to the Consultation on the White Paper on Artificial \nIntelligence – A European Approach in which we (i) described the role of the video game industry \nin the development and use of AI, (ii) outlined the policy areas of particular importance in \nbuilding an ecosystem of excellence to support the development and uptake of AI across the EU \neconomy, and (iii) expressed our position as regards the proposed regulatory approach as \noutlined in  the White Paper.  \n \n2.  ISFE represents the video games industry in Europe and is based in Brussels, Belgium. Our \nmembership  comprises  national  trade  associations  in  18  countries  across  Europe  which \nrepresent in turn thousands of developers and publishers at national level. ISFE also has as direct \nmembers the leading European and international video game companies, many of which have \nstudios with a strong European footprint, that produce and publish interactive entertainment \nand educational software for use on personal computers, game consoles, portable devices, \nmobile phones and tablets.  The video games sector represents one of Europe’s most compelling \neconomic success stories, relying on a strong IP framework, and is a rapidly growing segment of  \nthe creative industries. In 2019, Europe’s video games industry saw €21 billion in revenues. We \nhave seen a growth rate of 55% over the past 5 years in European key markets 1. Video games \nhave  a  proven  ability  to  successfully  drive  new  business  models  and  adapt  to  digital \ntransformation: online and app-based gaming represents 76% of the industry’s total European \nrevenue.  Via the emergence of on-demand and streaming services and the launch of new high-\nperformance consoles, together with the strong growth of mobile gaming, the industry offers \nplayers across Europe and in all age groups the possibility to enjoy and engage with video \n \n1 ISFE Key Facts 2020  from GameTrack Data by Ipsos MORI and commissioned by ISFE https://www.isfe.eu/isfe-\nkey-facts/  \nINTERACTIVE SOFTWARE FEDERATION OF EUROPE aisbl. \nRue Guimard 15 | 1040 Brussels | Belgium  \n  T +32 2 612 17 78 | info@isfe.eu | www.isfe.eu', 'games2.  Today 51% of Europe’s population plays videogames, which is approximately 250 \nmillion people, and 45 % of the players are women.  \n \n3.  The video game sector plays an important role in research and development of Artificial \nintelligence (AI), and AI is used in innovative and creative ways by the industry to create new \ncompelling experiences for players. Also, the simulated worlds of video games constitute a \nresearcher-friendly  environment:  video  games  are  rich  and  complex,  but  controllable \nenvironments providing important feedback to the researcher, in particular on which data  to \ncollect to further refine research in this area. For further details please consult the ISFE response \nto the consultation on the White Paper.  \n \nComments  on  the  proposed  policy  options  in  the  Inception  Impact \nAssessment \n \n4.  ISFE welcomes the opportunity to provide its view on the policy options presented in the \nInception Impact Assessment. In accordance with our previous contribution to the consultation \non the White Paper, ISFE believes that any new legislation must be considered with a cautious \napproach.  New  compulsory  requirements  to  low-risk  applications  would  result  in  a \ndisproportionate diversion of industry resource, stifling innovation, and damaging Europe’s \ncompetitiveness in global AI development. Because many AI applications pose no, or a low risk \nto individuals or society, it is important that the approach proposed by the European Commission \nprovides the necessary clarity to ensure that legislation remains proportionate and targeted to \nfulfil its objectives, to avoid that sectors that do not pose significant risks fall under a specific \nlegislative framework.  \n \n5.  Therefore a legislative act covering all types of AI would not be proportionate and would \ndisregard the legislative framework that is currently in place which apply already today to many \nexisting AI applications. The White Paper recognised that developers and deployers of AI are \nalready subject to European legislation on fundamental rights (e.g. data protection, privacy, \nnon-discrimination), consumer protection, and product safety and liability rules. For example the \nGDPR already imposes the obligation to inform data subjects of automated decision making and \nprovides them with the right not to be subject to a decision based solely on automated \nprocessing if it produces legal effects on them or similarly affects them. The GDPR also places an \nobligation on organisations to carry out Data Protection Impact Assessments to mitigate any \nhigh risks that AI applications may pose before such an AI application is implemented.  \n \n6.  New regulation and/or updating existing legislation in some areas of applications of AI may be \nnecessary, especially where damages may cause substantial harm or pose a material risk of \nconsequential impact on individuals and society, such as facial recognition technology where \ndeployers process and store the data that is collected for example.  It is important, however, that \nany regulatory framework is flexible enough to embrace the evolution of wide AI technology to \nensure  that  Europe  remains  competitive  in  strategic  areas  in  the  global  ecosystem.   An \nimportant aspect of a high-risk AI regulatory framework would be to clarify which sectors and \n \n2 See also https://www.isfe.eu/data-key-facts/ \n2', 'uses should be covered by applicable legislation and which obligations would apply to which \nactors. \n \n7.  Policy option 3 (b) suggests that the EU legislative instrument could be limited to “high-risk” AI \napplications, which in turn could be identified on the basis of two criteria as set out in the White \nPaper (sector and specific use/impact on rights or safety) or could be otherwise defined.  The \nWhite Paper proposed to define a high risk AI application if it is subject to two cumulative \ncriteria: (i) the application is employed in a sector where significant risks can be expected to \noccur; and (ii) the application is used in such a manner that significant risks are likely to occur. \nMoreover the White Paper mentions that each obligation should be addressed to the actors who \nare best placed to  address any potential risks.  Such clarification regarding which obligations or \nrequirements should apply to developers and which should apply to deployers is important as it \nis important to recognise the separation, and unique responsibilities, of such roles. \n \n8.  A trustworthy and responsible AI approach is important also for no or low risk AI applications. \nThe video game sector promotes responsible data management and takes great care to protect \nplayer data – whether used traditionally or by AI - and makes sure the data is used in a manner \nconsistent  with  privacy  principles  and  regulations,  such  as  the  GDPR  and  the  ePrivacy \nDirective.   Many  video games companies have adopted or are in the process of adopting their \nown internal policies or best practices around using AI to address any bias and transparency \nissues. When decisions are made by AI that  affects a player or needs to be further explained, a \nhuman-supervised approach may often be used, but not all no or low risk AI application may \nrequire this. Video game companies are taking these ethical challenges seriously by developing \ninternal policies on these issues. \n \n9.  The Inception Impact Assessment and the White Paper suggest a  voluntary labelling system for \nAI systems that are not considered high-risk. Such a  voluntary scheme,  as proposed in policy \noption 2, and further outlined in the White Paper, would be legally binding for those operators \nthat join such a scheme, and in return they would use the AI trust label for their AI application. \nThis may negatively impact AI applications that would not join the voluntary labelling scheme \nsimply because these may apply industry best practices and voluntary frameworks already in \nplace3.  A  guiding  principle  for  the  European  Commission  should  instead  be  to  support \norganisations that are developing or deploying AI to allow them to identify risks or harms and \nto ensure guidance on  process to follow to mitigate any risks.  \n \n10. Flexibility and recognition of best practices should be a focal area of the Commission, to avoid \nchannelling all low risk AI applications into a voluntary legally binding scheme, which could \ncreate significant administrative burdens as well as disincentivising investment and innovation \n \n3 For example, Digital Innovation Hubs, such as the Digital Catapult have developed useful guidance for start-ups \nwhich support them in their use of, and approach to an ethical AI.  Also, the UK regulator the Information \nCommissioner’s Office (ICO) in partnership with the Alan Turing Institute have very recently issued guidance to \ngive organisations practical advice to help explain the processes, services and decisions delivered or assisted by AI \nto the individuals effected by them. Most recently, in July 2020 the Independent High-Level Expert Group on AI set \nup by the EU Commission has published “The Assessment List for Trustworthy AI (ALTAI)” which provides an initial \napproach for the evaluation of Trustworthy AI and the ICO has published its “Guidance on AI and Data \nProtection”. \n3', 'in AI. Guidelines or best practices that are not part of such a voluntary scheme could be as \nefficient in creating responsible and trustworthy AI. This is especially important to consider \nbecause low risk AI must already comply with EU legislation in the field of fundamental rights \n(e.g. data protection, privacy, non-discrimination), consumer protection legislation, and product \nsafety legislation. \n \n11. ISFE believes that for low risk AI such an approach, i.e. policy option 1, should be further \nsupported instead of a ‘regulation first’ approach. As technology is evolving at a rapid pace, a \nEuropean AI framework needs to be flexible enough to ensure that Europe remains competitive \nin strategic areas in the global ecosystem. Competition, technology, and customers are the \nnatural drivers of the market. Steps that would artificially channel developments along a \nparticular path needs to be envisaged with caution. \n   \n12. From the perspective of video games where AI applications appear in a controlled virtual \nenvironment,  while some requirements can be useful guidance because of how AI is used (e.g. \nto improve the player experience, to ensure that the gameplay environment is safe both from a \nnetwork security perspective and from a player safety perspective) such as: \n \na.   information and transparency on the purpose and the nature of automated decision \nmaking such as AI systems are important to create trust with the player, and where \ninformation requirements are foreseen in the GDPR,  \nb.  robustness and accuracy of AI systems are also important, and  \nc.  human oversight in relation to any online interactions such as in the moderation of \nonline communities to support player safety, \n \nsuch requirements can be part of guidelines or best practices but do not necessarily need to be \npart of a binding scheme to be efficient.  More generally, given the nature of the proposed low \nrisk voluntary labelling system, which would become binding once a developer or deployer opted \nin, it would appear disproportionate and harmful to innovation and Europe’s competitiveness to \nrequire compliance with all the requirements that are required in high risk applications.  This \nwould effectively categorise low risk applications as ‘high-risk’ once a developer or deployer \nopted in.  Furthermore, different requirements are more relevant to different sectors and ISFE \nwould respectfully suggest that the low risk approach should consider the nuances of different \nsectors. \n   \n13. As regards enforcement, a careful approach should be taken for low risk AI applications. As \nstated in the White Paper, the existing EU legislative framework already applies to AI, in \nparticular in the area of fundamental rights. The existing legislation includes enforcement \nmechanisms  specifically  targeted  to  any  breaches,  whether  that  is  consumer  protection \nlegislation or data protection or privacy legislation. Therefore, for low risk AI, the existing \nenforcement framework is sufficient.  \n \nEND \n \nContact: \nAnn Becker | Head of Policy & Public Affairs | ann.becker@isfe.eu \n4']"
F551016,10 September 2020,Lodewijk Noordzij,Behörde,Eurocities,mittel (50 bis 249 Beschäftigte),12493392840-79,international,Belgien,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"['People-centred Artificial \n  Intelligence (AI) in cities \n \nResponse to EU’s white paper on AI  \n \n \n \nMarch 2020 \n \nResponse to EU’s white paper on AI \n1.  AI needs data. A huge amount of data produced and collected in cities, crucial for local \ngovernments to improve public services and policies, is currently gathered and owned \nby the private sector. A single market for data needs to benefit all ecosystem players, \nincluding local governments. We call for legislative action to ensure access and use of \nbusiness to government (B2G) data sharing in the proposed Data Act planned for 2021. \n2.  Liability and accountability in AI are key to guaranteeing people’s safety. AI safety and \nreliability tests are burdensome; local public authorities do not have the expertise or \nthe budget to carry them out. A central EU body or agency should develop the necessary \nverification and validation procedures, and guarantee security and public safety. \n3.  Some AI uses can be considered high-risk. More detailed and clear descriptions of the \npossible  high-risk  uses  are  necessary  to  clearly  understand  when  a  local  public \nadministration is affected. We propose establishing a task force composed by AI experts \nand local public administration experts to better define the different possible high-risk \nuses. \n4.  AI could have a disruptive effect on the future of the job market and skills development \nin cities. ESF+ and the Youth Guarantee funding should focus more on digital literacy \ntraining, skills development and gender equality. \n5.  Local governments are crucial to fostering an ecosystem of excellence and trust in AI in \nEurope. The EU must work with local governments in the development of the future \nregulatory framework for a trustworthy AI, taking into account the principles defined by \nthe Cities for Digital Rights coalition. \nArtificial intelligence and cities  \nThe Covid-19 pandemic has disrupted our societies beyond precedent. The global health crisis has \nenormous economic and social repercussions. It has also highlighted the urgency of securing a \npeople-centred digital transformation in Europe, which is essential to ensure social and economic \nresilience in our cities. European digital strategies must fully support the mainstreaming of \ndigitalisation as part of rights-based public service delivery.  \nWe recognise the crucial role of AI technologies to respond to global public health emergencies. \nAt the same time, human rights and European fundamental values must be fully protected. The \nnext EU regulatory framework on trustworthy AI must specifically address the use of AI in public \nhealth emergency prediction and management making sure AI is used without undermining \nethics and fundamental rights.  \nMore generally, artificial intelligence is an enabler of change for local governments. AI is already \ntransforming  the  governance  of  the  city  and  society.  Public  administrations  can  increase \nApril 2020 | www.eurocities.eu | EUROCITIES statement on AI  1', 'efficiency and productivity while reducing costs through automation of processes and tasks. City \ngovernments can use algorithms and machine learning applications to predict service demand \nand  anticipate  urban  problems,  improve  decision  making  and  the  delivery  of  more  and \ninnovative public services.  \nAs  AI  is  becoming  more  advanced  and  more  accessible,  city  authorities  are  increasingly \nexperimenting and piloting AI, in many cases in combination with other tools such as IoT, 5G or \nBig Data technology, leveraging their potential while understanding new patterns and trends. \nAs with the first smart city projects1, cities use the results and lessons from AI experimentation \nto develop action plans and strategies, often in collaboration with national governments and \nwith the support of local and regional stakeholders.   \nHowever, AI adoption in cities is a long and costly process. Good data collection, processing, \nmanagement and opening is expensive and requires specific, high-level competences as well as \nnew governance approaches. A broader uptake of AI  technology locally requires intensive \nfinancial investment from the national and EU level to ensure adequate technological and skills \ndevelopment in city administrations. \nOpportunities and challenges for cities \nAI is a powerful means to fully transform Europe’s cities into sustainable, inclusive and smart \nplaces for people to live2. AI development also helps cities to reach the sustainability goals of \nthe Green Deal3 especially in sectors such as air quality and mobility. Local authorities use AI \nto leverage IoT applications and to predict the level of pollution in cities. City governments can \nget valuable information by analysing social media data on tourists’ behaviour and to adapt \ncultural policies and investments to needs. Cities use machine learning to predict parking space \navailability  to  then  efficiently  redirect  drivers  into  a  free  area.  Through  AI  chatbots, \ngovernments can communicate faster with citizens and stakeholders increasing their sense of \nparticipation. \nWhile AI adoption is increasing in cities, challenges connected to it are too. Disruption in the \nlabour market and skills gap, safety and liability, and digital rights protection are the most \nimportant ones for cities.  \n•  Disruption in the labour market and skills gap  \nThe rapid and exponential technological progress in AI will have a disruptive effect on the job \nmarket.  While  it  is  still  difficult  to  predict  to  which  extend  AI  and  robotics  will  affect \nunemployment rate, polarisation of jobs, income inequalities or discrimination in the labour \nmarket, the risks seem high. AI, powered by machine learning, automates tasks that might cause \ndisplacement or even replacement of workers, polarisation of job demand between high-skilled \nand low-skilled jobs, and worsen the status of already fragile groups of people, such as the \ndigitally excluded, long-term unemployed and low-skilled people. There is a strong need in \ncities for skills development including long-life learning programmes, training for low-skilled \npeople and early education on digital skills - also oriented to engage more young women into \ntechnology4.  \n•  Safety and liability \nAI applications can hide bias or amplify existing bias. Machine learning, powered by data, has \nno consciousness, reflects the human prejudices and opinions of the developers, and repeats \nand perpetuates data which, if flawed or incorrect, can lead to misinterpretations and errors. \nWhen AI is used, for example, for urban mobility (e.g. autonomous cars) or environmental (e.g. \nair pollution or water security) purposes, data bias might lead to higher risks for people’s \n \n–––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– \n \n1\n Becoming cities of the future, lessons learned from experimenting smart cities, October 2016, https://bit.ly/30Hy7Rj   \n2\n EUROCITIES statement ‘Smart cities in the age of the digital revolution’, March 2019, https://bit.ly/38khQoB  \n3\n EUROCITIES statement on the Green Deal (to be finalised), March 2020, https://bit.ly/3aNHkNg  \n4\n EUROCITIES report on equal opportunities and access to the labour market, December 2018, https://bit.ly/2wrt8tL  \nApril 2020 | www.eurocities.eu | EUROCITIES statement on AI  2', 'safety5. AI systems and algorithms must undergo rigorous testing to check their reliability and \nsafety, meaning time, the right expertise and costs. Liability issues are also a major concern \nfor cities. If a driverless bus runs over a pedestrian, who is responsible? There are many parties \ninvolved in an AI system (data provider, designer, manufacturer, programmer, developer, user \nand AI system itself), liability is difficult to establish when something goes wrong and where \nthere are many factors to be taken into consideration.  \n•  Digital rights  \nFundamental human rights might be put at risk in an AI age. AI systems can impair freedom of \nexpression, privacy and data protection, equality and fairness. Local governments are actively \ncommitted to protecting, promoting and monitoring citizens’ human rights in the digital sphere. \nThrough the Cities for Digital Rights6 coalition, supported by EUROCITIES, over 50 cities all over \nthe world are working to provide trustworthy and secure digital services and infrastructures for \nthe common good. From apps that gamify participation in local consultations to video-based \nsolutions  that  promote  community  interaction,  cities  are  developing  services  to  decrease \ninequalities,  discrimination  and  help  reach  traditionally  excluded  communities.  Local \ngovernments are implementing mobility plans and actions in cooperation with local stakeholders \nto secure people’s privacy while obtaining crucial real-time data visualisation through traffic \ncameras.  \nGuiding AI principles for Europe \nDespite the differences in the level of AI experiments and deployment across Europe, local \ngovernments share the same values and principles when it comes to using AI.  \n−  People-focused AI: People are at the centre of AI deployment in cities. AI should be used to \nfacilitate access and deliver better services to citizens - not to track, control or direct people’s \nbehaviour. AI systems should serve people, and solutions should be based on EU societal and \nethical values. \n−  Collaborative intelligence for successful AI deployment in cities: AI must complement and \naugment human capabilities, not replace them.  \n−  Data is the engine of AI: The quantity, quality and transparency of used data is a key success \nfactor for AI adoption. High quality annotated open data should be more available for use by all \nactors. Those using the data have the responsibility to ensure its integrity, authenticity, \nconsistency and accuracy. A description of the data on which an algorithm is trained should be \npublished.  \n−  Safety and security: AI must serve and protect people; systems should be accurate and perform \nreliably. Security and privacy should be integrated into systems from the design phase. \n−  Accountability and transparency: As the impact of AI on people’s safety can be high, strong \naccountability  and  transparency  measures  and  mechanisms  should  be  ensured.  Oversight \nmeasures covering responsibility and liability need to be put in place.    \nResponse to EU’s white paper on AI \n•  Facilitate access and use of private data \nAI thrives on data. Full access, share and use of data by all ecosystem actors is a prerequisite for \nAI development and implementation. A growing amount of data is generated every day in cities \n \n–––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– \n \n5\n EUROCITIES statement on Integrating transport automation in the urban system, February 2019, https://bit.ly/3atmlyN  \n6\n Cities for Digital Rights https://citiesfordigitalrights.org/ \nApril 2020 | www.eurocities.eu | EUROCITIES statement on AI  3', 'by  people  and  machines,  but  its  use  and  re-use  is  not  fully  exploited7.  We welcome the \nCommission proposal to improve access to data establishing common European data spaces that \nwill facilitate data being used and shared between different players, also cross-border, as well as \ndata quality, interoperability and standards within and across sectors. We strongly believe in \nstrenghtening this effort through initiatives that focus on semantic interoperability by shared \ndefinitions, both technical and multilingual. Ensuring GDPR compliance, we will monitor the \ngovernance process and decisions on which data will be used and in which situations. We will also \nmonitor the standardisation of data formats, protocols and registry that could help promote \nexchange of data and knowledge. However, a huge amount of data is currently gathered and \nowned by the private sector. The use and management of that data is crucial for public authorities \nto improve public services and policies including urban planning, mobility and housing while \nensuring democratic control over data and mitigating the negative societal impact of AI. Access \nand use of business to government (B2G) data needs specific regulation. We therefore call for \naction to be taken on B2G data sharing in the proposed Data Act planned for 2021.  \n•  Accountability and transparency measures  \nAI systems and algorithms can hide bias which might lead to high risks for people’s safety. Safety \nand reliability tests of AI systems are burdensome; local public authorities do not have the \nexpertise or the budget to carry them out. A central EU body or agency should develop the \nnecessary verification and validation procedures, and guarantee security and public safety.  \nLiability and accountability in AI are key to guaranteeing people’s safety. Tangible measures \nmust be applied to close the accountability gap such as: \n-  full access to the algorithm code by the competent authorities whenever needed for \ninspection or verification purposes \n-  obligations to report which algorithms are used \n-  a framework for algorithmic auditing that supports AI system development end-to-end \n-  fostering an open source code philosophy \n \n•  Scope of a future EU regulatory framework \nWe agree with the Commission’s opinion on applying the high-risk approach and to use the \ncumulative criteria of the ‘sector’ where AI applications are employed as well as the intended \n‘use’ of AI within each sector. More detailed and clear description of the possible uses that are \nconsidered high-risk is essential to clearly understand when a local public administration is \naffected by a specific use. Considering the multitude of possible uses of AI in several sectors of \nresponsibility  for  public  authorities  and  the  complexity  of  the  classification  exercise,  we \nrecommend a task force composed of experts on AI and public administration experts, also at \nlocal level, to better define the different possible high-risk uses.  \n•  Funding for skills development \nLocal governments need support in managing the digital transition and to tackle the possible \ndisruptive effects of emerging technologies in the labour market. City governments are key \nplayers  in  supporting the creation  of  new jobs  as well  as the  development  of  skills and \nknowledge; they are the right level to facilitate the dialogue between all actors involved, \nincluding national and regional governments, educational institutions and the private sector. \nDigital skills development must be enhanced in city administrations, but local governments face \nstrong competition from global technological companies in terms of attracting employees with \nthe right digital skills. We welcome the Commission’s strong focus on skills development as part \nof its digital strategy including the update of the EU Skills Agenda, the dedicated funding in the \n \n–––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– \n \n7\n Data, People, Cities – EUROCITIES citizen data principles in action, November 2019, https://bit.ly/3afmF3Y  \n \nApril 2020 | www.eurocities.eu | EUROCITIES statement on AI  4', 'proposed Digital Europe Programme to advanced digital skills and the update of the Digital \nEducation Action Plan to increase education quality through AI. However, cities should also be \nable to access and use ESF+ and the Youth Guarantee funding that focus more on digital literacy \ntraining and skills development to tackle disruption in the labour market and skills gap or \nmismatch. This should specifically include women empowerment measures to combat gender \nimbalance in STEM.  \n•  Building an ecosystem of excellence and trust together \nLocal governments are crucial to fostering an ecosystem of excellence and trust in AI in Europe. \nWith increasing urban populations and access to talent and skills, universities, companies and \ninfrastructures, city authorities can facilitate research and innovation activities and processes, \nand create the right conditions to boost AI deployment, including by SMEs. Acting as open \nparticipation and collaboration platforms, using and making data and information available, city \nauthorities enable crowd-creation, foster experimentation, share technological expertise and \nco-develop ideas and solutions.  \nLocal governments are key to building confidence and trust in AI. Through experimentation and \nearly adoption of AI, city governments identify possible safety and fundamental rights risks, and \npropose trustworthy solutions. As the level closest to citizens, local authorities engage with \ncitizens, understand their fears and concerns and develop together possible solutions. The EU \nmust work with local governments in the development of a future regulatory framework for a \ntrustworthy AI that takes into account the principles defined by the Cities for Digital Rights \ncoalition8 and that supports cities to uptake and upscale AI in Europe.  \n \n–––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– \n \n8\n Cities for Digital Rights https://citiesfordigitalrights.org/ \nApril 2020 | www.eurocities.eu | EUROCITIES statement on AI  5']","EUROCITIES welcomes the Inception Impact Assessment (IIA) on a proposal for a legal act laying down requirements for Artificial Intelligence (AI). 
AI is an enabler of change for local governments and is already transforming the governance of cities and society. As the level of governance closest to people, with increasing populations, access to talent and skills, universities, companies and infrastructures, local governments are crucial to fostering an ecosystem of excellence and trust in AI in Europe. 
The problems described in the IIA correctly outline several key challenges related to ethical and legal application of AI. The lack of transparency for effective enforcement of existing law and liability compensations, a legislative void and the unavailability of tools for intervention present challenges for businesses, citizens and authorities alike. Missing requirements for preventing and/or mitigating negative outcomes of AI tools cause a lack of trust for embracing AI, slowing down its deployment.  
In its problem analysis, the IIA should also note that AI Adoption in cities is a long and costly process, which requires specific, high-level competences as well as new governance approaches. Moreover, the rapid exponential technological progress in AI will have a disruptive effect on the job market and creates the need in cities for skills development among its citizens. 
We support and underline the overarching objectives set out in the IIA for effective enforcement mechanisms to protect safety and fundamental rights. Emphasis should be put on the need to facilitate rights-based public services and the protection of European fundamental values. Building an ecosystem of excellence and trust is an important condition for a broad and swift uptake of AI, a condition for which local governments are crucial. 
Indeed, relevant documentation is needed for the purpose of private and public enforcement of EU rules. We add to that the need for documentation that is available and understandable for citizens. Everyone should have access to clear and accurate information about the technological, algorithmic and artificial intelligence systems that impact their lives, and the ability to question and change unfair, biased or discriminatory systems. 
The impact assessment should focus on those policy options that would achieve the overarching objectives. Emphasising the importance of safety and fundamental rights, voluntary schemes provided for and maintained by industry would not suffice. An assessment of a legislative instrument establishing mandatory requirements for AI, taking into consideration the possibility of addressing specific categories of (high-risk) AI applications would be a positive step.  
To ensure compliance with these requirements, a central EU body or agency should develop necessary verification and validation procedures to guarantee safe and ethical AI, protecting citizens’ fundamental rights.  
More detailed descriptions of possible high-risk use cases are needed to clearly understand effects on local public administrations. We encourage combining funding, capacities, competences and mechanisms to realise the competences for adopting AI, emphasising local governments as crucial capacities to fostering an ecosystem of excellence and trust in AI in Europe. 
The assessment of expected impacts should include the social and economic impacts for cities in the areas outlined under likely economic impacts (i.e. costs for public institutions to comply with mechanisms). 
Local authorities engage with citizens, understand their fears and concerns and provide collaborative platforms for citizens, businesses and academia to develop solutions together. The EU must work with local governments in the development of a future regulatory framework for trustworthy AI to incentivize its use. Therefore, the consultation strategy should recognise city authorities as stakeholders and involve representatives in any targeted consultations. "
F551015,10 September 2020,Liga Semane,Wirtschaftsverband,European Banking Federation,klein (10 bis 49 Beschäftigte),4722660838-23,Belgien,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"The European Banking Federation (EBF) welcomes the opportunity to respond to the European Commission’s Consultation on its Inception Impact Assessment (IIA) on a Proposal for a legal act of the European Parliament and the Council laying down requirements for Artificial Intelligence (AI). 
Please see the attached document for our comments. The responses to the different elements in the IIA complement the EBF’s response to the European Commission’s Consultation on the White Paper on Artificial Intelligence. 

","['10 September 2020 \nEBF_042415 \n \n \n \n \nEBF response to the European Commission Inception Impact Assessment on a \nProposal for a legal act of the European Parliament and the Council laying down \nrequirements for Artificial Intelligence \n \nThe European Banking Federation (EBF) welcomes the opportunity to respond to the \nEuropean Commission’s Consultation on its Inception Impact Assessment (IIA)  on a \nProposal  for  a  legal  act  of  the  European  Parliament  and  the  Council  laying  down \nrequirements for Artificial Intelligence (AI).  \nThe  responses  to  the  different  elements  in  the  IIA  complement  the  EBF’s \nresponse to the European Commission’s Consultation on the White Paper on \nArtificial Intelligence.  \n \n1.  Context and Problem Definition \nEBF members agree that AI can contribute to a wide array of economic and societal \nbenefits across the entire spectrum of industries. For the European banking sector, AI \nprovides significant opportunities ranging from enhancing customer experience, improving \nfinancial  inclusion,  cybersecurity,  and  consumer  protection,  to  strengthening  risk \nmanagement and process efficiency.  \nTo realize these benefits, the EU regulatory environment needs to be fit for the use \nof AI and enable innovation, while providing legal certainty and maintaining a \nstrong level of consumer protection. This is essential for AI development and uptake \nacross industries, including in the financial sector. It is also crucial for the competitiveness \nof the EU globally, something which the “context and problem definition” section in the IIA \ndoes not mention.  \n \n2.  Objectives and Policy Options \nBefore considering the different Policy Options proposed in the IIA, we would like to \nemphasize  the  need  to  avoid  any  duplication  of  requirements  or  conflicting \nrequirements for areas that are already regulated in order to prevent additional \nburdens and legal uncertainties. For example, the General Data Protection Regulation \n(GDPR) and Law Enforcement Directive already provide strong privacy and data protection \nregulation, which should not be duplicated.  \nOn the policy options specifically, we are of the opinion that new, AI specific legislation \nis not required and believe it is key to take into account the consequences that any rules \ncould have for the competitiveness of European companies globally. This is particularly \nEuropean Banking Federation aisbl   \n    \n \nBrussels / Avenue des Arts 56, 1000 Brussels, Belgium / +32 2 508 3711 / info@ebf.eu \nFrankfurt / Weißfrauenstraße 12-16, 60311 Frankfurt, Germany   \nEU Transparency Register / ID number: 4722660838-23  www.ebf.eu', 'the case for prescriptive, wider regulation, as proposed in Option 3 (sub-option \nthree), which could hamper the adoption of AI in Europe by companies of all \nsizes.   \nAs a general principle, we believe that regulation should remain technology neutral. \nLegal requirements or ethical principles should not apply to the underlying technology but \nto its application. \nWe would instead recommend a policy option centred on guidance developed by \ncompetent authorities on how to apply existing requirements to AI use cases, which could \nhelp firms to effectively apply their obligations under different regulatory regimes. This \nshould be a collaborative process, with competent authorities working with each other, \nwith input from industry and civil society. In this view, Option 1 would be preferable. \nAny guidelines should take a technology neutral approach and not be overly \nprescriptive, as this would be in friction with the rapidly evolving nature of AI-\nrelated technologies.  \nHowever, EBF members also have some concerns on Option 1, particularly the lack of \ndetail on what the proposed self-reporting/assessment on voluntary compliance would \nentail, and we would welcome clarification on them.  \nOn Option 2, an EU legislative instrument setting up a voluntary labelling scheme, \nEBF  members  have  significant  concerns.  A  “trustworthy”  label  for  non-high  risk \napplications implies that any application that does not carry the label is deemed “not \ntrustworthy”. By asserting this kind of social value, the label can no longer be considered \ntruly voluntary. This runs counter to a risk-based framework as it would implicitly \nincrease the requirements placed upon applications that are not high-risk, due \nto market discipline pressures. \nWe would also disagree with the assertion in the IIA that voluntary labelling would likely \nentail limited costs as there are many elements to consider. Labelling schemes are \ndifficult to set up, and are likely to complicate and prolong the development and \nimplementation of AI systems (which are often scalable or self-learning building blocks, \nor encapsulated into larger systems in the form of internal or external components that \nare difficult to isolate), which could therefore imply additional costs. The supply chain \nperspective on a labelling scheme is also missing from the IIA (and in the different policy \noptions overall, see below). Would organisations service consumers that are placed later \non the chain have to inherit all the high risks resulted from service providers?  \nFinally, the IIA states that “the label would function as an indication to the market \nthat the labelled AI application is trustworthy”. There are three concerns with \nthis:  \na)  As the label would be voluntary, what would be the ability of the consumer to \nrecognize whether, if a particular product or service does not have a label, the \ncompany did not apply for it or whether its absence means that the product/service \ndoes not comply with the requirements of the label scheme. \nb)  Subscribing to a scheme that would only show trustworthy AI, regardless of the \nother technologies that may be in use, could be misleading. The consumer needs \nto have access to trustworthy products and services as a whole, regardless of the \ntechnologies behind it.  \nc)  Subscribing to a labelling scheme of a trustworthy AI application is something \n \n \n \n2 \nwww.ebf.eu', 'different that the trustworthy output of an AI application. A label on a trustworthy \nAI application itself does not need to  cohere with the trustworthiness of the \ngenerated  output.  For  the  user  of  a  service,  this  distinction  might  not  be \nunderstandable.  \nIf the Commission proceeds with horizontal AI regulation and takes option 3, as described \nin the AI White Paper, we would generally support the proposed approach to focus only \non high risk AI applications (Sub-option b). In this case, the Commission should \ndevelop clear and objective criteria for high-risk applications ensuring that only \nthose applications that could cause serious harm to citizens (e.g. by putting their \nlives at risk) are captured. These potential new requirements should be consistently \napplied only to use cases that pose the same level of high risk regardless of the type of \nprovider, and not be extended to other cases with significantly lower risks. \nWe believe that in the financial sector, current regulation (both sector specific \nregulation  as  well as  cross-sectorial  regulation  such  as  the  GDPR)  and  the \ninternal  processes  it  requires  is  already  sufficient  to  guarantee  consumer \nprotection,  risk  management,  financial  stability  and  data  protection,  in  all \nservices provided to customer, including those applications that could include \nthe use of AI. Best practices from the financial services sector could serve as inspiration \nfor creating or adapting the legal framework to address AI-specific risks.  \nFinally, several EBF members would like to point out that the Objectives and Policy Options \nsection does not include the aspect of the responsibility of third party providers (TPPs). \nWhen AI is provided by a TPP and is then integrated into a service/product provided by \nthe final service provider, only the TPP will have the full knowledge of all the specific \nfeatures of the AI at any particular time, not the final service provider. This makes the \nsupply chain perspective an important one and we recommend that it is included in the \nCommission’s assessment of policy options. \nIn addition, some providers and solutions have a predominant influence on the market; a \nfailure in these cases could cause wider disruption and needs to be considered. \n \n3.  Definition of AI  \nThe IIA states that “Another core question relates to the scope of the initiative, \nnotably how AI should be defined (narrowly or broadly) (e.g. machine learning, deep \nneural networks, symbolic reasoning, expert systems, automated decision-making).” \nEBF members agree with the European Commission that the question of how to define AI \nwithin any future initiative is a crucial one. We would like to stress that any definition of \nAI must be future-proof and avoid being overly broad in a  way  that could \ninadvertently include technologies that are not AI and do not pose the same \nrisks.  It should be clear, narrow, and specific, as well as future-proofed, for example \nby focusing on the technology’s adaptive qualities. Using a definition such as the one \nproposed by the AI High-Level Expert Group (HLEG), risks that any system, including \ngeneral automation processes, is subject to AI specific rules. In terms of the risks, it is \nalso important to keep in mind that it would not be fair to have a bad human-made-rule \nbased system not treated as risky, but a well-validated data-driven ML-system as high-\nrisk.  \n \n \n \n3 \nwww.ebf.eu', 'In addition, it seems essential, for a good understanding of the issues related to AI, to \nexclude from the scope of any possible, future Regulation any reference, even implicit, to \nthe emergence of a ""strong artificial intelligence"" or ""artificial consciousness"". It follows \nthat the AI to which we are referring can only be considered as a technical object, therefore \nnot having in any way the character of an autonomous subject.  \nRegarding the different policy options in the IIA, if Policy Option 3, sub-option b is pursued \nby the Commission, the definition should not bring into scope non-high risk applications.  \nWe welcome the opportunity to discuss in more detail, ahead of the issuance of potential \nlegislation, a potential definition of AI that focuses on its key characteristics and is fit for \nregulatory purposes, i.e. easily understood, unambiguous and succinct. \n \n4.  Governance \nOn  the  proposal  to  create  a  framework  for  the  cooperation  of  national  competent \nauthorities, we are concerned that this may not fully address the risk of fragmentation of \nsupervisory and regulatory practices. If such a structure is created, we recommend that \nencouraging  information  sharing  and  cooperation  on  AI  issues  by  different  sectoral \nauthorities is included as part of the mandate. This would help to avoid situations \nwere standards for similar activities are regulated more rigorously in some \nsectors than in others. Especially important is the collaboration between Data Protection \nAuthorities (DPAs) and sectoral authorities in order to avoid overlapping of contradictory \npractices. \nIt would also be important to help authorities in this network increase their knowledge of \nAI, to better understand the technology and gain insight on how companies are using AI \non specific applications.  \n \n5.  Preliminary assessment of expected outcomes \nIn regard to likely environment impacts, we would encourage the Commission to continue \nexploring the environmental considerations with regards to AI.  \n \n \n \n \nENDS \n \n \n \n \n \n \n \n \n \n4 \nwww.ebf.eu', 'For more information: \nLiga Semane  \nPolicy Adviser – Data & Innovation \nl.semane@ebf.eu \n \nAbout the EBF \n \nThe European Banking Federation is \nthe voice of the European banking \nsector, bringing together 32 national \nbanking associations in Europe that \ntogether  represent  a  significant \nmajority  of  all  banking  assets  in \nEurope, with 3,500 banks - large \nand small, wholesale and retail, local \nand international – while employing \napproximately  two  million  people. \nEBF members represent banks that \nmake  available  loans  to  the \nEuropean economy in excess of €20 \ntrillion and that reliably handle more \nthan  400  million  payment \ntransactions per day. Launched in \n1960, the EBF is committed to a \nsingle market for financial services \nin  the  European  Union  and  to \nsupporting  policies  that  foster \neconomic growth. \n \nwww.ebf.eu  @EBFeu']"
F551014,10 September 2020,MARJORIE VOLLAND,Unternehmen/Unternehmensverband,TECH IN FRANCE,sehr klein (1 bis 9 Beschäftigte),-,Frankreich,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"Par la contribution à l’étude d’impact initiale sur l’Intelligence artificielle (IA) ouverte, TECH IN France souhaite poursuivre son implication vis-à-vis des travaux et des réflexions conduits par la Commission européenne dans le cadre de sa stratégie dédiée à l’IA.  

Ainsi, le souhait de la Commission européenne de créer un système européen à même de garantir la confiance des citoyens et stimuler l’adoption des usages IA, tout en assurant celle des entreprises dans le déploiement de leurs produits et applications IA et la capacité d’innovation en Europe, nous apparaît une stratégie ambitieuse et adaptée aux enjeux de développement du potentiel numérique en Europe. De nombreuses entreprises membres de TECH IN France se sont d’ailleurs exprimées publiquement au cours des derniers mois en faveur du principe de l’élaboration d’une régulation. 

Dans cette optique, TECH IN France et ses adhérents souhaitent réitérer ici l’attention portée à l’adoption d’une approche pragmatique et équilibrée dans la définition d’un nouveau cadre règlementaire par la Commission européenne. Ce principe d’équilibre dans l’élaboration d’une régulation européenne de l’IA apparaît comme essentiel pour créer les conditions de l’innovation via le développement d’un écosystème de l’excellence, dès le stade de la recherche et de l’innovation, incitant à une adoption rapide des solutions IA par les entreprises, notamment les TPE et les PME ; et pour être à même de stimuler l’innovation, en donnant aux entreprises la sécurité juridique nécessaire. Ce nouveau cadre réglementaire devra également éviter de faire peser sur les entreprises des contraintes disproportionnées par rapport à leurs compétiteurs internationaux, et devra répondre à leurs besoins de compétitivité dans la bataille mondiale de l’IA et des données.   

Dans le cadre de cette étude d’impact, il est proposé de revenir plus précisément sur les différentes options de régulation de l’IA envisagées par la Commission européenne et présentées au sein du Livre Blanc sur une approche européenne de l’intelligence artificielle en février dernier. Il nous apparaît ainsi important de préciser à nouveau les éléments et points de vigilance, formulés par TECH IN France au sein de sa réponse à la consultation sur le Livre Blanc [...]. 

","['September 2020\n \n \n \n \n \nResponse from TECH IN France to the Study\ninitial impact on artificial intelligence\nof the European Commission\n1', 'By contributing to the initial impact study on Open Artificial Intelligence (AI), TECH IN\nFrance wishes to continue its involvement in the work and discussions carried out by the\nEuropean Commission as part of its strategy dedicated to AI.\nThus, the wish of the European Commission to create a European system able to guarantee the\ntrust of citizens and stimulate the adoption of AI uses, while ensuring that of companies\nin the deployment of their AI products and applications and the capacity for innovation in Europe, we\nappears an ambitious strategy adapted to the challenges of developing digital potential\nin Europe. Many member companies of TECH IN France have expressed their views\npublicly in recent months in favor of the principle of developing regulation.\nWith this in mind, TECH IN France and its members wish to reiterate here the attention paid to\nthe adoption of a pragmatic and balanced approach in the definition of a new framework\nregulation by the European Commission. This principle of balance in the development of a\nEuropean regulation of AI appears to be essential to create the conditions for innovation via\nthe development of an ecosystem of excellence, from the research and innovation stage,\nencouraging the rapid adoption of AI solutions by companies, in particular VSEs and SMEs; and\nto be able to stimulate innovation, by giving companies legal certainty\nnecessary. This new regulatory framework should also avoid burdening companies with\ndisproportionate constraints compared to their international competitors, and will have to respond\nto their needs for competitiveness in the global battle of AI and data.\nAs part of this impact study, it is proposed to return more precisely to the various\nAI regulation options considered by the European Commission and presented in the Book\nBlanc on a European approach to artificial intelligence last February. It appears to us like this\nimportant to specify again the elements and points of vigilance, formulated by TECH IN France on 2\nwithin its response to the consultation on the White Paper.\n \nGeneral remarks on the AI \u200b\u200bregulatory framework\n \nThe European Commission proposes to develop and graduate European regulation in terms of\nusing an approach based on the level of “risk”. It also specifies in its White Paper\nof February 2020 that the principle of proportionality must guide the development of this new framework\nregulation for AI, which should achieve its objectives efficiently without being excessively\nnormative, at the risk of creating a disproportionate burden, in particular for SMEs. It is indeed\nessential that future legislation be proportionate and not too complex to\nimplemented by SMEs, so as not to penalize them. The standard must be able to play a supporting role\neconomic development and not counter-productive.\n \nIn this context, it seems essential that the scope, concepts and application criteria\nof a specific regulation are defined clearly and pragmatically, in order to allow\ncompanies subject to regulation to benefit from the best visibility on the schemes\napplicable laws. These elements are therefore essential conditions for any safety\nlegal framework and the dynamics of business innovation.', 'Scope of the AI \u200b\u200bregulatory framework: the definition of AI\nIn order to determine the scope of the regulation to be defined, the European Commission specifies\nin its strategy that it is a question of clearly defining upstream the notion of “intelligence\nartificial”. It is interesting to remember that there is to date no fixed and accepted definition\nby the entire ecosystem, whether by experts, academics or companies. For\nTo do this, the Commission refers to the main elements that make up AI: data and algorithms\nand thus seems to retain a broad definition of AI, which, according to TECH IN France, calls for points of\nvigilance.\nFirst of all, it seems important to us to specify here that a clear definition of AI and easily\nunderstandable to all, is a condition for effective regulation. Defining AI by essentializing\ndata and algorithms implies de facto that AI covers a very wide perimeter. In this sense, he\nIt would be appropriate to define more precisely the eligible scope falling under AI in relation to that of\nthe algorithm in general. Indeed, any data processing, including massive, does not come under\nnecessarily within the scope of AI.\nIn addition, some existing definitions of AI (including one produced by the European Commission\nin 2018 in its communication “AI for Europe”), some make a distinction between the\n“learning” AI systems, which “demonstrate intelligent behavior by analyzing their\nenvironment and taking action, with some degree of autonomy, to achieve\nspecific objectives” and classic AI systems, “which can be purely software, acting\nin the virtual world but also in material devices. Following such logic in a\ndefinition of AI would make it possible to make regulatory distinctions according to the categories of AI, in 3\nrealizing a focus on “learning” systems, thus reducing the field of application of\nregulatory intervention to be defined and leading to not creating excessive regulation.\n \n \nCreating a Trust Ecosystem: Options for Regulating Trust\nAI\n \nOn the “0” option and the importance of the consistency principle\nThe European Commission in its ""0"" option is considering the possibility of not creating a\nregulations specific to AI applications and systems, with specific rules and requirements,\ninsofar as existing legislation is already applicable to AI.\nIndeed, it appears useful that an assessment of existing legislation and its ability to apply\nrelevant to AI technologies, is carried out beforehand; this before considering any\ndedicated regulations.\nAbove all, it seems important to remember that in the event of new regulations, the principle of\nConsistency is essential. Indeed, consistency is necessary between a new', 'regulation of AI and the many existing European or national texts, which apply\nfully as noted by the European Commission, such as the GDPR, the directive\nrelating to general product safety, the Directive on equal treatment between persons\nregardless of racial or ethnic origin, the Directive on equal treatment in matters of\nof employment and work or the directive on consumer rights.\nIt is therefore a question of ensuring that the development of any other form of new regulation is\nconsistent with existing texts.\nIn practice, companies have acquired considerable experience in terms of compliance\n(process, tools, documentation, self-regulation, etc.), particularly in the context of the implementation of the\nGeneral Data Protection Regulation (GDPR).\nThus, with regard to automated decisions, the GDPR imposes restrictions, but also the\nright of individuals to be informed about the rationale behind these decisions. Likewise, the\nregulation provides, in particular in its article 22, a specific framework for the processing of\npersonal data. It will therefore be necessary for any new regulation in terms of AI not to\nnot ""overlap"" with the provisions already provided for by the GDPR. Above all, this principle of\ncoherence, guarantee of harmonization of legislation, will also allow companies to\nmaintain the legal framework to which they have adapted for several months now;\nelement necessary for legal certainty and therefore for the creation of an environment favorable to the\ntrust and innovation.\n \nOn option 1, a soft law approach\n4\nThe European Commission in its option 1 proposes not to enact any legislative instrument or\nbut to facilitate, prioritize and promote industry intervention and initiatives\nin AI. Voluntary approaches would thus be preferred, based on good\npractices, the development of standards and principles…Any support to the industry and the encouragement\nto the definition and sharing of good practices should, in our view, be encouraged.\n \nOn option 2 and the principle of voluntary labeling\nThe voluntary labeling scheme defined by a legislative instrument, envisaged in option 2\ncould be a contributing factor in building user confidence in AI systems,\nthus facilitating adoption. This compliance system should nevertheless, in our opinion, leave\ncompanies the flexibility to choose the most relevant tools to certify their compliance. Of the\nconcrete tools such as codes of conduct, certifications, the concept of privacy by design/ by\ndefault, would allow companies to benefit from this flexibility. Business adoption,\nespecially start-ups and SMEs, this labeling system will depend on this flexibility.\n \nOn options 3b) and 4, a risk-based approach\nThe European approach adopted in options 3b) and 4 of the initial impact study is based on the\nconcept of “risk”, which should make it possible to graduate the regulatory response. This approach seems\nrelevant, in particular because it makes it possible a priori not to discourage the development and\nthe use of AI, by applying a regulatory logic adjusting to the diversity of applications\nAI; and seems suited to evolving technologies like AI and machine learning. She', 'would also make it possible not to over-regulate AI applications with little\nrisks to the physical integrity or fundamental rights of individuals, for example, which is the\ncase for most AI applications.\nThe cumulative approach to determining the degree of risk: a pragmatic approach\nTo determine the level of risk of the AI \u200b\u200bapplication, the criteria taken into consideration must be\nclear and intelligible, as already stated in the White Paper of the European Commission, ""the\nelements to establish that an AI application is high risk should be clear, easy to\nunderstand and apply to all parties involved”. Thus, the cumulative approach consisting in\ntake into account the sector in which AI is used, but also the use made of it at the\nwithin this same sector, appears reasonable and pragmatic. This combination of criteria\nto determine whether an AI application is high-risk or low-risk, allows not to\nstigmatize certain sectors, which by nature would be considered high risk, while\nthe use in question made of the application would in fact involve a low risk, even in\ncertain cases would help reduce the risk.\nNevertheless, it seems important in practice that the distinction between the two types of risk\nenvisaged by the European Commission be clarified to guarantee the legal certainty of\nbusinesses, and in particular avoid placing small economic players in situations\ncomplex legal matters.\nIndeed, the criteria can be seen as still rather vague, which could raise fears that\nthe notion of “risk” is interpreted broadly. Additional elements\ncould thus be taken into account when assessing the risk (allowing companies to\neasier to achieve), such as considering the potential damage, but also the 5\nopportunities for society and citizens, and therefore to ""measure"" the cost of not using\npractice AI. Such a balancing would appear useful in order not to discourage the use of AI\nsince it would present risks, which can, in many cases, be mitigated by\npractice. Similarly, the determination of the level of risk could take into account, beyond the\npotential seriousness of damage, its probability. Furthermore, the exceptional cases mentioned by\nthe European Commission in its White Paper, which would be considered by nature to be high-\nrisk, and this, regardless of the risk assessment according to the cumulative criteria carried out, can be\nperceived as creating a real vagueness in the determination of the risk and the application of the\nspecific regulation, and therefore as an important factor of legal uncertainty.\nIt would also be appropriate to put performance expectations in terms of AI into perspective, because although everything\ncan be done to minimize risks and errors, the algorithms serving the AI \u200b\u200bsystem\nmay be biased or contain errors, this being the inherent nature of the algorithm,\nof providing statistical results and probabilities based on data. The Commission\nEuropean Commission also recalls that biases and discriminations are not specific to the machine and are\ninherent in any society and economic activity.\nFinally, it seems that a more precise classification of data would also allow companies\nto assess the notion of risk more precisely and to develop appropriate standardization strategies.', 'Mandatory Requirements for High-Risk AI Applications\nThe European Commission in its option 3b) refers to legal requirements and obligations imposed\nto developers and professional users of AI, which would only apply in cases\nAI applications considered high risk.\nIn view of the obligations envisaged in the White Paper, care should be taken when drafting\nof these rules and as to their practical interpretation: it is important that this is done with\npragmatism and that the requirements are applicable in practice by companies, at the risk of\ndrastically curb innovation.\n \nCompliance and Enforcement\nThe European Commission is considering that AI applications will be subject to an assessment of\nprior compliance to ensure that the mandatory requirements, in the case of application to ""high\nrisk” are well respected. This “prior assessment” could include test procedures,\ninspection or certification, or even checks of the algorithms and data sets used in\nthe development phase.\nCompanies may be concerned about setting up such an ex ante compliance procedure.\nIndeed, many concrete questions are emerging, to which it will be important to bring\ndetails. For example, what about the retroactive application of such a procedure to products already\non the market ?\n6\nEffective alternatives to this prior compliance assessment, and which can be implemented\nmore easily by companies, could be considered, for example on the basis of what already exists\nunder the GDPR in terms of assessing the impacts on data protection (“impact\nassessment”). Similarly, the combination of ex ante and ex ante risk assessments by companies\npost, could allow assessment of compliance with mandatory requirements, without imposing burdens\ntoo burdensome for companies, relying for example on legal practices or\nethics.\nFurthermore, a certification process should target more the processes implemented in\nthe development phase, as is the case for example in sensitive industries, that the\nFinished products.\nWith regard to product safety risk assessments, current legislation\nappears sufficiently flexible to take into consideration the majority of the existing risks associated with\nthe use of these emerging technologies. This is why the notion of “risk” linked to AI should not\nnot be considered from a broader angle, which could have the effect of creating\nexcessive constraints and therefore slow down innovation. It should be noted that the developers of\nIA solutions regularly update their products. Self-regulation in this area\ntherefore exists and deserves to be underlined. It is also fundamental for companies whose\ncustomers are increasingly demanding in terms of security, and this confidence that the customer can\ntuning in the product encourages good practices in the trade.\nFurthermore, as part of the reflections on the implementation of new procedures for the evaluation of\nrisks for the products, the reflection should be undertaken along two axes, which depend on the type', 'of AI in question. Indeed, for AI systems learning on so-called “static” games, the product\nresulting end product is not intended to evolve further after it is placed on the market. On the other hand, for\nAI systems that continue to learn on test cases, they can sometimes evolve to the point\nto achieve such strong breaks in the behavior of the system that the final product can be\namended. Thus, in the first case, it would not be necessary to set up new\nrisk assessment procedures, since the finished product will not be subject to modification or\naltered. Putting this procedure in place could even act as a brake on innovation. It suits\nindeed to relativize the scalability of AI algorithms in this case since the technologies are\nstabilized.\n \n \n \n \n \n \n \n \n \n  7\n \n \n \n \n[About TECH IN France]\nCreated in 2005, TECH IN France is a professional association under the 1901 law which aims to\nbring together and represent publishers of software, internet services and platforms in France.\nSpokesperson for the digital industry, TECH IN France has 400 member companies: from\nstart-ups to multinationals, including SMEs and large French groups; i.e. 8 billion euros\nand 90,000 jobs. TECH IN France has set itself the task of carrying out a permanent reflection on\nthe evolution of the digital industry and promote the attractiveness of the sector.\nwww.techinfrance.fr']"
F551012,10 September 2020,Arthur HILLIARD,Wirtschaftsverband,Insurance Europe,klein (10 bis 49 Beschäftigte),33213703459-54,Belgien,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"Insurance Europe welcomes the initiative to encourage ethical and responsible development of AI in the EU and supports the creation of an ecosystem of trust to stimulate its uptake. A European approach is necessary to limit fragmentation of the digital single market, ensure fair competition and protect European citizens & businesses.
As noted by the Commission’s HLEG on AI in its recommendations, the development & use of AI is already covered by a wide body of existing EU legislation, such as on fundamental rights, privacy & data protection, as well as product safety & liability. This is further complemented by national & sectoral regulatory frameworks. A horizontal, proportionate, principles- and risk-based framework that builds on this, addressing potential gaps where necessary, will help support the development & uptake of AI and avoid unnecessary burden.
We believe that option 1 would therefore be the preferable policy option, as it provides for a soft-law approach to facilitate and spur industry-led intervention. It would also be the most relevant approach to encourage coordination on a single set of AI principles, while enhancing enforcement of existing rules. 
To promote the uptake of AI & prevent innovative technologies from being stifled by premature regulation, the ethical use of AI should be supported by, and reinforced through, voluntary and/or non-legislative instruments as far as possible. Voluntary certifications have traditionally proven to be an effective means of ensuring high & transparent standards (eg in the area of IT security). They enable customers to easily identify trustworthy products and allow companies to demonstrate and promote the quality of their products. Many companies would have an interest in voluntarily opting to certify their AI applications to enhance consumer confidence, stay ahead of the competition and demonstrate compliance with current standards and regulation.
Moreover, an approach that focuses mainly on voluntary instruments (eg industry-developed codes of conduct or guidelines) remains compatible with the option to introduce legislative instruments containing mandatory requirements for certain AI applications, as set out in option 3. However, it is important to ensure that any EU legislative instrument that may be introduced is horizontal, proportionate & risk-based, and limited only to “high-risk” AI applications that are determined on the basis of clear criteria (as suggested in White Paper). Inclusion in the scope of such requirements of low-risk, common automation processes or applications that pose little or no risk to the rights of customers would hinder innovation and the uptake of new technologies, give rise to additional costs, and create a disproportionate burden in view of their low risk.
From the outset, the insurance industry has made extensive use of data and algorithms, eg in the calculation of insurance premiums. Such methods of analysis are long-established and already subject to supervision. The development of AI tools can help insurers to improve underwriting as well as to better monitor & predict risk, and thereby advise policyholders on how to reduce risk, which can in turn help reduce the frequency and severity of losses over time.
We would also stress that monitoring the use of AI applications should continue to fall within the competence of the relevant sectoral supervisory or regulatory authorities, as they remain best placed to understand the market in question and the specific context of the AI application vis-à-vis the applicable regulatory framework.
The existing liability regime (PLD) is fit for purpose to address the risks posed by AI but could benefit from additional guidance in certain key areas. Existing product safety legislation should be reviewed as to its fitness. New legislation should only be adopted where clear evidence demonstrates a need for action and should be proportionate to the concrete risks posed by the AI application in question."
F551011,10 September 2020,Cristina Cartes Andres,Wirtschaftsverband,Adigtal,klein (10 bis 49 Beschäftigte),972127919039-72,Spanien,Künstliche Intelligenz – ethische und rechtliche Anforderungen,Document attached,"['ADIGITAL CONTRIBUTION  \nEuropean Commission Public Consultation on  \nArtificial intelligence- ethical and legal requirements \n \n1.  Scope  \nAdigital welcomes the opportunity to respond to the Commission’s Inception Impact Assessment on \nthe proposed legislative initiative on AI. We welcome the policy Objective and Aims of the initiative, \nin particular the intent to ensure coherence and complementarity with other possible initiatives, e.g. \naffecting the Machinery Directive, the General Product Safety Directive or the product liability regime. \nWe support targeted policies that address companies’ accountability for developing and operating \ntrustworthy AI. Below are our comments on the legislative options outlined \nIn general, Adigital cautions the Commission on considering to significantly expand the scope of the \nfuture AI regulation to the open ended category of “automated decision making.” This would go \nagainst the initial, thoughtful direction proposed in the AI whitepaper that proposes to focus on the \nrisk-based, double-criterion for sectorial and application/use-based AI technologies. If AI were defined \nas  “automated  decision making”  for the  purpose  of  the  future  AI  regulation,  it would  create \nunproportional,  unjustified  regulatory  obligations  that  would  not  only  deter  development  and \ndeployment of AI-based applications in Europe, but also automated systems that do not pose any risk \nnor harms. \nOn a more specific note, Adigital reiterates the concerns around introducing the open-ended concept \nof “immaterial harm” into the future AI legislation. We propose as an alternative to refer to \n“significantly restricting the exercise of fundamental rights,” which would closer align with the existing \nlegislative framework. \n2.  Comments on the policy options proposed \n➔  Option 0 (baseline) \nThere is merit in ensuring that the existing EU regulation is properly implemented with regards to AI \nbefore putting in place any new prescriptive AI-specific rules. Currently AI does not operate in a \nvacuum and is subject to a number of existing rules, including GDPR, medical devices regulation, and \nfundamental rights aquis. Therefore, before embarking on a new legislative instrument, a thorough \n1', 'gap analysis should be performed. Most of the current legislative gaps to achieve trustworthy AI can \nbe achieved by updating and amending current legislation, while also reviewing and empowering \nenforcement mechanisms and oversight bodies. Only the gaps that cannot be filled by the former \nrequire new regulation. \n➔  Option 1 (industry-led intervention) \nNo matter what policy options are pursued, lending support to the industry in establishing and \nimplementing norms of responsible practices and sharing best practices is worthwhile. We support \nthis Option for low-risk AI applications. \n➔  Option 2 (legislation on voluntary labelling)  \nVoluntary labelling schemes can be helpful to consumers or end-users in some markets but we do not \nbelieve a single, one-size-fits-all labelling scheme would be effective across such a broad field as AI, \ngiven the hugely diverse range of products and services that will be deployed across all sectors. \nTherefore, we remain skeptical on the impact of a labelling scheme on the uptake of trustworthy AI in \nEurope. An administrative burden on SMEs to comply with the onerous labelling obligations -- if \ndrafted on the basis of the update Assessment List for Trustworthy AI from the EU High-Level Expert \nGroup on AI -- could significantly outweigh the benefits of such a scheme. \n➔  Option 3 (legislation with mandatory requirements) \nOverall, the opportunity cost of not using AI should be part of the assessment when considering any \nfuture legislation aiming to reduce risk and harms from the use of AI applications. Legislation should \nensure legal certainty, be proportionate and increase trust in AI without unduly hindering AI-driven \ninnovation. \n●  Option 3a: We support the need for a public dialogue on the use of facial recognition \ntechnologies,  which  could  lead  to  targeted  legislation.  Remote  biometric  identification \nsystems may be a good example of an application to which mandatory requirements could be \napplied in a risk-based framework. \n \n●  Option 3b: Legislation should focus on high-risk applications, particularly applications where \nhuman autonomy or judgment are substantially ceded to an AI system. There should be a \nsingle risk assessment framework to identify high-risk AI, regardless of sector and without lists \nof exceptions. \nAny mandatory requirements for high-risk AI systems should be addressed to the actors best \nplaced to address the risks. Similarly, liability is best allocated to the actor closest to the risk, \n2', 'as liability is highly context-specific. In a B2B context, contractual liability works well and \nshould be maintained, allowing parties to negotiate an efficient allocation of risk that takes \naccount of the specific use case. We support a well-defined risk-based approach to the AI \nregulation that takes into account both the severity and likelihood of harm. A sector and \nuse/application base criteria -- as proposed by the EC’s whitepaper -- at large seem to be a \ngood starting point. \n●  Option 3c: We do not support this option as it would significantly hamper the uptake and \ndevelopment of  AI  in  the  EU,  against  the stated  Objective and  Aims of the  initiative. \nTherefore,we strongly caution against an EU legislative act for all AI applications, that would \nmake no distinction between the AI applications that can pose significant risk/harm and those \nwith  no  or  lower  risk  profile.  Such  a  legislative  instrument  would  be  significantly \nunproportional to  the  problems  so  far identified by  the  European Commission,  create \nsignificant barriers for AI adoption (additional costs, delay, administrative burden) in Europe, \nresult in opportunity costs in some applications due to lack of AI implementation, and risk \nlowering the bar for those AI applications that are very likely to raise significant risks. \n●  Option 4: A combination of the options above taking into account the different levels of risk \nthat could be generated by a particular AI application. \n \nWe believe the Objective and Aims can best be met with a combination of Options 1, 3a and \n3b, implemented through a co-regulatory approach and supported by globally recognized \nstandards and industry-led codes of conduct. \nIn relation to a European governance structure on AI, we believe that in sectors where \nestablished structures already exist (medical devices, aviation etc.) these existing bodies are \nbest placed to cover high-risk AI in their sectors, having the necessary sectoral expertise, \noperational relationships and track-record with relevant stakeholders. There may be value in \na new European mechanism that provides best practice sharing and guidance across sectors, \nbut its scope must be limited and its relationship to existing regulatory bodies clearly defined, \nso as to avoid fragmentation, inconsistency and the risk of stifling innovation. \nIn summary, we agree with the need for a consistent EU-wide regulatory framework for \ntrustworthy AI. This will be essential to give stakeholders the confidence to develop and adopt \nAI-based solutions and realize the enormous benefits they offer.  Building trust requires \nacknowledging valid concerns that exist regarding accountability, transparency, fairness, \nprivacy and security, and putting in place appropriate regulatory mechanisms to manage \nthose risks, while continuing to promote ongoing innovation and experimentation – getting \nthat balance right requires a precision regulation approach that is clear and targeted. \n3', 'An adequate scenario would include a combination of options 1, 2 (with the specificities above \nmentioned), 3a and 3b, providing EU citizens with a graded level of assurance / control / \noversight of compliance with EU ethical guidelines, values and legal requirements aligned with \nthe risk posed by AI uses and businesses . Companies or industries should be able to define \ntheir own guidelines, codes of conduct and best practices (Option 1); additionally, those \naiming for higher trust on complying with EU ethical guidelines and values could voluntarily \nadhere to a voluntary labelling scheme (option 2) for already identified applications of AI \nposing a relevant risk to human rights, as could be the case of remote public biometric \nidentification, should be subject to a set of mandatory targeted requirements (option 3b); \nfinally,  based  on  a  risk  approach  using  an  identification  criteria  based  on  sector  and \nuse/application as proposed in the EC’s White Paper,  for the existing gaps in current \nlegislation that cannot be covered by an update or amendment of such legislation,  new ex-\nante obligations should be defined and adopted.     \n3.  Enforcement \nAdigital supports the ex post enforcement for when problems arise as the most appropriate and \nproportionate  mechanism,  except  in  fields  where ex-ante  assessments  are already  established \npractice.  In  those  situations,  we  recommend  aligning  any  ex-ante  assessment  with  existing \nprocedures. \nIf the Commission insists on the ex-ante enforcement, we strongly caution against a third party ex-\nante  assessment  and  recommend  instead  self-assessment  procedures  based  on  clear  “due \ndiligence” guidance from the regulators. A practical approach would be for regulators to provide \ndetailed templates and guidance on how to carry out and document the risk assessment, but delegate \nresponsibility to those using and most familiar with the AI system to conduct an accurate assessment.  \n4']"
F551010,10 September 2020,Marie TIMMERMANN,Universität/Forschungseinrichtung,Science Europe,klein (10 bis 49 Beschäftigte),73131808686-11,Belgien,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"Science Europe welcomes the opportunity to react to the European Commission’s (EC) Inception Impact Assessment for a ‘Proposal for a legal act laying down requirements for Artificial Intelligence’. Science Europe Member Organisations, major national research funding (RFOs) and performing organisations (RPOs), are important users and producers of data. As such they can contribute to and be impacted by Artificial Intelligence (AI) developments in many ways. Science Europe considers that the implications of an AI legislation on the research and innovation (R&I) sector are currently not sufficiently taken into account. 

The importance of AI for R&I
The importance of AI for R&I is growing quickly. Future EU legislation on AI could have an impact on the activities of RPOs and RFOs, including:
•	Research on the development of AI systems;
•	Implementation and embedding of AI methods in, possibly, all fields of research;
•	Use of AI in research administration and governance, for example to identify reviewers for grant applications or pre-assess project applications.

The EC analysis of the answers to the consultation in spring 2020 shows that fostering R&I on AI is of great importance to many respondents. It is therefore surprising that the EC Inception Impact Assessment from 23 July 2020 does not refer to the R&I sector at all and, instead, is purely business oriented. 
Science Europe would like to see R&I properly addressed in EU legislation on AI. In this respect, Science Europe invites the EC to take into account the following three points:

1.	Developers’ and users’ liability
The question of liability for developers and users of AI systems does not only concern industry, as implied in the Inception Impact Assessment. Researchers are important contributors to the development of AI systems. The EC should ensure that potential EU legislation tackling liability issues provides the necessary safety guarantees to both users and developers of AI systems while at the same time fostering R&I on AI. 

2.	Links between AI and Open Science
Science Europe members define and implement numerous Open Science policies, including Open Access to research publications and research data. Science Europe promotes Open Science policies and, moreover, Science Europe and some of its members are actively engaged in the development of the European Open Science Cloud (EOSC). 
EOSC will make data available on a large scale and across borders for researchers to use, based on the principle as open as possible as closed as necessary.’ The increased availability of data will lead to many researchers using AI technologies for data analysis. Healthy, unbiased datasets and data protocols are needed for researchers to produce quality research. The EC needs to ensure that future legislation on AI supports Open Science policies.

3.	Solid scientific results depend on robust and reliable data
Researchers using AI systems to conduct their research, as well as research organisations using AI in their daily business for research administration and governance, need to be sure that the data and the AI systems they use are reliable. The quality of results provided by AI applications depends on robust data collection methodologies and tools, transparency of data sets, as well as monitoring and curation of data sets and algorithms. This is critical to ensure that AI systems do not cause erroneous results because of biased data. The EC services should take these aspects into account when considering a potential legislative or soft law approach on AI and ensure coherence with other legislative projects such as the proposal on European data spaces.

Science Europe would be happy to collaborate with the EC by providing further input from its member organisations and their expertise on AI in the R&I sector. Future EU legislation on AI needs to strike the right balance between safeguards for users and developers of AI systems and a legal environment that fosters R&I.
"
F551005,10 September 2020,Johann CAS,EU-Bürger/-in,-,-,-,Österreich,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"Comments by Johann Čas on the European Commissions’ consultation on the inception impact assessment as part of the initiative “Artificial intelligence – ethical and legal requirements”.
Johann Čas is a senior researcher at the Institute of Technology Assessment of the Austrian Academy of Sciences. Among other things he is co-author of the study “When algorithms decide in our place: the challenges of artificial intelligence in Switzerland” (https://www.oeaw.ac.at/en/ita/projects/the-social-effects-of-artificial-intelligence/)  (the main text is only available in German) and a team member of the H2020 PANELFIT project consortium (https://www.panelfit.eu/). 
All opinions and views expressed are personal.

Welcoming the opportunity of providing feedback on this legislative initiative I would like to provide the following comments and suggestions:

A. Context, Problem definition and Subsidiarity Check

The introduction appears to be based on overoptimistic perspective. Whereas the positive potentials certainly exist it should be also mentioned that they are also accompanied by corresponding risks, that their materialization depends on the creation of suitable framework conditions, which must also include a fair distribution of the possible benefits. A more realistic and sober assessment of opportunities and risks would be more helpful as a basis for deciding on regulatory options. To explain this recommendation in more detail, here an excerpt from a commentary I wrote during the consultations on the Ethics guidelines for trustworthy AI of the High-Level Expert Group on Artificial Intelligence (HLEG):
“Last but not least, the working document [draft Ethics guidelines for trustworthy AI] appears to overestimate the actual and potentially positive contributions of AI to solve the grand challenges our world is facing, missing to provide evidence for this positive overall evaluation. Whereas large and important positive potentials can be envisaged, past and current use of AI does not appear to support this judgement. Taking increasing economic inequality as an example, AI has rather contributed to it – e.g. in form of a key enabling technology of high-frequency trading on financial markets - but I’m not aware of making serious attempts to use AI to resolve imbalances on labour markets. On the political level, AI is rather threatening civil liberties and democratic systems than empowering citizens. On a global level, AI is rather supporting the establishment of worldwide monopolies than empowering consumers. Data based businesses possess unprecedented economic capacities, unparalleled political influence, powers to shape the results of democratic votes, unique possibilities to influence or to manipulate individuals in the information they receive or decisions they take. By disproportionately stressing the potential positive impacts and neglecting the already materialised threats the working document in the current form might contribute to an inappropriate reliance on AI when tackling urgent problems of the EU and the world. By missing to mention these dangers and threats it also misses to analyse them and consequently to develop measures and policies to counter them. This leads back to the ... [next] critical comment: we are primarily not in need of a trustworthy technology but of making good use of opportunities offered by technology in the human interest and keeping human agency.”

Complete comments are available in the attached pdf file.
","['Comments by Johann Čas on the European Commissions’ consultation on the inception impact \nassessment as part of the initiative “Artificial intelligence – ethical and legal requirements”. \nJohann Čas is a senior researcher at the Institute of Technology Assessment of the Austrian Academy \nof Sciences. Among other things he is co-author of the study “When algorithms decide in our place: \nthe challenges of artificial intelligence in Switzerland” (https://www.oeaw.ac.at/en/ita/projects/the-\nsocial-effects-of-artificial-intelligence/)  (the main text is only available in German) and a team \nmember of the H2020 PANELFIT project consortium (https://www.panelfit.eu/). All opinions and \nviews expressed are personal. \nWelcoming the opportunity of providing feedback on this legislative initiative I would like to provide \nthe following comments and suggestions: \n \nA. Context, Problem definition and Subsidiarity Check \n \nThe introduction appears to be based on overoptimistic perspective. Whereas the positive potentials \ncertainly exist it should be also mentioned that they are also accompanied by corresponding risks, \nthat their materialization depends on the creation of suitable framework conditions, which must also \ninclude a fair distribution of the possible benefits. A more realistic and sober assessment of \nopportunities and risks would be more helpful as a basis for deciding on regulatory options. To \nexplain this recommendation in more detail, here an excerpt from a commentary I wrote during the \nconsultations on the Ethics guidelines for trustworthy AI of the High-Level Expert Group on Artificial \nIntelligence (HLEG): \n“Last but not least, the working document [draft Ethics guidelines for trustworthy AI] appears to \noverestimate the actual and potentially positive contributions of AI to solve the grand challenges our \nworld is facing, missing to provide evidence for this positive overall evaluation. Whereas large and \nimportant positive potentials can be envisaged, past and current use of AI does not appear to \nsupport this judgement. Taking increasing economic inequality as an example, AI has rather \ncontributed to it – e.g. in form of a key enabling technology of high-frequency trading on financial \nmarkets - but I’m not aware of making serious attempts to use AI to resolve imbalances on labour \nmarkets. On the political level, AI is rather threatening civil liberties and democratic systems than \nempowering citizens. On a global level, AI is rather supporting the establishment of worldwide \nmonopolies than empowering consumers. Data based businesses possess unprecedented economic \ncapacities, unparalleled political influence, powers to shape the results of democratic votes, unique \npossibilities to influence or to manipulate individuals in the information they receive or decisions \nthey take. By disproportionately stressing the potential positive impacts and neglecting the already \nmaterialised threats the working document in the current form might contribute to an inappropriate \nreliance on AI when tackling urgent problems of the EU and the world. By missing to mention these \ndangers and threats it also misses to analyse them and consequently to develop measures and \npolicies to counter them. This leads back to the first [next] critical comment: we are primarily not in \nneed of a trustworthy technology but of making good use of opportunities offered by technology in \nthe human interest and keeping human agency.” \n \nThe term "" trustworthy AI"" should be avoided, although it is used in many key documents. This term \nsuggests that, provided certain conditions are met, this technology can be used without further \nconcern or human supervision. Thus, the use of this pair of terms contradicts one of the key \nrequirements of "" trustworthy AI"", namely human agency and oversight. To explain this', 'recommendation in more detail, here another excerpt from the commentary that I wrote during the \nconsultations on the Ethics guidelines for trustworthy AI of the High-Level Expert Group on Artificial \nIntelligence (HLEG): \n“The first critical comment is related to the selected title as such: there are only a few technologies \nimaginable that can be regarded as trustworthy on their own. In the case of AI, the naming of “ethics \nfor trustworthy AI” is in appropriate and misleading for several reasons. Depending on the concrete \nAI technology in mind, the results produced by these technologies are at least prone to statistical \nerrors, some also show completely unexplainable (and unpredictable) behaviour. The labelling as \nguidelines for trustworthy AI contains at least implicitly the message that trust in these technologies \nis in principle justified as long as the developed guidelines are respected, neglecting the fact that it is \nthe use of technology only, which could deserve this marking. In the case of AI this comment, which \nmight be regarded as a linguistic sophistry, is doubly important. AI is threatening human autonomy \nand agency, as acknowledged in the working document; neglecting this fact in the very title is \nadditionally endangering human agency. At least a renaming in the form of “ethics guidelines for \ntrustworthy use of AI” or something similar should be considered. Otherwise the guidelines could \nbecome self-contradicting to one of the core principles mentioned in the document.” \nThe statement that “Just like for actions and decisions taken by humans, the use of AI to either \ndirectly take decisions or to support decision-making may lead to violations of fundamental rights, as \nguaranteed by and implemented in EU law.” negates fundamental existing ethical concerns and legal \nrestrictions on automated individual decision making in the European Union. By including direct \ndecisions by AI as a possibility, it pre-empts debates on this issue and the extent to which we should \ngrant such powers to AI. \nB. Objectives and Policy options \n \nAI is a very potent technology, affecting almost all areas of life and the economy, which brings with it \ngreat opportunities but correspondingly high risks. Accordingly, option 4 should be chosen and \ndesigned, as none of the previous options adequately addresses the positive and negative potentials. \nOption 4 does not exclude corresponding initiatives by industry or voluntary forms of labelling. \nHowever, such activities can by no means be sufficient, but must be embedded in an appropriate \nregulatory framework. \nThe regulatory framework should be designed gradually according to the magnitude of risks, in \nanalogy to the risk pyramid of the German Data Ethics Commission. AI applications that do not or \nonly indirectly affect humans do not require specific AI regulations. This includes, for example, AI \napplications for the analysis of large amounts of data in astronomy or physics, or applications in the \nfield of industrial automation. However, the second example does require political measures, more \non this below. \nFor all other AI applications, appropriate precautions must be taken according to the risks involved. \nFor example, recommendation systems or filter algorithms in social networks also need to meet \ntransparency requirements. The restriction to high risk applications proposed in Option 3B is totally \ninsufficient. First, it is not clear why two risk conditions have to be fulfilled simultaneously for AI \napplications to fall into this category of need of regulation. Second, such classification criteria hinder \ndynamic adaptations to new applications and associated risks. Moreover, high-risk applications \nshould always be questioned in principle, taking into account not only the effects on individuals but \nalso societal consequences (see below). \nIn principle, a broad, technology-neutral definition of AI is preferable.', 'C. Preliminary Assessment of Expected Impacts \n \nIn the section on possible economic impacts, a comparison of compliance costs caused by regulation \nwith the benefits of AI is addressed as one basis for decision making on legal obligations. The \npotential costs of non-compliance are, however, almost completely neglected. It is in principle \nquestionable whether calculations which compare the protection of fundamental rights with \neconomic costs should be permissible in democratic societies. \nIf such comparisons are made, however, at least the costs of non-compliance should be considered \naccordingly. This includes not only the economic costs, for example in the form of external effects or \ncosts caused for the industry by acceptance problems, but also the costs for society, social \ncoexistence and democracy. If the broad and deep impacts of AI applications are realistic, which are \nalways emphasised in the case of positive effects, they should also be taken into account in the case \nof possible negative effects. These include in particular effects on the social and political climate that \nhave already occurred or are becoming apparent. The principle of precaution requires that \npreventive effects be assessed and controlled in order to avoid future poisoning of social coexistence \nor overheating of the global political climate in analogy to the environmental pollution or global \nwarming \nThe section on possible social impacts contains a statement in which two auxiliary verbs have been \nmixed up: “… while AI-enabled automation may replace some jobs, the use of AI will simultaneously \nlead to the creation of new jobs …”, correctly it should read like this: “… while AI-enabled automation \nwill replace some jobs, the use of AI may simultaneously lead to the creation of new jobs …”, \nAutomation will in any case replace jobs as a direct effect; no company will invest in new \ntechnologies unless the investment in technology is expected to yield more than compensating \nsavings in labour costs. The extent to which these lost jobs can be partly or fully compensated by \nnewly created jobs depends on a number of external factors that cannot be predicted. A trustworthy \npolicy must ensure and guarantee that, whatever the actual development will be, the benefits of \nautomation are fairly distributed and that this does not translate into high and persistent \nunemployment. This demand goes far beyond the direct regulation of AI applications, but it is \nindispensable for a broad acceptance and therefore also the fullest possible use of the rationalisation \npotential of AI if the social coherence and political stability of the European Union is not to be \njeopardised even more. \n \nD. Evidence Base, Data collection and Better Regulation Instruments \n \nAs the response to this initiative also shows, online consultation is not sufficient for a broad \nparticipation of citizens. In view of the great importance and the many changes that AI brings to \ncitizens of the European Union, targeted citizen participation activities involving all Member States \nwould be appropriate and should be considered.']"
F551004,10 September 2020,Edward Haynes,Wirtschaftsverband,American Chamber of Commerce to the European Union (AmCham EU),klein (10 bis 49 Beschäftigte),5265780509-97,Belgien,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"The American Chamber of Commerce to the European Union (AmCham EU) has long called for a risk-based approach to artificial intelligence (AI) regulation, and fully supports the view that AI legislation must be targeted and focused on problems which are not already covered by existing legislation.

Any risk assessment (or indeed rules/prohibitions) must take into account the context when assessing risk, since an AI application used for the same purpose will pose different risks depending on the way it is integrated into business operations. While similar AI technologies in different use cases may present very different risk profiles. The focus should always be on the specific use case, not on the broad class of application or technology. As such, risk assessments must reflect the probability and severity of potential harm. We therefore agree with the Commission that any additional regulation should be focused on high-risk applications, and we encourage the Commission to propose a narrow definition of AI systems which would focus strictly on AI high-risk applications.

Please refer to our complete submission in the attached document.","['Consultation response \n \n \n \n   \nAmCham EU’s roadmap response on the proposal for \nartificial intelligence – ethical and legal requirements \nAmCham EU speaks for American companies committed to Europe on trade, investment and competitiveness issues. It aims to ensure a growth-orientated \nbusiness and investment climate in Europe. AmCham EU facilitates the resolution of transatlantic issues that impact business and plays a role in creating better \nunderstanding of EU and US positions on business matters. Aggregate US investment in Europe totalled more than €2 trillion in 2018, directly supports more \nthan 4.8 million jobs in Europe, and generates billions of euros annually in income, trade and research and development. \nAmerican Chamber of Commerce to the European Union  Avenue des Arts/Kunstlaan 53, 1000 Brussels, Belgium • T +32 2 513 68 92 \nSpeaking for American business in Europe  info@amchameu.eu • amchameu.eu • European Transparency Register: 5265780509-97', 'Consultation response  \n \n  10 Sep 2020 \n \n \nThe American Chamber of Commerce to the European Union (AmCham EU) has long called for a risk-based \napproach to artificial intelligence (AI) regulation, and fully supports the view that AI legislation must be targeted \nand focused on problems which are not already covered by existing legislation. \n \nAny risk assessment (or indeed rules/prohibitions) must take into account the context when assessing risk, since \nan AI application used for the same purpose will pose different risks depending on the way it is integrated into \nbusiness operations. While similar AI technologies in different use cases may present very different risk profiles. \nThe focus should always be on the specific use case, not on the broad class of application or technology. As such, \nrisk assessments must reflect the probability and severity of potential harm. We therefore agree with the \nCommission that any additional regulation should be focused on high-risk applications, and we encourage the \nCommission to propose a narrow definition of AI systems which would focus strictly on AI high-risk applications.  \n \nThe appropriate requirements for training data, record keeping, transparency, accuracy, and human oversight \nvary depending on the nature and use of an AI system. Outcomes of an AI-application are often only as good as \nthe data they are fed, so high-quality, curated data carries benefits on several levels, such as safety and accuracy. \nThe Commission should therefore avoid imposing one-size-fits-all requirements around these categories. The \nCommission should rather encourage companies to strive for high data quality and bias mitigation by making \nreasonable efforts, rather than imposing overly prescriptive requirements based on unattainable objectives. The \nemphasis should be on testing output, not on training data quality. Rather than putting requirements on training \ndata, it would be better to have requirements based on testing model performance using benchmark datasets, \nto make sure that the outputs are within an acceptable range, since it is the model output that ultimately \ndetermines the real-world impact of an AI system. \n \nMinimising the potential for businesses in Europe to choose different data sets around the globe or force them \nto ‘retrain’ AI systems on European data would produce lower quality AI outcomes. Moreover, AI systems \nbecome customised and evolve as a result of continuous training serving particular customers and markets. The \ntraining would be based on the specific business needs, location and business model. Consequently, any legal \nobligations concerning an operational AI system should apply to the operator of the system that has further \ntrained and evolved the AI as opposed to the original producer. \n \nWe support the need for a public dialogue on the use of facial recognition technologies, which could lead to \ntargeted legislation and specific requirements applying to high-risk applications. We would welcome further \nconsultation with industry on this point. \n \nWhile voluntary labelling schemes can be helpful to consumers or end-users in some markets, we do not believe \nit would be effective across such a broad field as AI, given the hugely diverse range of products and services that \nwill be developed and deployed across all sectors in the coming years. \n \nThe Commission should not impose ex-ante conformity assessment for AI systems, as these could turn into \nbarriers to enter the market. We would instead support a combination of ex-ante self-assessment by entities \nusing high-risk applications, followed by ex-post market surveillance. In order to demonstrate compliance with \npotential ex-ante requirements, the availability and use of relevant technical standards, testing protocols and \nthe availability of notifies bodies with adequate expertise and bandwidth are crucial for AI developers wishing \nto enter the market. A potential regulatory framework should take into consideration such availability, or risk \nundermining the potential of AI powered solutions. AmCham EU would also like to underline the existence of a \nsector-specific product safety framework, and the upcoming review of the General Product Safety Directive, and \ncalls on the Commission to ensure close coordination and avoid duplication in order to ensure legal certainty for \nall players. \n \nThe use of AI systems, and therefore any resulting liability, is context-specific. Therefore, the focus of risk should \nlie on a specific application and the context of its use. There is often a complex chain of various producers and \n   \n \n \n  AI, ethical and legal requirements  2', 'Consultation response  \n \n  10 Sep 2020 \n \n \nintermediaries involved, legal requirements for high-risk AI applications should therefore be addressed to the \nactors best placed to address potential risks. This is also why having more than a single operator who is liable or \nintroducing joint liability would not be workable. \n   \n \n \n  AI, ethical and legal requirements  3']"
F551003,10 September 2020,Irina Michalowitz,Unternehmen/Unternehmensverband,Twilio,groß (250 oder mehr Beschäftigte),067223231522-58,Vereinigten Staaten,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"Twilio thanks the European Commission for the opportunity to respond to the Inception Impact Assessment on developing requirements for Artificial Intelligence. Twilio has already submitted a response to the European Commission’s White Paper on Artificial Intelligence – A European Approach where the company highlighted its support for this important discussion and Twilio’s willingness to contribute constructively. 
Twilio wishes to see the development of a trusted ecosystem for AI development and deployment in Europe so that the company’s customers can easily and confidently adopt AI-enabled tools that transform how they connect with end-user consumers. With a well calibrated policy framework, particularly regarding the delicate issues of a risk-management and liability regime for AI deployments, Twilio believes that AI can be a key driver of business innovation and economic growth in Europe for years to come. 
Twilio looks forward to further discussion with the European institutions and to contributing to an AI policy framework that delivers real benefits for all Europeans. Please find enclosed Twilio's response to the impact inception assessment.
","[""Comments of Twilio to the Inception Impact Assessment on \ndeveloping requirements for Artificial Intelligence \nTwilio thanks the European Commission for the opportunity to respond to the Inception Impact \nAssessment on developing requirements for Artificial Intelligence. Twilio has already submitted a \nresponse to the European Commission’s White Paper on Artificial Intelligence – A European \nApproach where the company highlighted its support for this important discussion and Twilio’s \nwillingness to contribute constructively.  \nTwilio and the use of AI \nTwilio is a leading global cloud communications services provider. Twilio's products and services \nallow organizations to embed communications capabilities into their web, desktop, and mobile \napplications, enabling them to communicate more efficiently and effectively and reinvent how they \nengage with their customers. Organizations have used Twilio to allow users to contact their teacher \nor students, alert the public about an emergency, video chat with their doctor, speak to their \nrideshare driver, make a bank transaction, shop online, authenticate an account, and interact with \nelected officials, among many other activities. \nAs a business to business (B2B) company, Twilio currently handles more than 800 billion interactions \nevery year for more than 200,000 organizations globally – from the world’s largest enterprises to \nsmall- and medium-sized businesses (SMEs), which constitute the vast majority of Twilio’s customers \n– and across a broad range of industries including education, financial services, healthcare, logistics, \nnon-profit, and government services. Twilio is paid by organizations for the services it provides and \ndoes not monetize consumer data.  \nTwilio leverages artificial intelligence (AI) because it supports the company’s mission of enhancing \ncommunications. Twilio uses AI and high-quality data training sets to create products that help \ncompanies build better relationships with their consumers, help stop and prevent fraud, and better \ndetect unauthorized log-ins. These products empower innovative companies with AI tools that drive \nefficiency, responsiveness, and customer satisfaction. Twilio products that incorporate AI include: \n●  Twilio Flex, a programmable cloud contact center platform. Twilio Flex allows customers to \neasily use a virtual agent that can resolve a variety of customer issues and, if required, \ntransfer customers to a live agent who will receive suggested responses based on previous \ninteractions.  \n●  Twilio Autopilot, an AI interface that bridges the gap between human agents and self-service \nbots. Autopilot allows developers to build intelligent IVRs, bots, and Alexa apps that are \npowered by Twilio-built Natural Language Understanding and Machine Learning \nframeworks. It turns nested phone trees into simple “what can I help you with” voice \nprompts and allows customers to use voice search to access a knowledge base.  \n●  Twilio Understand, which is transforming how companies interact with customers by using \napps that convert intent into smarter IVRs or bots. It uses natural language to determine the \n1 \n \nTransparency Register identification number: 067223231522-58"", 'intent of what someone is saying or texting, which allows developers to build more intuitive \nphone trees or smarter messaging bots. It turns speech into structured data objects and \nanalyzes text from any communications channel (voice, video, text, Facebook messenger, \nhome assistants like Alexa) with a single natural language understanding model.  \n●  Twilio TaskRouter AP. This product helps route callers to the proper destination quickly, \nskipping some – or all – of the traditional phone menu. TaskRouter dynamically assigns \nmessages to the human agents that can best handle them and can use bots to detect intent \nsentiment. That means that TaskRouter can leverage the power of artificial intelligence to \nunderstand the emotion in the caller and route the call accordingly. Messages and other \ntypes of data can be routed based on the “skills” required and the priority set.  \nTwilio’s view on the European Commission’s policy option proposals \nBefore going into detail on the different policy options proposed, Twilio would like to reiterate the \nimportance of considering and promoting the benefits and efficiency gains resulting from the uptake \nof AI systems, especially in non-tech companies and industries, to the benefit of consumers. Twilio \nrecommends that the EU work to encourage the adoption of AI as part of Europe’s digital \ntransformation, propose new legislation only when necessary to address identified gaps in current \nlaw, and produce clear liability frameworks that recognize the role of contract law and established \nprinciples for consumer redress. AI is a new technology, but it is being applied in areas where there \nare already existing legal regimes – such as data protection, consumer protection, product liability, \nand anti-discrimination – governing business activities. As the White Paper rightly notes, the existing \nbody of EU law already applies to AI. Europe’s future economic growth depends to a large extent on \nthe ability of European companies across all industries to maximize the benefits of digitization and \ndigital transformation. AI applications like those provided by Twilio present important opportunities \nfor businesses to leverage digital technologies for growth. This is especially important for SMEs, \nwhere access to digital tools helps these companies to rapidly deploy enhanced capabilities in areas \nsuch as customer relations that were previously only available to larger companies. Beneficial \ndeployments of AI can also play a role in preventing existing harms, for example, by identifying \nsuspicious banking transactions or the use of telecommunications services for fraud or spam.  \nTwilio encourages European policymakers to take active steps to facilitate the use of low-risk or safe \nAI to improve the performance of companies and allow them to innovate, become more productive, \nand grow more quickly. A framework which uses a risk-based approach and assesses the risks from \nboth the types of AI applications and their use cases strikes an appropriate balance between \nestablishing rules for AI where they are required and using existing legal frameworks where they are \nnot. This is the most effective way to encourage the deployment of AI in the EU while protecting \nindividuals from harmful uses of AI. To support this transformation, the EU’s larger digital strategy \nshould focus on facilitating economical access to and use of the cloud infrastructure that underpins \nAI for all European companies. \nTo this end, Twilio supports leveraging and ensuring consistency of any AI legislation with existing EU \nrules such as the Product Liability Directive and the Product Safety Directive. AI-specific legislation \nrisks creating overlapping obligations or differential impacts on different types of technology, \nparticularly when it comes to national implementation. This outcome would not provide consumers \n2 \n \nTransparency Register identification number: 067223231522-58', 'or businesses with legal certainty when it comes to the use of AI or create an enabling environment \nfor the uptake of AI. \nTwilio would also like to comment specifically on the issue of liability. As a general principle, liability \nshould fall on the entity best positioned to mitigate the risk, and the liability framework should \nconsider the intended use of products versus how companies may choose to deploy them. Most of \nthe time, the entity best positioned to mitigate the risk will be the entity deploying an AI system, as \nit can define the use case for an AI system, foresee risks, and is closest to consumers who may suffer \nharm, making it easier for consumers to seek redress. Moreover, the risk of selling an AI product in \nan application not intended by the company that developed the AI should be with the entity making \nthat conscious choice. As a result, Twilio recommends that liability issues in a B2B context be \nprimarily governed by contract so that enterprises deploying AI can establish the capability and \nintended use of their AI system and take liability for its performance vis-à-vis the final consumer \nwhen used as intended or agreed in contract. Should the technology fail, the deploying entity will be \nable to seek redress through the enforcement of its contract with its AI developer. \n \nOption 1: Soft-law approach \nAI is context-specific. Many different specialized applications perform different functions while even \nmore generalized technologies may be deployed in different ways. Soft-law approaches would be \nbetter able to take account of this diversity while also allowing the EU to benefit from the range of \nexisting initiatives on AI at the national, EU, and international levels, such as work on principle and \nethics codes, industry codes of conduct, and the work of the High-Level Expert Group on AI. For most \ndeployments of AI, this approach would minimize unintended obstacles to the roll-out of AI across \nthe EU and avoid the development of overly-prescriptive rules, which may not be suitable for future \nAI applications. This approach would allow for the co-creation of a risk-based framework for AI that \nprotects consumers and ensures companies and policymakers focus on and deal with the most \nimportant issues. Twilio would welcome soft-law approaches and has already engaged in industry \ninitiatives leading to self-developed rules and principles. Twilio is already an active member of \nbusiness associations active in such matters, including ITI and BSA. An example of successful self-\nregulating work is Twilio’s engagement against fraud and spam: Twilio sits on the board of the \nAlliance of Telecommunications Industry Solutions (ATIS) and co-chairs the Robocalling and \nCommunication ID Spoofing group, which has been coordinating work on the implementation of the \nSTIR/SHAKEN protocol in the United States with the goal to implement a system to digitally sign and \nverify calls, allowing consumers and business users to be confident that the caller ID information \nthey are seeing has been verified.  \nThe High-Level Group on AI has already worked extensively on developing guidelines. Sub-groups \ncould be added that take into account different aspects of AI applications - B2B AI applications could \nbe one of them. \n \nOption 2: Voluntary labeling scheme \n3 \n \nTransparency Register identification number: 067223231522-58', 'A trusted environment depends on the ability of customers to understand possible risks, their rights, \nand how to enforce them. However, Twilio cautions against voluntary labelling schemes as these \nmay not be the most effective means of ensuring a trusted environment. Labelling, whether \nvoluntary or mandatory, may create confusion for businesses and consumers alike because AI \napplications can be trustworthy in one setting while they may produce different effects in another \nsetting, especially when deployed in a manner that the provider may not have envisioned. These \nnuances cannot easily be captured with a given set of labelling criteria. Labelling may also create a \nfalse sense of security among businesses deploying AI products, leading to careless or uncritical \ndeployments that do not take adequate account of the full context and possible effects of AI. \nFurther, premature labelling rules may curtail experimentation from industry groups that are also \nworking to develop approaches to explain AI applications and their uses. Policymakers should \nencourage this experimentation and focus on effective approaches to transparency, which include \nboth the AI system and the context in which it is used prior to pursuing labelling schemes. \nOption 3: mandatory requirements on specific issues  \nSub-option a) limited to specific categories of AI applications, e.g. Remote Biometric Identification \nSystems \nTwilio does not use biometric identification systems. As a general principle, Twilio would like to \nemphasize the need for facilitating technological innovation within the boundaries of fundamental \nrights and safety. Rather than issuing case-by-case application-specific regulation, policymakers and \nthe technology industry should work together to establish the framework of clear principles for a \nrisk-based approach within which innovation can flourish. Rather than focus on the technology or \napplication in the abstract, this should seek to balance risks related to the context of an application’s \nuse, the harms that may result, and the relevant legal framework already in existence. This \nframework can then be applied to biometric facial recognition or other future AI applications \nwithout a requirement for application-specific legislation for every new use of AI. This risk-based \nframework  protects individuals from harmful uses of AI while encouraging the deployment of safe \nand trustworthy AI applications. \n \nSub-option b):  Legislation for high-risk AI applications only \nTwilio welcomes risk-based approaches to AI and encourages policy makers to consider the full \ncontext and the purpose of an AI system as they develop approaches to evaluating its risk. \nSpecific legislation to cover high-risk AI applications should only be considered if no existing laws \nsufficiently cover the risks presented by these applications. When identifying high-risk uses and high-\nrisk sectors for the use of AI, policy-makers should explicitly define the high-risk uses subject to \nregulation and apply proportionate measures to maximize beneficial deployments while minimizing \nharms. Regardless of sector, when AI is used in back office or administrative functions where it does \nnot present a high risk to consumers, there is little need to subject it to heightened standards such \nas conformity assessments or strict oversight. Any legislation for high-risk applications should use \n4 \n \nTransparency Register identification number: 067223231522-58', 'the risk-based framework outlined above and address identified gaps in the current legal framework \nthat are specific to AI. \nSub-option c): Legislation covering all AI applications \nSince AI risks will be context-specific, Twilio believes it is best to use existing context-specific \nlegislation rather than omnibus AI regulation. Such legislation would likely have difficulty balancing \nthe broad range of AI deployment scenarios and differing levels of risk involved. It would be \nextremely difficult to issue general legislation regarding all types of AI applications that covers all \npotential future harm to consumers, including by applications that cannot be clearly defined at this \nstage. Such legislation would leave businesses in legal uncertainty and would severely hamper AI \ndevelopments or applications in Europe. Using a risk-based framework that addresses high-risk \napplications and their context and takes into account existing legislation is a better option than \nblanket regulation of all AI applications. \nOption 4: A combination of all the above options \nAs previously noted, AI encompasses many different types of applications whose impacts vary based \non wider context. In order to take account of this diversity and the risks of harm presented, a mix of \napproaches may be appropriate, in particular soft-law approaches for most applications that are low \nrisk combined with a risk-based and context-aware approach to addressing high-risk applications.  \nTwilio does not support legislation that would include policy options that are not clearly defined in \ncontent and scope, in particular technology-specific regulation or high-risk designations that do not \ntake full account of context. \nWhen it comes to the economic cost assessment of the Commission, Twilio would like to highlight \nthat the actual economic cost goes beyond the application of each policy option. In particular, the \nEuropean Commission does not consider the economic cost for businesses and society of not being \nable to develop and test certain applications. In addition, more regulation does not equal an \nincrease in trust per se – consumer trust is linked to more aspects than a regulatory framework, \nespecially if that framework is not understandable or visible to the end user.   \nTwilio wishes to see the development of a trusted ecosystem for AI development and deployment in \nEurope so that the company’s customers can easily and confidently adopt AI-enabled tools that \ntransform how they connect with end-user consumers. With a well calibrated policy framework, \nparticularly regarding the delicate issues of a risk-management and liability regime for AI \ndeployments, Twilio believes that AI can be a key driver of  business innovation and economic \ngrowth in Europe for years to come. Option 1 alone, or in combination with a carefully crafted \noption 3 b), may be a step into this direction.  \nTwilio looks forward to further discussion with the European institutions and to contributing to an AI \npolicy framework that delivers real benefits for all Europeans. \n5 \n \nTransparency Register identification number: 067223231522-58']"
F551002,10 September 2020,Jens BJÖÖRN,Unternehmen/Unternehmensverband,Fortum Oy,groß (250 oder mehr Beschäftigte),03501997362-71,Schweden,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"Fortum would like to see a regulatory framework for AI based on the following principles: 

Accountability should be the key element when creating an AI regulatory framework instead of creating exhaustive lists of sectors and critical use with demands of prior conformity assessments and approval. Accountability-based compliance and governance programs enable organizations to operationalize principles-based laws into risk-based, verifiable, demonstrable and enforceable corporate practices and controls, supported by technology tools. 

New regulation should only be introduced when there is a gap in GDPR or other legislation. It’s important to avoid regulatory overlaps and conflicts. 

Sectors that already have a primary authority for control and oversights should not be burdened with additional supervision. This would be the case with nuclear where new applications and systems usually gets approval from the national radiation authorities.  "
F550994,10 September 2020,Jelle Hoedemaekers,Unternehmen/Unternehmensverband,Agoria vzw/asbl,mittel (50 bis 249 Beschäftigte),68004524380-10,Belgien,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"“Agoria is the Belgian federation for the technology industry. We are paving the way for all technology-inspired companies in Belgium pursuing progress internationally through the development or application of innovations and which, together, represent some 300,000 employees. We are proud that more than 1,900 member companies trust in the three pillars of our services: consulting, business development and the creation of an optimal business environment.”. 

Agoria thanks the European Commission for its efforts to foster Artificial Intelligence and for the opportunity to provide inputs regarding its Proposal for a legal act laying down requirements for Artificial Intelligence, which presents several options for future legislation. 

Firstly, Agoria is in favour of Option 1 of the alternative options to the baseline scenario, i.e. EU “soft law”, implying a non-legislative approach to facilitate and spur industry-led intervention without putting forward an EU legislative instrument. In addition, we believe that a “soft law” approach could build upon existing national initiatives and encourage a quicker industrial transformation using AI-driven systems to automate and reinvent fundamental industrial processes. Secondly, we acknowledge that others may see specific risks relating to the usage of AI in certain situations, justifying a legislative intervention. However, we must stress that the possibility of such risks arising at some point in the future does not imply that this is the case today, neither sporadically nor systematically. We urge the European Commission to analyse what is wrong in practice, before considering the optimal means of resolving these issues. Once this is clear, requirements for these specific high-risk situations will become evident and remedies may be needed. Where remedies are needed, enforcement methods should be considered, but not beforehand. Suggesting ex-ante enforcement mechanisms without any background is causing a lot of uncertainty, and SME’s specifically will be put at a disadvantage by this. 

Agoria believes that before choosing any option for a legislative intervention, existing regulation must be analysed carefully, identifying potential gaps precisely. This analysis should help to propose the adequate tools to fill those gaps and to establish a legally sound definition of AI. Given that AI has many meanings for different people and that many definitions of this term exist already, it is vital for the success of any AI related legislative action to define AI in a robust way. The most important aspect to keep in mind is that AI is not a product in itself, but rather a technology embedded in products, services and applications, which may be used in an extensive array of sectors, which puts all concerns related to AI in another perspective. For example, the health sector is already heavily regulated, and additional requirements should fit into the existing mechanisms. Additional unnecessary AI-related requirements within the EU product legislation should be avoided.


"
F550993,10 September 2020,Anissa Kemiche,Gewerkschaft,Syntec Numérique,klein (10 bis 49 Beschäftigte),565078326484-60,Frankreich,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"Syntec Numérique soutient la Commission dans son ambition de stimuler le développement et l'adoption de l'IA et des nouvelles technologies, tout en veillant à ce que les risques potentiels soient traités de manière adéquate. Afin de garantir la cohérence avec la législation sectorielle existante, la future proposition de réglementation de l’IA devra tenir compte de la réglementation européenne existante et qui couvre déjà l’application de l’IA (protection des droits fondamentaux, des consommateurs et sécurité des produits). 

Compte tenu de la diversité des applications et des technologies de l'IA, nous recommandons à la Commission d'adopter une approche ciblée et fondée sur les risques. Une telle approche devrait être basée sur des définitions claires. Elle doit prendre en compte le risque posé par le déploiement d'un système d'IA, le domaine d'application, le type de déploiement et la nature des risques.

Vous trouverez ci-joint un détail de nos propositions sur les quatre options présentées dans l'analyse d'impact.
"
F550992,10 September 2020,Angeliki Dedopoulou,Unternehmen/Unternehmensverband,Huawei,groß (250 oder mehr Beschäftigte),114467111412-38,Belgien,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"We are grateful to the European Commission for this opportunity to provide input towards proposals for legislative action on requirements for Artificial Intelligence (AI). We encourage EU policymakers to pursue a collaborative and inclusive approach for preparation of their proposals including the perspective from global industry.
We feel that the challenge of risk assessment should be addressed as a priority in the proposals. A naïve approached focused only on high-risk applications can be an unintentionally blunt approach to risk assessment of AI. A change to the contextual integrity of AI systems may trigger re-evaluation, re-training, or re-development. Huawei encourages a tech-neutral approach for risk assessment of applications according to scenarios instead of generic technologies. A better definition is needed for high-risk applications to ensure legal clarity and certainty. The definition of AI high risk applications provided in the AI White Paper is not the same as the one provided in the civil liability report. The majority of machine learning applications are not high-risk and would not be constrained by ex-ante requirements. 
Regulatory oversight complements legislation through consistency and harmonisation of decisions and requirements from competent authorities at the national-level and EU-level. Any definition of high-risk scenarios for conformity assessment should be as clear as possible and regularly reviewed through an industry-led, transparent process.
We recognize the challenge to achieve consistency with other relevant legislation. To ensure Europe is fit for the digital age the EU must enable continued trust in AI. Trust needs to be based on facts. Facts must be verifiable, and verification must be based on international, globally-recognized, common standards. We believe that this is an effective model for building trust in the digital era. Labelling is potentially problematic, and so what is required is a thorough analysis of the various approaches to this, such as certification, third—party audit, benchmarking and ratings etc. Huawei has commissioned a paper which will be published in Q4 2020 that speaks to the need for an assurance scheme that considers the complexity of the AI market – we are a rich and complex ecosystem that needs to come together to ensure high quality and high trust for customers. We are committed to supporting industry initiatives that, at a global level, achieve the design and promotion of standards.
It is also essential to work with standards organizations to further codify the best practices of AI governance into global standards and specifications. Closer partnership between the public and private sector would effectively collect industry best practices for AI governance and provide minimum acceptable standards for different parts of the supply chain.
Therefore, we recommend these proposals include a healthy diet of industry-led activity with self-regulation that complements any formal regulation on requirements for AI. This is why we also believe that a combination of the four different sets of policy options will better fit in the current context. We need multi-stakeholder initiatives and governance for a healthy and competitive market that can deliver sustainable economic growth and bring real benefits to people and society at large. Together, we can pass the benefits of digital technology to everyone.
Europe needs to lead the world out of the coronavirus pandemic and into its recovery. We believe in the power of innovation. Huawei has been a trusted partner in Europe for 20 years. We are committed to support EU policymakers prepare their proposals in the policy area of AI and deliver a Europe fit for the digital age.","['Huawei’s input to DG CNECT Inception Impact Assessment on AI \n \nHuawei started as a small company in China and we have grown to be a world leader in \ndigital technologies. We have been a trusted partner in Europe for 20 years and we are \ncommitted to staying in Europe. Today, we employ over 13000 people in Europe, running \ntwo regional offices and more than 20 research sites. Huawei is a world leader of digital \ntechnologies working with European industry partners to support a diverse ecosystem of \nfull-stack, all-scenario AI offerings. For example, Huawei MindSpore AI framework, Huawei \nAscend AI microprocessors, Huawei contributions to open source hardware and software \nfor AI and cloud computing communities, Huawei Harmony IoT operating systems, and \nmobile services for navigation and search. \nFollowing the fast deployment of AI applications and use cases in different sectors of the \neconomy, we believe that now is the right moment to further expand the discussion about \nthe product safety and liability aspects of AI technology. We need to ensure consumer \nprotection and a robust regulatory framework which should remain technology neutral. High-\nrisk applications should be regulated under a clear legal framework which provides on the \none hand a legal certainty for enterprises and on the other hand adequate protection for \ncitizens. \nThere is a need for a better definition of high-risk applications because it is complex to \nunderstand and explain Artificial Intelligence under one fixed definition without the context \nof different use cases and applications. There is also a need to build trust in AI through \nawareness and promotion of digital skills and AI knowledge by incorporating new ways to \nlearn AI and ICTs in primary and higher education curricula and by providing EU-wide free \ndata, statistics, and AI courses for all (adults and children).This would help citizens to better \nrecognise different aspects of AI technologies and promote the uptake of AI.  \nWe believe that AI will be used in almost every sector of the economy, however we still need \nto observe the technology until it gets mature. Complexity, opacity and autonomy are \nvariable terms when it comes to the definition of AI. There is no clear delineation between \nsoftware and AI. Regarding the definition of AI we propose that: \n•  AI application risk assessments should focus on intended use of the application and \nthe type of impact from the AI function; \n•  Detailed assessment lists and procedures must enable self-assessment to reduce \nthe cost of initial risk assessment; and \n•  Assessment lists and procedures should match sector-specific requirements. \nThe existing legal framework based on fault-based and contractual liability is sufficient for \nthe  state-of-the-art  including  AI  so  extra  regulation  is  unnecessary,  would  be  over-\nburdensome and discourage the adoption of AI. \n \nThe risk assessment must take an application-inclusive approach involving not only the \nproperties of the algorithms and training data but also aspects such as the reliability of', 'sensors/data  sources  and  user  interfaces,  which  can  significantly  impact  application \nperformance in real-world use. \nFor the  assessment of  high-risk  AI applications,  a  bottom-up  approach may  be  more \neffective, where industry should play an important role in finding and establishing clear \nassessing criteria, and providing practical assessment guidance. There is a need to bring \ntogether  consumer  organisations,  academia,  member  states  or  businesses  to  assess \nwhether an AI system may qualify as high-risk. The Commission and the standing Technical \nCommittee high risk systems (TCRAI) could assess and evaluate an AI system against high-\nrisk criteria both legally and technically. \nA voluntary labelling system could complement the requirements for high-risk applications. \nIn the complex supply chain of an AI system with multiple stakeholders, these labels should \nbe designed for different roles based on their technical specifications and aligned with \nspecific sectors, since a ‘one-size-fit-all’ scheme may be ineffective. A governance model \nthat considers the supply chain should develop the right criteria and target the intended goal \nof transparency for consumers/businesses, and incentivize responsible AI development and \ndeployment. \nAs regards the quality of training data sets, it is indisputable that any AI use case will benefit \nfrom high quality and representative data sets. However, there is an element of subjectivity \ninvolved in defining what high quality may mean for a particular use case. Setting a fixed \nstandard may unnecessarily constrain the work of data scientists.  \nHuawei further refers to our response to the AI Whitepaper consultation in the ANNEX below \nwhere we elaborated on these aspects in more detail. \nANNEX \nHuawei’s Response to the European Commission’s Whitepaper on \nArtificial Intelligence \n \nOn the Ecosystem of Trust  \nA. Establishment of a multi-actor governance framework  \nThe AI chain is complex with multiple market participants providing different component such \nas  chips,  sensors,  datasets  and  software.  As  the  typical  activities  and  technical \nspecifications involved are different for each role, a “one-size-fit-all” approach cannot meet \nthe requirements for all of the participants. For example, data controllers need to consider \nhow to disclose the source, usage, and measures for handling datasets, while algorithm and \napplication  providers  should  mainly  consider  how  to  provide  objective  and  easy-to-\nunderstand explanations for their algorithmic models. In order to provide clear guidance for \ndifferent actors, it’s necessary to identify the typical activities and requirements for each role.  \nSuch a multi-actor framework may be defined as followed:', 'Collaboration of diverse market participants would form a dynamic ecosystem and facilitate \ninnovation along the value chain, but also makes the whole system more complex and hard \nto regulate. If an AI system takes undesirable actions, it may be difficult to identify the source \nof the problems and hold the most relevant actors accountable. By establishing a multi-actor \ngovernance framework and reducing vulnerabilities of each parts, the robustness, security \nand quality of the AI system could be enhanced. \nEach participant should focus on different measures when they provide AI components at \ndifferent layers. There are some recommendations for the typical roles: \n1. Data processors/controllers: Ensure data management and other related operations \nfully comply with General Data Protection Regulation (GDPR) and other applicable laws and \nregulations. \n2. AI computing platform providers: \n- Information security hardening for hardware; \n- Defence mechanisms for machine learning; \n- Trustworthy implementation of operators; \n- Secure and robust operation environment for AI frameworks and AI application enablement;', '- Traceability, privacy protection, security, and robustness of software. \n3. AI algorithm providers: Research and develop algorithms that meet a certain standard \nof security and robustness, and use statistical analysis, semantic verification, and other \nmethods to continue enhancing the capabilities of algorithm programs to run as expected.  \n4. AI application providers: Provide services, applications, sub-systems, and supporting \nO&M  mechanisms  that  meet  trustworthiness  requirements  in  the  domain  by  using  AI \nalgorithms and computing platforms, like the cloud, edge, and devices, that meet certain \nsecurity standards.  \n5. Solution providers: Ensure AI systems meet industry-specific needs and are trustworthy, \nand provide solutions and supporting services that meet the safety and ethical requirements \nof each scenario. This can be achieved through various methods, such as pre-event analysis, \nin-event intervention, and post-event audit and evaluation.  \n6. Deployers/Operators: Perform acceptance tests to determine whether AI systems can \nfulfil  their  intended  purposes,  and  help  identify  potential harmful  outcomes  and  make \nappropriate corrections to ensure that the deployment goals are achieved. Effectively control \nand prevent risks related to AI security and privacy during deployment and operations.  \n7. Consumers/Customers: They have the right to choose whether to use AI systems. If \nthey use AI systems, they must follow product/service instructions to ensure products or \nservices are used safely. Avoid using AI systems for purposes that violate laws or ethical \nrules, such as developing intentionally misleading photos or video and audio content.  \nSuch  a  multi-actor  framework  would  help  further  refine  and  divide  AI  governance \narchitecture at a nuance level. This will help practitioners identify emerging risks related to \ndifferent components, and proactively take measures to prevent these risks. In addition, \nsuch a multi-actor framework would help increase the traceability of different layers of AI \nsystems and determine the scope of responsibility of each layer. For example, we can use \ntechnical means such as data security labelling (i.e. labelling data security, identifying data \ntampering, and finding the cause), model signature tracing, and model watermarks to help \ndetermine the source of data tampering or leakage, or the root cause of a security failure. \nIt’s essential to work with standards organizations to further codify the best practices of AI \ngovernance  into  global  standards  and  specifications.  Also,  the  close  public-private \npartnerships could help effectively collect best practices of AI governance and provide \nminimum acceptable standards for different layers.  \nB. Regulatory framework of high-risk AI applications  \nWe strongly support the fundamental concept of basing regulatory requirements for AI \napplication on an assessment of the degree of risk posed by an AI application. We agree \nwith the approach to determine “high-risk” AI applications considering both the sectors and \nthe use, and would like to know about the concrete definition and a list of specific “high-risk” \napplications at a granular level, which would bring legal clarity and certainty.  \n1. Ex ante and ex post requirements  \nFor high risk applications, some ex ante requirements would be desirable. This will depend \non the particular use case and context. Generally, requiring the involvement of domain \nexperts  would  be  desirable: for example,  with  digital healthcare  solutions,  an  ex-ante \nconformity assessment should involve the expertise of doctors and nurses.', ""Ex ante requirements can also be helpful to companies as the legal certainty allows them to \nabsorb costs in advance and tailor products or solutions accordingly. After a solution is given \nthe green light, the relevant regulator will need to monitor compliance (for example and if \nappropriate, through regular checks, as the FCA might do with financial products), but also \ndetermine whether the original requirements were adequate. If not, these could be modified \nand/or an ex post requirement might be added. Regulators should ensure that imposing any \nadditional action is subject to a thorough cost-benefit analysis. \nMeanwhile, we believe the majority of machine learning applications should not be deemed \nhigh risk and so should not be constrained by ex-ante requirements. In this case, we expect \nex post market surveillance to be sufficient and innovation-friendly, provided regulators and \nnational authorities are adequately trained. \n2. Institutional framework  \nLegislation alone cannot guarantee a safe and secure environment for citizens without \nadequate regulatory oversight to enforce it. The foundation for ensuring that AI is trustworthy \nand secure is to ensure national regulators are sufficiently resourced (in terms of talent, \ntraining, and tools) to be able to scrutinize AI-related risks in their respective verticals.  \nEnsuring regulators from different Member States are connected to one another is another \nimportant  requirement:  the  focus  should  be  on  consistency  mechanisms  and  the \nharmonization of decisions and requirements from the member states' competent authorities, \nas well as the relevant institutional coordinating within EU. Guaranteeing this from the start \nwill ensure future regulatory frameworks will not suffer from the shortcomings such as those \nhighlighted in DigitalEurope’s response to the DG JUST Roadmap Consultation on GDPR.  \n3. Conformity assessment  \nWe would propose that a clear definition of “high risk” scenarios should be provided and \nreviewed regularly with transparent process and should be applied to all vendors.  \nRegarding the process and criteria for assessing the risk level of an AI application, we \nacknowledge  the  intent  behind  the  cumulative  sectoral  +  intended  use  approach,  but \nconsider  the  reliance  on  allocation  of  “high-risk  sectors”  to  be  extremely  difficult  to \nmeaningfully implement in practice, as indicated by the examples of “exceptional instances” \nthat were already given in the AI white paper. This problem is further complicated by the \ndifficulty of establishing a useful application independent definition of AI. As an alternative, \nwe would propose: \n- AI application risk assessments that focus on intended use of the application and the type \nof impact the AI function has. E.g. does the intended use involve potential long term \nconsequences? Doe the AI determine the output of the application in a way that makes it \ndifficult for humans to assess if the outcome is correct?  \nThe risk assessment has to take an application inclusive approach involving not only the \nproperties of the algorithms and training data but also aspects such as the reliability of \nsensors/data sources and user interfaces etc., which can significantly impact application \nperformance in real-world use. \n- Some compliance requirements are based on subjective judgments, e.g. practitioners and \nusers have different understanding and requirements on Explainability issues, where an \nexternal conformity assessment procedure operated by third parties is more professional \nand reliable."", '- Reduce the cost of initial risk assessment by providing detailed assessment lists and \nprocedures to enable preliminary self-assessment by the industry. \n- Acknowledge sector specific requirements by providing/modifying the assessment lists and \nprocedures to match sector demands. \n- The risk assessment lists/procedures should be co-developed with multi-stakeholder input \nincluding Standards Development Organizations, and will need to be periodically revised. \n- Beyond application specific factors, the risk assessment may also need to take into account \nadditional factors such as: anticipated number of users, especially if the risk is to society as \nin the case of news recommender systems; dependence on support infrastructure, e.g. \npatient embedded medical devices that will fail if the specialized service provider for the \ncloud based AI ceases operating  \nC. Voluntary labelling scheme  \nLabelling seeks to address concerns that regulation doesn’t reach, it can help to strike the \nright balance between protecting users and encouraging innovations. A labelling scheme \ncan provides transparency with information that can be easily understood and assessed by \nusers and other stakeholders, and help to increase trust.  \nFor this purpose we would propose that a labelling scheme with clear specification should \nbe provided through public and private partnership. An industry-led approach would be \neffective, where industrial associations with the technological and industrial know-how could \nbe encouraged to explore in voluntary labelling frameworks within different scenarios, and \nprovide best practices and guidance for different applications.  \nD. International collaboration  \nSince the digital economy driven by AI typically involves an international value chain, a \nfragmented governance framework may lead to regulatory arbitrage and vicious competition \nacross  different  regions.  Establishment  of  a  multilateral  AI  governance  mechanism \nconsisting of members from governments, civil society and private-sector would be essential \nto promote a basic consensus of trusted AI across the world and avoid fragmentation of \nresponsibilities globally.  \nWe may learn from the practical experience of such multilateral governance mechanisms in \nthe ICT industry, especially the success story of the 3rd Generation Partnership Project \n(3GPP). Although the nature of telecommunications technology is different from AI, it could \nbe worthwhile analyzing the multilateral collaboration mechanism formed in the ICT industry, \nas part of the efforts to drive a world-wide consensus on AI governance frameworks. \nThe 3GPP is a collaborative project initiated by multiple partners/members to promote the \nstandards  development  and  adoption  of  emerging  telecommunications  technologies. \nThanks to the open multilateral governance mechanism of the 3GPP, 5G has seen the \nindustry converge on a universal set of standards, avoiding the fragmentation of standards \nin 2G, 3G, and 4G. The coordination of standards have benefited all stakeholders across \nthe  value  chain.  This  will  also  further  incentivize  investment  in  5G  and  accelerate \ncommercial deployment. \nThe success of 3GPP has shown that a multilateral international mechanism could be an \neffective approach to coordinate the global governance landscape of emerging technologies \nlike AI, where a specialized, permanent international governance organization or a non-\npermanent international mechanism is essential.', 'The 3GPP consists of three types of members taking different roles in the collaboration: \n-  Organizational  Partners  (OPs) 1 ,  specifically  regional  Standards  Development \nOrganizations coming from different regions, which are the most important members of \n3GPP, working together to determine the general strategy of 3GPP. They authorize the \n3GPP to produce Technical Specifications (TS), and then set standards for their own regions \nbased on these TS. \n- Market Representation Partners (MRPs), such as GSMA, Next Generation Mobile Network \n(NGMN) Alliance, 3G Americas, and UMTS Forum. These MRPs offer market advice to the \n3GPP  and  keep  the  3GPP  informed  of  the  market  consensus  on  requirements  for \ntechnologies falling under the remit of the 3GPP, so as to contribute to the application and \ndevelopment of mobile communications networks worldwide. \n- Individual Members (IMs) who can contribute technically to one or more of the Technical \nSpecification Groups within the 3GPP scope. \nRefer to the 3GPP, the multilateral AI governance mechanism may have the following \ncharacteristics and functions: \n- Consist of standards organizations authorized by regional governments. The AI principles \nand governance frameworks developed within this mechanism could be recognized and \nincorporated by governments in different regions. \n- Foster public-private partnerships. The private sector should be encouraged to play its role \nin technology R&D and standards formulation, and foster public-private partnerships to \nrealize well-thought-out AI governance principles. \n-  Drive  the  development  of  international  standards.  International  standards  and \nspecifications for AI technologies (e.g. for security and trustworthiness) can be developed \nthrough collaboration with standards organizations, with a particular focus on the formulation \nof acceptable minimum baselines. \n- Establish an authorized certified test mechanism. Authorize independent international test \norganizations  to  evaluate  and  test  AI  products  or  services  based  on  unified  testing \nspecifications to ensure that these products or services meet the requirements for market \naccess. \n- Promote the application of technologies. Collect data and advice on market requirements \nin order to promote the application and development of AI in various industries. \nBased on Huawei’s experience of open collaboration in the ICT industry, we are willing to \nsupport Europe to lead in establishment of such a multilateral, democratic, open, and \ntransparent international AI governance platform, which would promote collaboration among \nstakeholders cross the world, and advance the uptake and adoption of AI in a fair manner. \n                                            \n1 The 3GPP unites seven Organizational Partners from six regions: ETSI from Europe, ATIS from USA, ARIB \nand TTC from Japan, CCSA from China, TSDSI from India, and TTA from Korea.']"
F550987,10 September 2020,Magda Bublewicz,Wirtschaftsverband,Związek Pracodawców Branży Internetowej IAB Polska,klein (10 bis 49 Beschäftigte),050841318640-60,Polen,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"Związek Pracodawców Branży Internetowej IAB Polska przedkłada w załączeniu wkład do konsultacji Komisji Europejskiej dot. Sztucznej Inteligencji.

","['Contribution of the Internet Industry Employers\' Association IAB Polska in consultation with the Commission\nEuropean Union on Artificial Intelligence\n \n1. Scope:\n○ First of all, we warn the European Commission against extending the scope of future regulations\non Artificial Intelligence (AI) to the broadly defined category of ""automatic\nmaking decisions. ”To do so would be against the original course\nproposed in the White Paper on Artificial Intelligence, where the focus was on\nhigh-risk AI applications, the so-called sector criterion. If only Artificial Intelligence\nhas been defined as ""automated decision-making"", this would have caused it to arise\ndisproportionate, unjustified regulatory obligations. It would discourage not only to\ndevelopment and implementation in Europe of applications based on AI solutions, but also\nautomated systems that pose no risk or cause any harm.\n \n○ Definition of non-pecuniary damage: We renew our concerns about the introduction of the concept\n""immaterial harm"" to future AI legislation. As\nas an alternative, we propose to refer to the concept of ""significant limitation of the enjoyment of rights\nbasic ""which would be closer to the existing legal framework.\n \n2. Variants of the approach to regulatory policy:\n○ Option 0 (baseline): We believe that proper implementation should be ensured\nexisting EU regulations on AI before any new ones are introduced\nArtificial Intelligence legal regulations. Currently, AI does not operate in a vacuum and is subject to\nnumerous existing laws, including GDPR, regulations on medical devices, and\ncatalog of fundamental rights.\n \n○ Option 1 (Industry Driven Intervention - Self-Regulation): Whatever\npolitical strategic policy options are being implemented, it is worth supporting the industry in\nsetting, implementing and sharing standards for responsible practices.\n \n○ Option 2 (voluntary labeling regulation): We remain skeptical about\nthe impact of a labeling system for signaling that specific products and services are based on\nArtificial Intelligence are trustworthy in Europe. Administrative burden for SMEs related to\ncompliance with onerous labeling obligations - if any\ndeveloped on the basis of updated recommendations for Trustworthy AI i\nprepared by the EU High Level Expert Group on Artificial Intelligence -\ncould far outweigh the benefits of such a system.\n \n○ Option 3 (regulation with mandatory requirements): Opportunity cost of not using\nArtificial Intelligence should be part of the assessment when considering any future\n        Internet Industry Employers Association IAB Polska\n      ul. Puławska 39/77 02-508 Warsaw tel. +48 22 415 54 44 fax + 48 22 212 87 74\n                                                                      www.iab.org.pl', 'regulations aimed at reducing the risk and harm arising from the use of the application\nbased on Artificial Intelligence. Any future regulation should guarantee\nlegal certainty, be proportionate and increase confidence in Artificial Intelligence, no\nat the same time obstructing the unjustified introduction of AI-based innovations.\n■ 3a: Remote biometric identification systems can be a good example\na solution to which mandatory requirements could be applied based on\nrisk analysis.\n■ 3b: We support a well-defined, risk-based approach to regulatory implementation\non Artificial Intelligence, taking into account both seriousness and\nthe likelihood of harm occurring. We consider a good starting point\nthere seem to be sectoral criteria as well as core criteria concerning\nuse / application - proposed in the EC White Paper.\n■ 3c: We strongly warn against the adoption of any EU legislation\non the regulation of all Artificial Intelligence applications, which lacks\nthere would be a distinction between the uses of AI that can create significant\nrisk / harm, and those that do not pose such a risk or have a lower profile\nrisk. Such a legislative instrument would be disproportionate to the\nproblems identified so far by the European Commission, it would create\nsignificant barriers to the adoption of Artificial Intelligence (additional costs, delays,\nadministrative burden) in Europe, would generate additional costs in some\napplications due to the lack of implementation of AI-based solutions, as well as\nwould run the risk of lowering the regulatory requirements for these Artificial applications\nIntelligence that can pose significant risks.\n3. Law enforcement:\n○ We support ex post enforcement when problems arise as\nthe most appropriate and proportionate mechanism, except in areas where ex\nante are already established practice (e.g. medical processes). In such situations, we recommend\nalign any ex-ante evaluations with existing sectoral procedures and practices.\n○ If the Commission insists on ex-ante enforcement, we strongly warn against\nex-ante evaluation by any third party and recommend developing\nself-assessment procedures based on clear guidelines on ""due diligence""\nregulatory authorities. In practice, this would be provided by regulators\ndetailed guidance on how to carry out and document the assessment\nrisk, while delegating responsibility for carrying out a thorough assessment\npeople using Artificial Intelligence and those who know it best.\n \n        Internet Industry Employers Association IAB Polska\n      ul. Puławska 39/77 02-508 Warsaw tel. +48 22 415 54 44 fax + 48 22 212 87 74\n                                                                      www.iab.org.pl']"
F550986,10 September 2020,Nicole Santiago,Sonstiges,SIENNA Project,klein (10 bis 49 Beschäftigte),-,Vereinigtes Königreich,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"SIENNA, a European Horizon 2020-funded project, is looking into ethical, legal and human rights issues and is developing ethical guidelines for human genomics, human enhancement and AI & robotics. We welcome the language that puts fundamental rights and societal values first. Unlike the European Commission White Paper on AI where the focus was on building consumers’ and businesses’ trust in AI to increase uptake of the technology, the Inception Impact Assessment rightly prioritises the protection and safety of individuals and society at large. While we recognize there are many beneficial applications of AI, any deployment must be accompanied by a necessity and proportionality assessment, as well as ensure adequate and sufficient measures are taken to protect the fundamental rights and societal values from potential harms.

We recommend a combination of Option 2 (EU legislative instrument setting up a voluntary labelling scheme) and Option 3 (EU legislative instrument establishing mandatory requirements). A legislative instrument should go beyond high-level principles and obligations. However, the framework cannot be too specific or detailed, as that risks unintentionally narrowing the scope and impact of the framework. Furthermore, the legislative framework must be agile enough to respond to the rapid development of AI and related technologies. As such, an EU legislative instrument should involve both ex-ante and ex-post enforcement mechanisms. 

We believe that Option 3(b) (limiting the scope to high-risk applications) may be sufficient if the determination of ‘high-risk’ is well-constructed. However, given this determination will be a crucial element of the whole framework and will have a significant impact on whether the framework will effectively serve its purpose, extreme caution should be taken to set the criteria in a manner that is robust. Regardless of whether the framework is limited to high-risk applications, specific rules will be necessary for specific categories of AI applications. 

Option 2 (EU legislative instrument setting up a voluntary labelling scheme) can be one complementary way to enhance trust and verify compliance with certain rules. However, it is vital that this must not be understood as a replacement for legal responsibility. Additionally, voluntary certification labels must not become self-serving or a business-manipulated front for hiding risks and harms.

We do not agree that there are no direct significant negative social impacts or environmental impacts expected from the proposed measures. Further assessment of expected impacts associated with the proposed regulatory framework for AI is needed, particularly in regard to social and environmental impacts. 

","['SIENNA submission to the European Commission:  \nFeedback on the Inception Impact Assessment on the \nproposal for a legal act for artificial intelligence \n10 September 2020 \nIntroduction \nThis document provides feedback on the European Commission’s Inception Impact Assessment on the \nproposal for a legal act laying down a requirement for artificial intelligence, based on the findings and \nresults of SIENNA, a European Horizon 2020-funded project (2017-2021).1 This submission reflects only \nthe views of contributors2 that have prepared this input based on their research in the SIENNA project.  \nSIENNA (Stakeholder-Informed Ethics for New technologies with high socio-ecoNomic and human \nrights impAct) is looking into ethical, legal and human rights issues and is developing ethical guidelines \nfor human genomics, human enhancement and AI & robotics. It has received funding under the \nEuropean Union’s H2020 research and innovation programme under grant agreement No 741716. \n \nRecommendations on ‘A. Context, Problem definition and Subsidiarity Check’  \nWe welcome the language that puts fundamental rights and societal values first. In particular, we \nstrongly support the statement that the “ultimate objective is to foster the development and uptake of \nsafe and lawful AI that respects fundamental rights across the Single Market by both private and public \nactors while ensuring inclusive societal outcomes.” Unlike the European Commission White Paper on \nAI where the focus was on building consumers’ and businesses’ trust in AI to increase uptake of the \ntechnology, the Inception Impact Assessment rightly prioritises the protection and safety of individuals \nand society at large. While we recognize there are many beneficial applications of AI, any deployment \nmust be accompanied by a necessity and proportionality assessment, as well as ensure adequate and \nsufficient measures are taken to protect the fundamental rights and societal values from potential \nharms. \nIn developing this objective further, impacted human rights that must be considered include social, \neconomic, and political rights (e.g., right of self-determination; to work; to the enjoyment of just and \nfavourable conditions of work; to social security; to education; to the enjoyment of the highest \nattainable standard of physical and mental health; to vote; to equality before the law; to an effective \nremedy and to a fair trial), as well as related  impacts on democratic processes. Additionally, attention \n \n1 SIENNA project: https://www.sienna-project.eu.  \n2 Konrad Siemaszko (Helsinki Foundation for Human Rights); Rowena Rodrigues, Anais Resseguier, Nicole \nSantiago (Trilateral Research); Javier Valls Prieto (University of Granada); Robert Gianni (Maastricht University). \n \n \nThe SIENNA project - Stakeholder-informed ethics for new technologies with high socio-economic and \nhuman rights impact - has received funding under the European Union’s H2020 research and \ninnovation programme under grant agreement No 741716', 'must be paid to enhancing protection of vulnerable groups and individuals, who might be especially \naffected by the adverse impacts of AI.3 \nRecommendations on ‘B. Objectives and Policy options’ \nWe recommend a combination of Option 2 (EU legislative instrument setting up a voluntary labelling \nscheme) and Option 3 (EU legislative instrument establishing mandatory requirements). A legislative \ninstrument should go beyond high-level principles and obligations. However, the framework cannot \nbe too specific or detailed, as that risks unintentionally narrowing the scope and impact of the \nframework. Furthermore, the legislative framework must be agile enough to respond to the rapid \ndevelopment of AI and related technologies. As such, an EU legislative instrument should involve both \nex-ante and ex-post enforcement mechanisms.  \n \nIn regard to the sub-options of Option 3, we believe that Option 3(b) (limiting the scope to high-risk \napplications) may be sufficient if the determination of ‘high-risk’ is well-constructed. However, given \nthis determination will be a crucial element of the whole framework and will have a significant impact \non whether the framework will effectively serve its purpose, extreme caution should be taken to set \nthe criteria in a manner that is robust. We do not believe the approach proposed in the European \nCommission White Paper and reiterated in this document is sufficient. To avoid the danger of leaving \nsome high-risk applications under-regulated, this approach could be supplemented with an open-\nended clause, for instance, with a requirement to conduct a human rights impact assessment (HRIA) \nor other relevant impact assessment. Additionally, those applications deemed high-risk should be \nsubject to rigorous prior conformity assessment. \n \nRegardless of whether the framework is limited to high-risk applications, specific rules will be \nnecessary for specific categories of AI applications, such as biometric identification. In some cases, this \nshould include a ban on some applications, including AI-enabled large-scale scoring of individuals,4 AI-\nbased racial profiling systems and biometric recognition facilitating mass surveillance (understood as \na surveillance that is indiscriminate, not targeted against a specific individual5).  \n \nOption 2 (EU legislative instrument setting up a voluntary labelling scheme) can be one complementary \nway to enhance trust and verify compliance with certain rules. However, it is vital that this must not \nbe understood as a replacement for legal responsibility. Additionally, voluntary certification labels \nmust not become self-serving or a business-manipulated front for hiding risks and harms. \n \n \n3 Jansen, Philip., et al, ‘SIENNA D4.1: State-of-the-art Review: AI and robotics’, April 2018, https://www.sienna-\nproject.eu/digitalAssets/787/c_787382-l_1-k_sienna-d4.1-state-of-the-artreview--final-v.04-.pdf. \n4 High-Level Expert Group on Artificial Intelligence, Policy and Investment Recommendations for Trustworthy \nArtificial Intelligence, Brussels, 26.09.2019, p. 20, \nhttps://ec.europa.eu/newsroom/dae/document.cfm?doc_id=60343.  \n5 EDRi, Ban Biometric Mass Surveillance A set of fundamental rights demands for the European Commission and \nEU Member States, Brussels, 13.05.2020, https://edri.org/wp-content/uploads/2020/05/Paper-Ban-Biometric-\nMass-Surveillance.pdf.  \n2', 'We strongly recommend against Option 0 (baseline) or Option 1 (no EU legislative instrument). We \nbelieve that the EU should lead in setting baseline standards of protection of fundamental rights and \nsocietal values. There are already many soft initiatives in existence, to varying degrees of efficacy, and \nthey have a role to play in protecting and promoting ethical values and fundamental rights. However, \nthe role of the EU should be in establishing a strong legal framework to guarantee the protection and \npromotion of these rights, including, for example, addressing legal issues of liability and other ensuing \nharms.  Without  EU  leadership,  Member  States  will  continue  to  act  individually,  which  creates \nfragmentation and confusion for all stakeholders. \n \nRecommendations on ‘C. Preliminary Assessment of Expected Impacts’ \nWe do not agree that there are no direct significant negative social impacts or environmental impacts \nexpected from the proposed measures. Further assessment of expected impacts associated with the \nproposed regulatory framework for AI is needed, particularly in regard to social and environmental \nimpacts. For example, more understanding is needed on how the various proposed options influence \nthe way individuals and society understand trust and whether certain options might promote a false \nsense of trustworthiness in AI and algorithmic decision-making. Additionally, as AI is a technology with \na significant environmental impact (e.g., energy consumption, resource extraction, disposal), certain \nproposals that encourage the development and use of AI will lead to more environmental impacts. \nFurthermore, the impact assessment should evaluate the risks of not acting. \n \n \n \n \n \n  \n3']"
F550985,10 September 2020,Rowena RODRIGUES,Sonstiges,SHERPA Project,klein (10 bis 49 Beschäftigte),-,Vereinigtes Königreich,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"SHERPA is an EU-funded Horizon 2020 project that focuses on ethical and human rights aspects of smart information systems (artificial intelligence and big data analytics). We welcome language in this Inception Impact Assessment that prioritises protection of human rights and recognises both the benefits and harms of AI. The protection of human rights and safety is a societal priority, and not only a means to create (or the outcome of creating) trustworthy AI.

We also strongly agree that existing EU law needs more effective enforcement, that effective redress mechanisms are needed, and that we must be concerned about intended and unintended negative outcomes. 

We believe that it is absolutely critical that the EU develop a mandatory regulatory framework of ex-ante and ex-post enforcement mechanisms to ensure that AI systems are safe and do not violate fundamental rights and ethical principles.  A strong legal standard must be set at the EU-level that establishes a baseline, encourages high standards of protection of fundamental rights and societal values, and minimizes inconsistency and fragmentation at the Member State-level. Therefore, the SHERPA project strongly supports a combination of Option 2 (EU legislative instrument setting up a voluntary labelling scheme) and Option 3 (EU legislative instrument establishing mandatory requirement for all or certain types of AI). Depending on how risk is determined, by whom, and which criteria are used, the SHERPA project might support limited regulation for high-risk only AI (sub-option 3(b)).

The SHERPA project also strongly recommends further assessment of expected impacts. An impact assessment should also evaluate the risks associated with failure to act, and not only the impacts of a potential regulatory framework.
","['Shaping the ethical dimensions of smart information systems– a \nEuropean perspective (SHERPA) \n \n \nFeedback to the  \nEuropean Commission on its  \n \nInception Impact Assessment  \nfor a legal act on the regulation of artificial intelligence \n \nBased on research from the SHERPA project1 \n \n \nSeptember 10, 2020 \n \n \n \nThis project has received funding from the \n \nEuropean Union’s Horizon 2020 Research and Innovation Programme \nUnder Grant Agreement no. 786641\n \n1 Coordinated by Trilateral Research, with contributions from De Montfort University, F-Secure, Pineapple Jazz, \nand University of Twente. \n1', 'SHERPA (Shaping the ethical dimensions of smart information systems (SIS) – a European perspective) is \nan EU-funded project that focuses on ethical and human rights aspects of smart information systems \n(artificial intelligence and big data analytics). SHERPA aims to ensure that its recommendations and \nfindings help to move the AI ecosystems in the desirable ethical and human-rights respectful direction. \nThe final recommendations from SHERPA are due in April 2021. \n \nThe SHERPA consortium welcomes the opportunity to provide feedback to the European Commission on \nits Inception Impact Assessment on the proposal for a legal act of the European Parliament and the Council \nlaying down requirements for AI. This feedback is a follow-up to previous comments by the SHERPA \nconsortium in response to the public consultation on the European Commission’s White Paper on AI.  \n \nFeedback on ‘A. Context, Problem definition and Subsidiarity Check’ \nThe SHERPA consortium welcomes the language in section A that prioritises protection of human rights \nand recognises both the benefits and harms of AI. We have highlighted some strong statements below, \nand we recommend the Commission keep these statements and concerns as a central part of the \nproposed regulatory framework. \n \nWe strongly agree with the articulation that the “ultimate objective is to foster the development and \nuptake of safe and lawful AI that respects fundamental rights across the Single Market by both private \nand public actors while ensuring inclusive societal outcomes.” The protection of human rights and safety \nis a societal priority, and not only a means to create (or the outcome of creating) trustworthy AI. To make \nthis clearer, the regulation should explicitly acknowledge the potential for misuse and abuse of artificial \nintelligence, and hold developers, deployers, and users to the ethical principle of non-maleficence. \n \nWe strongly agree that existing EU law needs more effective enforcement, particularly related to \nfundamental rights and freedoms. An EU approach should build on existing legislation and enforcement \nmechanisms. A comprehensive gaps analysis is needed to identify & prioritise required regulatory reform. \nSHERPA has analysed various regulatory options and specific measures that support the ethical and/or \nresponsible development of AI and big data which might be useful in this context.2  \n \nWe strongly recommend the creation and/or promotion of effective redress mechanisms, particularly as \n“[AI] may make it difficult for persons having suffered harm to obtain compensation under the current EU \nproduct liability legislation.” This should be an explicit part of the regulatory framework. \n \nWe strongly agree that we must be concerned about intended and unintended negative outcomes. An EU \napproach must be based on comprehensive assessment of risks; relying on self-reported risks related to \nintended use will not capture realistic concerns and potentially significant negative outcomes.  \n \n \n \n \n \n \n \n \n2 SHERPA, Report on Regulatory Options, December 2019, https://doi.org/10.21253/DMU.11618211 \n \n2 \n \n \nThis project has received funding from the European Union’s Horizon 2020 Research and Innovation Programme Under Grant Agreement no. \n786641', 'Feedback on ‘B. Objectives and Policy options’ \n \nComments on objectives \n●  Aim (a): We recommend clarification on the definition of ‘illegal discrimination’ to ensure it is in \nline  with  the  principle  of  non-discrimination.  Additionally,  the  meaning  of  “avoid  illegal \ndiscrimination by ensuring the relevant documentation for the purposes of private and public \nenforcement of EU rules” is unclear. Discrimination is avoided by actors taking appropriate actions \nto prevent unjust or prejudicial treatment, not by ‘relevant documentation’.  \n●  Aim (c): We recommend removing ‘where possible’, as that significantly dilutes the potential \nimpact. \n●  Aim (e): We recommend further consideration of the creation of a central coordinating institution \nat the EU level. SHERPA is currently developing Terms of Reference for an EU Agency for AI, in \nconsultation with experts and stakeholders, which should inform the Commission. A central body \nwith sufficient funding and legal and technical competences will be critical to the effectiveness of \na network of national competent authorities. \n●  Aim (f): We recommend removing “ensuring a level playing field”. This is not practically possible \nand may indirectly encourage measures that undermine the ultimate objective of the regulatory \nframework to protect human rights and safety. \n \nComments on policy options \nThe SHERPA project strongly supports Option 4, which would be a combination of Option 2 (EU legislative \ninstrument setting up a voluntary labelling scheme) and Option 3 (EU legislative instrument establishing \nmandatory requirement for all or certain types of AI). The SHERPA project might support limited \nregulation for high-risk only AI (sub-option 3(b)), depending on how risk is determined, by whom, and \nwhich criteria are used. \n \nWe believe that it is absolutely critical that the EU develop a mandatory regulatory framework of ex-ante \nand ex-post enforcement mechanisms to ensure that AI systems are safe and do not violate fundamental \nrights and ethical principles.  This regulatory framework must have a “higher degree of detail and \nspecificity” in order for developers and users to understand their legal obligations, for EU and national \nauthorities to monitor compliance, and for the general public to have trust in the regulatory framework. \nThere is already a proliferation of largely ineffective “high-level principles and obligations … completed by \nindustry-led norms such as in the form of standards or codes of conduct.”  Furthermore, these initiatives \nare not always consistent and are therefore confusing for researchers, developers and users. \n \nA complementary voluntary labelling scheme would be useful, but only if robust, clearly articulated, and \nnot symbolic.  \n \nWe strongly recommend against the baseline option (no action) and Option 1 (EU ‘soft law’). While soft \nlaw initiatives may be useful in some context and could complement legislation, they do not address legal \nissues of liability and other ensuring harms, which is critically needed. Additionally, Member States will \ncontinue to develop and implement their own regulatory frameworks if no action is taken at the EU-level, \ncontributing to further fragmentation and confusion. A strong legal standard must be set at the EU-level \nthat establishes a baseline and encourages high standards of protection of fundamental rights and societal \nvalues.  \n \n \n \n3 \n \n \nThis project has received funding from the European Union’s Horizon 2020 Research and Innovation Programme Under Grant Agreement no. \n786641', 'Feedback on ‘C. Preliminary Assessment of Expected Impacts’ \nThe SHERPA project strongly recommends further assessment of expected impacts. An impact assessment \nshould also evaluate the risks associated with failure to act, and not only the impacts of a potential \nregulatory framework. The social impacts of the proposed options have not been addressed at all (e.g., \nimpacts on how society understands trust or how the options may create and/or promote a false sense \nof trustworthiness).  \nSHERPA Contributions to the AI Ethics and Human \nRights Ecosystem \nSHERPA, itself part of an ecosystem consisting of other Swafs projects (e.g., SIENNA, PANELFIT) \nand numerous research groups and projects, is working on the following activities: \n \n1. Contribution to knowledge base and curricula \nSHERPA has done extensive empirical and conceptual research3 allowing the consortium to build \nup capacity to contribute to the definition of a required body of knowledge and model curricula. \nIn addition to factual knowledge, SHERPA has also produced audiovisual material and artistic \nrepresentations of key aspects of AI and ethics to allow reaching out to a broader audience. \n \n2. Terms of Reference for regulator \nSHERPA has undertaken a review of regulatory options4 and is working on the terms of reference \nfor a regulatory body that could be at the heart of the framework of AI regulation. \n \n3. Ethics by design \nSHERPA (in collaboration with SIENNA) has developed guidelines for AI developers and AI users \nthat are based on the principles of ethics by design.5 It has provided an initial training session for \nabout 80 Project and Policy Officers of the EC (December 2019). SHERPA can contribute to the \ndesign and delivery of further relevant training. \n \n4. Standardisation \nSHERPA members are involved in ISO SC 42 and are contributing to the development of ethically-\naware standardisation. \n \nSHERPA aims to ensure that its recommendations and findings help to move the AI ecosystems \nin the desirable direction and is requesting feedback from stakeholders to help it further shape \nrecommendations and outputs. For more information, please visit: https://www.project-\nsherpa.eu \n \n3 SHERPA, SIS Workbook, https://www.project-sherpa.eu/workbook/. \n4 SHERPA, Report on Regulatory Options, December 2019, https://doi.org/10.21253/DMU.11618211. \n5 SHERPA, Guidelines for the Ethical Use of AI and Big Data Systems, https://www.project-sherpa.eu/wp-\ncontent/uploads/2019/12/use-final.pdf. \n \n4 \n \n \nThis project has received funding from the European Union’s Horizon 2020 Research and Innovation Programme Under Grant Agreement no. \n786641']"
F550978,10 September 2020,Geraldine PROUST,Wirtschaftsverband,FEDMA,sehr klein (1 bis 9 Beschäftigte),39300567160-02,Belgien,-,"FEDMA previously answered the consultation on the white paper on AI. We would like to share the below insights: 
•	The work of the High-Level Expert Group on criteria for risk assessment and the feedback that they are receiving from the industry should be considered. 
•	Self-learning systems are not new nor is information gathering for a better more objective decision process (e.g. bank loan; better information might often help to get the loan). AI is simply an optimization of both. 
•	If personal data is involved, Art 22 GDPR covers the data protection issues. 
•	It is of the utmost importance that the GDPR be implemented in a fair and balanced manner. Indeed, legitimate interest and pseudonymization are key GDPR tools for companies to use AI (e.g. cleaning a contact list with a Robinson list). When the industry relies on legitimate interest for processing of personal data, the controller must do a legitimate interest assessment test in 3 steps: (a) is the purpose of the processing legitimate? (b) is the processing necessary? (c) do the interests of the controller override the rights and freedoms of the individual? Currently, legitimate interest is a legal basis which is under growing pressure and the EDPB has started the discussions on guidelines for LI. If the right balance is not met between consent and LI, then many industry sectors, such as data marketing, will disappear or will not be able to use AI, leaving behind a less competitive market dominant by large tech intermediaries.
•	FEDMA previously referred to the EUTA High Level Principles on AI. In line with those principles, we have strong reservations about extending legal requirements to all AI applications. As the majority of AI day-to-day applications today are low risk, we urge the EU institutions to conduct a thorough assessment of the existing legislation, especially the civil liability regime, before introducing any new proposals specifically targeting AI-driven technologies and applications. Regarding the possibility of new risk assessment being imposed upon businesses to define whether they are using low or high risk AI applications, we encourage the European Commission to provide a framework for internal risks assessments for low-risk applications, and thoroughly assess who would be responsible for leading external risk assessments for high-risks applications. Indeed, it is likely that external auditors or agencies will need to gain access to sensitive personal data under GDPR to assess the fairness or degree of risk carried out by one AI system Checking the fairness or risks level in any algorithm will necessarily require the use of particularly sensitive data under GDPR, such as those related to sex, ethnicity, age, religion, etc. Soft law and voluntary labelling should be preferred to new regulations where feasible, as they create only marginal administrative burden where binding requirements could create further red tape for EU businesses. At the same time, providing a harmonised EU framework would reduce compliance costs for businesses operating across the EU and avoid a plethora of conflicting AI rules. We need to ensure that the future EU framework is sufficiently flexible so that it is not outdated in a short span of time. We disagree with the Commission’s assessment that the social impact of the future AI framework will be limited, AI will have an extensive positive social impact. 
","['FEDMA answer to the Inception Impact Assessment \non Artificial Intelligence \n \n \n \nFEDMA previously answered the consultation on the white paper on AI.  \n \nWe would like to share the below insights:  \n•  The work of the High-Level Expert Group on criteria for risk assessment and the feedback \nthat they are receiving from the industry should be considered.  \n•  Self-learning systems are not new nor is information gathering for a better more objective \ndecision process (e.g. bank loan; better information might often help to get the loan). AI is \nsimply an optimization of both.  \n•  If personal data is involved, Art 22 GDPR covers the data protection issues.  \n•  It is of the utmost importance that the GDPR be implemented in a fair and balanced \nmanner.  Indeed,  legitimate  interest  and  pseudonymization  are  key  GDPR  tools  for \ncompanies to use AI (e.g. cleaning a contact list with a Robinson list). When the industry \nrelies on legitimate interest for processing of personal data, the controller must do a \nlegitimate interest assessment test in 3 steps: (a) is the purpose of the processing legitimate? \n(b) is the processing necessary? (c) do the interests of the controller override the rights and \nfreedoms of the individual? Currently, legitimate interest is a legal basis which is under \ngrowing pressure and the EDPB has started the discussions on guidelines for LI. If the right \nbalance is not met between consent and LI, then many industry sectors, such as data \nmarketing, will disappear or will not be able to use AI, leaving behind a less competitive \nmarket dominant by large tech intermediaries. \n•  FEDMA previously referred to the EUTA High Level Principles on AI. In line with those \nprinciples, we have strong reservations about extending legal requirements to all AI \napplications. As the majority of AI day-to-day applications today are low risk, we urge the \nEU institutions to conduct a thorough assessment of the existing legislation, especially the \ncivil liability regime, before introducing any new proposals specifically targeting AI-driven \ntechnologies  and  applications.  Regarding  the  possibility  of  new  risk  assessment  being \nimposed upon businesses to define whether they are using low or high risk AI applications, \nwe  encourage  the  European  Commission  to  provide  a  framework  for  internal  risks \nassessments for low-risk applications, and thoroughly assess who would be responsible for \nleading external risk assessments for high-risks applications. Indeed, it is likely that external \nauditors or agencies will need to gain access to sensitive personal data under GDPR to assess \nthe fairness or degree of risk carried out by one AI system Checking the fairness or risks level \nin any algorithm will necessarily require the use of particularly sensitive data under GDPR, \nsuch as those related to sex, ethnicity, age, religion, etc. Soft law and voluntary labelling \nshould be preferred to new regulations where feasible, as they create only marginal \nadministrative burden where binding requirements could create further red tape for EU \nbusinesses.  At  the  same  time,  providing  a  harmonised  EU  framework  would  reduce \ncompliance costs for businesses operating across the EU and avoid a plethora of conflicting \nFEDMA AISBL – \nFederation of European Direct and Interactive Marketing \nAvenue des Arts, 43 \nBE-1040 Brussels, Belgium \nPhone: +32 2 779 4268 \n \nVAT: BE 0464 157 569', 'AI rules. We need to ensure that the future EU framework is sufficiently flexible \nso that it is not outdated in a short span of time. We disagree with the \nCommission’ s assessment that the social impact of the future AI framework \nwill be limited, AI will have an extensive positive social impact.  \nFEDMA AISBL – \nFederation of European Direct and Interactive Marketing \nAvenue des Arts, 43 \nBE-1040 Brussels, Belgium \nPhone: +32 2 779 4268 \n \nVAT: BE 0464 157 569']"
F550977,10 September 2020,Antonio Grasso,NRO (Nichtregierungsorganisation),Pharmaceutical Group of the European Union (PGEU),sehr klein (1 bis 9 Beschäftigte),00086317186-42,Belgien,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"The Pharmaceutical Group of the European Union (PGEU), the organization representing community pharmacists in 32 European countries, welcomes the European Commission’s Roadmap aimed at addressing a number of ethical and legal issues raised by Artificial Intelligence (AI). 
We support the Commission’s general objective to ensure the development and uptake of lawful and trustworthy AI across the Single Market through the creation of an ecosystem of  trust. 

As AI applications develop fast, that AI will have a major impact on society and on a wide array of sectors of the economy is no longer a question. 

In the healthcare sector, PGEU acknowledges the value of innovative technologies such AI and Big Data analytics and considers these technologies to be a useful tool to support health professionals and EU health systems. In routine pharmacy practice at national level, we recommend that these tools shall always be accompanied by the supervision of pharmacists’ expert and professional advice, to use them to improve workflow efficiency, while promoting patient safety, therapy effectiveness and offering the highest standard of pharmacy services and pharmaceutical care to patients.

While we see great value in using AI in healthcare for enabling meaningful innovation, supporting health professionals and enhancing patient care, we are aware that it may also entail significant risks, for example in relation to the use of health and patient data. 
In order to fully harness the benefits of AI in healthcare, a key requirement is to develop trust by all stakeholders involved through guaranteeing a high level of data protection. Patient data must be processed under a high level of data protection standards within trustworthy infrastructures that enable the access to secure data services. It also has to be ensured that data access and analysis are amenable to European rules for privacy and data protection. 

Therefore, we consider an EU legislative initiative on AI, as envisaged under most policy options proposed by the Commission in the Roadmap, to be an adequate way forward to address the risks linked to the development and use of certain AI applications. 

We welcome in particular an EU legislative initiative following a risk-based approach as already defined in the White Paper on AI published in February 2020. Taking this into account, we support Commission’s ‘’Option 3’’ proposal in the Roadmap, with a preference for the second sub-option, recommending a EU legislative instrument which could be limited to ‘’high-risk’’ AI applications , including those which are likely to be deployed in the healthcare sector. We would as well support the proposed ‘’Option 4’’, which envisages to implement the sub-option above while taking into account the different level of risks that could be generated by a particular AI application. 

In view of the next steps in addressing ethical and legal issues related to AI, we urge the European Commission to involve community pharmacists, as experienced users of digital health tools, in the formulation of such policies as well as in the development of guidelines and methods on the deployment of AI in healthcare. To provide a full overview of our position on AI, please find attached to this feedback the PGEU Position Paper on Big Data & AI in Healthcare. ","['Ref 19.02.20E 001 \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n  Position Paper on \n \n \nBig Data &  \n \n \nArtificial Intelligence in Healthcare  \n \n \n \nThe  Pharmaceutical  Group  of  the  European  Union  (PGEU)  is  the  association  representing \ncommunity pharmacists in 31 European countries. In Europe over 400.000 community pharmacists \nprovide services throughout a network of more than 160.000 pharmacies, to an estimated 46 \nmillion European citizens daily.  \n \nPGEU’s objective is to promote the role of pharmacists as key players in healthcare systems \nthroughout Europe and to ensure that the views of the pharmacy profession are taken into \naccount in the EU decision-making process.', '1. Introduction  \n \n   \nDgdgd \nThe  shift  towards  the  digital  economy  has  accelerated  the  pace  at  which  new  technologies  are \n \ntransformidn g the healthcare sector. Health systems in Europe are awash with data, whose range and \nvolume are growing exponentially. Increasingly generated data is opening up possibilities for the use of \n \ntechnologies such as Artificial Intelligence (AI) and blockchains which are poised to disrupt healthcare on \na global scale.  \nDdffdf \nCommunity pharmacy acknowledges the value Big Data and AI can have for European health systems \n \nand is ready to ensure that the use of new, innovative and automated technologies is always accompanied \nby expert and professional advice, such that their potential can be fully utilized to deliver more efficient, \ndfdf\nsustainable and high-quality healthcare services to the patients, supporting and complementing the face-\n \nto-face interaction between healthcare providers and patients. \n \nIn order to accomplish this, European community pharmacists are committed to: \n1.  Build on their fully computerized systems and on innovative technologies to improve workflow \nefficiency while promoting patient safety, therapy effectiveness and offering the highest standard \nof pharmacy services to its patients. \n2.  Remain a trusted source of reliable and independent health information for patients in the era \nof digitalization and of multiplication of information sources, by making the innovative digital \nsolutions integral to community pharmacy practice. \n3.  Play a pivotal role in the design, development, testing, implementation and uptake of ICT \ninnovations to ensure they are fit for practice. \n4.  Continue to provide advanced pharmacy services and promote remote monitoring and care, \nread-write access to shared electronic health records, use of electronic prescription and secure \nanalysis of big data repositories, registries and other pharmacy-held databases.  \n5.  Use their unique position at the heart of European communities and leverage the potential of Big \nData and AI to provide more personalized advice to patients and robust, evidence-based \ninformation on issues related to their therapies while promoting safe and rational medicines use.  \nThis paper is aimed to show how community pharmacists are equipped to address the challenges and \nopportunities arising from digitalization in healthcare. It also provides key policy recommendations to take \nfull benefit of the potential of Big Data and AI in healthcare and promote sustainable and resilient health \nsystems in Europe. \n \nPage 2 of 8', '2. Big Data and AI in Healthcare explained \n \n \nDgdgd \nBig Data is generated today through a plurality of sources and is defined in various ways. In healthcare, \n \nBig Data refers to large routinely or automatically collected data, which is electronically stored. This data \nd \ncan be reused and comprise links among existing databases to improve health system performance.1  \n \nThe possibility to merge and connect existing databases is envisaged by the European Commission strategy \nDdffdf \nto boost healthcare data sharing in the EU2. PGEU welcomes the Commission’s strategy: community \npharmacists want to secure patients access to health data and – subject to each patient’s consent – \n \npromote the sharing of their data across borders to enable more personalised diagnoses and medical \ntreatments. \ndfdf\nWithin the Digital Sin gle Market strategy, the European Commission has put forward a proposal for a \nEuropean  approach  to  boost  investment  and  set  ethical  guidelines  in  Artificial  Intelligence  (AI)3, \n \nencouraging uptake of these technologies by public and private sectors. The European Commission intends \nto use the potential of new technologies to improve healthcare and medical research, by setting out a plan \nto secure citizens’ healthcare while fostering European cooperation and health data sharing.  \nThe implementation of eHealth, mHealth, telemedicine is linked to the collection, analysis and the speed \nin the application of Big Data in health. 4 The remarkable amount of data in health contributed so far to \nthe  widespread  adoption  of  electronic  health  records  and  e-prescribing  systems,  with  community \npharmacists being at the forefront of these developments in several European countries. As it turns out \nfrom daily practice, more and more patients ask pharmacists to provide advice on how to interpret health \n(medicine) information they acquire from other sources, such as the media, the internet or mobile apps. \nThis involves the interpretation by pharmacists of health data generated through wearable devices and \ndigital point-of-care tests in community pharmacies, which have an enormous capability in early detection \nof undiagnosed chronic disease and potential adverse events and in monitoring of adherence and \neffectiveness of therapies. \nAlong with increasing availability of Big Data, there have been advancements in AI techniques that enable \nmachines and computers to sense and act, either on their own or to augment human activities. In \nhealthcare, as in other sectors, AI has the potential to introduce new sources of growth, changing how \n                                                           \n1 Source: https://ec.europa.eu/health/sites/health/files/ehealth/docs/bigdata_report_en.pdf  \n2 Source : http://europa.eu/rapid/press-release_IP-18-3364_en.htm  \n3 Source : http://europa.eu/rapid/press-release_IP-18-3362_en.htm  \n4 Big Data is often referred to as being characterized by four dimensions: Volume, Velocity, Variety and Veracity – the latter being a \nmix of variability and complexity - the so-called four’s V of Big Data.  \n \nPage 3 of 8', 'work is done and reinforcing the role of people to drive growth in the sector.5 AI and machine learning can \npotentially free health professionals from routine tasks and save lives through efficient early detection.  \nIn the pharmacy sector, the widest use of AI is automated dispensing technology: in Europe between 30-\n40% of community pharmacies use this technology.6 This is applied in the pharmacy as automated pack \ndispensing robots, central filling systems and automated daily dosing systems. After deployment of this \ntechnology, pharmacists would see significant benefits in terms of safe dispensing and saved working time \non dispensing which pharmacists can spend on providing patients with professional advice and services. \nThe take-up of automation in community pharmacy is set to grow, with robots becoming smaller and more \naffordable.  \nThe evolution of automated dispensing technology goes hand in hand with the growing implementation \nof clinical decision support systems in community pharmacy7. Clinical decision support systems interact \nwith electronic health record systems by receiving the patient data and medicine characteristics as input \nand by providing alerts for potentially expected adverse reactions (e.g. medication interactions, allergies) \nand medication errors (e.g. overdosing). These are increasingly linked with algorithms in the form of clinical \nrules8 and take into account more and more relevant patient data and medicine characteristics (where \nthese are available to the pharmacist) such as lab and pharmacogenetic test results9. These allow for rapid \nand comprehensive assessments of the patients’ medication safety at the point of dispensing in the \npharmacy.  \nAnother type of AI technology with great potential in healthcare is blockchain.10 At its core, blockchain is \na technology to create immutable and distributable data and transaction records which can be shared peer \nto peer between networked database systems. Data stored in blockchain cannot be changed or recognized \nuntil it reaches the recipients – that is what makes blockchain a theoretically secure technology concerning \ndata integrity.  \nManaging and securing data within healthcare and supply chain management are two clear examples of \nprincipal concepts influencing and being impacted by possible blockchain adoption. Blockchain’s records \ncan be used to provide health records for individuals, while giving all patients more control over their own \ninformation through verifiable consent.  \n                                                           \n5 Source: https://www.accenture.com/us-en/insight-artificial-intelligence-future-growth  \n6 Source : https://www.pharmacymagazine.co.uk/the-robots-are-coming...  \n7 PGEU Statement on eHealth Annex: eHealth Solutions in European Community Pharmacies  \n8 Helmons PJ, et al. J; Drug-drug interaction checking assisted by clinical decision support: a return on investment analysis;  Am Med \nInform Assoc 2015;22:764–772. doi:10.1093 \n9 https://www.knmp.nl/downloads/poster-pharmacogenomics.pdf  \n10 Source: https://www2.deloitte.com/us/en/pages/public-sector/articles/blockchain-opportunities-for-health-care.html  \n \nPage 4 of 8', '3. Big Data and AI to make European health systems \n \nmore sustainable \n   \nDgdgd \nEuropean  health systems are facing major challenges related to the sustainability and quality of healthcare \nprovision, da s a consequence of demographic change, population ageing and rising prevalence of chronic \nconditions . Public expenditure on health and long-term care has been increasing over the last decades, \naccounting for 8.5% of GDP in the EU and is expected to rise by an additional 2% to 4% of GDP by 2060.11 \nEU MembeDr Stdatefs’ fcadpacfit y to provide high quality care to all will depend on whether health systems will \nmanage to become more resilient and sustainable. To this end, the ‘’State of Health in the EU’’12 \nrecommen ded Member States to pursue the following policies: strengthening health promotion and \nprevention; investing in primary care systems and shifting healthcare out of the hospital sector towards \nmore costd-efffecdtivef primary and ambulatory care; as well as promoting integrated care.  \n \nIn this context, innovative solutions that make use of digital technologies, including eHealth, Big Data, AI \n \nare seen by the European Commission as opportunities to transform healthcare systems.13  Big data and \nimproved data analytics capabilities, as well as the use of clinical decision support systems by health \nprofessionals and use of mobile health tools for individuals to manage their own health and chronic \nconditions  are  just  some  of  the  possibilities  offered  by  digitalization  to  achieve  more  sustainable \nhealthcare14.  To promote the use of eHealth, Big Data and AI in health systems, PGEU believes that \nrecommending,  monitoring  and  advising  patients  on  their  conditions  with  mHealth  and  eHealth \nsolutions should be a reimbursed service for community pharmacists. Given the excellent accessibility \nof the community pharmacist, a healthcare professional at the heart of each community, and given the \npharmacists’  existing  services  (in  pharmacovigilance,  managing  side  effects,  adverse  reactions, \ninteractions, dose adjustments, therapeutic recommendations, providing information to patients and \npublic health promotion), the PGEU believes community pharmacists are well placed to deliver such a \nfunded service. \nCommunity pharmacists use eHealth tools and mHealth applications daily, when dispensing electronic \nprescriptions, checking for medication interactions, accessing electronic medications records or providing \nsupport for adherence via a mobile app. As such, they should be seen by European and national \n                                                           \n11 Source: https://ec.europa.eu/info/sites/info/files/file_import/ip037_vol1_en_2.pdf  \n12 Source: OECD/EU (2016), Health at a Glance: Europe 2016 – State of Health in the EU Cycle, OECD Publishing, Paris. \nhttp://dx.doi.org/10.1787/9789264265592-en    \n13 Source: Health and care has been identified by most of the digital Public-Private Partnerships in Horizon 2020 as a core business \narea where digital technologies can play a major role. The Digitising European Industry (DEI) high level group recently established a \nworking group on health. \n14 Source : https://eur-lex.europa.eu/LexUriServ/LexUriServ.do?uri=OJ:C:2017:440:0003:0009:EN:PDF  \n \nPage 5 of 8', 'policymakers as key reference in the formulation of eHealth policies and in developing policies on Big \nData & AI. \n \n4. What is next for Community Pharmacy?  \n \n \nDgdgd \nThe advent of digitalization, Big Data and AI to healthcare presents the pharmacy professions with \n \nchallenges and opportunities which can be summarized as follows:  \nd \n \n4.1. Challenges  \nDdffdf \nThe extent to which Big Data and AI will have a positive impact on improving accessibility of care, \nintegration of primary care systems, health outcomes (e.g. in terms of medicine safety and therapy \n \neffectiveness) for the patients as well as on increasing cost effectiveness of health interventions, will \ndepend on a number of factors. \ndfdf\n \nThese factors include, for instance, the usability, quality and interoperability of data collected as well as \nthe quantity of those data. There is indeed one crucial prerequisite for the use and the development of \n \nAI-driven health technologies: the availability of a large amount of informative health data. The more the \ndata, the higher is the level of intelligence which machine learning tools can produce. The more relevant \nand informative the data, the more accurate AI predictions will be. \nIn addition, it can be expected that the adoption of big data and related analytics technologies in \nhealthcare will also raise some barriers and challenges concerning the use of sensitive information \nbelonging to patients. To keep patients’ trust in health systems unchanged, it will be essential that the \ncollection of patient data and information will be done in compliance to GDPR (General Data Protection \nRegulation). As the closest and most accessible point of access to care in Europe, community pharmacists \nare key to bridge patients and health systems and ensure patients are well informed on how their \nhealthcare data is used to improve the safety and quality of their treatment.  \n4.2. Opportunities \nAs the quality of pharmacy healthcare services continues to grow, there are three main ways by which \npharmacy can leverage the Big Data and AI disruption in healthcare to shape healthcare outcomes: \n \nPage 6 of 8', '1.  Being  the  most  accessible  and  affordable  point  of  access  in  health  systems,  community \npharmacists can use AI and new digital technologies to dedicate more working time to the \nprovision of healthcare services and to direct patient care. \n2.  Big Data and AI in pharmacy, if adopted within interoperable information systems, can use patient \ndata and clinical history to support the pharmacists in providing more personalized healthcare \nservices and expert advice.  \n3.  The potential of eHealth and mHealth tools can be used to provide real-time capture of data \nwhich can enable community pharmacists to follow up with at-risk patients on their conditions \nand to monitor their progress during therapy. \nIn addition, pharmacists already have an early form of AI in place: it is the pharmacy software which \nprovides housing for data concerning medication history of the patient, patient use of medication, clinical \nrules (clinical decision support) and adherence data, gathered in compliance with GDPR. The next \ngeneration of pharmacy software using AI to implement a technology-based information expert system to \nidentify timely adverse drug-reaction or medicines interaction problems based on patient data captured \nfrom the pharmacy system and other external data systems.  \nIn this way, pharmacists would need to spend less working time on identifying serious drug-related \nproblems. These time savings coupled with potential automation of dispensing process could free a \nsignificant amount of time for the pharmacists to provide a broader range of patient-centered healthcare \nservices.  \n \n  5. Conclusion \n \n \nDgdgd \nPharmacists can contribute to the gathering of large amount of data in healthcare. They have been using \n \nelectronic health records for almost 20 years to provide better patient care and monitor the patients’ \nd \nconditions. Informatic tools in pharmacies have been crucial as an information source for medicines and \n \nmedical devices, use of e-prescription systems, repeat prescriptions systems, invoicing, follow-up services \nfor patients, traceability and pharmacy services.  \nDdffdf \nThis variety of sources can generate Big Data which can be further analyzed, provided that they are \n \ncompliant with the provisions of the GDPR. Big data analytics and AI may be useful tools to provide patients \nwith  better  guidance  on  how  to  use  their  medication;  to  optimize  value  of  data  from  m-health \ndfdf\n \n \n  Page 7 of 8', 'technologies; to promote prevention and better everyday lifestyle guidance; to support patient monitoring \nand adherence as well as to obtain better health outcomes.  \nCommunity pharmacists have the infrastructure, culture and expertise to make use of the potential of \nBig Data and AI in healthcare. These technologies can increase the efficiency of processes within the \npharmacy  which  in  turn  can  facilitate  added  value  service  implementation.  Therefore,  community \npharmacists  acknowledge  the  benefits  of  appropriately  integrated  digital solutions  to  complement \npractice. \nTo fully take advantage of the potential of Big Data & AI in healthcare and promote sustainable and \nresilient health systems in Europe, this paper includes the following five recommendations:  \n1.  Involve community pharmacists as experienced users of digital health tools in the formulation of \ndigital policies at local, national and European level as well as in the development of guidelines \nand methods on the sharing of Big data and deployment of AI in healthcare.  \n2.  Reward  with  reimbursement  community  pharmacy  services  involving  recommending, \nmonitoring and advising patients via mHealth and eHealth tools.   \n3.  Facilitate the production of Big Data in healthcare, via linking electronic health records with e-\nPrescribing systems, allowing health professionals involved in patient care to access the necessary \npatient’s information, subject to the patient’s consent. Promote interoperability of information \nsystems in Europe to foster exchange of data across community pharmacies. \n4.  Enable community pharmacists to update electronic health records, if needed, to identify and \naddress potential medication and patient safety-related issues. \n5.  Harness the potential of AI in healthcare and use it to promote more collaboration across many \ndifferent health professionals serving the same patients as well as to promote integration of \nprimary care systems. \n \nEND. \n \n \n \nPage 8 of 8']"
-,-,-,-,-,-,-,-,-
F550975,10 September 2020,Jochen MISTIAEN,Wirtschaftsverband,DIGITALEUROPE,klein (10 bis 49 Beschäftigte),64270747023-20,Belgien,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"DIGITALEUROPE thanks the Commission for this opportunity to provide feedback on the AI Inception Impact Assessment. In our attached contribution, we analyse the various Options proposed by the Commission and give our comments and recommendations.","['', 'Our position', '', '', '', '', '', '']"
F550974,10 September 2020,Erik O DONOVAN,Wirtschaftsverband,Ibec,mittel (50 bis 249 Beschäftigte),479468313744-50,Irland,Künstliche Intelligenz – ethische und rechtliche Anforderungen,Many thanks for the opportunity to comment. Please attached document.,"['Implementing an \nopen European \ndigital future\nIbec response to the \nEuropean Commission White \nPaper on Artificial Intelligence', '', 'Contents\nKey messages ...........................................................................................................2\nKey recommendations on White Paper on Artificial Intelligence .............2\nIntroduction ...............................................................................................................4\nViews on EU White Paper on Artificial Intelligence .............................................8\nGeneral comments on proposed approach ..........................................8\nAim of the proposed approach ......................................................8\nPlaying to strengths and realising opportunities ..............................8\nSpecific comments on proposed approach ..........................................9\nEcosystem of excellence – promoting AI development and uptake ....9\nEcosystem of trust – proposed regulatory framework ....................13\nReferences and notes............................................................................................18\nAbout Ibec  ..............................................................................................................20\nIbec response to the European Commission White Paper on Artificial Intelligence 1', 'Key messages\nThe shape of Europe’s digital future matters. Ibec envisage a more \ncompetitive, smarter low carbon economy, with a sustainable enterprise \nbase that provides quality jobs and enables a high quality of life. We \nenvisage an outward looking, dynamic and successful EU, that provides \nthe conditions for organisations and individuals to adapt to technological \nchange and reach their full potential. Under the right conditions, Artificial \nIntelligence (AI) are a suite of transformative technologies or systems that \ncan enable that vision. Developing a strong European data economy and \nensuring excellence and trust are crucial to that success.\nKey recommendations on White Paper  \non Artificial Intelligence\n• The outcome of the EU approach must speed up not slow down Europe’s capacities in data \ninnovation and data application.\n• The shared ambition of the EU and its Member States must be to lead on further digital \ndevelopment, including AI development and adoption. Europe’s digital frontrunners should seek \nto bring along other Member States, share best practice and build an inclusive agenda that \nensures a rising [digital] tide raises all boats. Europe must enable and champion further digital \nand data innovation, enterprise and trade.\n• Play to strengths and realise opportunities. Further develop a coherent European ecosystem \nthat enables our public sector, our private sector and individuals to invest, adopt, innovate and \nsucceed with AI, bringing benefits for all.\n• Work with Member States. Revise the EU’s Co-ordinated Plan on AI to enhance commitment \nand coherence in approach. Deliver the investment promised in the next long-term EU budget to \nsupport the desired ecosystem of excellence in AI.\n• Focus efforts in research and innovation. Increase investment and intensify collaboration \nbetween government, regulators, enterprise and the research community in developing Europe’s \nAI research and innovation ecosystem.\n• Enable everyone to accomplish more through the right knowledge, training and skills. We \nsupport investment, education and inclusion in helping organisations1 and individuals prepare \nfor AI adoption.\n• Enable smarter entrepreneurship. The implementation of AI-based solutions across all sectors \nis where the real economic benefits will be found.\n2 Implementing an open European digital future', '• The proposed partnership with the private sector should be open and have clear structures \nfor leadership and co-ordination; a clear vision of purpose and objectives; and the necessary \nfunding for delivery.\n• The public sector should act as a catalyst for enabling further cross-border and cross-sectoral \ndigital opportunities and raising Europe’s collective digital performance.\n• Invest in digital and data infrastructure.\n• Shared global standards, including ISO, in emerging technologies will further enable trust, fair \ncompetition and avoid market distortions.\n• Take a principled and proportionate approach to AI governance and laws that apply to AI that \nis based on sound, clear principles, evidence and risk. Harness existing regulatory frameworks \nwhere possible.\nIbec response to the European Commission White Paper on Artificial Intelligence 3', 'Introduction\nThe shape of Europe’s digital future matters. Ibec2, \nIreland’s business group, welcome the publication \nof the European Commission’s Communication, \n‘Shaping Europe’s digital future’, as bringing a \nnecessary focus to the importance of: digital \nleadership; enabling further development of our \ndigital capacities; and championing further digital \nand data innovation, enterprise and trade. Ibec \nand its members have outlined nine policy \nrecommendations across these three priority \nareas to EU policy makers and influencers. We \nenvisage a Europe that provides the ambition \nand tools to enable its Member States, \nbusinesses, innovators and citizens to lead and succeed in \nthe local and global opportunities offered by further digital transformation, \nenabling further innovation, quality jobs, better services and enhanced well-\nbeing in period 2020-20243.\nExcellence and trust in artificial intelligence (AI) matters\nOur vision of a future Ireland is for a more competitive, smarter low carbon economy, with a sustainable \nenterprise base that provides quality jobs and enables a high quality of life for its citizens. An inclusive \nIreland at the heart of an outward looking, dynamic and successful EU, that provides the conditions for \norganisations and individuals to adapt to technological change and reach their full potential. Under the right \nconditions, Artificial Intelligence (AI) are a suite of transformative technologies or systems that can enable \nthat vision. Ibec and its members have outlined policy recommendations to policy makers and influencers \non the future European approach to AI4 and our national AI strategy5. These include the delivery of physical \nand data infrastructure and enabling governments, public and private organisations and individuals to \nfurther collaborate, innovate and succeed with AI. We envisage national AI strategies evolving, aligned with \nEU initiatives and globally relevant standards for interoperable and trustworthy AI, that enable governments, \norganisations and individuals across the EU to: embrace innovation and technological change; address \npolicy issues of strategic importance; deliver quality jobs and enhance well-being in period 2020-2024.\n4 Implementing an open European digital future', 'A European data economy matters\nFor Europe to ‘become a global leader in innovation in the data economy and its applications’ it will need \naccess to quality data in quantities that may exceed those held by individual businesses or public bodies. \nThe EU needs an overarching, market-friendly data strategy to enable further digital transformation of its \neconomy and society. \nIn this context, Ibec and its members welcome the opportunity to contribute to the Commission’s public \nconsultations on further shaping Europe’s digital future, specifically in the areas of AI6 and data7. This paper \noutlines Ibec views on the Commission’s White Paper on AI. Ibec has also developed a separate response to \nthe proposed European strategy for data.\nIbec response to the European Commission White Paper on Artificial Intelligence 5', '6 Implementing an open European digital future', 'Ibec response to the European Commission White Paper on Artificial Intelligence 7', 'Views on EU White Paper  \non Artificial Intelligence\nGeneral comments on proposed approach\nAim of the proposed approach\nThe Commission’s stated aim is for Europe to ‘become a global leader in innovation in the data economy \nand its applications’ benefitting citizens, business and the public interest.\nDigital leadership is critical to Europe’s future economic success and well-being. Despite progress, the \nEuropean Union and Ireland face intense and growing global competition in the pace and level of our \ndigital transformation. The ability to capture the opportunities from AI may vary between EU Member \nStates, sectors, large and small firms. Embracing further technological change, such as AI, presents both \nopportunities and challenges across several economic and social domains. Concerns have been raised in \nrelation to the disruption of certain sectors, jobs displacement, safety and the protection of rights. In this \ncontext, public policy has an important role in ensuring the potential benefits of AI and digital automation \ncan be realised and potential risks mitigated through ambitious planning and implementation.\nEU policy makers can play a key leadership role in preparing Member States, organisations and individuals \nfor further technological change.  In this context, the shared ambition of the EU and its Member States \nmust be to lead on further digital development, including AI development and adoption. Europe’s digital \nfrontrunners should seek to bring along other Member States, share best practice and build an inclusive \nagenda that ensures a rising digital tide raises all boats. Europe must enable and champion further digital \nand data innovation, enterprise and trade.\nPlaying to strengths and realising opportunities\nIbec believes that the EU and Ireland are well placed to realise the significant opportunities that AI offers \nour economy and well-being . However, this will only happen with further commitment by the European \ninstitutions, Member States, business, innovators and citizens to engage, collaborate and build on Europe’s \ncomparative strengths , not just in regulation. We need to further develop a coherent European ecosystem \nthat enables our public sector, our private sector and individuals to invest, adopt, innovate and succeed with \nAI, bringing benefits for all.\n8 Implementing an open European digital future', 'Specific comments on proposed approach\nEcosystem of excellence - promoting AI development and uptake\nThe six actions proposed in Section 4 of the White Paper10 are very important and welcome. They can \ncontribute to the development of the supply and demand capacities necessary to enable further AI adoption \nand innovation across Member States, public services, researchers, businesses of all sizes.\nWorking with Member States\nA revision of the EU’s Co-ordinated Plan on AI11 will be necessary to take onboard the outcomes of this \npublic consultation and the AI-HLEG12 pilot of its assessment list for trustworthy AI13.\nRecommendations\nWorking together should:\n• Enhance commitment and coherence between the Member State approaches. It is important that not \nonly are all national AI strategies completed, but that they act in concert with the revised Co-ordinated \nPlan. At the end of February, there were 15 national AI strategies, with a further 7 at draft stage and 5 \nin progress across the EU Member States14. \n• Intensify the pace and level of Europe’s collective digital performance by demonstrating success \nstories, avoiding duplication and inefficiencies, building trust in digital transformation and deepening \ncollaboration between front runners and weaker performers in building a shared and inclusive agenda.\n• Deliver the investment promised in the next long-term EU budget to support the desired ecosystem of \nexcellence in AI15.\nFocussing efforts in research and innovation\nIbec supports increased investment and intensified collaboration between government, regulators, \nenterprise and the research community in developing Europe’s AI research and innovation ecosystem.  \nThe proposed actions (4A, 4C and 4E16) in the White Paper are all very important and welcome.\nIbec response to the European Commission White Paper on Artificial Intelligence 9', 'Views on EU White Paper on Artificial Intelligence / continued\nRecommendations\nGiven the stated urgency of the White Paper and the need to build further momentum:\n• Prioritise the PPP for industrial research and the co-ordination of existing AI research excellence \ncentres in the short-term with clear structures for leadership and co-ordination; a clear vision of \npurpose and objectives; and the necessary funding for delivery. The proposal to create a lighthouse \ncentre is an excellent initiative that could help, although it is vital to strike the right balance in the \ndegree of centralisation. A single institute spread across several locations in a number of Member \nStates could be a good model. If adopting a hub and spoke model, it will be important to ensure that \nresourcing is allocated in a sensible manner.\n• Invest further in applied data research, supporting our innovators and enterprise through development \nand real-world testing of new digital applications. Europe is successful in research output in AI. \nHowever, there is more to do in linking this to delivery of business needs and in co-ordinating efforts \nbetween Member States. Ensure further alignment between EU innovation and industrial policy. Use \nregulatory sandboxes to test and scale up research as appropriate. Enable shared learning between \ninnovators, enterprise and regulators. This should make it easier to launch and scale further digital \ninnovation across Europe.\nSkills\nThe planned reinforcement of the Commission’s skills agenda and the planned support of a network of \neducation institutions offering world-leading masters programmes in AI are welcome. The eventual EU \napproach should enable everyone to accomplish more through the right knowledge, training and skills. \nWe support investment and inclusion in helping organisations17 and individuals prepare for AI adoption. In \nparallel, there is a similar educational challenge to equip those in the wider ecosystem who will play crucial \nsupporting roles in guiding the uptake of AI. Information will need to be delivered at citizen and enterprise \nlevel so that there is an understanding of the potential benefits and risks inherent in the ecosystem.\nRecommendations\nFor consideration and in working with Member States:\n• Co-ordinate initiatives across the education system from primary to postgraduate, apprenticeships \nand alternative pathways to gain the relevant STEAM and transversal skills to embrace AI and digital \nautomation.\n• Encourage cross-faculty collaboration between computer science and other enterprise-focussed \nacademic fields to develop the indigenous pipeline of enabled AI talent.\n• Develop the indigenous pipeline of AI practitioners. Explore apprenticeship opportunities across the \neconomy where graduates with AI or relevant knowledge are encouraged to employ that knowledge to \nreal world challenges in the public and private sector across the EU.\n• Embed an inclusive and lifelong approach to the development of skills for a digital age. Enable \ngovernments, public and private organisations, educators, workers and jobseekers to keep pace with \ntechnical change through upskilling and reskilling as necessary.\n• Work with employers to manage digital transformation in the workplace. Employers will need to \nfacilitate new ways of working. Public policy on childcare, lifelong learning, technology fulfilment of \nroles, retirement, pensions and taxation must keep people engaged in the labour market in a way that \ndoesn’t discriminate or disincentivise work.\n10 Implementing an open European digital future', '• Deploy appropriate supports to people whose current jobs may be transformed or eliminated by \ntechnological change, with a focus on training, career guidance and social safeguards.\n• Promote the EU as a location for mobile AI talent.\n• General awareness and understanding of AI should be promoted too.  While digital skills will be \nimportant to our workforce, this aspect is important for all citizens and inclusion in an AI-empowered \neconomy and society. \n• Provide appropriate education and supports for those regulating and enforcing laws regarding the \nuse of AI across various sectors, so that the legal infrastructures and citizen rights are robust and \nproportionate. \n• Provide appropriate education regarding the regulatory environment for innovators and educators \nregarding the use of AI across various sectors, to foster trust and understanding, and avoid \nunnecessary setbacks through a lack of understanding of the relevant laws or failure to comply with \nsuch laws.\nFocus on SMEs\nThe proposed actions (4D18) for SMEs and start-ups are very important and welcome. The EU approach \nmust enable smarter entrepreneurship. The implementation of AI-based solutions across all sectors is \nwhere the real economic benefits will be found.\nRecommendations\n• Focus on enhancing comparative strengths.\n• Encourage enterprise in developing, adopting and deploying AI applications. Different sectors, \norganisations and individuals may be at different stages in their understanding and adoption of AI. \nSome may lack awareness of the opportunities from AI, while others may require support in the \ndevelopment and deployment of AI applications. A big challenge for smaller firms will be in assessing \nwhat technologies to adopt and how to deploy them. We must resource the response accordingly.\n• Leverage the EU Digital Innovation Hub (DIH) network further. Develop a coherent network that is \nenterprise led. DIHs should support further awareness, development, collaboration and deployment of \ndigital applications across the European economy for small and big firms alike. \nPartnership with the private sector\nThe proposed actions (4E19) for private sector collaboration are very important and welcome. Without \nenterprise-driven AI, the EU will not be able to add value to ongoing research in AI and lose out to \ncompetitors.\nRecommendations:\n• The PPP should have clear structures for leadership and co-ordination; a clear vision of purpose and \nobjectives; and the necessary funding for delivery.\n• Access to participate in the PPP should be open and non-discriminatory.\n• Information should flow from the PPP to enrich and facilitate agile development.\nIbec response to the European Commission White Paper on Artificial Intelligence 11', 'Views on EU White Paper on Artificial Intelligence / continued\nPromoting AI adoption by the public sector\nThe proposed actions (4F20) for private sector collaboration are very important and welcome. AI offers \nan opportunity to enhance public services. There is also an opportunity for the public sector to act as \na catalyst for enabling further cross-border and cross-sectoral digital opportunities and raising Europe’s \ncollective digital performance.\nRecommendations\n• The EU and its Member States should aim to lead on online Government services and the \ndigitalisation of public service delivery.\n• Intensify the development of GovTech ecosystems in Member States and across the EU.\n• Further develop institutional capacities and public-private partnerships (PPPs) to realise identified \nopportunities for online Government services and digitalised public service delivery for organisations \nand citizens. Deliver the Tallinn Declaration on e-Government.\n• Intensify work on open data. Define and extend the range of high value datasets that the public sector \nwill make available in promoting further digital innovation. Enhance transparency, promote further \nbusiness creation and innovative public services.\n• There are opportunities for governments themselves to benefit from the use of AI in their operations. \nAI has the potential to help public sector agencies respond faster and with greater nuance to citizen \nqueries. In addition, by showcasing how AI can be practically and sensitively applied, government \ncould help to lead the way as role models.\nDigital and data infrastructure\nAccess to computing and data infrastructure will underpin efforts to develop Europe’s capacities in AI21. \nBeyond the necessary digital infrastructure, the quantity, quality and type of data available are important \nto the successful development and deployment of AI. The development of a European data space requires \ncommitment by Member States.\n \nInternational aspect\nFurther EU engagement with its international partners can deepen our mutual understanding of \nthe opportunities, risks and may overcome shared technical and policy challenges in further digital \ntransformation. Shared global standards in emerging technologies, including ISO, will further enable trust, \nfair competition and avoid market distortions.\nRecommendations\n• Develop industry-led standards ‘bottom-up’, using the following principles: inclusiveness, consensus, \ntransparency, effectiveness, technology, neutrality and impartiality. This will ensure we can encourage \nand benefit from trustworthy AI outside the EU but also that AI developed within the EU can move \nacross borders easily.\n• Facilitate cross-border data flows and prevent forced data localisation measures. Ibec like several \nEuropean business groups and likeminded Member States, support EU legislation to remove \nunjustified restrictions to the free flow of data. We don’t agree that AI trained on non-European data \nwould throw up different results to that trained on EU data in every context.\n12 Implementing an open European digital future', '• Intensify the use of free trade agreements (FTAs) and mutual adequacy decisions as a vehicle \nto promote further (bilateral) digital trade and cross-border data flows; and to address digital \nprotectionism without prejudice to EU data protection rules.\n \nEcosystem of trust – proposed regulatory framework\nRegulatory concerns\nEmbracing further technological change, such as AI, presents both opportunities and challenges across \nseveral economic and social domains. Concerns are raised in the White Paper in relation to the protection \nof rights, explainability, safety and liability. In this context, public policy has an important role in ensuring \nthe potential benefits of AI and digital automation can be realised and potential risks mitigated through \nambitious planning and implementation.\nRegulatory approach\nIbec support a principled and proportionate approach to AI governance that is based on evidence and risk. \nWe broadly welcome the regulatory approach proposed in the Commission’s White Paper. There are already \nmany regulations and legal codes that are technology neutral in nature, and thus already apply to AI, but \nit is worth evaluating if there are gaps in the context of specific demonstrable risk. Any gaps identified \nshould be addressed via practical, principles-based rules which build on existing legislation, and address \ndemonstrable high risk, to avoid creating overly complex or conflicting legal obligations. Consultation with \nthe various interested groups prior to introducing new laws is key to success in this area.\nRecommendations\n• Take a human-centred approach22 to the governance and regulation of AI development and adoption. \nSpecifically:\n–  Help Member States, organisations and individuals to contextualise and operationalise EU23 and \nOECD principles24 in the development and deployment of ‘trustworthy AI’25 powered solutions \nacross several domains.\n–  Protect values (including human dignity and fundamental rights, freedom, democracy, equality \nand justice) and enable business to add value – innovation and enterprise are human \nendeavours that can enable higher standards of living across the EU.\n–  The opportunity cost of not using AI is not sufficiently reflected in policy debates. When \nconsidering the risks of AI, it is vital to acknowledge there are also flaws in existing (non-AI) \napproaches. We should compare the risks of using new AI systems against existing approaches. \nIf an imperfect AI system were shown to perform more accurately than the status quo at a crucial \nlife-saving task, for example, it may be irresponsible to not use the AI system, notwithstanding its \ninherent risk.\nIbec response to the European Commission White Paper on Artificial Intelligence 13', '14 Implementing an open European digital future', 'Ibec response to the European Commission White Paper on Artificial Intelligence 15', 'Views on EU White Paper on Artificial Intelligence / continued\n• Harness existing regulatory frameworks in AI governance. Specifically:\n–  Assess the need for new regulation against existing regulation. Many of the concerns raised in \nthe White Paper are addressed by applicable EU legislation, for example GDPR, however current \nlegislation may have some gaps. Given the range of AI systems and sectoral applications, \nexisting EU rules should be reviewed against AI-related concerns and requirements. \n–  Where regulatory or governance gaps are identified, we favour a response that is evidence-based, \ntechnology-neutral, proportionate and outcome-based. An outcome-based approach is favoured \nas the rate of technological and behavioural change can often outpace policy and regulatory \nprocesses. This can risk undue regulatory burden or obsolescence. Secondly, given the \nurgency expressed in the White Paper the key focus must be outcomes that speed up Europe’s \nengagement with AI.\n• Take a risk-based approach to AI governance. The heterogenous nature of AI and its applications \nmeans a one-size-fits-all legislative approach would be problematic or risk stifling the desired \nopportunities. Policy makers and stakeholders should work together to define any further appropriate \nsafeguards needed for sensitive use cases whilst continuing to encourage innovation. Specifically:\n–  The proposed introduction of new compulsory requirements limited to ‘high-risk’ applications is \nuseful as it aims to ensure a targeted and proportionate approach.\n–  The two cumulative criteria26 proposed in determining high-risk are helpful but more nuance and \nproportionality should be added to the risk assessment criteria to make it easier for companies \nto understand when their technology may fall into this category.\n–  The Commission should better reflect well-established interpretations of risk, reflect wider \noperational context when assessing risk, and factor in the opportunity cost of not using AI in the \ndefinitions/criteria.\n• Ensure the criteria of high-risk at a principles level are evidence-based, coherent, clear and \nfuture-proof.\n• Ensure any advisory committee involved in identifying risk should involve industry and \nsectoral participation.\n• Ensure consistency in approach for different applications in different sectors or \nintersectoral applications. Avoid risk of different and overlapping rules.\n• Ensure the legal definition of AI is clear and reflects both EU and OECD thinking27. Such an \napproach will allow Europe to lead in facilitating further digital trade and internationalise \ntrustworthy AI. Care should also be taken to ensure that the definition is focused on actual \nAI and is not conflated with any software. The “exceptional instances” clause is too open-\nended and should be removed as it creates legal uncertainty.\n–  Ensure secure, trustworthy AI through a combination of ex-ante28 and ex-post29 mechanisms. For \nthe vast majority of applications conformity assessment may be best done on a self-certified \nex-ante basis with ex-post third party market surveillance to ensure compliance. Checking \nevery system by a third party before it comes to market may be impossible regarding regulators \nresources and would hamper innovation. For some higher risk AI applications, third party ex-ante \nconformity assessment may be required.\n• Ensure extensive and meaningful engagement between actors across the AI value chain \nand relevant competent authorities to develop the assessment (including self-assessment), \ncompliance and enforcement mechanisms to achieve shared outcome of trustworthy AI.\n• Consider SMEs and start-ups requirements in the development of the mechanisms as well \nas support in implementation of the mechanisms.\n16 Implementing an open European digital future', '• Consider the lessons from the AI HLEG pilot outcome in the development and \nimplementation of the mechanisms.\n• The assessment regime must be pragmatic, well-resourced and transparent to ensure \nit is not overly burdensome for application providers, and also practical for designated \nassessment bodies to deliver, taking into account the level of expertise (sectoral and AI \nspecific) and resourcing required to implement in a timely fashion.\n• Ensure an international outlook in the development and implementation of mechanisms. \nConsider relevant international standards and developments at OECD level. Avoid \nfragmentation of the international AI market to the detriment of EU business and citizens.\n• Safety and liability legal frameworks should continue to ensure that all products and services, \nincluding those integrating emerging technologies, operate safely, reliably and consistently and that \ndamage having occurred is remedied efficiently. Specifically:\n–  Ensure an environment that adequately protects rights, values and assets30, while providing an \neffective framework to invest further in innovative digital products and services.\n–  Maintain legal certainty for business on liability around emerging technologies. The current \nframework, including the limitations to intermediary liability and Product Liability Directive \n(PLD), offer some certainty for digital supply chains and the delivery of AI applications in the \nB2C context. Transparency and accountability should be promoted in the B2B context, while \nrespecting contractual freedom. The ‘Digital Service Act’ discussions should seek to retain the \nprinciple of limited liability and develop a robust evidence base before proposing any change.\n–  Globally, strict liability frameworks are reserved for abnormally hazardous situations as they \nremove any consideration of intent or negligence. Expanding the scope of the PLD to software \nand all AI applications would mean that anyone involved in making an AI system could be held \nliable for problems for which they had no control.\n–  Carrying out a new risk assessment should only be required when there has been a significant \nchange to the functionality of the product which is likely to materially alter its outcome in testing \nor the safety disclosures made.\n–  The existing liability framework is solid and technology neutral, making it flexible enough to \ncover the challenges arising with emerging technologies. Changing such a foundational legal \nand societal framework should be done thoughtfully and only in response to significant and \ndemonstrable shortcomings with the current legislative framework. A strict liability regime for \nsoftware would stall innovation in Europe, stifling economic growth.\n• Leverage the existing legal frameworks. Take a risk-based, evidence-based and outcome \ndriven approach to any proposed interventions to the existing frameworks. Focus on specific \ntypes or levels of risk as well as specific applications or systems as appropriate\n• Security must be to the fore. We need to ensure AI and the networks and data it uses are \nas secure as possible.  Standards work needed here.  Consider cyber security by design.\n• Policy makers and stakeholders should work together to define any further appropriate \nsafeguards needed for sensitive sectoral use cases whilst continuing to encourage \ninnovation.\n• A voluntary labelling system could risk placing a significant burden on SMEs to comply. This could \nfavour large players who can afford to meet the requirements whilst delivering minimal benefit to \nconsumers. There should be broad agreement on standards to ensure such a scheme would be \nfeasible or helpful. Given the pace of change, any scheme would have to be very flexible to work as \nintended. Existing self-regulatory approaches should also be taken into account.\nIbec response to the European Commission White Paper on Artificial Intelligence 17', 'References and notes\n1.  For example: Government, public sector, private sector (of all sizes), sectoral regulators and educators.\n2.  www.ibec.ie/digitalpolicy\n3.  Ibec (2019a) Europe’s digital future – open for business, https://www.ibec.ie/connect-and-learn/\ninsights/insights/2020/02/07/europes-digital-future-open-for-business\n4.  Ibec (2019a) Ibid.\n5.  Ibec (2019b) Smarter technology for a better future, https://www.ibec.ie/connect-and-learn/insights/\ninsights/2020/02/20/smarter-technology-for-a-better-future\n6.  European Commission (2020a) White Paper on Artificial Intelligence – A European approach to \nexcellence and trust, COM (2020) 65 final\n7.  European Commission (2020b) Communication - A European strategy for data, COM (2020) 66 final\n8.  Ibec (2019a and b) Ibid.\n9.  For example, ICT, healthcare biopharmaceuticals and medical technology, agri-foods, financial, business \nservices, utilities and potential GovTech offerings.\n10.  Promoting AI development and adoption by working with Member States; focusing research and \ninnovation; developing skills; focusing on SMEs; PPPs and promoting public sector adoption.\n11.  https://ec.europa.eu/digital-single-market/en/news/coordinated-plan-artificial-intelligence\n12.  https://ec.europa.eu/digital-single-market/en/high-level-expert-group-artificial-intelligence\n13.  https://ec.europa.eu/futurium/en/ethics-guidelines-trustworthy-ai/register-piloting-process-0\n14.  European Commission (2020c) Presentation 26 February, ‘White Paper on AI – A European approach \nto excellence and trust’.\n15.  The European Multiannual Financial Framework (MFF), for the 2021-2027 period\n16.  Establishment of an AI ‘lighthouse’ research centre; network of existing AI research excellence \ncentres; PPP for industrial research.\n17.  For example: Government, public sector, private sector (of all sizes), sectoral regulators and educators.\n18.  Raising SME awareness of AI benefits; SME access to testing/reference centres; knowledge transfer \nto SMEs; collaboration between SMEs larger firms and academia on AI; and access to equity financing \nfor start-ups.\n19.  New PPP in AI, data and robotics to co-ordinate research and innovation; collaborate with other \nrelevant PPPs; and work with testing facilities and DIHs.\n20.  Sector dialogues on healthcare, rural administration and public services.\n21.  Proposed actions are in the parallel Commission data strategy. Ibec has developed a separate \nresponse to this.\n22.  Approach outlined by the European Commission’s High-Level Expert Group on AI (‘AI HLEG’) and \nOECD that encourages beneficial outcomes from AI for both humans and the planet that sustains \nthem. This approach encourages a respect for law, human rights and democratic values as well as a \nconsideration for the natural environment and sustainability.\n18 Implementing an open European digital future', '23.  European Commission High Level Expert Group on AI or ‘AI HLEG’ (April, 2019) Guidelines for trustworthy AI \n(https://ec.europa.eu/digital-single-market/en/news/ethics-guidelines-trustworthy-ai). Update to AI HLEG \nassessment list expected in June 2020.\n24.  OECD Council Recommendation (May, 2019), OECD Principles on AI (https://www.oecd.org/going-digital/ai/\nprinciples/)\n25.  Trustworthy AI is lawful, ethical and robust throughout its lifecycle. It requires transparency and accountability by \norganisations who deploy or use AI.\n26.  Based upon sectors and intended applications deemed to pose a high-risk to rights or safety.\n27.  European Commission High Level Expert Group on AI or ‘AI HLEG’ (2019) and OECD Council Recommendation \n(2019), OECD Principles on AI. It should be noted that the European Commission participated in the development \nof the OECD principles.\n28.  Prior to putting the system on the market.\n29.  After putting the system on the market.\n30.  This includes tangible and intangible assets.\nIbec response to the European Commission White Paper on Artificial Intelligence 19', 'About Ibec\nIbec is Ireland’s largest lobby group, representing Irish business both domestically and internationally. Our \nmembers span all sectors of the economy, collectively employing over 70% of the private sector workforce. \nOur policy work seeks to improve business conditions and thereby promote sustainable economic growth.\nwww.ibec.ie\nEU Transparency Register ID No. 479468313744-50\nContact us\nErik O’Donovan\nIbec Head of Digital Economy Policy\nemail: erik.odonovan@ibec.ie\nPat Ivory\nIbec Director of EU and International Affairs\nemail: pat.ivory@ibec.ie\n20 Implementing an open European digital future', '', 'ibec.ie']"
F550973,10 September 2020,Gianclaudio Malgieri,Universität/Forschungseinrichtung,Augmented Law Institute - EDHEC Business School,klein (10 bis 49 Beschäftigte),-,Frankreich,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"This is a common statement of the Augmented Law Institute of EDHEC Business School (for the list of Professors composing this group, please click on the link: https://www.edhec.edu/en/legaledhec-research-centre#dexp_tab_item_2063327793).

An extended version is attached. In this summary here we report just our main conclusions.

Our feedback is based on 3 parts: 1) a definitional part, which aims to reply to the question “what to regulate?”; 2) a methodological part, which aims to reply to the question “why regulate?” [Please see the attachment] ; and 3) a content-based part, which aims to reply to the question “how to regulate?”.

A first problem that we observe in the Impact Assessment and in the related White Paper is the lack of clear definitions. In particular, two terms urge to be clearly defined before to proceed:
1.	artificial intelligence;
2.	risk (to fundamental rights and freedoms, including human health and safety).

""AI"" was defined both in the EC Communication (2018)237 and then re-defined by the HLEG-AI in April 2019. Both the White paper and the Inception Impact Assessment are not clear about which definition of AI should be preferred. However, the implications can be relevant. In particular, since the definition of the Commission’s Communication does not define what “intelligent behaviour” is, there might be problems in understanding the scope of AI notion and whether some technological systems (e.g., Covid-19 Alert Exposure Apps, automatic entrance systems, etc.) are within or outside the scope of “intelligent behaviour”. Accordingly, we propose to prefer the HLEG definition, accompanied by an Annex with a non-exhaustive list of relevant examples of AI systems actually used.

The notion of risk is also very problematic. Section B of the Inception Impact Assessment is largely based on the notion of risk (e.g., options 3 and 4 of the “Alternative options to the baseline scenarios”). However, it is not clear what we should mean by “risk” and how the risk should be assessed for each AI system. We propose to clearly define what “risk” means and which level of severity and likelihood might amount to qualifying as “high risk”. In addition, we propose to use the White Paper’s definition of the severity threshold on “legal or similarly significant effects” to the rights and freedom of the subjects, possibly clarifying examples in an Annex where for each fundamental right or freedom mentioned in the EU Charter of Fundamental Rights there are some examples of legal or similarly significant effects. When analysing risks to fundamental rights and freedoms, particular attention should be dedicated to human health and safety.

In the EU law there are already several different pieces of legislation covering many different areas that are generally very relevant for AI systems (data protection law, anti-discrimination law, consumer protection & product liability law, administrative law): in the attached document we propose possible gaps in these fields that a Regulation on AI could cover and we suggest to consider also risks generated in public institutions.

The impact assessment shows 4 alternative options to regulate AI. Although each approach may have advantages and disadvantages, we consider that the voluntary labelling scheme is inefficient and ineffective for the declared purposes of a regulation on AI. Many other EU legal sectors recognise voluntary labelling scheme but only as an ancillary tool, related to other kinds of regulation. Depending on the definition of high-risk, the approach 3.b might be considered, but only with a clear blacklist of high-risk examples and with easy criteria to identify risks and to correlate each risk to a fundamental right or freedom as recognised by the EU Charter of Fundamental Rights and Freedoms. In addition, any sectoral amendment of existing laws should be advisable (including soft law actions, as amending the unfair commercial practice blacklist, etc.).","['Feedback for the Commission Impact Assessment  \n“Proposal for a legal act of the European Parliament and the Council laying \ndown requirements for Artificial Intelligence” \n \nThis  is  a  common  statement  of  the  Augmented  Law  Institute  of  EDHEC  Business  School \nhttps://www.edhec.edu/en/legaledhec-research-centre#dexp_tab_item_2063327793 (for the list of Professors \ncomposing this group, please click on the link). \n \nThe aim of this position paper is to analyse the Inception Impact Assessment of the European Commission \nabout a Proposal for an EU legal act regulating Artificial Intelligence. This report is based on 3 parts: 1) a \ndefinitional part, which aims to reply to the question “what to regulate?”; 2) a methodological part, which aims \nto reply to the question “why regulate?”; and 3) a content-based part, which aims to reply to the question “how \nto regulate?”. \n \n1.  Definitional part: What to regulate? \n \nA first problem that we observe in the Impact Assessment and in the related White Paper is the lack of clear \ndefinitions. In particular, two terms urge to be clearly defined before to proceed: \n1.  artificial intelligence; \n2.  risk (to fundamental rights and freedoms, including human health and safety). \n \n1.1.  The notion of AI \n \n•  The notion of Artificial Intelligence has been already defined by the EC in a Communication of 2018, as \nsystems that display “intelligent behaviour by analysing their environment and taking actions – with some \ndegree of autonomy – to achieve specific goals”.1 However, that definition has been criticized by the EC \nHigh-Level Expert Group on AI (hereinafter: AI HLEG) in a Document made public on April 2019.2 In \nthat document the HLEG proposed a new definition, which clarifies better what an “intelligent behaviour” \nis and what the environment analysis is: “systems designed by humans that, given a complex goal, act in \nthe physical or digital dimension by perceiving their environment through data acquisition, interpreting \nthe collected structured or unstructured data, reasoning on the knowledge, or processing the information, \nderived from this data and deciding the best action(s)”. \n \n•  In purely legal terms, the notion of AI is not present in any EU legal act. To the contrary, there is a notion \nof “software” (at Article 1(1) of the software directive3) and a notion of “automated decision-making” in \nthe General Data Protection Regulation - hereinafter GDPR - (see Article 22). But both of these notions \nare potentially limited and not comprehensive enough for the phenomenon of AI. \n \n•  Both the White paper and the Inception Impact Assessment are not clear about which definition should \nbe preferred. However, the implications can be relevant. In particular, since the definition of the \n \n1 Communication from the Commission to the European Parliament, the European Council, the Council, the European Economic and \nSocial Committee and the Committee of the Regions on Artificial Intelligence for Europe, Brussels, 25.4.2018 COM(2018) 237 final.  \n2 European Commission High-Level Expert Group on Artificial Intelligence, “A definition of AI: Main capabilities and \ndisciplines”, 8 April 2019, https://ec.europa.eu/digital-single-market/en/news/definition-artificial-intelligence-main-\ncapabilities-and-scientific-disciplines \n3 Directive 2009/24/EC of the European Parliament and of the Council of 23 April 2009 on the legal protection of \ncomputer programs (Codified version) \n  1', 'Commission’s Communication does not define what “intelligent behaviour” is, there might be problems \nin understanding the scope of AI notion and whether some technological systems (e.g., Covid-19 Alert \nExposure  Apps,  automatic  entrance  systems,  etc.)  are  within  or  outside  the  scope  of  “intelligent \nbehaviour”. \n \n•  Accordingly, what we propose is to prefer the HLEG definition, accompanied by an Annex with a non-\nexhaustive list of relevant examples of AI systems actually used. \n \n1.2.  The notion of Risk \n \n•  The notion of risk is also very problematic. Section B of the Inception Impact Assessment is largely \nbased on the notion of risk (e.g., options 3 and 4 of the “Alternative options to the baseline scenarios”). \nHowever, it is not clear what we should mean by “risk” and how the risk should be assessed for each \nAI system. \n \n•  As regards the notion of risk, interpreting the White Paper and the Inception Impact Assessment (and \nin accordance with the GDPR (Articles 24, 25, 27, 33-35)) it seems that it is a legal notion referring \nto the “risk for fundamental rights and freedoms”. However, such a legal notion should be based on \nsome criteria to identify what “high” risk means. In the GDPR, high risk refers to a certain level of \nseverity and likelihood for fundamental rights and freedoms. While in the White Paper the reference \nto “severity” seems to rely on the notion of “legal or similarly significant effects”4 (as referred to, e.g., \nin the GDPR, at Article 22(1) and 35(3)), in the Impact Assessment there is no mention of this severity \nthreshold. In addition, there are no examples of high risk AI systems. \n \n•  We propose to clearly define what “risk” means and which level of severity and likelihood might \namount to qualifying as “high risk”. In addition, we propose to use the White Paper’s definition of the \nseverity threshold on “legal or similarly significant effects” to the rights and freedom of the subjects, \npossibly clarifying examples in an Annex where for each fundamental right or freedom mentioned in \nthe EU Charter of Fundamental Rights there are some examples of legal or similarly significant effects. \nWhen analysing risks to fundamental rights and freedoms, particular attention should be dedicated to \nhuman health and safety. The Annex should devote specific criteria for the analysis of risk to human \nhealth and safety. \n2.  Methodological part: why to regulate? \n \n•  In the EU law there are already several different pieces of legislation covering many different areas \nthat are generally very relevant for AI systems. Both the White Paper and the Impact Assessment do \nnot seem to focus on existing rules. Identification and application of existing legal rules may mitigate \nAI-related risks and protect individuals. Accordingly, it would be helpful to understand whether there \nexists any gaps that the law should cover. The Inception Impact Assessment implies possible coverage \nand/or gaps in Products Liability Directive, but there are other rules that provide for protection against \nAI-related harm. A comprehensive analysis, beyond products liability, would be helpful. In this section \nwe will make just some preliminary examples. \n \n•  Data Protection Law. The GDPR and the related data protection pieces of legislation (e-privacy \ndirective, Law Enforcement Directive, EUDPR, PNR directive) are already requiring a certain level \nof accountability, transparency and fairness of AI (in particular for automated decision-making system \n \n4 European Commission, White Paper on AI, p.17. \n  2', 'based on personal data processing). However, we identify some areas where AI systems are not based \non personal data (as defined at Article 4(1) of the GDPR): as a preliminary example we can consider \nautomated online behavioural advertisement systems in public spaces (where emotion recognition \ntriggers different kinds of advertisements, without any need to identify the natural person, who is \ntargeted). If these automated emotion recognition tools could in principle imply high risks for \nfundamental rights and freedoms of natural persons, this might be a clear example of a “gap” that a \nnew EU legal act on AI could cover. Other examples may involve, e.g.: secondary effects of AI-driven \ndata processing that do not affect data subjects but a bigger audience of individuals who are not clearly \nidentified; adverse effects (e.g., anticompetitive acts) of AI on legal persons that are protectable neither \nunder Intellectual Property Law nor under data protection law (the GDPR protects only physical \npersons).  \n \n•  Anti-discrimination Law. EU antidiscrimination law is sectoral and category based. Accordingly, its \napproach, including existing burden of evidence rules, may—or may not—turn out to be highly \nineffective to identify and mitigate new forms of hidden AI-driven discriminations, based on, e.g., \ndata proxies or affinity profiling. \n \n•  Consumer Law and related Tort Law (product liability). EU protects consumers, e.g., through the \nlimitations to unfair commercial practices and through specific protections against defective products \nliability. Both these areas might be relevant to protect consumers adversely affected by AI-driven \ncommercial practices or AI-driven products. A proportionality assessment in the Impact Assessment \nshould explain why the current legislation is (or is not) adequate to deal with AI-based practices. For \nwhat concerns the unfair commercial practice directive, it might be helpful to add in the EU blacklist \nof unfair practice some cases of AI-related practice that could adversely affect mental freedom of \nconsumers. As regards product liability, we agree that clarification is needed for determining whether \nthe directive includes software. Consistent with our proposal of a comprehensive statement regarding \nthe effectiveness of existing law, liability for software defects should additionally be concerned with \nconsumer protection and basic contract law principles. It may be the case the Products Liability \nDirective requires no amendment.   \n \n•  Public Administration Law. Both the White Paper and the Inception Impact Assessment seem to refer \nonly to AI systems developed or used by private parties (businesses), while there is no mention of \npossible issues or pitfalls of AI used by public institutions. The Impact Assessment should clarify why \nthis approach was preferred and whether current administrative law (at least at Member State law \nlevel) is adequate to deal with risks to fundamental rights and freedoms for individuals generated by \nAI systems in public institutions (e.g., school admissions, public hospital interactions, tax anti-evasion \nsystems, etc.). \n   \n3.  Content-based part: how to regulate? \n \n•  The impact assessment shows several alternative options: 1) soft law; 2) voluntary labelling scheme; \n3) hard law (a. based only on some categories/sectors; b. based only on high-risk AI; c. covering all \nAI systems); 4. a combination of the previous approached. \n \n•  In our view, in order to make this decision, it is highly necessary to reply to point 1 and 2 of this paper, \ni.e. clarifying the definitions and the existing gaps. \n \n  3', '•  Although each approach may have advantages and disadvantages, we consider that the voluntary \nlabelling scheme is inefficient and ineffective for the declared purposes of a regulation on AI. Many \nother EU legal sectors recognize voluntary labelling scheme but only as an ancillary tool, related to \nother kinds of regulation. \n \n•  Depending on the definition of high-risk the approach 3.b might be considered, but only with a clear \nblacklist of high-risk examples and with easy criteria to identify risks and to correlate each risk to a \nfundamental right or freedom as recognized by the EU Charter of Fundamental Rights and Freedoms. \n \n•  In addition, any sectoral amendment of existing laws should be advisable (including soft law actions, \nas amending the unfair commercial practice blacklist, etc.). \n  4']"
F550966,10 September 2020,Matteo Quattrocchi,Wirtschaftsverband,BSA | The Software Alliance,mittel (50 bis 249 Beschäftigte),75039383277-48,Belgien,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"BSA would like to underline the need for the Commission to carry out an in-depth inventory of EU law, and its application to AI, before suggesting possible legislative actions. Consistent with the risk-based, context-specific approach of the White Paper, any proposed legislation should avoid one-size-fits-all mandates. 
Future proposals should focus on high-risk scenarios where the deployment of AI poses a threat to fundamental rights. The scope of any regulatory obligations should be a function of the degree of risk and the potential scope and severity of harm. 
BSA supports an incremental approach by limiting regulation to AI that is deployed in a high risk sector and used in a manner that significant risks are likely to arise. The Commission should extend this two-pronged approach to all possible high-risk scenarios, rather than identifying specific sectors where AI would be considered high-risk by default. Ensuring that the definition of high-risk is appropriately tailored will be critical, it is also crucial to provide a well-determined scope of application. 
Legal requirements for high-risk AI applications should be addressed to the actors best placed to address potential risks. In many cases developers may not know whether the technology is being deployed by an end-user in a manner that meets the definition of high-risk. Developers are better placed to describe the capabilities and limitations of an AI system, while disclosing the possible impact of AI use will typically be the responsibility of the deployer. 
The Commission should draw from existing concepts for establishing which entity is best placed to address potential risk, i.e. the entity that determines the purpose of the AI, similar to the concept of a controller under GDPR. In the context of AI, the AI controller will generally be the deployer. The processor/controller distinction provides organizations with a clear picture of their respective legal obligations, and helps ensure that data subjects rights are adequately protected.
Ensuring that the definitions of the entities involved are the same in different sectors, founded in established practices, would entail a more harmonized approach to AI. 
The Commission should not establish pre-marketing conformity assessment for AI systems, as such obligations are liable to turn into barriers to enter the market, a more scalable approach would be self-attestation.
BSA urges the Commission not to pursue a regulatory scheme based on prescriptive conformity assessment requirements. The risks that AI poses and the appropriate mitigation mechanisms are context-specific. The appropriate mechanisms and standards for training data, record keeping, transparency, accuracy, and human oversight vary depending on the nature and deployment of an AI system. The Commission should therefore avoid creating prescriptive, one-size-fits-all requirements around these categories. 
The Commission should articulate an impact assessment framework on high-risk AI for stakeholders, possibly on the basis of the HLEG ALTAI.
The Commission should consider specific rules for the use of remote biometric identification systems, by the public sector in particular, given the heightened risks inherent in governmental use of this technology.
BSA urges the Commission not to pursue the creation of a blanket voluntary labeling system for all no-high risk systems. Given the diverse range of AI products and services that will not be considered high risk, a one-size-fits-all labeling scheme would be unworkable. The benchmarks for evaluating whether AI systems are trustworthy are likely to be highly variable, driven in large part by system functionality and deployment context. A labelling system that could apply to all no-high risk AI would necessarily be very complex, and would limit customers understanding and engagement. Similarly, the governance of such a scheme would be exceedingly complex and would have to cover very diverse sectors and technologies.","[""Brussels, September 2020 \n \n \n \nBSA submission to the European Commission Consultation on the Inception Impact \nAssessment for a Proposal for a legal act of the European Parliament and the Council \nlaying down requirements for Artificial Intelligence \n \nBSA | The Software Alliance (“BSA”)1  welcomes the opportunity to offer thoughts on the \nEuropean Commission Inception Impact Assessment on a possible Proposal for a legal act of the \nEuropean Parliament and the Council laying down requirements for Artificial Intelligence (“the \nProposal”). BSA is the leading advocate for the global software industry before governments and \nin the international marketplace. Our members are at the forefront of software-enabled innovation \nthat is fueling global economic growth, including cloud computing and AI products and services. \nBSA members include many of the world's leading suppliers of software, hardware, and online \nservices to organizations of all sizes and across all industries and sectors. BSA members have \nmade significant investments in developing innovative AI solutions for use across a range of \napplications. As leaders in AI development, BSA members have unique insights into both the \ntremendous potential that AI holds to address a variety of social challenges and the governmental \npolicies that can best support the responsible use of AI and ensure continued innovation. \nBSA agrees with the fundamental proposition of the European Commission’s White Paper \non Artificial Intelligence (“the White Paper”) that the public should “expect the same level \nof safety and respect of their rights whether or not a product or system relies on AI.”  Of \ncourse, the concerns presented by the European Commission are not unique to AI. The EU body \nof laws offers strong, technologically neutral safeguards against these concerns. BSA strongly \nrecommends that the Commission take stock of this body of legislation in a targeted way, identify \npossible gaps and only propose new legislation if there is no other way to rectify them, AI-specific \nor not. \nThe White Paper acknowledges the challenges and promise of AI tools, and at the same time \ncalls for a more thorough analysis of existing EU Legislation, to establish whether it is fit for \npurpose in protecting fundamental rights whilst fostering AI uptake. In the context of the work of \nthe High-Level Expert Group on AI (“HLEG”), BSA prepared a detailed analysis of EU legislation \nimpacting AI,2 which could prove helpful as the Commission moves to evaluate the sufficiency of \ncurrent laws. Moreover, BSA would like to emphasize that AI is not developed in a vacuum in the \nEU, and that while new technologies present new challenges, the protection and enforcement of \nFundamental Rights in the EU remain as strong as ever. BSA and its Members continue to work \n \n1 BSA | The Software Alliance (www.bsa.org) is the leading advocate for the global software industry before governments \nand in the international marketplace. Its members are among the world’s most innovative companies, creating software \nsolutions that spark the economy and improve modern life. With headquarters in Washington, DC, and operations in \nmore than 30 countries, BSA pioneers compliance programs that promote legal software use and advocates for public \npolicies that foster technology innovation and drive growth in the digital economy.  \nBSA’s  members  include:  Adobe,  Akamai,  Atlassian,  Autodesk,  Bentley  Systems,  Box,  Cadence,  Cloudflare, \nCNC/Mastercam, IBM, Informatica, Intel, Intuit, MathWorks, McAfee, Microsoft, Okta, Oracle, PTC, Salesforce, \nServiceNow, Siemens Industry Software Inc., Sitecore, Slack, Splunk, Trend Micro, Trimble Solutions Corporation, Twilio, \nand Workday. \n2 Please refer to our submission to the HLEG on EU Legislation here. \nAvenue des Arts 44      P  +32  (0)2 274 13 10 \n1040 Brussels      W bsa.org \nBelgium        EU Register of Interest Representatives 75039383277-48"", 'alongside EU Institutions and Member States to support a strong EU body of law that provides \nsafeguards for fundamental rights whilst fostering innovation.  \nAs the Commission moves to implement the White Paper, BSA would like to reiterate how \nimportant it would be for the Commission to carry out an in-depth inventory of EU law, \nand its application to AI, before suggesting possible legislative actions. Consistent with the \nrisk-based, context-specific approach the Commission has endorsed, any proposed legislative \nchanges should avoid one-size-fits-all mandates. The AI ecosystem is broad, encompassing a \ndiverse range of technologies, use cases and wide array of stakeholders. Legislative updates \nmust therefore be flexible enough to account for the unique considerations that may be implicated \nby specific uses cases. For instance, Business-to-Business (“B2B”) relations are radically \ndifferent than Business-to-Consumer (“B2C”), and entail a completely different consideration and \nallocation of risk. In the B2B context, entities should remain free to use contractual negotiations \nas a mechanism for allocating risks, liabilities, and obligations in a manner that corresponds to \nthe nature of the transaction. \nBSA agrees that future legislative proposals should focus on high-risk scenarios where \nthe deployment of AI-based technologies poses a threat to human life and fundamental \nrights. The scope of any regulatory obligations should be a function of the degree of risk and the \npotential scope and severity of harm.  Many AI systems pose extremely low, or even no, risk to \nindividuals or society. To this end, it will be important to carefully assess scenarios that should \nbe deemed as high-risk and hence be subject to legal requirements. BSA strongly recommends \nensuring stakeholder involvement in this context as much as possible, especially as it will be \nsector-dependent as much as use-case dependent. BSA and its Members have participated, and \nintend to continue to be active participants, to EU and Member States stakeholder consultations. \nBSA Members are uniquely positioned to provide essential insights in the assessment of high-\nrisk scenarios and use-cases of AI. \nBSA supports the possibility suggested by the White Paper to take an incremental \napproach by limiting regulation to AI systems that are (1) deployed in a high risk sector \nand  (2)  used  in  a  manner  that  significant  risks  are  likely  to  arise.  Moreover,  due \nconsideration should be given to AI applications that enhance human decision-making, whereby \nthe risk consideration – even when the two above conditions are fulfilled – is inevitably balanced \nby the human involvement and control. Furthermore, BSA urges the Commission to extend \nthis two-pronged approach to all possible high-risk scenarios, rather than identifying \nspecific sectors where – regardless of its purpose and use –  AI would be considered high-risk \nby default. This would allow for a more homogeneous application and understanding of the \npossible requirements for high-risk AI, providing for the necessary proportionality and legal \ncertainty as AI technologies and tools are developed and deployed.  \nEnsuring that the definition of high-risk is appropriately tailored will be critical. Given the \npotentially far-reaching requirements of new legislative requirements for high-risk AI, it is crucial \nfor AI developers and users to be able to determine with certainty if their application might fall \nwithin the scope of high-risk. The complexity of defining “high-risk” AI is exacerbated by that fact \nthat  AI may be developed for a multitude of uses – and determining whether it is “high-risk” will \nturn on how it is deployed (i.e., whether it is deployed by an end-user in a high-risk sector and \nused in a manner that creates a significantly likelihood of risk.). The methodology behind the \ndefinition of high-risk sectors needs to be precise and robust, with only limited exemptions. This \nwill guarantee that the list remains targeted and up to date as new technologies and use cases \nemerge.  \nAvenue des Arts 44      P  +32  (0)2 274 13 10 \n1040 Brussels      W bsa.org \nBelgium    EU Register of Interest Representatives 75039383277-48', 'BSA agrees with the Commission’s analysis that legal requirements for high-risk AI applications \n“should be addressed to the actor(s) who is (are) best placed to address any potential risks” \n(White Paper p. 22). In many cases – especially in the cases of general-purpose AI systems – \ndevelopers will not be in the position to know whether the technology is being deployed by an \nend-user in a manner that meets the definition of high-risk. Similarly, in B2B relations the \nallocation of risk will be one part of the contractual agreement between two entities, and once \nagain the developer will not be best place to establish whether the application is to be deployed \nin an high-risk scenario, and what obligations that may entail in the specific sector. Developers \nare better placed to describe the capabilities and limitations of an AI system, while \ndisclosing the possible impact of AI use will typically need to be the responsibility of the \ndeployer.  \nIn this context, and in a similar vein in the liability context (please see below in the relevant \nsection for more information), the Commission may want to draw from existing concepts for \nestablishing which entity is “best placed to address any potential risk”, i.e. the entity that \ndetermines the purpose of the AI, similar to the concept of a “controller” under the GDPR.  \nArticle 29 Working Party guidance on controllers and processors (WP 169) describes this party \nas the “determining body” that decides the “how” and the “why” of a processing operation.  \nApplying this concept in the context of AI, the “AI controller”  will generally be the deployer of an \nAI system (e.g., a vehicle manufacturer that integrates an AI-driven language recognition system \ninto an automobile, or a bank that uses an AI tool to score consumers for loans).  In some \ninstances, it may also be the operator of the AI system (e.g., a physician using assistive tools \nduring surgery). \nUnder the GDPR, controllers and processors have different levels of responsibility for achieving \nprivacy  outcomes  that  reflect  their  different  roles.  In  particular,  controllers  have  primary \nresponsibility for satisfying certain legal privacy and security obligations and for honoring data \nsubject rights requests. On the other hand, processors, which handle data on behalf of the \ncontroller to implement the controller’s objectives, are responsible for securing the personal data \nthey maintain and following the instructions of a controller, pursuant to their agreements with \nrelevant controllers. The processor/controller distinction not only provides organizations \nwith a clear picture of their respective legal obligations, it also helps to ensure that data \nsubjects rights are adequately protected. Accordingly, BSA recommends that the Commission \npreserves in possible AI legislation the careful distinction between controllers and processors, \nespecially at a time when data protection legislation, such as the General Data Protection \nRegulation, is finally establishing a level-set amongst organizations that process data. \nIn the context of enterprise AI, the tools that BSA companies provide are generally AI systems \nthat facilitate human decision-making, without replacing human decision-making. With this in \nmind, it becomes clear that a company using an AI service to enable its employees to make a \ndecision acts as a controller in deciding how and why that data is processed, and the AI system \nis used as a tool for processing data on behalf of that controller. Accordingly, the company \ndeveloping and providing the AI tool is appropriately treated as a processor. This key distinction \ncould also help inform the Commission’s AI workstreams, which will have to focus on sectors with \nvery different definitions and approaches to risk management. Ensuring that the definitions \nand requirements for the entities involved are the same in different sectors – and founded \nin well-established practices that are equally applied cross-sectorally – would ensure a \nmore harmonized approach to AI.  \nBSA  recommends  to  the  Commission  not  to  establish  pre-marketing  conformity \nassessment for AI systems, as such obligations are liable to turn into barriers to enter the \nmarket. A more scalable approach would be self-attestation, which would also be least likely to \nAvenue des Arts 44      P  +32  (0)2 274 13 10 \n1040 Brussels      W bsa.org \nBelgium    EU Register of Interest Representatives 75039383277-48', 'unnecessarily extend time to market or unduly burden smaller operators. BSA believe that \nprescriptive regulation of AI, requiring for example that every possible use of an AI system is “fair” \nor “unbiased”, will likely be unworkable in practice.  \nBSA urges the Commission not to pursue a regulatory scheme based on prescriptive \nconformity  assessment  requirements.  The  risks  that  AI  poses  and  the  appropriate \nmechanisms for mitigating those risks are largely context-specific. The appropriate mechanisms \nand standards for training data, record keeping, transparency, accuracy, and human oversight \nwill vary depending on the nature of the AI system and the setting in which it is being deployed. \nThe Commission should therefore avoid creating prescriptive, one-size-fits-all requirements \naround these categories. Such ex-ante requirements could impede efforts to address the very \nrisks they are intended to address, add unnecessary costs and require extremely complex \ncompliance checks. \nGiven the nascent nature of the technology and sociotechnical quality of many of its most \nsignificant challenges, BSA believes that a governance-based approach to legislation, which \nidentifies broad objectives and the processes that developers and deployers should follow to \nachieve them, will be more effective than a prescriptive one. Consistent with a governance-based \napproach, the Commission should articulate a framework that will enable stakeholders to perform \nan “impact assessment” on high-risk AI systems. In this context, BSA recommends building upon \nthe work done by the HLEG and many AI developers on the Assessment List for Trustworthy \nArtificial Intelligence, in particular as it may constitute a template for future assessment tools. \nThe goal of these governance processes should be to help developers and deployers of \ncovered AI systems identify and quantify any relevant risks of harm to individuals or \nsociety and, where those risks are determined to be significant, to implement measures \nto mitigate against them. Importantly, impact assessments allow for a more context-specific \nevaluation of the types of risk mitigation measures that are available, and which is ideally suited \nfor the particular deployment scenario.    \nBSA acknowledges the Commission’s concern that the deployment of biometric identification \nsystems can implicate heightened risks for fundamental rights. BSA welcomes the White Paper’s \nrecommendation for the Commission to launch an inquiry to examine the appropriate regulatory \nframework for biometric systems, which is also reflected in the Inception Impact Assessment. \nWhile EU law already provides clear parameters for assessing the lawfulness of biometric \ntechnologies from a data protection perspective, the rules that govern the ethics and other risks \nof Facial Recognition Technologies (“FRT”) deployments are less well defined. For that reason, \nthe Commission should consider specific rules governing the use of FRT by the public \nsector in particular, given the heightened risks inherent in governmental use of this \ntechnology. \nBSA  agrees  that  public  trust  in AI  is  essential  for  “[promoting]  the  overall  uptake  of  the \ntechnology”. However, we would urge the Commission not to pursue the creation of a \nblanket voluntary labeling system for all no-high risk systems. Given the diverse range of \nAI products and services that will be considered ""no-high risk AI applications"", a one-size-fits-all \nlabeling scheme would be unworkable. The benchmarks for evaluating whether AI systems are \ntrustworthy are likely to be highly variable, driven in large part by system functionality and \ndeployment context. The relevant benchmarks for evaluating the trustworthiness of an AI system \nthat recommends restaurants are likely to be quite different from those that are relevant to an AI \nsystem that is designed to identify what objects are in a photograph. A methodology for a labelling \nsystem that could apply to the entire universe of “no-high risk AI” would necessarily be very \ncomplex, which would limit customers’ understanding and engagement. Similarly, the governance \nAvenue des Arts 44      P  +32  (0)2 274 13 10 \n1040 Brussels      W bsa.org \nBelgium    EU Register of Interest Representatives 75039383277-48', 'of such a scheme would be exceedingly complex and would necessarily have to cover diverse \nsectors and technologies – likely in almost all industrial EU activities.  \nAvenue des Arts 44      P  +32  (0)2 274 13 10 \n1040 Brussels      W bsa.org \nBelgium    EU Register of Interest Representatives 75039383277-48']"
F550965,10 September 2020,wolfgang lindner,Wirtschaftsverband,WKO Austria,groß (250 oder mehr Beschäftigte),10405322962-08,Österreich,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"WKO Austria welcomes the opportunity to provide input to the Commission’s Inception Impact Assessment for the upcoming legislative framework on AI. We strongly believe, that it is very important to secure a future-proof framework that will support an innovative and competitive European market, and also too much complexity and over-regulation should be avoided.

General remarks:

-	The term ""AI"" must be clearly and unambiguously defined at international level; this is a basic requirement for the design of the legal framework.
-	The impacts of future jobs must be taken into account. It will be necessary to have more qualified (technical) ressources and new job profiles.

Options for regulation:

The aim should in any case to avoid complex administrative documentation obligations and legally uncertain considerations of fundamental rights.

WKO perspective has a preference for options 1 and 2 (soft law and voluntary labeling scheme)
Real laboratories could be an option for better regulation. Trial and development phases can help develop the right system of government.

If options 1 and 2 are probably not sufficient to provide an appropriate legal framework for this new technological challenge, it should go rather with option 3.a.
There should only be regulations where there are major risks in the area of application. In all other areas, no further regulation is required, only voluntary exchange and cooperation.
- Option 3b: From a purely technical point of view, it makes little sense to regulate according to the criteria presented in the White Paper (primarily according to economic sector). Any upcoming AI framework should be application-based and not technology-based
- Option 3.c would lead to possible over-regulation
"
F550964,10 September 2020,-,-,-,-,-,-,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"Liberty Global response to the roadmap (IIA) on Artificial intelligence – ethical and legal requirements (Proposal for a legal act of the European Parliament and the Council laying down requirements for Artificial Intelligence)

Liberty Global welcomes the opportunity to comment on the Commission’s roadmap on IIA of a possible initiative aimed at addressing a number of ethical and legal issues raised by AI. Liberty Global supports the Commission’s ultimate objective of ‘fostering the development and uptake of safe and lawful AI that respects fundamental rights across the Single Market by both private and public actors while ensuring inclusive societal outcomes’. In our response to the Commission’s AI White Paper (attached), we gave our support to the Commission’s twin objective of promoting the uptake of AI and of addressing the risks associated with certain uses of this new technology, noting that striking the right balance between the two is key to achieving an optimal outcome. We maintain that position in light of the updated, more precisely-defined objective. 

That objective, in Liberty Global’s opinion, can only be achieved in a balanced way through an EU legislative instrument establishing mandatory requirements for certain types of AI (policy option 3). Liberty Global considers a legislative instrument  appropriate specifically for and limited to ‘high-risk’ AI applications which can be identified on the basis of the two criteria set out in the aforementioned White Paper. In Liberty Global’s view, this approach also covers ‘specific categories’ of AI, such as biometric identification systems (first sub-option), for which extra safeguards are warranted. In order to promote innovation and investment, it is important that a potential legislative instrument create an adequate, high-level framework for the assessment of compliance. The detailed requirements for compliance themselves are then best laid down in Harmonized Standards. Such an approach will not prejudice the adoption of voluntary and industry-led interventions for non-‘high-risk’ applications, which would benefit from the work done by European standardisation organisations in the context of setting detailed requirements for those ‘high-risk’ applications. 

In light of the above, Liberty Global strongly supports the adoption of any potential legislative act on the basis of Article 114 TFEU, as is suggested by the on p. 3 of the IIA. The EU has a long and successful history of balanced legislation on this basis and its predecessors under the Treaties. Legislation, which through harmonisation furthers the (Digital) Single Market, creates the stable regulatory environment that is a prerequisite for innovation and investment – promoting consumer welfare – and at the same time pursues society’s wider interests. The harmonised application of strong and smart internal market regulation, using tools such as Harmonised Standards, gives the EU a global reach to promote values-based innovation in the forthcoming update of AI.

We recommend the Commission to seize this opportunity for the EU to become a leader in the development and uptake of lawful and trustworthy AI. This is only possible if regulatory fragmentation, through the adoption of diverging national approaches is prevented by a strong European approach. 

In our view, the prerequisites for the success of such a strong European approach are that it is technologically neutral, risk-based and laid down by a high-level framework which delegates the setting of detailed requirements for ‘high-risk’ AI applications to the European standards development organisations. This will, in turn, promote the adoption of and adherence to such standards, on a voluntary basis, for AI applications with a more limited risk profile. Through such an approach, European consumers and citizens will ultimately benefit from the swift uptake of values-based AI. For more detail, we refer to our supplementary position paper (attached).","['Liberty Global response to the European Commission’s White Paper on Artificial \nIntelligence - A European approach to excellence and trust \nLiberty Global welcomes the opportunity to comment on the European Commission’s White Paper on \nArtificial Intelligence - A European approach to excellence and trust (the White Paper).1 Liberty Global \nstrongly supports the European Commission’s continued commitment to engage with stakeholders on \nissues that are relevant to them, particularly in the rapidly evolving and impactful domain covered by \nthe EU’s digital strategy. Liberty Global is convinced that such engagement will help further achieving \na Europe fit for the digital age, by ensuring that this transformation will, indeed, work for both people \nand businesses.  \n \nThis written submission accompanies Liberty Global’s response to the European Commission’s public \nconsultation on this subject, a copy of which is provided in this document’s Annex.2 That annexed \nresponse  covers the full  breadth of  the issues  addressed by  the European Commission’s  public \nconsultation, whereas this written submission will focus more on the role of artificial intelligence (AI) \nin the electronic communications domain and the associated regulatory implications. \n \nIntroduction \nFostering a European approach to excellence and trust \nWith a view to launching a solid European approach to address the opportunities and challenges of \nAI, against a backdrop of fierce global competition, and to promote the development and deployment \nAI in line with European values, Liberty Global agrees with the European Commission that the EU must \nact as one. Therefore, Liberty Global commends the European Commission for its timely consultation \nof the relevant policy options to achieve the clear twin objectives identified in the White Paper.  \nLiberty Global agrees with the European Commission’s position, as set out in the White Paper, that a \n‘regulatory and investment oriented approach with the twin objective of  promoting the uptake  of  AI \nand  of addressing  the  risks  associated  with certain  uses  of this  new technology’ is the right \napproach. In Liberty Global’s opinion, striking the right balance is key to achieving an optimal outcome, \nboth as regards promoting investment – and innovation – and as regards addressing potential risks \nassociated with certain uses of AI.  \n \n                                                             \n1 European Commission, White Paper, On Artificial Intelligence – A European approach to excellence and trust, \nCOM(2020) 65 final (White Paper). \n2 Consultation on the White Paper on Artificial Intelligence - A European Approach; available at \nhttps://ec.europa.eu/eusurvey/runner/AIConsult2020?surveylanguage=EN.', 'The importance of AI applications for next-generation networking (and vice versa) \nArtificial intelligence and machine learning will transform the way networks operate \nAs one of the leading pan-European providers of electronic communications services and as the \noperator of a significant globe spanning network, Liberty Global is well positioned to speak to the \nimportance of AI – and its derivatives, such as machine learning (ML) – for the operation of next-\ngeneration networks, including DOCSIS 3.13 and 5G-based technologies, as well as the provision of \nnew and innovative communications services. \n \nTogether with CableLabs, the cable industry’s leading R&D consortium and membership organization, \nwe are working on a broad spectrum of AI-related innovations, which will transform networks and \nservices to the benefit of the millions of European end-users, who already have access to Gigabit \nspeeds even today, as  result of our continued commitment to innovation and investment. \n \nAs an example, the deployment of AI and ML applications will be critical for the deployment of next-\ngeneration networks, which will provide seamless connectivity to end-users by using a wide variety of \naccess networks and technologies, frequency bands and cells. To ensure this seamless experience and \nto provide optimal connectivity for a particular communication or application, leveraging AI and ML \nwill enable operators to: \n \n\uf0b7  Forecast the peak traffic, resource utilization and application types \n\uf0b7  Optimize and fine tune network parameters for capacity expansion \n\uf0b7  Eliminate coverage holes by measuring the interference and using the inter-site distance \ninformation.4 \n \nAdditionally, the deployment of AI and ML will be critical to the widespread deployment of, and \nprovision of services based on, network slicing and virtualization of network functions. AI and ML will \ngreatly  enhance  automation  and  adaptability,  enabling  efficient  orchestration  and  dynamic \nprovisioning of network slices to particular users or devices. In addition, ML and AI can process, in real \ntime, the vast amounts of information required for such applications.  \n \nLastly, ML applications such as pattern recognition are already critical in ensuring the integrity and \nsecurity of today’s networks, by  rapidly processing vast amounts of data to detect novel attack \nvectors. In view of the digital transformation of Europe and the persistence of such cyber threats, the \nimportance of such applications of AI will only be growing over the coming years. \n                                                             \n3 A. Gaydashenko et al., A Machine Learning Approach to Maximizing Broadband Capacity via Dynamic DOCSIS \n3.1 Profile Management, IEEE 18th International Conference on Machine Learning and Applications (ICMLA), \n(2019); C. Leddy, Can AI Make Cable Smarter?, Light Reading (2017), available at: \nhttps://www.lightreading.com/artificial-intelligence-machine-learning/can-ai-make-cable-smarter/a/d-\nid/739262; CableLabs, A Different Future for Artificial Intelligence, (2018), available at: \nhttps://www.cablelabs.com/different-future-for-artificial-intelligence.   \n4 CableLabs, Leveraging Machine Learning and Artificial Intelligence for 5G, (2019) available at: \nhttps://www.cablelabs.com/leveraging-machine-learning-and-artificial-intelligence-for-5g.  \nLiberty Global response to the European Commission’s White Paper on Artificial Intelligence - A European approach to \nexcellence and trust – 12 June 2020', 'Next-generation networks are key enablers for the uptake of AI in Europe  As the European \nCommission correctly assessed on the first page of its White Paper, ‘a large share of tomorrow’s far \nmore abundant data will come from industry, business and the public sector, and will be stored on a \nvariety of systems, notably on computing devices working at the edge  of  the network.  This  opens  up \nnew opportunities  for  Europe, which has a strong position in  digitised  industry  and  business-to-\nbusiness  applications, but a relatively  weak position in  consumer platforms.’ To ensure the successful \nuptake of AI in Europe, particularly in a way in which it works for both people and (small) businesses, \nit will be important to ensure the democratization of this technology and its enablers in an early stage. \nConnectivity, data storage, as well as computing power are such key enablers. \n \nLiberty  Global will  play  a key  part  in  this,  by  providing  people  and  businesses  with  first-rate \nconnectivity and value-added services. The advent of edge computing and data storage services will \nbe a key amplifier in this respect, as it gives people and particularly small businesses immediate access \nto those core technological resources needed for the development and large-scale deployment of AI \napplications. Therefore, Liberty Global encourages the European Commission to foster investments in \nthe digital domain, as these will be critical to the success of Europe’s AI uptake. \n  \nLiberty Global’s additional considerations as regards the White Paper \nA risk-based approach is key to achieve the twin objectives of promoting a rapid uptake of AI and \naddressing potential risks \nFurther to its response to the European Commission’s consultation (annexed below), Liberty Global \nwishes to emphasize the urgency of pursuing such an approach. The cumulative criteria identified by \nthe European Commission, as a threshold for the imposition of regulatory requirements, will – if \nadhered  to by  regulators and  policymakers  alike –  serve  as  powerful  indicator and  therefore \ncontribute significantly to regulatory clarity and predictability. Such clarity and predictability will, in \nturn, have a positive impact on innovation and investment in the relevant domains. \n \nThe merits of the European approach towards standardization \nLiberty Global is of the opinion that  the use of standardisation in the EU is a resounding success and \nthat it is well-established that harmonized standards are of vital importance for the proper functioning \nof the internal market. One example is the use of harmonised standards in the (self-)declaration by \nmanufacturers of the conformity of products with essential regulatory requirements, as laid down in \napplicable EU laws, such as, for example, the Radio Equipment Directive. \n \nStandardisation  boosts  the competitiveness  of  undertakings  in  our  sector –  and  others  –   by \nfacilitating,  in  particular,  the  free  movement  of  goods  and  services,  stimulating  (network) \ninteroperability, and by supporting innovation. Moreover, through coordination with international \nstandardisation bodies, such as the International Telecommunications Union, the EU is at a position \nto use standardisation processes to further the global competitiveness of European companies and to \npromote European values in the development of standards applicable to AI applications worldwide. \nLiberty Global response to the European Commission’s White Paper on Artificial Intelligence - A European approach to \nexcellence and trust – 12 June 2020', 'To facilitate the development of standards in the EU, the Standardisation Regulation provides an \nadequate legislative framework.5 \n \nThe European Commission  can issue  standardisation requests to the official European Standards \nDevelopment Organisations (SDOs). Liberty Global recommends the European Commission to closely \nwork with the SDOs – and to engage them in a timely fashion – to ensure this powerful tool can be \nused to its full extent. \n \nIn the context of the White Paper, standardisation can be particularly valuable in supporting voluntary \nself-labelling/self-certification initiatives in those sectors which have thus far not been identified as \nhigh risk by the European Commission. The application of standards will, contrary to the involvement \nof notified bodies, have less impact on the time-to-market and development costs in such non-high \nrisk domains and greatly facilitate market surveillance. Moreover, the application of standards will \nhelp  ensure a level  playing field  and provide  an  adequate yardstick for  regulators and  market \nsurveillance  authorities  tasked with  ex-post  regulatory oversight. In  situations  where   ex ante \nauthorisation will be required, standards can fulfil a similar role, by laying down objective criteria.  \n \nClosing remarks \nLiberty Global’s key recommendations for the European Commission \nIn closing, Liberty Global summarizes the crux of its contribution to the European Commission’s \nconsultation of its White Paper as follows: \n \n\uf0b7  The European Commission is pursuing the right approach in this domain, with a regulatory \nand investment oriented approach with the twin objective of  promoting the uptake  of  AI \nand  of addressing  the  risks  associated with its application in high-risk domains. \n \n\uf0b7  Next-generating networking will be a key enable for AI applications and vice versa. Moreover, \nfirst-rate connectivity and associated services will contribute vastly to the democratization of \ntechnologies such  as AI, by giving people  and small businesses  access to the necessary \ntechnological resources. Therefore, it is critical to promote investments in the digital domain. \n \n\uf0b7  A risk-based approach, as foreseen by the White Paper, is the right way to promote these twin \nobjectives and will contribute to the functioning of the internal market, if it is applied in a \nclear, consistent and harmonized way. Duplication of existing regulatory institutions and \ninstruments should be avoided, as should – in particular – regulatory divergence between \nMember States.  \n \n\uf0b7  The use of standards will improve the quality and consistency both voluntary self-certification, \nas well as ex ante and ex post regulatory oversight, and should be preferred over the use of \nnotified bodies designated by Member States. The use of standards will contribute vastly to \n                                                             \n5 Regulation (EU) No 1025/2012 of the European Parliament and of the Council of 25 October 2012 on \nEuropean standardisation OJ L 316/12 [2012]. \nLiberty Global response to the European Commission’s White Paper on Artificial Intelligence - A European approach to \nexcellence and trust – 12 June 2020', 'the ecosystem of trust as foreseen by the European Commission, as it enables voluntary self-\ncertification of quality products and services in non-high risk domains or applications, against \nthe same criteria which are mandatory for high risk applications in high risk domains.    \n \nNext steps: ensure close cooperation with key stakeholders \nLiberty Global agrees with the European Commission on the need for close involvement of all relevant \nstakeholders in the context of the development and implementation of the forthcoming European \ngovernance structure on AI. With this in mind, Liberty Global looks forward to engaging further with \nthe European Commission and other public stakeholders in this domain in the near future. \n \nAbout Liberty Global \nLiberty Global (NASDAQ: LBTYA, LBTYB and LBTYK) is one of the world’s leading converged video, \nbroadband and communications companies, with operations in six European countries under the \nconsumer brands Virgin Media, Telenet and UPC. We invest in the infrastructure and digital platforms \nthat empower our customers to make the most of the digital revolution.  \nOur substantial scale and commitment to innovation enable us to develop market-leading products \ndelivered through next generation networks that connect 11 million customers subscribing to 25 \nmillion TV, broadband internet and telephony services. We also serve 6 million mobile subscribers and \noffer WiFi service through millions of access points across our footprint.  \nIn addition, Liberty Global owns 50% of VodafoneZiggo, a joint venture in the Netherlands with 4 \nmillion  customers  subscribing  to  10 million  fixed-line  and 5 million  mobile  services, as well  as \nsignificant investments in ITV, All3Media, ITI Neovision, LionsGate, the Formula E racing series and \nseveral regional sports networks. \nLiberty Global response to the European Commission’s White Paper on Artificial Intelligence - A European approach to \nexcellence and trust – 12 June 2020']"
F550962,10 September 2020,Jussi Mäkinen,Wirtschaftsverband,Technology Industries of Finland,mittel (50 bis 249 Beschäftigte),39705603497-38,Finnland,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"Technology Industries of Finland (TIF) represents more than 1,600 companies, active in Finland in various sectors of technology industries. Our member companies cover both developers and deployers of AI. 

Our main messages are: 

•	There is no one, overarching AI. AI is a set of technologies for processing and extracting value out of data, automating decisions and extracting insights.
•	According to latest studies, solutions and tools of data economy, such as AI will play a major role in the green transformation of the industries. 
•	Existing Product Safety legislation is technology-neutral and has stood the test of time. We urge Commission to carefully analyse whether use of AI would call for a piece of technology-specific piece of regulation?
•	Instead of technology, the attention should be on data and the purpose of its processing. These define the riskiness of the use case. 
•	On assessment of regulatory options, it is wise to apply legislation only when it is needed. However, singling out high-risk cases may prove problematic in terms of predictability and legal certainty. 
•	Green transformation and ramping up productivity of European industries are our major challenges. Technology plays a major role when solutions are developed. To achieve this, we need steady and predictable environment for investments.
•	Europe needs to remain a predictable member in global value networks and not use regulation for short-sighted leaps. Excellence is best built by building on strengths and agile solutions for European data economy. 

Please see the attachment for detailed comments.","['1 (3) \n      \n     \n     \n       9/10/2020   \n \nTechnology Industries of Finland input to the Commission Impact Assessment on AI \n \nTechnology Industries of Finland (TIF) represents more than 1,600 companies, active in Finland \nin various sectors of technology industries. Our member companies cover both developers and \ndeployers of AI.  \n \nOur main messages are:  \n \n\uf0b7  There is no one, overarching AI. AI is a set of technologies for processing and extracting \nvalue out of data, automating decisions and extracting insights. \n\uf0b7  According to latest studies, solutions and tools of data economy, such as AI will play a \nmajor role in the green transformation of the industries.  \n\uf0b7  Existing Product Safety legislation is technology-neutral and has stood the test of time. We \nurge Commission to carefully analyse whether use of AI would call for a piece of \ntechnology-specific piece of regulation? \n\uf0b7  Instead of technology, the attention should be on data and the purpose of its processing. \nThese define the riskiness of the use case.  \n\uf0b7  On assessment of regulatory options, it is wise to apply legislation only when it is needed. \nHowever, singling out high-risk cases may prove problematic in terms of predictability and \nlegal certainty.  \n\uf0b7  Green transformation and ramping up productivity of European industries are our major \nchallenges. Technology plays a major role when solutions are developed. To achieve this, \nwe need steady and predictable environment for investments. \n\uf0b7  Europe needs to remain a predictable member in global value networks and not use \nregulation for short-sighted leaps. Excellence is best built by building on strengths and \nagile solutions for European data economy.  \n \nSupport for the Single Market \n \nTIF agrees with the Commission on the first point of the assessment: the EU-level is the right \nlevel of regulation in order to avoid legal fragmentation. We need streamlined digital single \nmarket, where regulation is in place when and were needed.  \n \nOn the IIA document, the need for regulation is based on lack of trust and potential threats to \nHuman Rights. This leads us to ask: is the link between trust and regulation as imminent as the \nCommission it depicts?  \n \n \nObjectives and Policy Options \n \nHigh level of protection of privacy is one of the core values of EU and it is well established that \nthe level of protection is indeed high in the EU. However, this does not seem to play in favour of \nEuropean (digital) companies. Basically, regulation establishes for the companies a set of \ntechnical and organisational requirements. Regulation is justified and good if these requirements \nlink directly to the objective that is desired and are proportionate to reach that end (e.g. \nrequirements in nuclear power plants to advance nuclear safety). \n \nIt is very challenging to come up with proper set of requirements, though, when the objective is \nas broad as protection of human rights – even more challenging it is when the domain is AI. AI is \nTechnology Industries of Finland  Eteläranta 10, P.O.Box 10, FI-00131 Helsinki \nTelephone +358 9 192 31 \nwww.techind.fi \nBusiness ID: 0215289-2', '2 (3) \n      \n     \n     \n          \n \na technology that can be used in wide array of solutions, it can be used for different purposes and \nfor processing of different sets of data.  \nThe regulatory option of applying full-scale regulation only to high-risk cases (Regulatory option 3 \nb) of application of AI seems plausible at first. But when analysed in greater detail, it becomes \nvery challenging to come up with a general criterion on how to single out cases where the risk is \nhigh. At the end, it is down to the data and the purpose of processing or usage of AI system that \ndefines the risk on a meaningful way. Therefore, we need a framework that helps developers and \ndeployers to identify, evaluate and mitigate the risks involved. In most cases, the risks stem from \nprocessing of personal data or the AI-driven systems/vehicles moving about freely among people, \nlike autonomous cleaning robots or vehicles. On biometric identification (option 3 a) it is not the \ntechnology but the purpose that is decisive: it is ok to use it – properly executed – to make \nthings easier, like opening one’s phone or laptop but most of us feel unease if it is used for mass \nsurveillance.  \n \nWhat are the cross-cutting key prerequisites for trust? People need to know who are in charge of \nthe systems (this is already established in Product Safety Law of the EU), people need to know \nwhether they are interacting with an AI system or a human being, people need to able to \nunderstand what data (and why and how) is being processed. It may also be wise to tune the \nframework on a manner that serves for careful mode of development and running of AI systems \nwhere key decisions are documented – on a way that is justified and proportionate.    \n \nAs written in the assessment, too many requirements are prone to lead to a situation where costs \noutweigh the benefits. Therefore, the regulatory option 1, and in some cases option 2, of the \nassessment are a solid starting point. We do recognise that there might be a need for actual legal \nrequirements such as in cases where AI-driven vehicles interact with humans and human-driven \nvehicles. These need to be carefully analysed, on basis of existing legislation and keeping open \nthe possibility to use existing, preferably technology neutral, regulatory base on a coordinated \nmanner.  \n \nWhen setting up the regulatory framework, the Commission should keep in mind that many \nEuropean AI companies tend to be of SME category, usually concentrating on one specific area of \ntechnology. If legal requirements are numerous, in very high detail and accompanied by \nextensive ex ante conformity assessment mechanisms, the big players will have the advantage. \nTherefore, as a general rule, we favour ex-post solutions for enforcement structures, when \nneeded.   \n \nThe same call for proportionality applies for the scope of future regulation. What would be the \nratio of applying regulation to simple automated decision-making systems? Would this in fact \nproduce an outcome – a relevant set of requirements – which help us to better protect human \nrights? – Or would it actually create an environment that encourages to use secondary solutions, \nsuch as an ever-going chain of ‘if-commands’ instead of adequate solutions.   \n \nThe key requirements for regulation that encourages development of and investments to AI, are \npredictability and proportionality. It should also be remembered that European firms operate on a \nglobal market. The regulatory framework should not establish unfounded requirements that make \ndevelopment of global data solutions and transfers challenging.   \n \n \nImpacts \n \nTIF does agree with the impact assessment as to the proportionality presented on economic \nimpacts and likes to point out that in order to meet the decarbonisation and productivity goals,', '3 (3) \n      \n     \n     \n          \n \nwe need to have proportionate and predictable legal environment to facilitate the necessary \ninvestments to the solutions of data economy.  \n \nAs to the social impacts, it is not only use of AI, but the digitalisation of industries that calls for \nre-skilling of workforce.  \n \nAs to the environmental impacts, there are various studies published in Finland pointing out that \nsolutions of data economy, such as AI, algorithms and data-driven optimisation are the most \nreadily available high-impact methods of bringing down co2 output throughout in the society, \ncoupled to use of fossil free electricity to replace fossil-based processes. According to the TIF \nroadmap to fossil free technology industries by 2035, it is stated that the objective can be \nreached, if we are able to fully deploy the new technologies and the environment is stable enough \nto facilitate the required investments. The option of not being able to use the optimal technology \nshould also be weighed. \n \nAs digital solutions facilitate new ways, we urge the Commission to look new kinds of co-\noperation between regulators and developers. New solutions, such as regulatory sandboxes \nfacilitate transfer of crucial information and building of trust between regulators, developers and \nusers of solutions. \n \nConclusion \n \nIn order to be able to create a future-proof and stable regulatory environment we still need to \nelaborate the objectives of the regulation and have enough fact-base combined to thorough \nanalysis of the actual gaps of the existing regulatory framework. The great challenges or our \ngeneration – climate change and productivity – call for ability to use all available technical \nsolutions on a rational manner in order to guarantee sustainable future for future generations. By \nhaving the facts in order and objectives clear, we can evaluate what is the proportionate and \nbalanced regulatory approach.  \n \n \nFurther information: \n \nHead of Digital Regulation, Jussi Mäkinen Jussi.makinen@techind.fi, +358 40 900 3066']"
F550961,10 September 2020,Marco CANTON,Unternehmen/Unternehmensverband,FUJITSU,groß (250 oder mehr Beschäftigte),441732425040-02,Belgien,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"Fujitsu is one of the leading global ICT companies and the largest in Japan.  Europe is at the heart of our global business.  

1	Protecting fundamental rights

We support the concept of trustworthy AI, based on European values and protecting Fundamental Rights. Fujitsu aims to incorporate ethics in the AI system lifecycle for all ‘high-risk’ AI solutions, consistent with guidance from the EU.
We recommend the European Institutions to create ""An Ecosystem of Trust"" where AI and Ethics can have a key guiding role in the development of new solutions (as set out in the Fujitsu Group AI Commitment ).  
Fujitsu is working on a design-oriented approach, based on our Human-Centric policy. We recognize the importance of ensuring transparency. 

2	Safety

Today, discussions on the safety and risks of AI-powered products have just begun on a sector-by-sector basis, and it is premature to expect then to converge. Today’s discussion is not about the safety requirements that apply to all AIs, but about which cases and sectors AI solutions should be implemented, in particular starting with high-priority areas. 
We would like to emphasize that the scope of the PL Directive should not be expanded to impose liability on AI-based technologies beyond those incorporated into the hardware. Such a change of scope could leave  AI system developers with  the responsibility for problems that they cannot have any control over, and could discourage industries from developing and using AI systems. 

3	Remote biometrics identification

Definition and correct understanding of ‘remote biometric identification’ needs clarification among stakeholders.  
The challenges with remote biometric identification technology, should be discussed separately to the question of its regulation, and without distinguishing between the three perspectives of remote biometrics identification (racial discrimination, abuse and privacy), which might be detrimental to the European market and industry. 
Personal data which biometrics use is already covered in the GDPR, and no new AI-specific regulation is needed. On the other hand, we recognize there is legal uncertainty amongst local regulations defining the use of biometric identification for country safety and security purposes. We strongly believe harmonization would be beneficial. 
We believe it is necessary to create a market environment in which companies that invest in protecting ethics, safety, and privacy do not have a price disadvantage in the market.
 
4	Fujitsu’s opinions on EC’s alternative options to the baseline scenario

Fujitsu would like to stress the importance of balancing innovation and regulation. 
We called it an “AI legislative comfort zone” to overcome the lack of clarity about what is possible and what is not with AI solutions. We support the Commission's proposal to prioritize high-risk AI applications following a risk-based approach based on the existing legislation and standards in sectors. 

In this sense “Option 4”, a combination of soft law, voluntary labelling schemes and EU legislation, establishing mandatory requirements for certain types of AI applications, could be the way forward to tackle the AI related challenges flexibly. 

Labels and standards for AI that are discussed at European level, should be constantly reviewed in cooperation with the international standards bodies, governments and other organizations representing industries, consumers and civil society.

5	Conclusion

Fujitsu strongly encourages the European Commission to pursue an effective AI approach and strategy for the European market, which is able to play a leading role at a global level in strong cooperation and alignment with the Japanese Government. 

We remain convinced that a step-by-step approach, based on the definition of concrete cases and risks, is the baseline. 
A combination of different regulatory tools should be the way forward in order to ensure sufficient flexibility and, at the same time, provide clear rules.
","['Fujitsu Response to the EU Inception Impact Assessment on AI \n \n \n Fujitsu Positioning Paper on the \n \nEU Inception Impact Assessment on Artificial Intelligence \n \n \n \nFujitsu is one of the leading global ICT companies and the largest in Japan.  We have around 129,000 Fujitsu colleagues \nworking with customers in over 180 countries. Fujitsu is committed to investing in R&D with Laboratories and Innovation \nCenters in Japan, Asia, Europe and US.  We use our experience and the power of ICT to shape the future of society with \nour customers. \nFujitsu’s Vision is to enable a Human Centric Intelligent Society that creates value by connecting infrastructure, \nempowering people and creatively defining new forms of intelligence.  We are transforming into a strong and reliable \nDigital Transformation Company by investing in our people and new key technologies such as AI, Blockchain and \nQuantum inspired solutions.  Our ambition is to contribute to the benefit of all citizens and society in line with the UN’s \nSustainable Development Goals by supporting customers from across the public and private sectors. Investing in AI and \nData is critical to achieving this. \nEurope is at the heart of our global business.  We employ 20,000 people and offer a full portfolio of business-technology \nproducts, solutions and services, ranging from workplace systems to datacenter solutions, managed services, and cloud-\nbased software and solutions.  In 20181 Fujitsu signed a long-term research and co-creation program with INRIA in Paris-\nSaclay to develop new AI and machine learning techniques by leveraging advanced mathematics and computing. The \nFujitsu AI Center of Excellence in France is now operational, employing a growing number of researchers and AI experts \ndeveloping R&D projects (including Horizon2020) adopting a co-creation approach with our customers.   \n \nWe value the efforts of the European Commission in creating a common approach to AI in Europe and welcome the \nopportunity to share our thinking on this vital topic. Fujitsu shares the EU’s commitment towards achieving Human \nCentric trustworthy AI2. Fujitsu strongly supports the European Commission’s key objective to foster the development \nand uptake of safe and lawful AI that respects fundamental rights across the Single Market while ensuring inclusive \nsocietal outcomes. Fujitsu, as an AI system developer and provider, is happy to discuss this further with the Commission. \n1  Protecting fundamental rights \nWe support the concept of trustworthy AI, based on European values and protecting Fundamental Rights. Fujitsu \naims to incorporate ethics in the AI system lifecycle for all ‘high-risk’ AI solutions, consistent with guidance from the \nEU. \nFujitsu is a founding member of the AI4People Stakeholders Forum, launched in 2018 at the European Parliament. We \nare currently participating in several committees to develop recommendations for “AI&Ethics” for the Financial and \nAutomotive sectors.  As part of this work, Fujitsu Laboratories is developing joint use cases and banking models focused \non the ethical challenges of AI technologies within the banking and finance sectors. Fujitsu Laboratories of Europe has \nalso been appointed as a senior/advisory member of the European Consultation on Artificial Intelligence (ECAI) High-\nLevel Group. Organised in conjunction with AI4People, the ECAI is heading up an extensive public consultation on AI \n                                                             \n1 Fujitsu Press Release 2018 - here \n2 Fujitsu Group AI Commitment 2019 - here \n \n Uncontrolled if Printed  Page 1 of 5 \nSelect Information Classification Copyright © FUJITSU 2020', 'Fujitsu Response to the Consultation on the “EU” White Paper on AI  \n \nacross 12 European countries, representing the first initiative of this kind, and Fujitsu is pleased to have the opportunity \nto play a key proactive role in helping to shape future policies that reflect public opinion on AI, its Ethics and its \nGovernance. \nIn coherence with the principles and values of the Guidelines of the High-Level Experts Group, Fujitsu supports the \nJapanese Government’s Social Principles of Human-Centric AI3 and the AI principles endorsed at the G20 Ministerial \nMeeting in Tsukuba last year. The European Commission and Japanese Government share many common principles \nand should continue to work together at international level to ensure these principles are endorsed by other countries.  \nWe recommend the European Institutions, Member States and all stakeholders to work together in order to create ""An \nEcosystem of Trust"" where AI and Ethics can have a key guiding role in the development of new solutions (as set out \nin the Fujitsu Group AI Commitment4).  In this regard, Fujitsu had the pleasure to participate in the “Deep dive \nInterview” on the AI and Ethics related assessment list. We had the opportunity to provide ideas to make this tool \neffective for companies of all sizes.  \n \nThrough these activities, we have observed that sometimes AI properties are misleading and do not reveal the great \npotential that AI actually has. Our recommendation to the European Commission is to address the essential issues \nregarding citizens’ safety, health and fundamental rights, through carefully unraveling each one of them and assessing \nthe risks. \nFor example, the issues related to ""discrimination caused by AI"" can be the result of two main challenges: the human \nbias hidden in the ‘learning data’ that humans provide and collect, and the bias due to the way AI systems work. The \nability to ensure the fairness of AI outcomes is still in development but we are convinced that our human-centric \napproach plays an important role in addressing them.  \nThe typical AI development process takes the approach of hypothesizing ""a fair status in the real world"" and narrowing \nthe gap between it and the mathematical model outcome. The idea is that the smaller the gap, the closer the AI \noutcome is to “a fair status in the real world"". However, our Labs found this approach has technical and ethical \nlimitations and the mathematical model cannot be a silver bullet to solve it. In particular, it is difficult to formulate \na basic hypothesis in the first place, because ""a fair status in the real world"" varies according to country, culture, \ngeneration, income group, religion, ethnicity, etc. \n \nTo overcome this barrier, Fujitsu is working on a design-oriented approach, based on our Human-Centric policy. \nWith this approach, we observe society first, then look at fairness, consider the standard, and reproduce a true \nrepresentation with AI. To make it successful, we have set out the Fujitsu Group AI Commitment and are working on \ninstilling it in our members all over the world. In this sense, we are truly committed to engaging with the European \nCommission, EU Member States, Japanese and other Governments to align our policies. \nWe also recognize the importance of ensuring transparency. Interestingly, the case5 where AI can avoid gender \ndiscrimination or creating disadvantages for women, shows that the challenge is actually in the processing and use of \nthe data, and not the classification and identification, (such as ‘men and women’). To this end, the purpose of \ntransparency should not be to disclose all technologies and algorisms. Instead, we believe it is important to make the \nAI system transparent by allowing users to understand it, and third parties to verify the process of the AI systems, \nsuch as how to test the fairness of AI, and what standards are used to assess it.  \n \n                                                             \n3 Japanese government AI Strategy here \n4 Fujitsu Group AI Commitment – 5 Principles here \n5 MIT Techreview There’s an easy way to make lending fairer for women. Trouble is, it’s illegal.’ \nhttps://www.technologyreview.jp/s/172585/theres-an-easy-way-to-make-lending-fairer-for-women-trouble-is-its-illegal/ \n \n Uncontrolled if Printed  Page 2 of 5 \nUnclassified  Copyright © FUJITSU 2020', 'Fujitsu Response to the Consultation on the “EU” White Paper on AI  \n \nWe recommend the EU to encourage AI developers and providers who endeavor to ensure the transparency and \nfairness of AI outcomes and AI users, to choose vendors who implement such measures and exclude those that do \nnot. In the current market, where such mechanisms are lacking, vendors who invest in the transparency and fairness \nof their AI systems, do so at a price disadvantage to those who provide AI systems without such an investment. \nTherefore, few businesses can proactively promote such AI systems. We need to change this situation, and we would \nlike to ask the European Commission to create mechanisms to deploy trusted and human-centric AI. \n \nAs the examples above show, ethical perspectives are still in discussion amongst key stakeholders and we need \ncontinuous effort to discuss AI and Ethics on a case-by-case approach. Indeed, in many business cases our Public \nSector customers have stopped adopting new AI solutions due to the difficulties in proving that particular AI solution \nis transparent, reliable and consequently trustworthy. A “Technical Trial” for AI Algorithms could be a practical idea \nto create more trust, in particular with the Public Sector, by defining minimum baseline requirements and guidelines \nwith the customers. This would help both developers and users in Public Sector to understand if the AI solution they \nare considering is going in the “right direction” and could work both from a legal and technical perspective.  \n2  Safety \nToday, discussions on the safety and risks of AI-powered products have just begun on a sector-by-sector basis, and \nit is premature to expect then to converge. The concept of ""safety"" and “risk”, which is the basis of discussion, differs \ndepending on each customer, such as manufacturing, healthcare, and security. Some of them rely on human judgment \nto ensure safety, while others rely on AI control to ensure safety without compromising on productivity. Therefore, the \ncurrent discussion is not about the safety requirements that apply to all AIs, but about which cases and sectors AI \nsolutions should be implemented, in particular starting with high-priority areas. \nWe look forward to the EU working with stakeholders (Industry associations, consortiums, governments, etc.) to \nadvance discussions on how to ensure safety at global level in order to ensure consistency. \n \nNevertheless, we would like to emphasize that the scope of the PL Directive should not be expanded to impose \nliability on AI-based technologies beyond those incorporated into the hardware. Such a change of scope could leave  \nAI system developers with  the responsibility for problems that they cannot have any control over, and could discourage \nindustries from developing and using AI systems.  \n3  Liability \nWe understand that the characteristics of AI may give rise to the case where it is difficult to trace the damage back to \na person and implement compensation approaches.   \nAI technology itself is neutral and can be a problem or a solution, depending on how it\'s used. Therefore, we believe \nthat placing legal liability on the AI developers could increase the risk of AI technology development itself, despite \nbeing neutral in nature, resulting in a decline in Europe\'s competitiveness in the sector. \n \n Uncontrolled if Printed  Page 3 of 5 \nUnclassified  Copyright © FUJITSU 2020', 'Fujitsu Response to the Consultation on the “EU” White Paper on AI  \n \n4  Remote biometrics identification \nDefinition and correct understanding of ‘remote biometric identification’ needs clarification among stakeholders. \nClassification of remote biometric identification, according to its purpose and use, might help to focus discussions on \nthe essential problem. Remote Biometric Authentication of public spaces is also a neutral technology that can be a \nproblem or solution depending on the application. In particular, we should not forget that facial-recognition \ntechnology contributes to protecting the fundamental rights of EU citizens through maintaining safety and security in \nthe public space. We, as a vender, do not, however, highly rate \'remote-biometrics\' as a form of personal identification.  \nBiometric authentication which uses ‘non-contact devices’ or ‘non-contact multi-biometric identification service system’ \ncannot identify a specific enrolled user within one million. In fact, it is very difficult to identify a single person from \nlarge groups of people using only remote facial-recognition or facial data. In general, individuals can be identified by \ncombining facial data with data collected by other IT systems or even by humans, such as addresses and specific \nactivities. \nThe challenges with remote biometric identification technology, should be discussed separately to the question of its \nregulation, and without distinguishing between the three perspectives of remote biometrics identification (racial \ndiscrimination, abuse and privacy), which might be detrimental to the European market and industry. Personal data \nwhich biometrics use is already covered in the GDPR, and no new AI-specific regulation is needed. On the other hand, \nwe recognize there is legal uncertainty amongst local regulations defining the use of biometric identification for \ncountry safety and security purposes. We strongly believe harmonization would be beneficial.  \n \nIn addition, technologies that enable more trusted biometric authentication, such as technologies that can generate \na plurality of different codes from biometric information, or that can be collated while encrypted, are being developed \none after another. We believe it is necessary to create a market environment in which companies that invest in \nprotecting ethics, safety, and privacy do not have a price disadvantage in the market.  \n5  Fujitsu’s opinions on EC’s alternative options to the baseline scenario \nFujitsu would like to stress the importance of balancing innovation and regulation in order to realize the benefits of \nan advanced and reliable data-driven society, while also ensuring global regulatory harmonization that avoids \nunnecessary fragmentation. We called it an “AI legislative comfort zone”. One of the main reasons that AI solutions \nare not being developed in Europe is due to the lack of clarity about what is possible and what is not. This is preventing \nlot of business discussions with both public and private customers who appreciate the potential of AI solutions in \nsolving their challenges but prefer not to take risks related to regulation compliance.  \nWe support the Commission\'s proposal to prioritize high-risk AI applications following a risk-based approach to address \nthe concerns about the use of AI. The definition and assessment of ""high risk"" should be based on the existing \nlegislation and standards in sectors, and also in international standardization organizations.  \nIn this sense “Option 4”, a combination of soft law, voluntary labelling schemes and EU legislation, establishing \nmandatory requirements for certain types of AI applications, could be the way forward to tackle the AI related \nchallenges flexibly. This is important to ensure an effective level of respect of the European Values, Fundamental Rights \nand rules and at the same time ensuring a high level of technological competitiveness in Europe and cooperation with \nother countries. \n \n Uncontrolled if Printed  Page 4 of 5 \nUnclassified  Copyright © FUJITSU 2020', 'Fujitsu Response to the Consultation on the “EU” White Paper on AI  \n \nIn its 2019 Report entitled “On good AI Governance” the AI4People suggested a number of recommendations. Among \nthem, the need for a “S.M.A.R.T. Model of Governance, for both governments and businesses which is adequate for \ntackling the normative challenges of AI, while being Scalable, Modular, Adaptable, Reflexive, and Technologically-\nsavvy”. The concept of “Co-regulation”, – a type of “middle-out” approach is interesting. It refers to how legal \nregulation and self-regulation interact. A good example is the Regulation 2019/1150 on platform-to-business (P2B) \ntrading practices, where hard law is the basis for corporate codes of conduct and dispute resolution mechanisms, under \npublic sector surveillance and analysis of outcomes, with updates of the hard law requirements as a possible step at \nany time. A combination of top-down and bottom-up approaches with a strong use of developers and users public \nconsultations and participant mechanisms should be the right way to address a huge number of different AI solutions \nwith different levels of related risk.   \nWithin this scenario, an AI regulatory Comfort Zone should be promoted by adopting guidelines, which are shared \nwith businesses and users, and are open to consultations, debates and possible review. Some specific industrial sectors \nalready have a clear set of rules and these must be adapted (if necessary) to the new possible set of rules, \nrequirements, labels and standards for AI. \nLabels and standards for AI that are discussed at European level, should be constantly reviewed in cooperation with \nthe international standards bodies, governments and other organizations representing industries, consumers and civil \nsociety. We note that ISO/IEC JTC 1/SC 42 is drafting the definition of AI and will publish the TR in 2021. \n6  Conclusion \nFujitsu strongly encourages the European Commission to pursue an effective AI approach and strategy for the European \nmarket, which is able to play a leading role at a global level. We appreciated the opportunity to express our viewpoint \non the next possible legal initiatives and on the different options the EU is considering. We remain convinced that a \nstep-by-step approach, based on the definition of concrete cases and risks, is the baseline. A combination of different \nregulatory tools should be the way forward in order to ensure sufficient flexibility and, at the same time, provide clear \nrules. \nThe alignment of key principles for AI and new technologies between the Japanese Government and the European \nUnion is an important common ground for further cooperation at an international level. Fujitsu is committed to \ncontinue  working  closely  with  both  the  Japanese  Government  and  the  Commission  by  providing  advice  and \ncontributions from our experts in order to achieve a global view with a strong presence from both regions, with the \nultimate intent of delivering benefit for our societies and citizens. \n \n Uncontrolled if Printed  Page 5 of 5 \nUnclassified  Copyright © FUJITSU 2020']"
F550949,10 September 2020,Michael STRÜBIN,Wirtschaftsverband,MedTech Europe,klein (10 bis 49 Beschäftigte),433743725252-26,Belgien,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"Artificial Intelligence (AI) in medical technologies has the potential to deliver on the promise of better healthcare in Europe. To optimise the value of AI in the healthcare sector, policies need to remain flexible and follow the evolution of technological development, allowing space for technology to thrive. 

Therefore, we at MedTech Europe, the European trade association for the medical technology industry including diagnostics, medical devices and digital health, are keen to bring today our healthcare expertise and perspective to the AI policy conversation via our attached response to the public consultation on an EU legal act for Artificial Intelligence.

We greatly welcome the aim of the European Commission (EC) to address a number of legal and ethical issues, raised by AI, to foster the development and uptake of AI, and to avoid legal fragmentation across Member States. We agree that European citizens need to have confidence that their medical technologies, with or without an AI component, provide a high level of safety and quality for patients. In the medtech sector, the safety and effectiveness of medical device software, including ""AI with an intended medical purpose"", is already addressed through existing, recently reinforced, regulations, i.e. the Medical Device Regulation (EU) 2017/745 (“MDR”) and the In vitro diagnostics Regulation (EU) 2017/746 (“IVDR”).

It is for this reason, that MedTech Europe supports the idea that, while assessing the need for new legislation specific to AI, the application of existing regulations on AI should be taken into account to understand whether any gaps exist, to avoid overlap and conflicting regulations that could cause issues when AI is introduced.

Read our full response attached.","['MedTech Europe response  \nto the inception impact assessment  \non the Proposal for a legal act with requirements for  \nArtificial Intelligence \nFinal, 10 September 2020 \n \n \nPreface \n \nThis document is in response to the public consultation on the roadmap for artificial intelligence (“AI”) by DG \nCNECT, Unit A2, and addresses directly the Inception impact assessment document Ares(2020)3896535 for \nthe Proposal for a legal act of the European Parliament and the Council laying down requirements for Artificial \nIntelligence. MedTech Europe recently provided input to the public consultation on the White paper on AI \nand we have been following with keen interest the developments in the area of Sectoral Considerations for \nAI by the HLEG AI (we participated as an expert in the Healthcare focused workshops, organised by the \ngroup), as well as the work on the Liability implications of AI). We appreciate the opportunity to provide input \nto this conversation as well, bringing our specific healthcare expertise and perspective, and are open to \nanswer any questions. \n \nIn this document we: \n1)  Outline the specificities of AI in medical technologies & relevant existing sectoral legislation; \n2)  Give some background on the risk management in medtech; \n3)  Discuss the possible scope of the legal initiative;  \n4)  Provide our opinion on the pros and cons of the different policy options suggested in the impact \nassessment from a medical technology perspective; \n5)  Offer a conclusion.  \n \n1.  Introduction & setting the scene \n \nWe understand that through the above-mentioned legislative initiative, the European Commission will aim to \naddress a number of ethical and legal issues raised by AI, with the objective to foster the development and \nuptake of safe and lawful AI and to avoid fragmented regulations in the Member States. MedTech Europe \ngreatly supports and welcomes these goals, which are essential in the healthcare area and more specifically \nin medical technologies.  \n \nThe medical technology (medtech) industry is fully supportive of the need for its medical technologies utilising \nAI to be subject to an appropriate level of supervision, oversight and regulation. We agree that European \ncitizens need to be confident that their medical technologies, with or without an AI component, provide a high \nlevel of safety and quality for patients. It is precisely for this reason that the Medical Device Regulation (EU) \n2017/745 (“MDR”) and In vitro diagnostics Regulation (EU) 2017/746 (“IVDR”) provide a high level of \nprotection of health for patients and users, whilst considering the small- and medium-sized enterprises that \nrepresent most part of the industry. At the same time, the regulations set high standards of quality and safety \nwww.medtecheurope.org  Page 1 of 9', 'for medical technologies in order to meet common safety requirements for such products. Both objectives \nare being pursued simultaneously and are inseparably linked, with one not being secondary to the other.  \n \nAI in medical technologies  \n \nMedical technologies (“medtech”) cover any products, services or solutions used to save and improve \npeople’s lives and which can be used in a care setting, ranging from disposables, diagnostics, capital \nequipment and surgical innovations, to implant technology, biomaterials, sensors, 3D printing, regenerative \nmedicine and connected health IT such as eHealth, mHealth, human genome decoding, disease prediction, \nbiobanks, biomarkers, and many more. \n \nThe medtech industry has long-standing experience with operating in a highly regulated environment, \nincluding stringent self-regulation, in a field dealing with safety- and quality-critical application domains and \nwith exposure to highly sensitive data. The core goal of AI technology in medtech is to save and improve \npeople’s lives. Therefore, safety, quality and ethical requirements (for example, as prescribed by the HLEG \nAI)  are  addressed  throughout  the  development,  deployment  and  life  cycle  of  AI-supported  medical \ntechnologies.  \n \nFor more details on how the medtech sector addresses those requirements (privacy and data governance, \ndiversity, societal well-being, accountability, human oversight, transparency, technical robustness and \nsafety), please refer to our detailed response of November 2019 to the pilot on the Trustworthy AI assessment \nlist, “Trustworthy Artificial Intelligence (AI) in healthcare”. \n \nSoftware as a medical device  \n \nIn most cases in healthcare, AI is a tool and methodology used in the development and functioning of other \nhealthcare products, and not a separate entity of its own. As such, several policies, principles and regulatory \nframeworks currently applying for medical device software also apply for AI (with an intended medical \npurpose). \n \nSoftware with an intended medical purpose is already subject to strict existing regulations. Whether an \nartificial intelligence (AI) solution is embedded in a medical device or is a self-standing medical device \nsoftware, it would be covered by the medtech sectoral regulations, i.e. MDR and IVDR. These regulations \ntrigger considerably stricter legal requirements and liabilities than software without a medical purpose, as the \nmanufacturer must thoroughly substantiate that the software is safe, performs as intended, and delivers a \nclinical benefit. \n \nSafety & performance certification of medical devices (CE marking) \n \nThe principle behind MDR and IVDR is to ensure device performance and patient and user safety, by \nrequiring that the manufacturer demonstrates this to regulatory authorities through the EU CE marking \nprocess. Therefore, all software with an intended medical purpose will have already gone through a rigorous \nwww.medtecheurope.org  Page 2 of 9', 'certification system before being placed on the EU market (regardless of whether it includes AI or not), which \nincludes addressing safety and reliability as part of the design. Monitoring of safety and quality continue when \na product is on the market, through broad post market surveillance and vigilance obligations. \n \nAs such and as previously explained, we believe that, for our specific sector, existing laws and regulations \nsufficiently cover the identified risks of AI with an intended medical purpose. Therefore, we support the \nsuggested approach to exclude specifically regulated sectors from the scope of a new AI-specific legislation, \nin particular in view of avoiding conflicts between the various AI-relevant regulations as well as excessive \nbarriers for the development of AI-supported medical technologies. \n \nManaging risks and addressing liability in AI in medtech \n \nThe medtech sector is already regulated through a system which has its foundations in the identification of \nrisk profiles and the management of risk, and which also builds on the Product Liability Directive’s liability \nregime (Art 10.16). Many requirements that the EU Commission has rightfully identified as being relevant for \nAI are already included in the MDR/IVDR (such as ensuring the safety and the performance of AI that \nconstitutes a medical device or IVD), and as such these requirements are already taken into account in the \ndevelopment and design of the product, via defined requirements regarding processes, evidence collection, \nand risk/benefit assessment and security assessments. In any case, the manufacturer will need to comply \nwith a proper risk management framework, which includes striking a balance between benefit and risk. \n \nIt is also important that any new proposal takes in due consideration existing liability rules applicable for \nmedtech manufacturers (i.e. manufacturing defects; design defects; warning defects). In case of any material \ngaps identified, any “new” AI-related liability risks (including liability of healthcare professionals) need to \nconsider and build on existing rules. \n \nAdditional elements to consider when drafting the future policy approach towards AI in medtech: \n \n•  Legal certainty for businesses: it is essential to consider existing sectoral legislation, in order to \navoid creating legal uncertainty for businesses resulting from incoherence and overlaps, which would \nimpede the development and adoption of AI solutions in healthcare. \n-  Engagement with relevant sectoral authorities: when it comes to governance, it will be essential \nthat the guidance and enforcement of MDR/IVDR on AI systems, which comprise medical devices \nor IVDs, rely on sectoral regulatory authorities and bodies (i.e. MDCG, Notified Bodies) which are \nbest positioned to assess risk and potential management strategies, based on their expertise in the \nhealthcare sector. \n-  Alignment with international instruments: there are various relevant standards, principles & \ncodes of conduct at international level (EU & non-EU). It will be important to map and align with those \nto decrease  international  fragmentation, to support the goal of the  EU Digital  Single Market \nharmonisation and to advance EU competitiveness on the global scene. \n  \nwww.medtecheurope.org  Page 3 of 9', '2.  Considerations on risk management in medtech \n \nIn addition to the above, given the risks identified by the European Commission in the inception impact \nassessment, the medtech industry would like to comment on how these risks are managed under applicable \nregulations in the medtech sector. \n \n▪  Safety: Medical device software, including AI with an intended medical purpose, must fulfil the \nGeneral Safety and Performance Requirements (Annex I) of the MDR and IVDR, which means \nimplementing a design only after the risks have been reduced as far as possible.  \n \n▪  Traceability,  documentation  and  transparency:  In  the  medtech  sector,  traceability  and \ntransparency are assured through detailed technical documentation and retention obligations, and \nthrough labelling obligations. Device design and manufacturing processes are each subject to \ndetailed technical documentation requirements under quality management systems which allow the \ndesign stages applied to an individual device to be understood and all sites and suppliers or \nsubcontractors to be identified. This detailed documentation, which must be retained for a specified \ntime after the lifetime of the device model, must include the manufacturer’s risk management plan to \nreduce risks as far as possible; a benefit-risk analysis for the product; the risk management solutions \nadopted to design, manufacture, monitor and manage risk throughout the Total Product’s Lifecycle \n(TPLC); the validation and verification testing conducted and the pre-clinical and clinical trial data \nincluding in particular data for any software verification and validation as well as data on performance \nand safety in clinical use. Over time the post-market clinical-follow-up, safety and performance data \n(including periodic safety update reports) for the device are added to the technical documentation so \nsafety and performance of the device are monitored over the lifetime of the device. In addition, \ntransparency and documentation is also ensured by manufacturers via compliance with the GDPR \nand in particular the accountability principle laid out in Article 5.2 of GDPR, providing that a data \ncontroller “must be able to demonstrate compliance with paragraph 1 [the other data protection \nprinciples]. This includes the principle that personal data are processed in a transparent manner in \nrelation to the data subject.”  \n \n▪  Usability: The inception impact assessment references the black box nature of AI and suggests \npotential risks related to interpretability. All medical devices and IVDs, including medical device \nsoftware leveraging AI, must undergo usability/human factors testing, and IEC 62366 is the reference \nstandard that addresses usability testing in medtech. Such usability assessments directly address \nthe challenges of interpretability with respect to device risk and overall safety. \n \nThe mandatory processes of clinical or performance evaluation and risk management require that \nthe performance of the AI is understood and validated, in order for the AI to be approved for use as \na medical device, which limits black box risks.  \n \n▪  Risk Management: A key consideration for any AI-based product is appropriate risk management \nboth before and after product launch. For AI solutions with an intended medical purpose, the MDR \nwww.medtecheurope.org  Page 4 of 9', 'and IVDR regulate the AI solution, mandating both pre-market and post-market risk management \nrequirements, and ISO 14971 is the reference standard that developers typically follow for risk \nmanagement. In addition, specific standards for risk management in the design process apply, such \nas IEC 62304. \n \n▪  Post-market Surveillance: The inception impact assessment specifically calls out risks related to \npost-market  surveillance,  but  the  MDR/IVDR  already  include  specific  and  quite  far  reaching \nobligations in that regard, such as developing a post-market surveillance plan appropriate for the \ntype of device, which includes post-market clinical evaluation reports, periodic safety update reports, \nand post-market surveillance reports. \n \n▪  Privacy: Medical device software must conform to GDPR requirements, and these requirements are \nequally applicable to AI-based medical device software. Medtech companies should consider \nincluding a GDPR Data Protection Impact Assessment as part of both the design iteration and risk \nmanagement-driven (re)design cycles. In addition, the GDPR prescribes implementation of privacy \nand security by design. \n \n▪  Cybersecurity: The MDR and IVDR General Safety and Performance Requirements include \nmedical  device  software  security  requirements,  and  EU  guidance  related  to  medical  device \ncybersecurity already exists (for example, MDCG 2019-16 - Guidance on Cybersecurity for medical \ndevices). Data protection-driven security is part of quality management software (QMS) processes. \nThese requirements, guidances, and frameworks are equally applicable to AI-based medical device \nsoftware, and there is no need to create specific frameworks for AI-based medical device software. \n \n▪  Change Management: The inception impact assessment states as a risk that “…such legislation \nfocuses on safety risks present at the time of placing the product on the market and presupposes \n‘static’ products, while AI systems can evolve.” Medical technologies, including medical device \nsoftware  and  ""AI  with  an  intended  medical  purpose"",  are  subject  to  change  management \nrequirements, as described, for example, in MDR/IVDR Annex VII.4.9. Therefore, manufacturers of \nsuch products are already required to assess the impact that changes can have on device safety \nand effectiveness, and they must have the related processes in place. This also applies to continuous \nlearning AI-based medical device software products. Manufacturers must work with their Notified \nBodies to develop processes related to change initiation, assessment, and commercialisation. While \nwe believe guidance related to approaches to change management of AI-based medical device \nsoftware would be beneficial to industry and regulators alike, this should be managed under existing \nregulatory frameworks, specifically, the MDR and IVDR. \n \n3.  Scope of the European Commission’s legislative initiative on AI \n \nAs suggested above and in previous consultations, MedTech Europe supports the idea that, while assessing \nthe need for new legislation specific to AI, the application of existing regulations on AI should be taken into \nwww.medtecheurope.org  Page 5 of 9', 'account to understand whether any gaps exist, to avoid overlap and conflicting regulations that could cause \nissues when AI is introduced.  \n \nAs explained above, in the medtech sector, the safety and effectiveness of medical device software, including \n""AI  with  an  intended  medical  purpose"",  is  already  addressed  through  existing,  recently  reinforced, \nregulations, i.e. MDR and IVDR. Further, data protection, security and privacy considerations related to such \nsoftware are already addressed through GDPR. We believe these laws and regulations provide adequate \nrequirements, and therefore the scope of any proposed AI legislative initiative should not include safety, \neffectiveness, nor privacy considerations related to AI-based medical device software.  \n \nSimilarly, we would advise caution when considering introducing new  generally applicable AI-specific \nlegislation, in particular in view of the risk of creating conflicts between the various AI-relevant regulations, \nand  of  the  need  to  avoid  creating  additional  barriers  for  the  development  of  AI-supported  medical \ntechnologies. In addition, any such legislation should focus on providing definitions and guidance criteria for \nrisk assessment on AI, and, where necessary, could be further developed and adapted per sector or area of \napplication. Should such legislation be considered necessary in view of identified gaps, MedTech Europe \nsuggests an approach to any new regulation to be based on the intended use of the technologies and not on \nthe technology that drives them.  \n \nIn the highly regulated medtech sector, guidance that provides interpretation and describes novel approaches \nto (existing) requirements fulfilment may promote more development efforts and enable developers to more \nreadily navigate the challenging EU regulatory environment for medtech. An example of additional guidance \nin the medtech sector which may be helpful is in the field of documentation. While the performance of \nhealthcare products incorporating AI can often be described with traditional indicators, and existing guidance \ncomprehensively assesses the safety and performance of AI systems, further guidance governing the \nvalidation  and/or  approval  of  such  products  may  be  helpful  to  support  authorities’  assessment  and \nharmonising requirements for manufacturers, particularly with unique  AI features such as continuous \nlearning. Such guidance would further the common understanding, transparency and trust of all stakeholders. \n \nFinally, for any legislative initiative on AI to be effective, we believe that the term ‘high risk AI’ should be \ndefined as narrowly as possible in view of the risk of overlap and incoherence with existing regulatory \nframeworks that also include risk classification (Annex VIII of MDR, Annex VIII of IVDR). \n \n4.  Policy options \n \nThe medical technology industry would like to re-iterate its support for EC’s efforts to promote and drive the \nadoption of driving AI in Europe. More widespread deployment of AI in particular in healthcare will benefit \nEuropean citizens, patients, and healthcare professionals, and advance the sustainability of Europe’s \nhealthcare systems. It will also further the competitiveness of EU industry by requiring AI design and \ndeployment against state of the art standards. \n \nwww.medtecheurope.org  Page 6 of 9', 'In this section, we would like to outline further our views about the different policy options listed in the \nInception Impact Assessment. Considerations are focused on our specific sector even though any additional \nlegislation should only be considered if there is a clear gap, in which case, such gaps should be addressed \nas much as possible as part of existing regulations to ensure consistency (e.g. General Product Safety \nDirective (GPSD)). \n \nOption 0 (baseline scenario)  \n \nGenerally speaking, given the existing regulatory framework for ""AI with an intended medical purpose"" under \nthe EU MDR and IVDR, there may be no need for any specific AI legislative initiative unless the sector-\nspecific gap assessment would reveal a material risk or gap that is not sufficiently addressed by the current \n(and upcoming) MDR and IVDR frameworks.  \n \nOption 1  \n \nMedTech Europe supports many of the principles described in Option 1. Specifically, we support industry-\nled coordination of AI principles specific to medtech and propose that such principles are described within \nguidance rather than “soft law.”  \n \nAs outlined earlier, the medtech sector is already heavily regulated, and this sector includes AI with an \nintended medical purpose. Where appropriate, guidance can be used to further common understanding of \nthese requirements in the context of AI with an intended medical purpose and/or develop approaches \napplicable to AI-based medical device technologies under the existing regulatory frameworks.  \n \nAs a ‘hybrid approach’, option 1 can be used to address part of the need for a ‘legislative initiative’. As an \nexample, “soft law”, such as guidance or industry-led guidelines, may be useful to streamline the regulatory \nprocesses (especially when technology is fast moving). Another area where MedTech Europe sees value in \ndeveloping guidance is on liability. While we believe that the Product Liability Directive (PLD) provides the \nright liability framework for medical technologies, developing guidance – in relation to specific deployment \nscenarios, AI technology concerned (e.g. self-learning and self-adapting) and applications – could clarify \ncertain issues under the Directive such as the roles and responsibilities of parties in an AI value chain \nincluding user and end-user, e.g. by developing case studies. This could include e.g. whether the producer \nof a product will typically be the same entity as the one that is responsible for the safety of the product under \nother legislation (e.g. Medical Device Regulation (MDR) or in vitro diagnostic medical devices regulation \n(IVDR)); whether the producer of a product can be held liable where that product is defective (i.e. does not \npresent the level of safety which consumers are entitled to expect) as a result of a failure in its software, \nincluding in respect of cyber-risks; etc. \n \nThis option may not, at first sight, be considered by some stakeholders as sufficient to ensure citizens’ trust. \nHowever, to the extent that AI in medtech is strictly regulated under the MDR and IVDR in terms of safety \nand the CE-mark is the European symbol of safe and high-quality products, this strengthens patients’ trust \nin the digital development while at the same time enabling innovative and competitive businesses in Europe. \nwww.medtecheurope.org  Page 7 of 9', 'Option 2  \n \nThe medtech industry advises caution towards an approach as per Option 2, which may lead to confusion \nand incoherence in those sectors, such as ours, where there are already compulsory labelling requirements, \nsuch as MDR and IVDR’s CE marking. Such labelling demonstrate conformity with a number of key \nrequirements which are essential for the safe and proper use of the device, including those related to safety, \nperformance, security, etc. Moreover, it is important to underline that labelling can be a significant cost for \ncompanies, especially for items that need frequent/regular labelling changes.  \n \nOption 3 \n \nAs indicated above, in sectors where, as opposed to medtech, comprehensive sectoral regulation does not \nexist and therefore cannot mitigate material AI related risks, it may be appropriate for the European \nCommission to consider AI regulation for (to be defined) high risk AI applications.  \n \nThat said, as noted previously, the medtech industry is already sufficiently regulated and governed by \nmandatory  requirements,  and  MDR/IVDR  address  the  need  to  safeguard  patient  safety  in  view  of \ntechnological progress. Both regulations address risk classes and as such an AI solution in the scope of \nthese regulations may in principle fall in any of the risk categories (low to high).  \n \nIt is important that any new horizontal AI regulation explicitly excludes medical technologies regulated by \nMDR or IVDR, to avoid overlap or otherwise conflict with existing rules designed to evaluate the safety and \neffectiveness of life saving technology in a timely manner. We expect that if such explicit exclusion is not \nmade, horizontal regulation could slow down patient access to AI technology with a medical purpose, as \nexisting design and approval processes may be negatively impacted (also due to potential conflicts with \nmedtech regulation), and/or sector-specific issues that are not appropriately regulated by the medtech \nregulation remain unaddressed as they will not have been individually identified and commented on by \naffected stakeholders. \n \nOur concern is that while the EC recognised that EU legislation may class “risks” differently, it is unclear how \nto reconcile these classifications and thus, any additional legislation based on risk could duplicate or conflict \nwith existing vertical regulatory requirements. In addition, should such new legislation include medtech, it \nmay impact business decisions to invest in AI with an intended medical purpose.  \n \nOption 4  \n \nOption 4 was described to a lesser extent in the Inception Impact Assessment document. As such, this option \nmay have strong benefits, if explained to mean that any legislative initiative on AI fully takes into account the \nwww.medtecheurope.org  Page 8 of 9', 'specialised strengths and knowledge of the sector-specific regulators for the medtech and IVD industry, and \ntherefore: \n-  Focusing on Options 0 and 1 to further address AI ‘with an intended medical purpose’ under the \nMDR/IVDR frameworks; \n-  As appropriate, consider also options 2 and 3 for high risk AI that is not covered by MDR/IVDR.  \n \n5.  Conclusion \n \nIn order to take full advantage of the impact and benefits of AI in the healthcare sector for patients, healthcare \nprofessionals  and  healthcare  systems,  policies  need  to  remain  flexible  and  follow  the  evolution  of \ntechnological development, allowing space for innovation both within big and smaller companies. \n \nWith the MDR and IVDR we already have a strict regulatory framework in place. Industry believes that \nguidance that would provide interpretation and describe novel approaches to  meet the requirements \nfulfilment may promote innovation and competitiveness, and enable developers to more readily navigate the \nchallenging European medical technology regulatory environment.  \n \nCitizen’s trust would help increase the adoption and use of these technologies in healthcare. There will be \nmany factors driving that trust, one of them being the transparency and explainability of the purpose, use, \nbenefits and limitations of AI systems. For example, a well-trained healthcare workforce on AI would help \nincrease that understanding and build trust, in particular in the medtech field where the primary user is the \nhealthcare professional. \n \nWe look forward to the impact assessment, which will follow after the inception impact assessment. \n \n \n \nAbout MedTech Europe \n \nMedTech Europe is the European trade association for the medical technology industry including diagnostics, \nmedical devices and digital health. Our members are national, European and multinational companies as \nwell as a network of national medical technology associations who research, develop, manufacture, distribute \nand supply health-related technologies, services and solutions.  \n \nFor more information, visit www.medtecheurope.org.  \n \n \nFor further enquiries, please contact: \n \nMichael Strübin \nDirector Digital Health  \nm.strubin@medtecheruope.org \nwww.medtecheurope.org  Page 9 of 9']"
F550948,10 September 2020,Christophe TISSIER,Gewerkschaft,CEMA,sehr klein (1 bis 9 Beschäftigte),489575310490-58,Belgien,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"CEMA considers the initiative from the European Commission to deal with ethical and legal requirements on AI must be coordinated to take into account:
-	Initiatives already initiated, for example the revision of the Machinery Directive to take into account new technologies
-	Initiatives initiated at international level to ensure consistency for our manufacturers

CEMA is in favour of Option 1, as it offers the possibility to establish an overall harmonized European framework that takes into account the Human Fundamental Rights and ethical aspects without interfering with specific product regulations that defines e.g. Functional Safety requirements.

For our sector, the Machinery Directive 2006/42/EC already covers safety for those machines using AI-enhancements and/or Machine learning capabilities, since it has technology neutrality as one of its core principles."
F550947,10 September 2020,-,-,-,-,-,-,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"We welcome the opportunity to provide feedback to this Roadmap. The creation of a policy framework that encourages the use of lawful and trustworthy AI in Europe is an important initiative by the European Commission. The uptake of this technology is accelerating fast so clear guidelines and rules are needed. 

We agree with the Commission’s recommendation to adopt as a general principle a ‘risk-based approach’. As stated by the Commission ‘As a matter of principle, the new regulatory framework for AI should be effective to achieve its objectives while not being excessively prescriptive so that it could create a disproportionate burden, especially for SMEs. To strike this balance, the Commission is of the view that it should follow a risk-based approach’ (White paper on Artificial Intelligence, page 17).

A risk-based approach would ensure that companies that develop and deploy low risk AI applications could continue to innovate and invest in this technology without being burdened with unnecessary rules. At the same time, this approach would also ensure that AI applications which may endanger people’s lives or negatively impact fundamental rights would be regulated.

For these reasons, in our opinion the policy option that the Commission should adopt is a combination of options 1, 2 and 3b. EU ‘soft law’ (option 1) and voluntary labelling scheme (option 2) are the most appropriate instruments for low-risk AI applications. Voluntary labelling schemes could be particularly helpful: they would allow companies to demonstrate and promote the uptake of best practices in ethical applications of AI. As rightly suggested by the Commission, these schemes but also other initiatives and standards could be built upon the existing set of Ethics guidelines for trustworthy AI. 

However, an EU legislative instrument seems necessary for high-risk AI applications (3b). The scope of this instrument should be clearly limited to high-risk AI applications.  The criteria to identify and asses the level of risk could be the sector of application and the specific use/impact on rights or safety.

We remain at the Commission’s disposal for any comments or questions."
F550945,10 September 2020,Jasmien CESAR,Unternehmen/Unternehmensverband,Mastercard,groß (250 oder mehr Beschäftigte),58204758673-16,Belgien,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"Mastercard welcomes the opportunity to provide feedback to the European Commission’s Inception Impact Assessment (“IIA”) on an EU legislative initiative for AI.

Mastercard wishes to express its support for Policy Option 4 as outlined in the IIA. We believe the Objectives highlighted by the IIA can be achieved in an appropriate and sufficient manner through a combination of Policy Options 1 (EU soft law (non-legislative) approach to facilitate industry-led intervention), 2 (EU legislative instrument setting up a voluntary labelling scheme) and 3b (EU legislative instrument establishing mandatory requirements for “high-risk” AI applications).

As outlined in our response to the Commission’s Public Consultation on the AI White Paper, we believe that regulatory guidance and/or codes of conduct (for instance on AI auditability) in combination with a voluntary labelling system at EU level allowing AI actors to demonstrate their compliance with an AI normative framework for no-high risk AI applications may serve to establish consumer trust in the use of AI technologies in the EU.

Whereas we support the idea in principle that the introduction of new compulsory requirements should be limited to high-risk applications, this is conditional on key aspects that are not yet clearly delineated, such as the test to determine if an application is “high-risk”, how and when an application will be assessed, and a clear definition of AI. We believe that the high-risk criteria outlined in the White Paper do not allow for the necessary flexibility for market actors and society to benefit from AI without producing onerous requirements. The qualification of certain specific sectors as high-risk does not consider the fact that sectors are constantly evolving and that AI applications may be used across sectors. The qualification of certain uses as “high-risk as such”, e.g. the use of AI for recruitment, is too rigid and ignores the potential benefits the AI application may bring as well as any available measures to mitigate risk.

In relation to an enforcement mechanism to ensure effective compliance with any applicable requirements related to fundamental rights, we believe an appropriate enforcement approach combines an ex-ante self-assessment and an ex-post compliance assessment similar to what is established in GDPR. A self-risk assessment process should be based on the structure and content of a Data Protection Impact Assessment under GDPR (as the use of AI often requires a DPIA) but could include additional AI-specific sections related to bias, explainability and auditability. If a self-risk assessment indicates an AI application is likely to create high risk to fundamental rights, in light of severity and likelihood of potential harm, an organization should take steps to reduce the risk, for instance by implementing technical & organizational controls. If after the remediation steps have been taken, the high risk remains, companies should consult with the relevant competent authorities prior to deploying the AI application. This is similar to the consultation process in Article 36 GDPR.

In relation to an enforcement mechanism for risks to safety, we believe that existing conformity assessment procedures in sectors where established structures already exist (automotive, healthcare, etc.) should be leveraged to address high safety risks engendered by AI applications in these sectors.

Finally, any effective legislative initiative on AI requires a clear definition of the technology. In this respect, it should be considered that certain AI-like applications, e.g. regression analysis, have been around for years without engendering risks that cannot be addressed by the existing regulatory framework. For the purpose of delineating the scope of this legislative initiative, the definition should cover applications entailing additional risks that cannot be addressed by existing legislation, such as unsupervised machine learning."
F550944,10 September 2020,Benjamin LEDWON,Wirtschaftsverband,Bitkom,mittel (50 bis 249 Beschäftigte),535 183 0264 - 31,Deutschland,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"Bitkom welcomes the open consultation approach of the AI Inception Impact Assessment of Assessment of the European Commission for a proposal for a legal act of the European Parliament and the European Council regarding the „requirements for Artificial Intelligence“ (Future regulatory framework for AI in the European Union). The main basis for discussion on the future regulatory framework for AI is the White Paper on AI which Bitkom commented in detail. 

At the outset, we would like to stress that we welcome the objectives of the European Commission in general: Support the EU in becoming leader in AI (chapter 4: ecosystem of excellence) complemented by introducing new safeguards for citizens (chapter 5: ecosystem of trust). In principle, we do not see the need for a specific AI-regulation throughout Europe. Before such regulation is introduced, it should be examined in detail from a legal point of view where there are blank spots on the EU regulation map and where significant restrictions of the digital single market are imposed by regulations in member states. This applies in particular to potential regulation that is explicitly introduced as a consequence of the increased use and dissemination of artificial intelligence in the economy and society. In our view, it has not yet been proven that the considerations made in the paper give rise to a general need for additional and especially horizontal regulation of AI. Furthermore the regulatory framework must be designed as technology neutral as possible.

Conclusion

We would like to underline again as Bitkom that we reject all proposals that would mandate specific requirements on all AI applications (i.e. Option 3c). This would simply not be proportionate or effective towards the Commission’s intended goal. 

We favour a combination of policy options 1 and 3b: The basis should be a “soft law”-approach. This will leave enough room for innovation and the development of AI-solutions , but also allow a precise assessment of the real risks of certain AI applications and potential regulatory gaps. 

If the mapping of existing regulatory frameworks in the sectors considered to be high-risk in the White Paper lead to the detection of blanks spots we recommend a further development of these existing regulatory frameworks and also of the enforcement regimes in these sectors. Therefore we support the introduction of a high-risk scheme as described in the whitepaper in this case in the respective sectors (evolutionary development from scenario 3a scenario 3b regarding the impact assessment scenarios). Regula-tory sandboxes as a lean and agile approach complement this framework and create a framework in which the promotion of innovation is in the foreground.

At least as important as the choice of scenario are the following questions. We have explained this in detail in our statement on the White paper:
Consideration and systematic mapping of existing sectoral regulatory frame-works and existing control, audit and enforcement authorities in the potential high-risk areas. All activities regarding future regulatory frameworks and enforcement authorities must be linked to the existing frameworks. 

There is no agreed mechanism for classifying AI applications as such. If a future regulatory framework plans to regulate AI, the concepts of AI and algorithmic systems must be defined in a way which makes them easy to handle for the eco-nomic operators involved to determine, if a specific data-driven application meets the criteria AI/algorithmic system.Furthermore clear and legally compliant processes that define which applications are high-risk AI applications are needed. 

Clarification of the question and provision of criteria where conformity as-sessements (ex-ante or ex-post) are needed and where self-assessements and self-regulation, for example through Code of Conducts, are sufficient. 

(Please find additional remarks in the attached paper)","['Comments on Impact Assessement \nBitkom comments on Inception Impact Assessement \n \nI.  General Remarks \nBitkom welcomes the open consultation approach of the AI Inception Impact Assessement \nof Assessment of the European Commission for a proposal for a legal act of the European \nParliament and the European Council regarding the „requirements for Artifical Intelli-\ngence“ (Future regulatory framework for AI in the European Union). The main basis for \ndiscussion on the future regulatory framework for AI is the White Paper on AI which \nBitkom \nBitkom commented in detail (Link). \nBundesverband  \n  Informationswirtschaft, \nAt the outset, we would like to stress that we welcome the objectives of the European  Telekommunikation  \nund Neue Medien e.V.  \nCommission in general: Support the EU in becoming leader in AI (chapter 4: ecosystem \n(Federal Association  \nof excellence) complemented by introducing new safeguards for citizens (chapter 5:  for Information Technology, \nTelecommunications and \necosystem of trust). In principle, we do not see the need for a specific AI-regulation \nNew Media) \nthroughout Europe. Before such regulation is introduced, it should be examined in   \n \ndetail from a legal point of view where there are blank spots on the EU regulation map \nLukas Klingholz \nand where significant restrictions of the digital single market are imposed by regula- Big Data & Artifical Intelligence \ntions in member states.  This applies in particular to potential regulation that is explicit- T +49 30 27576 101 \nl.klingholz@bitkom.org \nly introduced as a consequence of the increased use and dissemination of artificial \n \nintelligence in the economy and society. In our view, it has not yet been proven that the  Benjamin Ledwon  \nHead of Brussels Office \nconsiderations made in the paper give rise to a general need for additional and espe-\nOffice +32 2 60953-21 \ncially horizontal regulation of AI. Furthermore the regulatory framework must be de- b.ledwon@bitkom.org \n \nsigned as technology neutral as possible.  \n \n  Albrechtstraße 10 \nII.  Conclusion  10117 Berlin \nGermany \n \nPresident \nWe would like to underline again as Bitkom that we reject all proposals that would \nAchim Berg \nmandate specific requirements on all AI applications (i.e. Option 3c). This would simply \n \nnot be proportionate or effective towards the Commission’s intended goal.   CEO \nDr. Bernhard Rohleder \n \n \nWe favour a combination of policy options 1 and 3b: The basis should be a “soft law”-\n \napproach. This will leave enough room for innovation and the development of AI-\nsolutions , but also allow a precise assessment of the real risks of certain AI applications \nand potential regulatory gaps.  \n \nIf the mapping of existing regulatory frameworks in the sectors considered to be high-\nrisk in the White Paper lead to the detection of blanks spots we recommend a further \ndevelopment of these existing regulatory frameworks and also of the enforcement \nregimes in these sectors. Therefore we support the introduction of a high-risk scheme \n \nwww.bitkom.org', 'Bitkom comments on the White Paper on Artificial Intelligence – A \nEuropean approach to excellence and trust \nPage 2|2 \n \n \n \n \n \nas described in the whitepaper in this case in the respective sectors (evolutionary develo-\npment from scenario 3a scenario 3b regarding the impact assessement scenarios). Regula-\ntory sandboxes as a lean and agile approach complement this framework and create a \nframework in which the promotion of innovation is in the foreground. \n \nAt least as important as the choice of scenario are the following questions. We have ex-\nplained this in detail in our statement on the White paper \n\uf0b7  Consideration and systematic mapping of existing sectoral regulatory frame-\nworks and existing control, audit and enforcement authorities in  the potential \nhigh-risk areas. All activities regarding future regulatory frameworks and en-\nforcement authoritites must be linked to the existing frameworks. \n\uf0b7  There is no agreed mechanism for classifying AI applications as such. If a future \nregulatory framework plans to regulate AI, the concepts of AI and algorithmic \nsystems must be defined in a way which makes them easy to handle for the eco-\nnomic operators involved to determine, if a specific data-driven application \nmeets the criteria AI/algorithmic system.Furthermore clear and legally compliant \nprocesses that define which applications are high-risk AI applications are needed.  \n\uf0b7  Clarification of the question and provision of criteria where conformity as-\nsessements (ex-ante or ex-post) are needed and where self-assessements and \nself-regulation, for example through Code of Conducts, are sufficient. \n\uf0b7  Consideration of the legal conflicts with GDPR. Overall the future regulatory \nframework, together with the evolving European dataspaces and the existing \nregulatory framework like GDPR should be characterised by data sovereignity \nand data availability rather than the existing principles of data thrift and data \navoidance to enable a innovative European data economy. \nBitkom represents more than 2,700 companies of the digital economy, including 1,900 direct members. \nThrough IT- and communication services alone, our members generate a domestic annual turnover of 190 \nbillion Euros, including 50 billion Euros in exports. The members of Bitkom employ more than 2 million \npeople in Germany. Among these members are 1,000 small and medium-sized businesses, over 500 startups \nand almost all global players. They offer a wide range of software technologies, IT-services, and telecommu-\nnications or internet services, produce hardware and consumer electronics, operate in the digital media \nsector or are in other ways affiliated with the digital economy. 80 percent of the members’ headquarters are \nlocated in Germany with an additional 8 percent both in the EU and the USA, as well as 4 percent in other \nregions of the world.  Bitkom promotes the digital transformation of the German economy, as well as of \nGerman society at large, enabling citizens to benefit from digitalisation.  A strong European digital policy \nand a fully integrated digital single market are at the heart of Bitkom’s concerns, as well as establishing \nGermany as a key driver of digital change in Europe and globally. \n \nwww.bitkom.org']"
F550938,10 September 2020,Benoît Stockbroeckx,Wirtschaftsverband,EURALARM,sehr klein (1 bis 9 Beschäftigte),94201247949-87,Belgien,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"Euralarm welcomes the inception impact assessment on Proposal for a legal act of the European Parliament and the Council laying down requirements for Artificial Intelligence. Euralarm shares the view that artificial intelligence (AI) can drive economic growth and improve the security and safety for the benefit of the citizens, the economic actors and the Member States by enabling new products, services and solutions.

For AI-enabled products, Euralarm is in favour of Option 1 of the alternative options to the baseline scenario: “soft law” (non-legislative) approach to facilitate and spur industry-led intervention (no EU legislative instrument). Embedded AI-based applications are considered products both by the EU (1) product safety legislation (before the placing on the market) and in the (2) Product Liability Directive (PLD - after the placing on the market). While the first set of rules imposes essential safety requirements for products to be assessed and thus distributed onto the market, the latter aims at compensating victims for the harm suffered from the use of defective good. Hence Euralarm doesn’t see any need for additional legislation addressed to AI-enabled products. Nevertheless, this “soft law” should be accompanied by provisions ensuring that no national rules would impair the free circulation of these products.

For AI-enabled services, Euralarm is in favour of Option 3.b: EU legislative instrument establishing mandatory requirements for “high risk” applications. The qualification of “high risk” should not be a consequence of an intended use or sector but should be based on a risk analysis. In order to ensure liability, the risk analysis should be guided by a list of clearly identified risks. However, this requires more detailed analysis and discussions with the industry, especially when it comes to the different levels of risks generated by AI applications. In Euralarm’s view, the quality of any future regulation will depend on the possibility to identify a common, transparent and easily applicable definition of “AI” and understanding of “high-risk”. High-risk situations should be defined in cooperation with industry, based on risk-benefit considerations and adjusted when needed.

Euralarm believes that the rollout of AI must not come with the new regulation as per se. The new regulation must be used only where it is necessary (e.g. addressing clear and proven risks) and where it delivers clear benefits (e.g. helps to uptake the new technologies by creating a level playing field, ensures safety etc.). General scrutinising of AI-technologies which hampers innovation and creates uncertainty must be avoided. Clear criteria must be established for identifying critical areas in a way that is legally certain.

Europe’s security and safety industries represent companies that innovate at the crossroads of digital and physical technology. AI offers opportunities to European industries not only for further grow at global level but also to build a safer and more secure Europe, provided that the right choices are made, particularly at EU level, to support its development and deployment.","['Public document \n \n \n \n \nPOSITION PAPER \n \nDate: 10 September 2020 \nSubject: Euralarm feedback on EC Roadmap “Artificial Intelligence – ethical and \nlegal requirements” (draft) \n \nEuralarm welcomes the inception impact assessment on Proposal for a legal act of the \nEuropean Parliament and the Council laying down requirements for Artificial Intelligence. \nEuralarm shares the view that artificial intelligence (AI) can drive economic growth and \nimprove the security and safety for the benefit of the citizens, the economic actors and the \nMember States by enabling new products, services and solutions. \n \nFor AI-enabled products, Euralarm is in favour of Option 1 of the alternative options to the \nbaseline scenario: “soft law” (non-legislative) approach to facilitate and spur industry-led \nintervention (no EU legislative instrument). Embedded AI-based applications are considered \nproducts both by the EU (1) product safety legislation (before the placing on the market) and \nin the (2) Product Liability Directive (PLD - after the placing on the market). While the first \nset of rules imposes essential safety requirements for products to be assessed and thus \ndistributed onto the market, the latter aims at compensating victims for the harm suffered \nfrom the use of defective good. Hence Euralarm doesn’t see any need for additional \nlegislation  addressed  to  AI-enabled  products.  Nevertheless,  this  “soft  law”  should  be \naccompanied by provisions ensuring that no national rules would impair the free circulation \nof these products. \n \nFor AI-enabled services, Euralarm is in favour of Option 3.b: EU legislative instrument \nestablishing mandatory requirements for “high risk” applications. The qualification of “high \nrisk” should not be a consequence of an intended use or sector but should be based on a \nrisk analysis. In order to ensure liability, the risk analysis should be guided by a list of clearly \nidentified risks. However, this requires more detailed analysis and discussions with the \nindustry,  especially  when  it  comes  to  the  different  levels  of  risks  generated  by  AI \napplications. In Euralarm’s view, the quality of any future regulation will depend on the \npossibility to identify a common, transparent and easily applicable definition of “AI” and \nunderstanding of “high-risk”. High-risk situations should be defined in cooperation with \nindustry, based on risk-benefit considerations and adjusted when needed. \n \nEuralarm believes that the rollout of AI must not come with the new regulation as per se. \nThe new regulation must be used only where it is necessary (e.g. addressing clear and \nproven risks) and where it delivers clear benefits (e.g. helps to uptake the new technologies \nby creating a level playing field, ensures safety etc.). General scrutinising of AI-technologies \nwhich hampers innovation and creates uncertainty must be avoided. Clear criteria must be \nestablished for identifying critical areas in a way that is legally certain. \n \nEurope’s security and safety industries represent companies that innovate at the crossroads \nof digital and physical technology. AI offers opportunities to European industries not only for \nfurther grow at global level but also to build a safer and more secure Europe, provided that \nthe  right  choices  are  made,  particularly  at  EU  level,  to  support  its  development  and \ndeployment. \nGD-2020-019']"
F550937,10 September 2020,Catelijne Muller,NRO (Nichtregierungsorganisation),ALLAI,sehr klein (1 bis 9 Beschäftigte),-,Niederlande,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"First and foremost, we would like express our support for the European Commission's efforts to establish an appropriate regulatory framework for AI. In establishing such a framework, one should both look at existing laws and regulations and determine if they are 'fit for purpose' for a world with AI as well as consider establishing new rules where current legislation is not adequate. 

In general, we recommend to broaden the description of the problem that the initiative aims to tackle, i.e. addressing a number of ethical and legal issues raised by AI, and include ""societal issues raised by AI"". In the same spirit, we recommend to broaden the description of the ultimate policy objective of the proposal, i.e. to foster the development and uptake of safe and lawful AI that respects fundamental rights across the Single Market by both private and public actors while ensuring inclusive societal outcomes, so as to include ""fair societal outcomes"". 

The issue of defining the scope of a new legislative initiative for AI is the core element that needs to be addressed. Whereas the Inception Impact Assessment mentions a number of AI-techniques that either should or should not be covered by the instrument, we would like to recommend a different approach toward defining the scope of the instrument: an approach that looks at the level of impact of the technology on people and society at large, rather than (merely) on the technical specifications of a particular AI-system. An impact-level based approach lowers the risk of loopholes that could be exploited. 

As for existing legislation, we call for a broad legal AI stress test, because we see a large number of additional legal lacunae where it comes to AI, that were not mentioned in the Inception Impact Assessment, such as the GDPR, law enforcement, competition law, transportation, trade of dual use technology, medical devices, energy and the environment, to name a few. 

The Inception Impact Assessment lays down 5 policy options ranging from keeping the 'Baseline scenario' to a combination of several policy options. 

ALLAI would be most in favour of a combination of the policy options 2, 3a and 3b as described in the Inception Impact Assessment. This combination would entail soft law for low impact AI applications (or uses) including volulntary labelling, and EU instrument with mandatory labelling covering two elements: (i) clear restrictions, conditions, safeguards and/or boundaries for a limited number of exceptionally impactful AI-applications or uses and (ii) mandatory requirements for medium to high impact AI based on common denominators to determine the level of impact.

Finally, we call for an ex-durante (which would include ex-ante and ex-post mechanisms) mechanism to ensure a continuous, systematic, socio-technical governance approach, looking at the technology from all perspectives and through various lenses. For this we recommend to set up European AI Authority as part of a global framework of AI Authorities. 
","['FEEDBACK ON THE  \nINCEPTION IMPACT ASSESSMENT \n     \n \nProposal for a legal act of the European Parliament and the Council  \nlaying down requirements for Artificial Intelligence \n \n \n \nSeptember 10, 2020 \n \nVirginia Dignum \nCatelijne Muller \nAndreas Theodorou', 'In its Whitepaper on Artificial Intelligence, Europe took a clear stance on AI; \nfoster uptake of AI technologies, underpinned by what it calls ‘an ecosystem of \nexcellence’, while also ensuring their compliance with to European ethical norms, \nlegal  requirements  and  social  values,  ‘an  ecosystem  of  trust’.  The  Inception \nImpact Assessment of a ""Proposal for a legal act of the European Parliament and \nthe Council laying down requirements for Artificial Intelligence"" now presents a \nnumber of objectives and policy options. This paper provides feedback to these \nobjectives and policy options.  \n \nBy Virginia Dignum, Catelijne Muller and Andreas Theodorou \n \n \nExecutive Summary  \n \nFirst and foremost, we would like express our support for the European Commission\'s \nefforts to establish an appropriate regulatory framework for AI. In establishing such a \nframework, one should both look at existing laws and regulations and determine if their \nare \'fit for purpose\' for a world with AI as well as consider establishing new rules where \ncurrent legislation is not adequate.  \n\t\nIn general, we recommend to broaden the description of the problem that the initiative \naims to tackle, i.e. addressing a number of ethical and legal issues raised by AI, and \ninclude ""societal issues raised by AI"". In the same spirit, we recommend to broaden the \ndescription  of  the  ultimate  policy  objective  of  the  proposal,  i.e.  to  foster  the \ndevelopment and uptake of safe and lawful AI that respects fundamental rights across \nthe Single Market by both private and public actors while ensuring inclusive societal \noutcomes, so as to include ""fair societal outcomes"".  \n \nThe issue of defining the scope of a new legislative initiative for AI is the core element \nthat needs to be addressed. Whereas the Inception Impact Assessment mentions a \nnumber of AI-techniques that either should or should not be covered by the instrument, \nwe would like to recommend a different approach toward defining the scope of the \ninstrument: an approach that looks at the level of impact of the technology on people \nand society at large, rather than (merely) on the technical specifications of a particular \nAI-system. An impact-level based approach lowers the risk of loopholes that could be \nexploited.  \n \nAs for existing legislation, we call for a broad legal AI stress test, because we see a large \nnumber of additional legal lacunae where it comes to AI, that were not mentioned in the \nInception Impact Assessment, such as the GDPR, law enforcement, competition law, \ntransportation,  trade  of  dual  use  technology,  medical  devices,  energy  and  the \nenvironment, to name a few.  \n\t\nThe Inception Impact Assessment lays down 5 policy options ranging from keeping the \n\'Baseline scenario\' to a combination of several policy options.  \n \n2', 'ALLAI would be most in favor of a combination of the policy options 2, 3a and 3b as \ndescribed in the Inception Impact Assessment. This combination would entail soft law \nfor low impact AI applications (or uses) including volulntary labelling, and EU instrument \nwith  mandatory  labelling  covering  two  elements:  (i)  clear  restrictions,  conditions, \nsafeguards  and/or  boundaries  for  a  limited  number  of  exceptionally  impactful  AI-\napplications or uses and (ii) mandatory requirements for medium to high impact AI \nbased on common denominators to determine the level of impact. \n \nFinally, we call for an ex-durante (which would include ex-ante and ex-post mechanisms) \nmechanism to ensure a continuous, systematic, socio-technical governance approach, \nlooking at the technology from all perspectives and through various lenses. For this we \nrecommend to set up European AI Authority as part of a global framework of AI \nAuthorities.  \n   \n \nA. Context, Problem Definition and Subsidiarity Check \n \n1. Defining AI for regulatory purposes \n \nWhile we realize that the Inception Impact Assessment is not the place to provide for a \ndefinition of AI, it rightly identifies the need to define the scope of the initiative as a core \nquestion that needs to be answered. We would like to make some remarks on the issue \nof defining AI for regulatory purposes.  \n \nAs  we  already  indicated  in  our  ""Analysis  of  the  EU  Whitepaper  on  Artificial \nIntelligence""1, we want to emphasize that AI is more than data and algorithms, powered \nby Computer Processing Power (CPU). While this is the case for the most widely used AI-\nsystems at present, this is only a very limited description of what AI is. AI is a container \nterm for many computer applications, some of which combine data and algorithms, but \nother,  non-data-driven  AI  approaches,  also  exist,  e.g.  expert  systems,  knowledge \nreasoning and representation, reactive planning, argumentation and others. This should \nbe  kept  in  mind  when  defining  the  scope  of  the  initiative,  to  avoid  over-  or \nunderinclusion.  \n \nIn the same analysis we have elaborated on the fact that there is no universally accepted \ndefinition of AI and how we would describe AI technically. Nevertheless, it should be \nnoted that many of the wide applied AI systems are indeed examples of data-driven AI, \nthat have a number of typical charachteristics that can make them brittle, unstable and \nunpredictable  (such  as  opacity,  correlation  in  stead  of  causality,  insufficient  or  low \nquality data, unclear \'goals\', a lack of common sense, etc.).  \n \n \n \n \n \n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n1 https://allai.nl/wp-content/uploads/2020/06/ALLAI-Final-Analysis-of-the-EU-Whitepaper-on-AI-consultation.pdf \n3', 'Legal versus technical definition - a need to look at the impact of AI \n \nIt is important to realize that legal definitions differ from purely technical definitions \nwhereas legal definitions should meet a number of different or additional requirements2 \nsuch  as  inclusiveness,  preciseness,  comprehensiveness,  practicability,  permanence, \nsome of which are legally binding, and some are considered good regulatory practice3.  \n \nIn general, we feel obliged to emphasise that the focus on a definition through any \nattempts at defining AI-techniques in order to determine what is and what is not AI \ncreates loopholes that could be exploited. This brings us to a number of important \ngroundrules that should be guiding AI regulation and how one should set the scope of \nsuch regulation or, in other words, define AI for regulatory purposes:  \n1.  We  recommend  to  focus  on  the  effects  and  impact  of  the  systems,  not  on  a \nparticular AI-technology/technique.  \n2.  AI-systems are more than just the sum of their technical or software components. AI \nsystems  also  comprise  the  socio-technical  system  around  it.  When  considering \nregulation, the focus should not just be on the technology, but more on the social \nstructures around it: the organisations, people and institutions that create, develop, \ndeploy, use, and control it, and the people that are affected by it, such as citizens \nin their relation to governments, consumers, workers or even entire society.   \n3.  An  ""AI  lifecycle  approach""  should  be  followed,  that  considers  not  only  the \ndevelopment stage of AI, but also the deployment and use stages.  \n4.  It should be kept in mind that most AI-applications currently being used could \nenshrine, exacerbate and amplify the impact on existing laws and fundamental rights \nas well as society at scale, affecting larger parts of society and more people at the \nsame time. \nALLAI Project: Defining AI for Regulatory Purposes \n \nALLAI is currently evaluating the feasibility of setting up a project called ""Defining AI for \nRegulatory Purposes"". Working with a small group of experts and scientists from different \nbackgrounds the project aims to provide guidance for the open core question relating to the \nscope of the initiative, notably how AI should be defined. \n \n \n \n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n2 A Legal Definition of AI Jonas Schuett Goethe University Frankfurt September 4, 2019 (Legal definitions must be: (i) \ninclusive: the goals of regulation must not over- or under-include. (Julia Black. Rules and Regulators. Oxford University \nPress, 1997. [32] Robert Baldwin, Martin Cave, and Martin Lodge. Understanding Regulation: Theory, Strategy, and \nPractice. Oxford University Press, 2nd edition, 2012.); (ii) Precise: it should be clear which case falls under the definition \nand which does not; (iii) Comprehensive: the definition should be understandable by those who are regulated; (iv) \nPracticable: legal professionals should be able to easily determine whether a case falls under the definition; (v) \nPermanent: the need for continued legal updating should be avoided. \n3 Inclusiveness can be derived from the principle of proportionality in EU law (art. 5(4) of the Treaty on European Union. \nThe criteria precision and comprehensiveness are based on the principle of legal certainty in EU law. The criteria \npracticability and permanent are considered good legislative practice. \n4', '2. Problem the initiative aims to tackle \nOn: ""Ultimate objective"" \n\t\nALLAI for the main part supports the ultmate policy objective of the proposal, i.e. to \nfoster the development and uptake of safe and lawful AI that respects fundamental \nrights  across  the  Single  Market  by  both  private  and  public  actors  while  ensuring \ninclusive societal outcomes.  \n\t\nALLAI would however advise to broaden the final part of the ultimate policy objective to \nnot only ensure inclusive societal outcomes, but to also ensure ""fair societal outcomes"". \nAs mentioned in the Inception Impact Assessment, the complexity (and many times \nopacity)  of  certain  systems  and  granular  applicability  of  outcomes  to  individuals  in \ncombination with the scalability of AI systems, presents a range of difficulties as regards \nenforcement of existing legislation meant to protect human rights and could generate \nnew safety risks. On top of that, these characteristics could enshrine, exacerbate and \namplify these risks and adverse impacts on society at scale, affecting more people at the \nsame time. As such there is a serious risk that unfair societal outcomes become ever \nmore  enshrined,  exacerbated  and  amplified,  thus  potentially  leading  to  wider  and \ndeeper  societal  gaps  between  groups  of  people,  propagating  inequality  and,  as  a \nconsequence, entrenching political polarisation. \n\t\nThe addition of fairness to ultimate policy objective would also reflect the European \nCommissions\' adoption of the 7 Requirements of the High Level Expert Group on AI, i.e. \nrequirement no. 5: Inclusiveness, non-discrimination and fairness.  \n\t\nOn: ""Harm caused by AI-systems and risks not covered by existing legislation"" \n \nWe  agree  with  the  indication  that  harm  caused  by  the  use  of  AI  may  be  the \nconsequence of multiple causes. We would like to stress however that the causes for \nharm go beyond just flaws in the technical and digital components (including data) and \ncharachteristics of the system. While these flaws do often play a major and sometimes \ndecisive role in causing harm, one should not forget that even the most technically \nrobust systems can still cause harm.  \n \nImagine a facial recognition system that does recognize people of all colors, genders, \nages, etc. correctly. Technically such a system could be considered robust and non-\ndiscriminatory. Lawfully however, the system could still cause harm. AI-driven (mass) \nsurveillance with facial recognition, involves the capture, storage and processing of \npersonal (biometric) data (our faces), but it also affects our \'general\' privacy, identity and \nautonomy in such a way that it creates a situation where we are (constantly) being \nwatched, followed and identified. As a psychological ‘chilling’ effect, people might feel \ninclined to adapt their behaviour to a certain norm, which shifts the balance of power \nbetween the state or private organisation using facial recognition and the individual.  \n \n \n \n5', 'In  legal  doctrine  and  precedent  the  chilling  effect  of  surveillance  can  constitute  a \nviolation  of  the  private  space,  which  is  necessary  for  personal  development  and \n \ndemocratic deliberation. Even if our faces are immediately deleted after capturing, the \ntechnology still intrudes our psychological integrity.   \n \nThis is just one example where the mere technical elements of a system are less relevant \nas  regards  harm  than  the  actual  use  of  the  system.  That  is  why  we  advocate  to \ncontinuously ask “question zero”: Do we want to allow this particular AI-system and \ntechnique in the first place, or are there reasons not to allow its use at all? And if we \nwere  to  consider  implementing,  deploying  and  using  such  a  system,  what  are  the \nconditions we should set for its use? \n \nIn either case, for the purposes of any regulatory framework we should not merely focus \non technical solutions at dataset or algorithm level, but devise socio-technical processes \nthat help us:  \na)  Understand  the  potential  legal,  ethical  and  social  effects  of  the  AI-system  and \nimprove our design and implementation choices based on that understanding;  \nb)  Audit  our  algorithms  and  their  output  to  make  any  undesirable  outcomes \ntransparent; and  \nc)  Continuously monitor the workings of the systems to mitigate the ill effects of AI.  \nOn: ""Risks not adequately covered by existing legislation"" \nALLAI agrees with the Inception Impact Assessment that there are ample risks that are \nnot  adequately  covered  by  existing  legislation  on  cybersecurity,  protection  of \nemployees and anti-discrimination. The Inception Impact Assessment identifies as main \nissues:  \n•  Effective enforcement of existing EU rules to protect fundamental rights \n•  Application of EU rules on safety \n•  Application of EU the rules on liability \n\t\nAI & the impact on fundamental rights \n \nALLAI strongly commends the focus on the protection of fundamental rights and would \nlike to draw your attention to a report it delivered to the Council of Europe on the \n""Impact of AI on Human Rights, Democracy and the Rule of Law"".4 This report identifies \nthose human rights, as set out by the European Convention on Human Rights (""ECHR""), \nits Protocols and the European Social Charter (""ESC""), that are currently most impacted \nor likely to be impacted by AI. It aims to provide a number of possible strategies that \ncould be implemented simultaneously, ranging from addressing the impact within the \nexisting framework of human rights, democracy and the rule of law to establishing new \nhuman rights should the existing framework fail to adequately protect us. \n \n \n \n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n4 https://rm.coe.int/cahai-2020-06-fin-c-muller-the-impact-of-ai-on-human-rights-democracy-/16809ed6da \n6', ""While  states  are  obliged  to  protect  individuals  and  groups  against  breaches  of \nfundamental  rights  perpetrated  by  other  actors,  appreciation  of  non-state  actors’ \ninfluence on human rights has steadily grown.5 As (large) tech companies have now \nbecome operators that are capable of determining and perhaps altering our social and \neven democratic structures, the impact of their AI(-use) on fundamental rights becomes \nmore prevalent. In this respect, AI might serve as a good opportunity and think of a \nstructure that goes beyond the 'horizontal effect' of the EU Charter of Fundamental \nRights of the European Union. Such a structure could entail a legal obligation for private \nactors to comply with fundamental rights and to grant access to justice if they fail to do \nso.6  \n \nAI & the impact on work \n \nThe Inception Impact Assessment mentions that there are legal issues as regads the \nprotection of employees, but does not yet specify these issues. We would like to stress \nthat he introduction and use of AI in the workplace can cause effects as to health and \nsafety in the workplace, job security, worker privacy, the balance between worker and \nemployer  and  so  on.  For  that  reason,  we  have  been  advocating  early  and  close \ninvolvement of workers and service providers of all types, including freelancers, the self-\nemployed and gig workers − not just people who design or develop AI, but also those \nwho purchase, implement, work with or are affected by AI systems. Social dialogue must \ntake place before the introduction of AI technologies in the workplace, in line with the \nrelevant applicable national rules and practices.  \n \nAdditionally, we would like to draw special attention to AI used in hiring, firing and \nworker assessment and evaluation processes. The White Paper on AI mentions AI used \nin  recruitment  as  an  example  of  a  high-risk  application  that  would  be  subject  to \nregulation irrespective of the sector. We recommend extending this use to include AI \nused in firing and in worker assessment and evaluation processes, but also to explore \nthe common characteristics of AI applications that would make for a high risk use in the \nworkplace, irrespective of the sector. AI applications that have no scientific basis, such \nas  emotion  detection  through  biometric  recognition,  should  not  be  allowed  in \nworkplace environments.  \n \nAdditional legal lacunae \n \nIn addition to the issues identified in the Inception Impact Assessment, ALLAI also \nidentifies a number of additional regulatory lacunae related to existing legislation. \n \nAs AI is evolving quickly and the wider impact of AI on the full acquis has still not been \nfully identified, these lacunae are not exhaustive but below are the areas for attention to \nthe extent that they can be identified today7: \n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n5 Business and Human Rights, A Handbook for Legal Practitioners, Claire Methven O’Brien, Council of Europe  \n6 This also means going beyond merely referring to the Recommendation CM/Rec(2016)3 on human rights and business \nof the Committee of Ministers of the Council of Europe (and the UN Guiding Principles on Business and Human Rights) \n7 See also: STOA Policy Briefing: Legal and ethical reflections concerning robotics, The Scientific Technopolis Group, \n\t \nMaastricht University, Scientific Foresight Unit (DG EPRS) of the European Parliament.\n7"", '1.  GDPR \n2.  Law Enforcement \n3.  Competition law \n4.  Transportation \n5.  Trade of dual-use technology \n6.  Consumer protection \n7.  Healthcare \n8.  Energy & Environment \n \nAd 1: GDPR \n \nA much cited existing EU regulation in the context of AI is the GDPR. This regulation \nprovides frameworks for data protection, the right to an explanation of AI decisions and \nsafeguards for the use of biometric recognition.  \n \nBiometric  recognition:  It  should  be  noted  that  GDPR  restricts  the  processing  of \nbiometric data only to some extent. Biometric data according to the GDPR is “personal \ndata resulting from specific technical processing relating to the physical, physiological or \nbehavioural  characteristics  of  a  natural  person,  which  allow  or  confirm  the  unique \nidentification of that natural person. The last part of the sentence is crucial, because if \nbiometric recognition is not aimed at identification (but for example at categorization, \nprofiling or affect recognition), it might not fall under the GDPR-definition. In fact, recital \n51 of the GDPR says that \'the processing of photographs [is considered] biometric data \nonly  when  processed  through  a  specific  technical  means  allowing  the  unique \nidentification or authentication of a natural person.\' \n \nMany  biometric  recognition  technologies  are  however  not  aimed  at  processing \nbiometric data to uniquely identify a person, but merely to assess a person’s behaviour \n(for example in the classroom) or to categorize individuals (for example for the purpose \nof determining their insurance premium based on their statistical prevalence to health \nproblems). These uses might not fall under the definition of biometric data (processing) \nin the GDPR.  \n \nRight to explanation of ADM, including profiling: Particularly with regard to the right \nto an explanation of automated decisions, a debate is underway about whether the \nGDPR gives a right to an explanation of automated decisions or not. Under the GDPR, \ncontrollers who use personal data to make automated decisions are obliged to inform \nindividuals  in  advance  and  to  provide  meaningful  information  about  the  logic  and \nimportance of the decision-making and the consequences for the data subject. The so-\ncalled art. 29 Working Group recognizes that ""the growth and complexity of machine \nlearning can make it challenging to understand how automated decision-making or \nprofiling works,"" but that, despite this, ""the company [must] find simple ways to tell the \nindividual about the reason, or the criteria on which the decision is based, without \nnecessarily  always  attempting  a  complex  explanation  of  the  algorithms  used  or \ndisclosure of the full algorithm."" \n \n8', ""Also,  the  GDPR  requires  a  controller  to  implement  appropriate  safeguards  when \ndesigning automated decisions, such as the right to human intervention and the right to \nexpress his or her point of view and contest the decision. The recitals of the GDPR also \ninclude the right to an explanation of a fully automated decision. \n \nIt is however debatable whether there is a full right to an explanation of an automated \ndecision in all cases. The fact that the right to explanation is included only in the recitals \nseems to be a hurdle that can be overcome, but in particular the fact that the right to \nexplanation  only  exists  as  regards  to  fully  automated  decision-making,  makes  it \ninsufficient to ensure adequate transparency in the automated decision-making process. \nAfter all, a single human 'check' on an automatic decision (regardless of whether this \nperson has been able to judge the decision on its merits, could lead to the conclusion \nthat an explanation within the meaning of the GDPR would not be necessary.  \n \nIt  must  also  be  assessed  whether  the  GDPR  offers  sufficient  protection  when  the \ndecision is based on non-personal data. AI-driven profiling and the categorization of \npeople or groups of people is  regularly being done by finding inferences made about \nan individual, even without using personal data or resulting in identification of a person. \nThere is no consensus whether these inferences in itself should count as personal data, \nbut there are experts arguing that all data processing that has an impact on people \nshould be protected8 or even that the distinction between personal and non-personal \ndata should be suspended9. \n \nHere the issue also ventures from the GDPR into other fundamental rights such as the \nright non-discrimination, covered in multiple EU directives and the ECHR. \n\t\nAd 2: Law enforcement \n \nIn line with the above, EU regulation such as the Police Directive (EU 2016/680), that \nregulates the processing of personal data in particular for the purpose of profiling by \nlaw enforcement in the Member States, does not adequately cover the issue of profiling \nthat is based on mere inferences rather than personal data, as described above. \n \nAd 3: Competition \n \nMany AI-applications are developed and deployed by a handful of private actors. These \nactors are also present in multiple market segments that are related to AI or use AI, such \nas  finance,  insurance,  etc.  If  too  much  market  power  in  these  different  markets  is \nconcentrated in a few companies, this could lead to unfair competition and difficulty for \nnew (smaller) players to enter the market. \n \n \n \n \n \n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n8 Purtova (2018) \n9 Koops (2014) \n9"", 'Ad 4: Transportation \n \nWhile  autonomous  driving  has  not  yet  fully  materialized,  the  autonomous  vehicles \nindustry is well underway to making vehicles behave more autonomous and needing \nless \'active involvement\' from their drivers.  \n \nThese developments call for a critical review of the various EU regulations and directives \nthat deal with transportation, such as Regulation (EC) 561/2006 and Regulation (EEC) \n3821/85  regarding  driving  and  resting  times  and  digital  tachygraphy  (for  truck \nplatooning),  Directive  2014/45/EU  on  Roadworthiness,  Directive  2010/40/EU  on \nIntelligent Transport Systems in the field of road transport and for interfaces  with other \nmodes of transport and  Directive 2003/59/EC on training and initial qualifications of \nprofessional drivers10. \n \nAd 5: Trade of dual-use technology \n \nFollowing an impact assessment in 2016, a new reform process was started to (a.o.) \nfuture-proof the export control regime for rapidly developing emerging technologies. In \nsum, the main goal of this reform is to control the export of information technologies \nthat can be used for the suppression of human rights, thus increasing the scope and \nscale of dual-use governance. In 2016 the focus was on the inclusion of so called ‘cyber-\nsurveillance technologies’11, but recently AI applications such as facial recognition have \nalso entered this debate as a possible next step for dual-use regulation.  \n \nAd 6: Consumer protection \n \nAt  this  point,  consumers  do  not  receive  adequate  protection  against  unacceptable \nimpact.12 Many  of  these  protections  would  need  to  be  better  covered  in  different \nexistsing legislative instruments. As an example, the EU legal protection of consumers \nagainst  unfair  AI-driven  personalized  pricing  is  mostly  principle-based,  leading  to \nuncertainty on its interpretation as long as there is no clarification in case law.13 \n \nAd 7: Health \n \nIn the Medical Devices Regulation software is considered a medical device, making AI-\ndriven  applications  in  healthcare  subject  to  certain  requirements  and  a  mandatory \nlabelling scheme. Alignment of this regulation with a new legislative instrument on AI is \nimportant.  \n \n \n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n10 Also: Directive 2009/103/EC on motor vehicle insurance; Directive 2007/46/EC on vehicle approval; Directive \n2006/126/EC on requirements for driving licences \n11 Immenkamp, 2019 \n12 BEUC, EU Rights for Consumers \n13 de Streel, Alexandre; Jacques, Florian (2019) : Personalised pricing and EU law, 30th European Conference of the \nInternational Telecommunications Society (ITS): ""Towards a Connected and Automated Society"", Helsinki, Finland, 16th-\n19th June, 2019, International Telecommunications Society (ITS), Calgary \n \n10', 'Also, now that the Medical Devices Regulation has been postponed to enter into effect \nin  May  2021,  existing  legislation,  and  the  current  practice  of  assessment  and \ncertification of AI-diven healthcare devices, should be critically looked at.  \n \nAd 8: Energy & Environment \n \nThe  High  Level  Expert  group  on  AI  (HLEG  AI)  has  argued  that  sustainability  and \necological responsibility of AI systems should be encouraged and has made ""Societal \nand Environmental Well-being"" one of the 7 requriements for Trustworthy AI of its \nEthics Guidelines for Trustworthy AI. \nAccording to the HLEG AI, it must be ensured that AI-systems operate in the most \nenvironmentally friendly way possible. The system’s development, deployment and use \nprocess, as well as its entire supply chain, should be assessed in this regard, e.g. via a \ncritical examination of the resource usage and energy consumption during training, \nopting for less harmful choices. Measures securing the environmental friendliness of AI \nsystems’ entire supply chain should be encouraged. This could trigger a necessery \nreview and possible adaptation of EU engery regulations such as Directive 2010/30/EU \non the indication by labelling and standard product information of the consumption of \nenergy and other resources by energy-related products.  \n3. AI stress test for EU regulation \n \nThe foregoing calls for a much broader AI stress test for existing EU regulation. Three \nquestions need to be answered in particular: \n1.  To what extent are the policy and legal objectives underlying these regulations \naffected by AI systems and in what ways? \n2.  What  are  the  existing  monitoring,  information  gathering  and  enforcement \nframeworks capable of providing meaningful and effective oversight to ensure that \npolicy and legal objectives are still effectively achieved? \n3.  To  what  extent  does  existing  legislation  work  in  a  way  that  they  promote  and \nsafeguard the ethical principles and requirements (as described in the AI HLEG\'s \nEthics Guidelines for Trustworthy AI)? \n \nALLAI points out that the AI HLEG has already advised the European Commission to \nperform this \'stress test\' at EU level, in order to arrive at an unambiguous regulatory \nframework across Europe.  \n \nIt should be noted that EU legislation does not affect all national legislative areas. \nWhere the EU has no legislative competence, ALLAI advises the Dutch legislator to \ninitiate the stress test at national level. Think of elements of labor law, social security \nlaw, administrative law and criminal law. \n \n \n \n \n11', '4. Basis for EU intervention \n \nALLAI fully agrees with the Inception Impact Assessment\'s reasoning that the objectives \ndescribed cannot be reached effectively by Member States alone, but can be better \nreached at Union level. We agree that it is important that fragmantation would prevent \nthe free circulation of goods and services containing AI and thus negatively affect the \nDigital  Single  Market.  Fragmentation  would  also  lead  to  divergence  in  levels  of \nprotection of citizens and society against the abovementioned harms and risks, which \ncould lead to an unacceptable ""race to the bottom"" of AI regulation and protections in \nan effort to attract more AI investment.  \n \n \nB. Objectives and Policy Options \n1. Objective \nALLAI supports the overall objective of the instrument, i.e. to ensure the development \nand uptake of lawful and trustworthy AI across the Single Market through the creation of \nan ecosystem of trust.  \nALLAI would like to suggest the following additional aims of the initiative:  \n•  To  fill  any  legal  lacunae  either  regarding  the  effectivenes,  applicabilty  or \nenforceability of existing EU law and where no EU law exists, so as to ensure that \noverall EU policy and legal objectives as regards trustworthy AI are promoted and \nsafeguarded; \n•  In addition to what is expressed in aim (c) we strongly recommend to broaden this \naim so as to include risks for people and society.  \n•  As part of aim (c) and aim (e) it is important to be able to effectively monitor the \nfuture developments of AI within and outside the Single Market, so as to make sure \nthat new opportunities for trustworthy AI are identified and promoted, but also that \nnew challenges are adequately and timely addressed.  \n•  In addition to aim (e), we suggest to set up a structure that includes not only the \nrelevant authorities in the Member States, but also all relevant stakeholders, such as \nworkers\'  and  business\'  representatives,  other  civil  society  organisations,  NGO\'s, \nacademia (various disciplines), policy makers, etc.  \n•  As an additional aim we reccommend to set up a European AI Authority as part of a \nglobal framework of AI Authorities. Such a framework could be set up as folows:  \no  A global AI Authority; \no  Several regional sub-authorities (e.g. EU, the Americas, Asia, Oceanea, Middle \nEast); \no  National  executive  authorities  (either  existing  or  new),  for  the  EU  to  be \nappointed or set up by the Member States. \n \n \n \n12', 'The roles and responsibilities of these authorities should be carefully considered, \nbut they should have broad expertise of the different elements and impact domains \nof AI, including but not limited to technical, legal and ethical expertise, as well as \nknowledge of behavioural effects, labour market effects, economic and societal \neffects of AI.   \n2. Policy Options \nOn Option 0: ""Baseline \nALLAI agrees that the current ""Baseline"" or Option ""0"" does not suffice to adequately \naddress the risks and potential harms connected to AI and thus considers this option not \nviable.  \nOn Option 1: ""EU soft law"" \nThe option of EU soft law should only apply to AI-systems or uses that have no adverse \nimpact  on  people,  society  or  the  environment  nor  on  our  (fundamental)  rights, \ndemocracy and the rule of law.  \nThe  two  factor  approach  that  was  suggested  by  the  European  Commission  in  its \nWhitepaper on AI does however not suffice to determine these types of AI-systems. As \nan example consider targeted online advertising. The Commission will likely qualify \nadvertising as a low-risk sector, and AI-driven targeted adds as a low risk AI-application. \nTargeted advertising however, has shown to have a potential segregating and dividing \neffect.  \nThis is the reason why we recommend looking at the level of societal or personal impact \nof an AI-system or use to determine the risk level of the system. \nOn Option 2: ""EU legislative instrument setting up a voluntary labelling scheme"" \n \nThe labelling scheme is not a standalone option, but could be split into two options, the \nfirst to be joined with option 1 and second to be joined with option 3.  \n \nWhere EU soft law would suffice (option 1), a voluntary labelling scheme could be an \ninteresting  addition  for  industry  players  to  gain  competitive  advantage,  or  to  feel \nconfident to define or explore a niche area of application.  \n \nWhere  an  EU  legislative  instrument  establishing  mandatory  requirements  for  AI  is \nnecessary (option 3), a mandatory labelling scheme (like the CE-marking) could be \nconsidered to avoid any untrustworthy AI being deployed on the Digital Single Market. \nCurrent  practices  around  certification  of  AI,  such  as  those  already  executed  in  the \nhealthcare sector, should be carefully reviewed, to see if those practices are sufficient, or \nshould be amended or replaced by a new mandatory labelling scheme to avoid overlap \nwith possible new labelling scheme.  \n \n13', 'Labels should not merely refer to the technical characteristics of the system, but more \nimportantly also to the effects and impacts of the system. \n \nOn Option 3:""EU legislative instrument establishing mandatory requirements for \nAI"" \n \nALLAI  is  in  favour  of  introducing  an  EU  legislative  instrument,  in  the  form  of  a \ncombination of sub-options (a) and (b).  \nExceptionally impactful AI \nThe following AI-systems or uses that are considered to be too impactful could give rise \nto the necessity of a ban, moratorium, strong restrictions or conditions for exceptional \nand controlled use: \n•  Indiscriminate use of facial recognition and other forms of biometric recognition \neither by state actors or by private actors; \n•  AI-powered  mass  surveillance  (using  facial/biometric  recognition  but  also  other \nforms of AI-tracking and/or identification such as through location services, online \nbehaviour, etc.); \n•  Personal, physical or mental tracking, assessment, profiling, scoring and nudging \nthrough biometric and (online) behaviour recognition in violation of fundamental \nrights (AI-enabled Social/Citizen Scoring); \n•  Covert AI systems and deep fakes; \n•  Implanted human-AI interfaces; \n \nExceptional use of these technologies, such as for national security purposes or medical \ntreatment or diagnosis, could be allowed but should be evidence based, necessary and \nproportionate and only be executed in controlled environments or cleary identified \ncontexts and (if applicable) for limited periods of time. \n \nAs regards biometric recognition14 (including facial recognition) we recommend that \nany  use  of  biometric  recognition  only  be  allowed  under  the  followoing  cumulative \nconditions: i) there is a scientifically proven effect; (ii) it is used in controlled environment \n(e.g. a hospital); (iii) it is used under strict conditions (e.g. limited in time, for a specific \npurpose,  etc.).  Widespread  and/or  public  use  of  AI-driven  biometric  recognition  to \nsurveil,  trace,  track,  assess  or  categorise  humans  or  human  behaviour  or  emotions \nshould not be allowed. \n\t\nAs regards AI-driven mass surveillance we refer to the recommendation of the HLEG \nAI in its Ethics Guidelines for Trustworthy AI that automatic identification raises strong \nconcerns of both a legal and ethical nature, as it may have an unexpected impact on \nmany psychological and sociocultural levels.  \n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n14 Biometric recognition of micro-expressions, gait, (tone of) voice, heart rate, temperature, etc. is being used in various \nways, one of which is to assess or even predict our behaviour, mental state, and emotions. As Barret et al. (Emotional \nExpressions Reconsidered: Challenges to Inferring Emotion From Human Facial Movements, 2019) have shown however, \nno sound scientific evidence exists to suggest that a person\'s inner emotions or mental state can be accurately \'read\' \nfrom their facial expression, gait, heart rate, tone of voice or temperature, let alone that (future) behaviour could be \npredicted by it. \n14', ""A proportionate use of control techniques in AI is needed to uphold the autonomy of \nEuropean  citizens.  Clearly  defining  if,  when  and  how  AI  can  be  used  for  mass \nsurveillance both by public or private actors, differentiating between the identification of \nan  individual  versus  the  tracing  and  tracking  of  individual,  will  be  crucial  for  the \nachievement of Trustworthy AI. \nAs regards AI-enabled Social Scoring we refer to the recommendation of the HLEG AI \nin its Ethics Guidelines for Trustworthy AI, that 'any form of citizen scoring should only \nbe used if there is a clear justification, and where measures are proportionate and fair. \nNormative  citizen  scoring  (general  assessmentof  “moral  personality”  or  “ethical \nintegrity”) in all aspects and on a large scale by public authorities or private actors \nendangers  these  values,  especially  when  used  not  in  accordance  with  fundamental \nrights, and when used disproportionately and without a delineated and communicated \nlegitimate purpose.' The HLEG AI also argues 'that citizen scoring – on a larger or \nsmaller scale – is already often used in purely descriptive and domain-specific scorings \n(e.g.  school  systems,  e-learning,  and  driver  licences).  Even  in  those  more  narrow \napplications,  a  fully  transparent  procedure  should  be  made  available  to  citizens, \nincluding information on the process, purpose and methodology of the scoring.' It also \nnotes that 'mere transparency cannot prevent non-discrimination or ensure fairness, and \nis not the panacea against the problem of scoring and that ideally the possibility of \nopting out of the scoring mechanism without detriment should be provided – otherwise \nmechanisms for challenging and rectifying the scores must be given. This is particularly \nimportant in situations where an asymmetry of power exists between the parties. Such \nopt-out options should be ensured in the technology’s design in circumstances where \nthis is necessary to ensure compliance with fundamental rights and is necessary in a \ndemocratic society.' \nAs regards covert AI systems and deep fakes we again refer to the recommendation \nof the HLEG AI in its Ethics Guidelines for Trustworthy AI that 'human beings should \nalways know if they are directly interacting with another human being or a machine, and \nit is the responsibility of AI practitioners that this is reliably achieved. AI practitioners \nshould therefore ensure that humans are made aware of – or able to request and \nvalidate the fact that – they interact with an AI system (for instance, by issuing clear and \ntransparent disclaimers ). Note that borderline cases exist and complicate the matter \n(e.g. an AI-filtered voice spoken by a human). It should be borne in mind that the \nconfusion between humans and machines could have multiple consequences such as \n \nattachment, influence, or reduction of the value of being human.'  \nAs  regards  Implanted  human-AI  interfaces  such  as  Elon  Musk's  Neuralink  brain \nimplant give rise to a plethora of legal (in particular as regards to (fundamental) rights \nthat protect physical and psychological integrity, autonomy, free will, privacy etc.) and \nethical concerns that should be carefully assessed and addressed in order to establish \nthe appropriate regulatory framework. \n \n15"", 'Common denominators for determining the level of impact of AI \nFor all other AI applications, mandatory requirements on issues such as robustness, \naccuracy  and  reproducibility,  traceability,  transparency,  human  oversight  and  data \ngovernance15 could be set, relative to the degree of impact of the AI-application.  \nWe would like to stress again that these mandatory requirements should not be aimed \nmerely at the data used to train and feed the AI-system, but also at the model(s) and \nalgorithm(s) that comprise the system. These requirements should be met prior to the \ndeployment of the system and be maintained throughout the use of the system.  \nWe already expressed our concerns on the suggested \'two-factor\' approach to identify \n\'high risk AI\' (high risk application + high risk sector) that would then be subject to \nmandatory requirements. We also expressed our concerns on the list based approach of \nAI-applications that would be considered high risk \'as is\', i.e. irrespective of the sector. \nFor both structures, we fear that multiple AI applications or uses could fall (or actively be \nkept) outside the scope of the legislative initiative.16 \n\t\nIn  stead  of  the  two-factor  approach,  we  recommend  to  determine  common \ndenominators for AI applications or uses that are to be considered high risk, or \nrather medium or high impact, and would fall within the scope of the legal instrument. \nThe following elements could serve as a guidance to set such common denominators: \n•  Common denominators should be determined based on the impact of the system \non people and/or society, rather than (merely) on the technical characteristics of the \nsystem, or the sector in which the system is being used; \n•  \'AI impact\' is to be considered both at individual and at societal/collective level \nwhereas AI can impact both the individual as well as larger parts of our collective \nsociety; \n•  Context, severity, scale and likelihood of the impact is important to determine the \nappropriate  and  proportionate  mandatory  requirements,  which  could  result  in \ndifferent requirements for different levels of impact; \n•  For high impact AI applications that generate unacceptable risks or pose threats of \nharm or systemic failure that are substantial, a precautionary and principle-based \nregulatory approach should be adopted; \n•  For  medium  impact  AI  applications  a  risk-based  approach  could  be  more \nappropriate.  \n \nOn Option 4: ""A combination of either or all of the previous options""  \n \nAs can be concluded from the above, ALLAI would be most in favor of a combination of \nthe policy options described in the Inception Impact Assessment. In short we would \nenvisage the following structure:  \n•  Soft law for low impact AI applications, including voluntary labelling \n•  EU instrument with mandatory labelling covering: \n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n15\tWe have elaborated on these requirements in our Final Analysis of the EU Whitepaper on AI, ALLAI (2020) \n16 Final Analysis of the EU Whitpaper on Artificial Intelligence, ALLAI (2020) \n16', 'o  Clear restrictions, conditions, safeguards and/or boundaries for a limited number \nof exceptionally impactful AI-applications or uses; \no  Mandatory requirements for medium to high impact AI based on common \ndenominators to determine the level of impact. \nOn: ""Ex-ante and/or ex-post enforcement mechanisms"" \nWhile  ex-ante  and  ex-post  enforcement  mechanisms  are  necessary,  in  our  opinion, \ntrustworthy  AI  primarily  needs  an  ex-durante  mechanism  to  ensure  a  continuous, \nsystematic, socio-technical governance approach, looking at the technology from all \nperspectives and through various lenses.  \nThis requires a multidisciplinary approach where policy makers, academics from a variety \nof  fields  (AI,  data-science,  law,  ethics,  philosophy,  social  sciences,  psychology, \neconomics, cyber security), social partners and NGO’s work together on an ongoing \nbasis.  \nIt also requires socio-technical governance processes and structures that facilitate:  \n•  Understanding of the potential legal, ethical and social effects of the AI-system and \nimprove the governance/legislative choices based on that understanding;  \n•  Monitoring of the workings and developments of AI to address and mitigate any \nnew ill effects. \nWe thus call for a European AI Authority, as part of a global stucture of AI Authorities as \nmentioned above under B.1. \n   \nC. Preliminary Assessment of Expected Impacts  \nWe agree that SME\'s should not be excluded from the application of the regulatory \nframework, precisely because of the high scalability of AI-systems.  \nAs regards societal impact, we would like to draw attention to the fact that is still not \nclear whether AI will cause major loss of jobs or that this loss outweighs the number of \nnew jobs it could bring.  \n \nThe maintenance or acquisition of AI skills is necessary in order to allow people to adapt \nto the rapid developments in the field of AI is therefore important. But policy and \nfinancial resources will also need to be directed at education and skills development in \nareas that will not be threatened by AI systems (i.e. tasks in which human interaction is \nvital, such as public interest services related to health, safety and wellbeing of people \nand based on trust, where humans and machines cooperate, or tasks we would like \nhuman beings to continue doing). \n\t\nAs for environmental impacts, we expect that if energy regulation would be adapted to \nalso cover AI, legislation could have a positive environmental impact.  \n ∞ \n17', 'Authors \nVirginia Dignum is professor of Artificial Intelligence at Umeå University, program director of the \nWallenberg AI, Autonomous Systems and Software Program – Humanities and Society (WASP-HS), co-\nfounder of ALLAI, member of the European High Level Expert group on AI and of the World \nEconomic Forum AI Board, and currently working as an expert advisor for UNICEF. \nCatelijne Muller is co-founder and president of ALLAI, member of the European High Level Expert \ngroup on AI, Rapporteur on AI for the European Economic and Social Committee, and currently \nworking as an expert advisor for the Council of Europe on AI & Human Rights, Democracy and the \nRule of Law. \nAndreas Theodorou is postdoctoral researcher on Responsible AI at Umeå University, member of the \nAI4EU consortium, member of the external ethics board of the ROXANNE project, and committee \nmember on the IEEE Standards Association P70xx series of standards on AI. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nALLAI refers to Stichting ALLAI Nederland, a foundation under Dutch Law. No entity or person connected \nto ALLAI, including its Board Members, Advisory Board Members, employees, experts, volunteers and \nagents, is responsible or liable for any direct or indirect loss or damage suffered by any person or entity \nrelying wholly or partially on this communication.  \n \n© 2020 ALLAI, The Netherlands \n18']"
F550936,10 September 2020,-,-,-,-,-,-,Künstliche Intelligenz – ethische und rechtliche Anforderungen,The feedback on behalf of ALLAI is attached.,"['FEEDBACK ON THE  \nINCEPTION IMPACT ASSESSMENT \n     \n \nProposal for a legal act of the European Parliament and the Council  \nlaying down requirements for Artificial Intelligence \n \n \n \nSeptember 10, 2020 \n \nVirginia Dignum \nCatelijne Muller \nAndreas Theodorou', 'In its Whitepaper on Artificial Intelligence, Europe took a clear stance on AI; \nfoster uptake of AI technologies, underpinned by what it calls ‘an ecosystem of \nexcellence’, while also ensuring their compliance with to European ethical norms, \nlegal  requirements  and  social  values,  ‘an  ecosystem  of  trust’.  The  Inception \nImpact Assessment of a ""Proposal for a legal act of the European Parliament and \nthe Council laying down requirements for Artificial Intelligence"" now presents a \nnumber of objectives and policy options. This paper provides feedback to these \nobjectives and policy options.  \n \nBy Virginia Dignum, Catelijne Muller and Andreas Theodorou \n \n \nExecutive Summary  \n \nFirst and foremost, we would like express our support for the European Commission\'s \nefforts to establish an appropriate regulatory framework for AI. In establishing such a \nframework, one should both look at existing laws and regulations and determine if their \nare \'fit for purpose\' for a world with AI as well as consider establishing new rules where \ncurrent legislation is not adequate.  \n\t\nIn general, we recommend to broaden the description of the problem that the initiative \naims to tackle, i.e. addressing a number of ethical and legal issues raised by AI, and \ninclude ""societal issues raised by AI"". In the same spirit, we recommend to broaden the \ndescription  of  the  ultimate  policy  objective  of  the  proposal,  i.e.  to  foster  the \ndevelopment and uptake of safe and lawful AI that respects fundamental rights across \nthe Single Market by both private and public actors while ensuring inclusive societal \noutcomes, so as to include ""fair societal outcomes"".  \n \nThe issue of defining the scope of a new legislative initiative for AI is the core element \nthat needs to be addressed. Whereas the Inception Impact Assessment mentions a \nnumber of AI-techniques that either should or should not be covered by the instrument, \nwe would like to recommend a different approach toward defining the scope of the \ninstrument: an approach that looks at the level of impact of the technology on people \nand society at large, rather than (merely) on the technical specifications of a particular \nAI-system. An impact-level based approach lowers the risk of loopholes that could be \nexploited.  \n \nAs for existing legislation, we call for a broad legal AI stress test, because we see a large \nnumber of additional legal lacunae where it comes to AI, that were not mentioned in the \nInception Impact Assessment, such as the GDPR, law enforcement, competition law, \ntransportation,  trade  of  dual  use  technology,  medical  devices,  energy  and  the \nenvironment, to name a few.  \n\t\nThe Inception Impact Assessment lays down 5 policy options ranging from keeping the \n\'Baseline scenario\' to a combination of several policy options.  \n \n2', 'ALLAI would be most in favor of a combination of the policy options 2, 3a and 3b as \ndescribed in the Inception Impact Assessment. This combination would entail soft law \nfor low impact AI applications (or uses) including volulntary labelling, and EU instrument \nwith  mandatory  labelling  covering  two  elements:  (i)  clear  restrictions,  conditions, \nsafeguards  and/or  boundaries  for  a  limited  number  of  exceptionally  impactful  AI-\napplications or uses and (ii) mandatory requirements for medium to high impact AI \nbased on common denominators to determine the level of impact. \n \nFinally, we call for an ex-durante (which would include ex-ante and ex-post mechanisms) \nmechanism to ensure a continuous, systematic, socio-technical governance approach, \nlooking at the technology from all perspectives and through various lenses. For this we \nrecommend to set up European AI Authority as part of a global framework of AI \nAuthorities.  \n   \n \nA. Context, Problem Definition and Subsidiarity Check \n \n1. Defining AI for regulatory purposes \n \nWhile we realize that the Inception Impact Assessment is not the place to provide for a \ndefinition of AI, it rightly identifies the need to define the scope of the initiative as a core \nquestion that needs to be answered. We would like to make some remarks on the issue \nof defining AI for regulatory purposes.  \n \nAs  we  already  indicated  in  our  ""Analysis  of  the  EU  Whitepaper  on  Artificial \nIntelligence""1, we want to emphasize that AI is more than data and algorithms, powered \nby Computer Processing Power (CPU). While this is the case for the most widely used AI-\nsystems at present, this is only a very limited description of what AI is. AI is a container \nterm for many computer applications, some of which combine data and algorithms, but \nother,  non-data-driven  AI  approaches,  also  exist,  e.g.  expert  systems,  knowledge \nreasoning and representation, reactive planning, argumentation and others. This should \nbe  kept  in  mind  when  defining  the  scope  of  the  initiative,  to  avoid  over-  or \nunderinclusion.  \n \nIn the same analysis we have elaborated on the fact that there is no universally accepted \ndefinition of AI and how we would describe AI technically. Nevertheless, it should be \nnoted that many of the wide applied AI systems are indeed examples of data-driven AI, \nthat have a number of typical charachteristics that can make them brittle, unstable and \nunpredictable  (such  as  opacity,  correlation  in  stead  of  causality,  insufficient  or  low \nquality data, unclear \'goals\', a lack of common sense, etc.).  \n \n \n \n \n \n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n1 https://allai.nl/wp-content/uploads/2020/06/ALLAI-Final-Analysis-of-the-EU-Whitepaper-on-AI-consultation.pdf \n3', 'Legal versus technical definition - a need to look at the impact of AI \n \nIt is important to realize that legal definitions differ from purely technical definitions \nwhereas legal definitions should meet a number of different or additional requirements2 \nsuch  as  inclusiveness,  preciseness,  comprehensiveness,  practicability,  permanence, \nsome of which are legally binding, and some are considered good regulatory practice3.  \n \nIn general, we feel obliged to emphasise that the focus on a definition through any \nattempts at defining AI-techniques in order to determine what is and what is not AI \ncreates loopholes that could be exploited. This brings us to a number of important \ngroundrules that should be guiding AI regulation and how one should set the scope of \nsuch regulation or, in other words, define AI for regulatory purposes:  \n1.  We  recommend  to  focus  on  the  effects  and  impact  of  the  systems,  not  on  a \nparticular AI-technology/technique.  \n2.  AI-systems are more than just the sum of their technical or software components. AI \nsystems  also  comprise  the  socio-technical  system  around  it.  When  considering \nregulation, the focus should not just be on the technology, but more on the social \nstructures around it: the organisations, people and institutions that create, develop, \ndeploy, use, and control it, and the people that are affected by it, such as citizens \nin their relation to governments, consumers, workers or even entire society.   \n3.  An  ""AI  lifecycle  approach""  should  be  followed,  that  considers  not  only  the \ndevelopment stage of AI, but also the deployment and use stages.  \n4.  It should be kept in mind that most AI-applications currently being used could \nenshrine, exacerbate and amplify the impact on existing laws and fundamental rights \nas well as society at scale, affecting larger parts of society and more people at the \nsame time. \nALLAI Project: Defining AI for Regulatory Purposes \n \nALLAI is currently evaluating the feasibility of setting up a project called ""Defining AI for \nRegulatory Purposes"". Working with a small group of experts and scientists from different \nbackgrounds the project aims to provide guidance for the open core question relating to the \nscope of the initiative, notably how AI should be defined. \n \n \n \n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n2 A Legal Definition of AI Jonas Schuett Goethe University Frankfurt September 4, 2019 (Legal definitions must be: (i) \ninclusive: the goals of regulation must not over- or under-include. (Julia Black. Rules and Regulators. Oxford University \nPress, 1997. [32] Robert Baldwin, Martin Cave, and Martin Lodge. Understanding Regulation: Theory, Strategy, and \nPractice. Oxford University Press, 2nd edition, 2012.); (ii) Precise: it should be clear which case falls under the definition \nand which does not; (iii) Comprehensive: the definition should be understandable by those who are regulated; (iv) \nPracticable: legal professionals should be able to easily determine whether a case falls under the definition; (v) \nPermanent: the need for continued legal updating should be avoided. \n3 Inclusiveness can be derived from the principle of proportionality in EU law (art. 5(4) of the Treaty on European Union. \nThe criteria precision and comprehensiveness are based on the principle of legal certainty in EU law. The criteria \npracticability and permanent are considered good legislative practice. \n4', '2. Problem the initiative aims to tackle \nOn: ""Ultimate objective"" \n\t\nALLAI for the main part supports the ultmate policy objective of the proposal, i.e. to \nfoster the development and uptake of safe and lawful AI that respects fundamental \nrights  across  the  Single  Market  by  both  private  and  public  actors  while  ensuring \ninclusive societal outcomes.  \n\t\nALLAI would however advise to broaden the final part of the ultimate policy objective to \nnot only ensure inclusive societal outcomes, but to also ensure ""fair societal outcomes"". \nAs mentioned in the Inception Impact Assessment, the complexity (and many times \nopacity)  of  certain  systems  and  granular  applicability  of  outcomes  to  individuals  in \ncombination with the scalability of AI systems, presents a range of difficulties as regards \nenforcement of existing legislation meant to protect human rights and could generate \nnew safety risks. On top of that, these characteristics could enshrine, exacerbate and \namplify these risks and adverse impacts on society at scale, affecting more people at the \nsame time. As such there is a serious risk that unfair societal outcomes become ever \nmore  enshrined,  exacerbated  and  amplified,  thus  potentially  leading  to  wider  and \ndeeper  societal  gaps  between  groups  of  people,  propagating  inequality  and,  as  a \nconsequence, entrenching political polarisation. \n\t\nThe addition of fairness to ultimate policy objective would also reflect the European \nCommissions\' adoption of the 7 Requirements of the High Level Expert Group on AI, i.e. \nrequirement no. 5: Inclusiveness, non-discrimination and fairness.  \n\t\nOn: ""Harm caused by AI-systems and risks not covered by existing legislation"" \n \nWe  agree  with  the  indication  that  harm  caused  by  the  use  of  AI  may  be  the \nconsequence of multiple causes. We would like to stress however that the causes for \nharm go beyond just flaws in the technical and digital components (including data) and \ncharachteristics of the system. While these flaws do often play a major and sometimes \ndecisive role in causing harm, one should not forget that even the most technically \nrobust systems can still cause harm.  \n \nImagine a facial recognition system that does recognize people of all colors, genders, \nages, etc. correctly. Technically such a system could be considered robust and non-\ndiscriminatory. Lawfully however, the system could still cause harm. AI-driven (mass) \nsurveillance with facial recognition, involves the capture, storage and processing of \npersonal (biometric) data (our faces), but it also affects our \'general\' privacy, identity and \nautonomy in such a way that it creates a situation where we are (constantly) being \nwatched, followed and identified. As a psychological ‘chilling’ effect, people might feel \ninclined to adapt their behaviour to a certain norm, which shifts the balance of power \nbetween the state or private organisation using facial recognition and the individual.  \n \n \n \n5', 'In  legal  doctrine  and  precedent  the  chilling  effect  of  surveillance  can  constitute  a \nviolation  of  the  private  space,  which  is  necessary  for  personal  development  and \n \ndemocratic deliberation. Even if our faces are immediately deleted after capturing, the \ntechnology still intrudes our psychological integrity.   \n \nThis is just one example where the mere technical elements of a system are less relevant \nas  regards  harm  than  the  actual  use  of  the  system.  That  is  why  we  advocate  to \ncontinuously ask “question zero”: Do we want to allow this particular AI-system and \ntechnique in the first place, or are there reasons not to allow its use at all? And if we \nwere  to  consider  implementing,  deploying  and  using  such  a  system,  what  are  the \nconditions we should set for its use? \n \nIn either case, for the purposes of any regulatory framework we should not merely focus \non technical solutions at dataset or algorithm level, but devise socio-technical processes \nthat help us:  \na)  Understand  the  potential  legal,  ethical  and  social  effects  of  the  AI-system  and \nimprove our design and implementation choices based on that understanding;  \nb)  Audit  our  algorithms  and  their  output  to  make  any  undesirable  outcomes \ntransparent; and  \nc)  Continuously monitor the workings of the systems to mitigate the ill effects of AI.  \nOn: ""Risks not adequately covered by existing legislation"" \nALLAI agrees with the Inception Impact Assessment that there are ample risks that are \nnot  adequately  covered  by  existing  legislation  on  cybersecurity,  protection  of \nemployees and anti-discrimination. The Inception Impact Assessment identifies as main \nissues:  \n•  Effective enforcement of existing EU rules to protect fundamental rights \n•  Application of EU rules on safety \n•  Application of EU the rules on liability \n\t\nAI & the impact on fundamental rights \n \nALLAI strongly commends the focus on the protection of fundamental rights and would \nlike to draw your attention to a report it delivered to the Council of Europe on the \n""Impact of AI on Human Rights, Democracy and the Rule of Law"".4 This report identifies \nthose human rights, as set out by the European Convention on Human Rights (""ECHR""), \nits Protocols and the European Social Charter (""ESC""), that are currently most impacted \nor likely to be impacted by AI. It aims to provide a number of possible strategies that \ncould be implemented simultaneously, ranging from addressing the impact within the \nexisting framework of human rights, democracy and the rule of law to establishing new \nhuman rights should the existing framework fail to adequately protect us. \n \n \n \n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n4 https://rm.coe.int/cahai-2020-06-fin-c-muller-the-impact-of-ai-on-human-rights-democracy-/16809ed6da \n6', ""While  states  are  obliged  to  protect  individuals  and  groups  against  breaches  of \nfundamental  rights  perpetrated  by  other  actors,  appreciation  of  non-state  actors’ \ninfluence on human rights has steadily grown.5 As (large) tech companies have now \nbecome operators that are capable of determining and perhaps altering our social and \neven democratic structures, the impact of their AI(-use) on fundamental rights becomes \nmore prevalent. In this respect, AI might serve as a good opportunity and think of a \nstructure that goes beyond the 'horizontal effect' of the EU Charter of Fundamental \nRights of the European Union. Such a structure could entail a legal obligation for private \nactors to comply with fundamental rights and to grant access to justice if they fail to do \nso.6  \n \nAI & the impact on work \n \nThe Inception Impact Assessment mentions that there are legal issues as regads the \nprotection of employees, but does not yet specify these issues. We would like to stress \nthat he introduction and use of AI in the workplace can cause effects as to health and \nsafety in the workplace, job security, worker privacy, the balance between worker and \nemployer  and  so  on.  For  that  reason,  we  have  been  advocating  early  and  close \ninvolvement of workers and service providers of all types, including freelancers, the self-\nemployed and gig workers − not just people who design or develop AI, but also those \nwho purchase, implement, work with or are affected by AI systems. Social dialogue must \ntake place before the introduction of AI technologies in the workplace, in line with the \nrelevant applicable national rules and practices.  \n \nAdditionally, we would like to draw special attention to AI used in hiring, firing and \nworker assessment and evaluation processes. The White Paper on AI mentions AI used \nin  recruitment  as  an  example  of  a  high-risk  application  that  would  be  subject  to \nregulation irrespective of the sector. We recommend extending this use to include AI \nused in firing and in worker assessment and evaluation processes, but also to explore \nthe common characteristics of AI applications that would make for a high risk use in the \nworkplace, irrespective of the sector. AI applications that have no scientific basis, such \nas  emotion  detection  through  biometric  recognition,  should  not  be  allowed  in \nworkplace environments.  \n \nAdditional legal lacunae \n \nIn addition to the issues identified in the Inception Impact Assessment, ALLAI also \nidentifies a number of additional regulatory lacunae related to existing legislation. \n \nAs AI is evolving quickly and the wider impact of AI on the full acquis has still not been \nfully identified, these lacunae are not exhaustive but below are the areas for attention to \nthe extent that they can be identified today7: \n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n5 Business and Human Rights, A Handbook for Legal Practitioners, Claire Methven O’Brien, Council of Europe  \n6 This also means going beyond merely referring to the Recommendation CM/Rec(2016)3 on human rights and business \nof the Committee of Ministers of the Council of Europe (and the UN Guiding Principles on Business and Human Rights) \n7 See also: STOA Policy Briefing: Legal and ethical reflections concerning robotics, The Scientific Technopolis Group, \n\t \nMaastricht University, Scientific Foresight Unit (DG EPRS) of the European Parliament.\n7"", '1.  GDPR \n2.  Law Enforcement \n3.  Competition law \n4.  Transportation \n5.  Trade of dual-use technology \n6.  Consumer protection \n7.  Healthcare \n8.  Energy & Environment \n \nAd 1: GDPR \n \nA much cited existing EU regulation in the context of AI is the GDPR. This regulation \nprovides frameworks for data protection, the right to an explanation of AI decisions and \nsafeguards for the use of biometric recognition.  \n \nBiometric  recognition:  It  should  be  noted  that  GDPR  restricts  the  processing  of \nbiometric data only to some extent. Biometric data according to the GDPR is “personal \ndata resulting from specific technical processing relating to the physical, physiological or \nbehavioural  characteristics  of  a  natural  person,  which  allow  or  confirm  the  unique \nidentification of that natural person. The last part of the sentence is crucial, because if \nbiometric recognition is not aimed at identification (but for example at categorization, \nprofiling or affect recognition), it might not fall under the GDPR-definition. In fact, recital \n51 of the GDPR says that \'the processing of photographs [is considered] biometric data \nonly  when  processed  through  a  specific  technical  means  allowing  the  unique \nidentification or authentication of a natural person.\' \n \nMany  biometric  recognition  technologies  are  however  not  aimed  at  processing \nbiometric data to uniquely identify a person, but merely to assess a person’s behaviour \n(for example in the classroom) or to categorize individuals (for example for the purpose \nof determining their insurance premium based on their statistical prevalence to health \nproblems). These uses might not fall under the definition of biometric data (processing) \nin the GDPR.  \n \nRight to explanation of ADM, including profiling: Particularly with regard to the right \nto an explanation of automated decisions, a debate is underway about whether the \nGDPR gives a right to an explanation of automated decisions or not. Under the GDPR, \ncontrollers who use personal data to make automated decisions are obliged to inform \nindividuals  in  advance  and  to  provide  meaningful  information  about  the  logic  and \nimportance of the decision-making and the consequences for the data subject. The so-\ncalled art. 29 Working Group recognizes that ""the growth and complexity of machine \nlearning can make it challenging to understand how automated decision-making or \nprofiling works,"" but that, despite this, ""the company [must] find simple ways to tell the \nindividual about the reason, or the criteria on which the decision is based, without \nnecessarily  always  attempting  a  complex  explanation  of  the  algorithms  used  or \ndisclosure of the full algorithm."" \n \n8', ""Also,  the  GDPR  requires  a  controller  to  implement  appropriate  safeguards  when \ndesigning automated decisions, such as the right to human intervention and the right to \nexpress his or her point of view and contest the decision. The recitals of the GDPR also \ninclude the right to an explanation of a fully automated decision. \n \nIt is however debatable whether there is a full right to an explanation of an automated \ndecision in all cases. The fact that the right to explanation is included only in the recitals \nseems to be a hurdle that can be overcome, but in particular the fact that the right to \nexplanation  only  exists  as  regards  to  fully  automated  decision-making,  makes  it \ninsufficient to ensure adequate transparency in the automated decision-making process. \nAfter all, a single human 'check' on an automatic decision (regardless of whether this \nperson has been able to judge the decision on its merits, could lead to the conclusion \nthat an explanation within the meaning of the GDPR would not be necessary.  \n \nIt  must  also  be  assessed  whether  the  GDPR  offers  sufficient  protection  when  the \ndecision is based on non-personal data. AI-driven profiling and the categorization of \npeople or groups of people is  regularly being done by finding inferences made about \nan individual, even without using personal data or resulting in identification of a person. \nThere is no consensus whether these inferences in itself should count as personal data, \nbut there are experts arguing that all data processing that has an impact on people \nshould be protected8 or even that the distinction between personal and non-personal \ndata should be suspended9. \n \nHere the issue also ventures from the GDPR into other fundamental rights such as the \nright non-discrimination, covered in multiple EU directives and the ECHR. \n\t\nAd 2: Law enforcement \n \nIn line with the above, EU regulation such as the Police Directive (EU 2016/680), that \nregulates the processing of personal data in particular for the purpose of profiling by \nlaw enforcement in the Member States, does not adequately cover the issue of profiling \nthat is based on mere inferences rather than personal data, as described above. \n \nAd 3: Competition \n \nMany AI-applications are developed and deployed by a handful of private actors. These \nactors are also present in multiple market segments that are related to AI or use AI, such \nas  finance,  insurance,  etc.  If  too  much  market  power  in  these  different  markets  is \nconcentrated in a few companies, this could lead to unfair competition and difficulty for \nnew (smaller) players to enter the market. \n \n \n \n \n \n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n8 Purtova (2018) \n9 Koops (2014) \n9"", 'Ad 4: Transportation \n \nWhile  autonomous  driving  has  not  yet  fully  materialized,  the  autonomous  vehicles \nindustry is well underway to making vehicles behave more autonomous and needing \nless \'active involvement\' from their drivers.  \n \nThese developments call for a critical review of the various EU regulations and directives \nthat deal with transportation, such as Regulation (EC) 561/2006 and Regulation (EEC) \n3821/85  regarding  driving  and  resting  times  and  digital  tachygraphy  (for  truck \nplatooning),  Directive  2014/45/EU  on  Roadworthiness,  Directive  2010/40/EU  on \nIntelligent Transport Systems in the field of road transport and for interfaces  with other \nmodes of transport and  Directive 2003/59/EC on training and initial qualifications of \nprofessional drivers10. \n \nAd 5: Trade of dual-use technology \n \nFollowing an impact assessment in 2016, a new reform process was started to (a.o.) \nfuture-proof the export control regime for rapidly developing emerging technologies. In \nsum, the main goal of this reform is to control the export of information technologies \nthat can be used for the suppression of human rights, thus increasing the scope and \nscale of dual-use governance. In 2016 the focus was on the inclusion of so called ‘cyber-\nsurveillance technologies’11, but recently AI applications such as facial recognition have \nalso entered this debate as a possible next step for dual-use regulation.  \n \nAd 6: Consumer protection \n \nAt  this  point,  consumers  do  not  receive  adequate  protection  against  unacceptable \nimpact.12 Many  of  these  protections  would  need  to  be  better  covered  in  different \nexistsing legislative instruments. As an example, the EU legal protection of consumers \nagainst  unfair  AI-driven  personalized  pricing  is  mostly  principle-based,  leading  to \nuncertainty on its interpretation as long as there is no clarification in case law.13 \n \nAd 7: Health \n \nIn the Medical Devices Regulation software is considered a medical device, making AI-\ndriven  applications  in  healthcare  subject  to  certain  requirements  and  a  mandatory \nlabelling scheme. Alignment of this regulation with a new legislative instrument on AI is \nimportant.  \n \n \n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n10 Also: Directive 2009/103/EC on motor vehicle insurance; Directive 2007/46/EC on vehicle approval; Directive \n2006/126/EC on requirements for driving licences \n11 Immenkamp, 2019 \n12 BEUC, EU Rights for Consumers \n13 de Streel, Alexandre; Jacques, Florian (2019) : Personalised pricing and EU law, 30th European Conference of the \nInternational Telecommunications Society (ITS): ""Towards a Connected and Automated Society"", Helsinki, Finland, 16th-\n19th June, 2019, International Telecommunications Society (ITS), Calgary \n \n10', 'Also, now that the Medical Devices Regulation has been postponed to enter into effect \nin  May  2021,  existing  legislation,  and  the  current  practice  of  assessment  and \ncertification of AI-diven healthcare devices, should be critically looked at.  \n \nAd 8: Energy & Environment \n \nThe  High  Level  Expert  group  on  AI  (HLEG  AI)  has  argued  that  sustainability  and \necological responsibility of AI systems should be encouraged and has made ""Societal \nand Environmental Well-being"" one of the 7 requriements for Trustworthy AI of its \nEthics Guidelines for Trustworthy AI. \nAccording to the HLEG AI, it must be ensured that AI-systems operate in the most \nenvironmentally friendly way possible. The system’s development, deployment and use \nprocess, as well as its entire supply chain, should be assessed in this regard, e.g. via a \ncritical examination of the resource usage and energy consumption during training, \nopting for less harmful choices. Measures securing the environmental friendliness of AI \nsystems’ entire supply chain should be encouraged. This could trigger a necessery \nreview and possible adaptation of EU engery regulations such as Directive 2010/30/EU \non the indication by labelling and standard product information of the consumption of \nenergy and other resources by energy-related products.  \n3. AI stress test for EU regulation \n \nThe foregoing calls for a much broader AI stress test for existing EU regulation. Three \nquestions need to be answered in particular: \n1.  To what extent are the policy and legal objectives underlying these regulations \naffected by AI systems and in what ways? \n2.  What  are  the  existing  monitoring,  information  gathering  and  enforcement \nframeworks capable of providing meaningful and effective oversight to ensure that \npolicy and legal objectives are still effectively achieved? \n3.  To  what  extent  does  existing  legislation  work  in  a  way  that  they  promote  and \nsafeguard the ethical principles and requirements (as described in the AI HLEG\'s \nEthics Guidelines for Trustworthy AI)? \n \nALLAI points out that the AI HLEG has already advised the European Commission to \nperform this \'stress test\' at EU level, in order to arrive at an unambiguous regulatory \nframework across Europe.  \n \nIt should be noted that EU legislation does not affect all national legislative areas. \nWhere the EU has no legislative competence, ALLAI advises the Dutch legislator to \ninitiate the stress test at national level. Think of elements of labor law, social security \nlaw, administrative law and criminal law. \n \n \n \n \n11', '4. Basis for EU intervention \n \nALLAI fully agrees with the Inception Impact Assessment\'s reasoning that the objectives \ndescribed cannot be reached effectively by Member States alone, but can be better \nreached at Union level. We agree that it is important that fragmantation would prevent \nthe free circulation of goods and services containing AI and thus negatively affect the \nDigital  Single  Market.  Fragmentation  would  also  lead  to  divergence  in  levels  of \nprotection of citizens and society against the abovementioned harms and risks, which \ncould lead to an unacceptable ""race to the bottom"" of AI regulation and protections in \nan effort to attract more AI investment.  \n \n \nB. Objectives and Policy Options \n1. Objective \nALLAI supports the overall objective of the instrument, i.e. to ensure the development \nand uptake of lawful and trustworthy AI across the Single Market through the creation of \nan ecosystem of trust.  \nALLAI would like to suggest the following additional aims of the initiative:  \n•  To  fill  any  legal  lacunae  either  regarding  the  effectivenes,  applicabilty  or \nenforceability of existing EU law and where no EU law exists, so as to ensure that \noverall EU policy and legal objectives as regards trustworthy AI are promoted and \nsafeguarded; \n•  In addition to what is expressed in aim (c) we strongly recommend to broaden this \naim so as to include risks for people and society.  \n•  As part of aim (c) and aim (e) it is important to be able to effectively monitor the \nfuture developments of AI within and outside the Single Market, so as to make sure \nthat new opportunities for trustworthy AI are identified and promoted, but also that \nnew challenges are adequately and timely addressed.  \n•  In addition to aim (e), we suggest to set up a structure that includes not only the \nrelevant authorities in the Member States, but also all relevant stakeholders, such as \nworkers\'  and  business\'  representatives,  other  civil  society  organisations,  NGO\'s, \nacademia (various disciplines), policy makers, etc.  \n•  As an additional aim we reccommend to set up a European AI Authority as part of a \nglobal framework of AI Authorities. Such a framework could be set up as folows:  \no  A global AI Authority; \no  Several regional sub-authorities (e.g. EU, the Americas, Asia, Oceanea, Middle \nEast); \no  National  executive  authorities  (either  existing  or  new),  for  the  EU  to  be \nappointed or set up by the Member States. \n \n \n \n12', 'The roles and responsibilities of these authorities should be carefully considered, \nbut they should have broad expertise of the different elements and impact domains \nof AI, including but not limited to technical, legal and ethical expertise, as well as \nknowledge of behavioural effects, labour market effects, economic and societal \neffects of AI.   \n2. Policy Options \nOn Option 0: ""Baseline \nALLAI agrees that the current ""Baseline"" or Option ""0"" does not suffice to adequately \naddress the risks and potential harms connected to AI and thus considers this option not \nviable.  \nOn Option 1: ""EU soft law"" \nThe option of EU soft law should only apply to AI-systems or uses that have no adverse \nimpact  on  people,  society  or  the  environment  nor  on  our  (fundamental)  rights, \ndemocracy and the rule of law.  \nThe  two  factor  approach  that  was  suggested  by  the  European  Commission  in  its \nWhitepaper on AI does however not suffice to determine these types of AI-systems. As \nan example consider targeted online advertising. The Commission will likely qualify \nadvertising as a low-risk sector, and AI-driven targeted adds as a low risk AI-application. \nTargeted advertising however, has shown to have a potential segregating and dividing \neffect.  \nThis is the reason why we recommend looking at the level of societal or personal impact \nof an AI-system or use to determine the risk level of the system. \nOn Option 2: ""EU legislative instrument setting up a voluntary labelling scheme"" \n \nThe labelling scheme is not a standalone option, but could be split into two options, the \nfirst to be joined with option 1 and second to be joined with option 3.  \n \nWhere EU soft law would suffice (option 1), a voluntary labelling scheme could be an \ninteresting  addition  for  industry  players  to  gain  competitive  advantage,  or  to  feel \nconfident to define or explore a niche area of application.  \n \nWhere  an  EU  legislative  instrument  establishing  mandatory  requirements  for  AI  is \nnecessary (option 3), a mandatory labelling scheme (like the CE-marking) could be \nconsidered to avoid any untrustworthy AI being deployed on the Digital Single Market. \nCurrent  practices  around  certification  of  AI,  such  as  those  already  executed  in  the \nhealthcare sector, should be carefully reviewed, to see if those practices are sufficient, or \nshould be amended or replaced by a new mandatory labelling scheme to avoid overlap \nwith possible new labelling scheme.  \n \n13', 'Labels should not merely refer to the technical characteristics of the system, but more \nimportantly also to the effects and impacts of the system. \n \nOn Option 3:""EU legislative instrument establishing mandatory requirements for \nAI"" \n \nALLAI  is  in  favour  of  introducing  an  EU  legislative  instrument,  in  the  form  of  a \ncombination of sub-options (a) and (b).  \nExceptionally impactful AI \nThe following AI-systems or uses that are considered to be too impactful could give rise \nto the necessity of a ban, moratorium, strong restrictions or conditions for exceptional \nand controlled use: \n•  Indiscriminate use of facial recognition and other forms of biometric recognition \neither by state actors or by private actors; \n•  AI-powered  mass  surveillance  (using  facial/biometric  recognition  but  also  other \nforms of AI-tracking and/or identification such as through location services, online \nbehaviour, etc.); \n•  Personal, physical or mental tracking, assessment, profiling, scoring and nudging \nthrough biometric and (online) behaviour recognition in violation of fundamental \nrights (AI-enabled Social/Citizen Scoring); \n•  Covert AI systems and deep fakes; \n•  Implanted human-AI interfaces; \n \nExceptional use of these technologies, such as for national security purposes or medical \ntreatment or diagnosis, could be allowed but should be evidence based, necessary and \nproportionate and only be executed in controlled environments or cleary identified \ncontexts and (if applicable) for limited periods of time. \n \nAs regards biometric recognition14 (including facial recognition) we recommend that \nany  use  of  biometric  recognition  only  be  allowed  under  the  followoing  cumulative \nconditions: i) there is a scientifically proven effect; (ii) it is used in controlled environment \n(e.g. a hospital); (iii) it is used under strict conditions (e.g. limited in time, for a specific \npurpose,  etc.).  Widespread  and/or  public  use  of  AI-driven  biometric  recognition  to \nsurveil,  trace,  track,  assess  or  categorise  humans  or  human  behaviour  or  emotions \nshould not be allowed. \n\t\nAs regards AI-driven mass surveillance we refer to the recommendation of the HLEG \nAI in its Ethics Guidelines for Trustworthy AI that automatic identification raises strong \nconcerns of both a legal and ethical nature, as it may have an unexpected impact on \nmany psychological and sociocultural levels.  \n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n14 Biometric recognition of micro-expressions, gait, (tone of) voice, heart rate, temperature, etc. is being used in various \nways, one of which is to assess or even predict our behaviour, mental state, and emotions. As Barret et al. (Emotional \nExpressions Reconsidered: Challenges to Inferring Emotion From Human Facial Movements, 2019) have shown however, \nno sound scientific evidence exists to suggest that a person\'s inner emotions or mental state can be accurately \'read\' \nfrom their facial expression, gait, heart rate, tone of voice or temperature, let alone that (future) behaviour could be \npredicted by it. \n14', ""A proportionate use of control techniques in AI is needed to uphold the autonomy of \nEuropean  citizens.  Clearly  defining  if,  when  and  how  AI  can  be  used  for  mass \nsurveillance both by public or private actors, differentiating between the identification of \nan  individual  versus  the  tracing  and  tracking  of  individual,  will  be  crucial  for  the \nachievement of Trustworthy AI. \nAs regards AI-enabled Social Scoring we refer to the recommendation of the HLEG AI \nin its Ethics Guidelines for Trustworthy AI, that 'any form of citizen scoring should only \nbe used if there is a clear justification, and where measures are proportionate and fair. \nNormative  citizen  scoring  (general  assessmentof  “moral  personality”  or  “ethical \nintegrity”) in all aspects and on a large scale by public authorities or private actors \nendangers  these  values,  especially  when  used  not  in  accordance  with  fundamental \nrights, and when used disproportionately and without a delineated and communicated \nlegitimate purpose.' The HLEG AI also argues 'that citizen scoring – on a larger or \nsmaller scale – is already often used in purely descriptive and domain-specific scorings \n(e.g.  school  systems,  e-learning,  and  driver  licences).  Even  in  those  more  narrow \napplications,  a  fully  transparent  procedure  should  be  made  available  to  citizens, \nincluding information on the process, purpose and methodology of the scoring.' It also \nnotes that 'mere transparency cannot prevent non-discrimination or ensure fairness, and \nis not the panacea against the problem of scoring and that ideally the possibility of \nopting out of the scoring mechanism without detriment should be provided – otherwise \nmechanisms for challenging and rectifying the scores must be given. This is particularly \nimportant in situations where an asymmetry of power exists between the parties. Such \nopt-out options should be ensured in the technology’s design in circumstances where \nthis is necessary to ensure compliance with fundamental rights and is necessary in a \ndemocratic society.' \nAs regards covert AI systems and deep fakes we again refer to the recommendation \nof the HLEG AI in its Ethics Guidelines for Trustworthy AI that 'human beings should \nalways know if they are directly interacting with another human being or a machine, and \nit is the responsibility of AI practitioners that this is reliably achieved. AI practitioners \nshould therefore ensure that humans are made aware of – or able to request and \nvalidate the fact that – they interact with an AI system (for instance, by issuing clear and \ntransparent disclaimers ). Note that borderline cases exist and complicate the matter \n(e.g. an AI-filtered voice spoken by a human). It should be borne in mind that the \nconfusion between humans and machines could have multiple consequences such as \n \nattachment, influence, or reduction of the value of being human.'  \nAs  regards  Implanted  human-AI  interfaces  such  as  Elon  Musk's  Neuralink  brain \nimplant give rise to a plethora of legal (in particular as regards to (fundamental) rights \nthat protect physical and psychological integrity, autonomy, free will, privacy etc.) and \nethical concerns that should be carefully assessed and addressed in order to establish \nthe appropriate regulatory framework. \n \n15"", 'Common denominators for determining the level of impact of AI \nFor all other AI applications, mandatory requirements on issues such as robustness, \naccuracy  and  reproducibility,  traceability,  transparency,  human  oversight  and  data \ngovernance15 could be set, relative to the degree of impact of the AI-application.  \nWe would like to stress again that these mandatory requirements should not be aimed \nmerely at the data used to train and feed the AI-system, but also at the model(s) and \nalgorithm(s) that comprise the system. These requirements should be met prior to the \ndeployment of the system and be maintained throughout the use of the system.  \nWe already expressed our concerns on the suggested \'two-factor\' approach to identify \n\'high risk AI\' (high risk application + high risk sector) that would then be subject to \nmandatory requirements. We also expressed our concerns on the list based approach of \nAI-applications that would be considered high risk \'as is\', i.e. irrespective of the sector. \nFor both structures, we fear that multiple AI applications or uses could fall (or actively be \nkept) outside the scope of the legislative initiative.16 \n\t\nIn  stead  of  the  two-factor  approach,  we  recommend  to  determine  common \ndenominators for AI applications or uses that are to be considered high risk, or \nrather medium or high impact, and would fall within the scope of the legal instrument. \nThe following elements could serve as a guidance to set such common denominators: \n•  Common denominators should be determined based on the impact of the system \non people and/or society, rather than (merely) on the technical characteristics of the \nsystem, or the sector in which the system is being used; \n•  \'AI impact\' is to be considered both at individual and at societal/collective level \nwhereas AI can impact both the individual as well as larger parts of our collective \nsociety; \n•  Context, severity, scale and likelihood of the impact is important to determine the \nappropriate  and  proportionate  mandatory  requirements,  which  could  result  in \ndifferent requirements for different levels of impact; \n•  For high impact AI applications that generate unacceptable risks or pose threats of \nharm or systemic failure that are substantial, a precautionary and principle-based \nregulatory approach should be adopted; \n•  For  medium  impact  AI  applications  a  risk-based  approach  could  be  more \nappropriate.  \n \nOn Option 4: ""A combination of either or all of the previous options""  \n \nAs can be concluded from the above, ALLAI would be most in favor of a combination of \nthe policy options described in the Inception Impact Assessment. In short we would \nenvisage the following structure:  \n•  Soft law for low impact AI applications, including voluntary labelling \n•  EU instrument with mandatory labelling covering: \n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n15\tWe have elaborated on these requirements in our Final Analysis of the EU Whitepaper on AI, ALLAI (2020) \n16 Final Analysis of the EU Whitpaper on Artificial Intelligence, ALLAI (2020) \n16', 'o  Clear restrictions, conditions, safeguards and/or boundaries for a limited number \nof exceptionally impactful AI-applications or uses; \no  Mandatory requirements for medium to high impact AI based on common \ndenominators to determine the level of impact. \nOn: ""Ex-ante and/or ex-post enforcement mechanisms"" \nWhile  ex-ante  and  ex-post  enforcement  mechanisms  are  necessary,  in  our  opinion, \ntrustworthy  AI  primarily  needs  an  ex-durante  mechanism  to  ensure  a  continuous, \nsystematic, socio-technical governance approach, looking at the technology from all \nperspectives and through various lenses.  \nThis requires a multidisciplinary approach where policy makers, academics from a variety \nof  fields  (AI,  data-science,  law,  ethics,  philosophy,  social  sciences,  psychology, \neconomics, cyber security), social partners and NGO’s work together on an ongoing \nbasis.  \nIt also requires socio-technical governance processes and structures that facilitate:  \n•  Understanding of the potential legal, ethical and social effects of the AI-system and \nimprove the governance/legislative choices based on that understanding;  \n•  Monitoring of the workings and developments of AI to address and mitigate any \nnew ill effects. \nWe thus call for a European AI Authority, as part of a global stucture of AI Authorities as \nmentioned above under B.1. \n   \nC. Preliminary Assessment of Expected Impacts  \nWe agree that SME\'s should not be excluded from the application of the regulatory \nframework, precisely because of the high scalability of AI-systems.  \nAs regards societal impact, we would like to draw attention to the fact that is still not \nclear whether AI will cause major loss of jobs or that this loss outweighs the number of \nnew jobs it could bring.  \n \nThe maintenance or acquisition of AI skills is necessary in order to allow people to adapt \nto the rapid developments in the field of AI is therefore important. But policy and \nfinancial resources will also need to be directed at education and skills development in \nareas that will not be threatened by AI systems (i.e. tasks in which human interaction is \nvital, such as public interest services related to health, safety and wellbeing of people \nand based on trust, where humans and machines cooperate, or tasks we would like \nhuman beings to continue doing). \n\t\nAs for environmental impacts, we expect that if energy regulation would be adapted to \nalso cover AI, legislation could have a positive environmental impact.  \n ∞ \n17', 'Authors \nVirginia Dignum is professor of Artificial Intelligence at Umeå University, program director of the \nWallenberg AI, Autonomous Systems and Software Program – Humanities and Society (WASP-HS), co-\nfounder of ALLAI, member of the European High Level Expert group on AI and of the World \nEconomic Forum AI Board, and currently working as an expert advisor for UNICEF. \nCatelijne Muller is co-founder and president of ALLAI, member of the European High Level Expert \ngroup on AI, Rapporteur on AI for the European Economic and Social Committee, and currently \nworking as an expert advisor for the Council of Europe on AI & Human Rights, Democracy and the \nRule of Law. \nAndreas Theodorou is postdoctoral researcher on Responsible AI at Umeå University, member of the \nAI4EU consortium, member of the external ethics board of the ROXANNE project, and committee \nmember on the IEEE Standards Association P70xx series of standards on AI. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nALLAI refers to Stichting ALLAI Nederland, a foundation under Dutch Law. No entity or person connected \nto ALLAI, including its Board Members, Advisory Board Members, employees, experts, volunteers and \nagents, is responsible or liable for any direct or indirect loss or damage suffered by any person or entity \nrelying wholly or partially on this communication.  \n \n© 2020 ALLAI, The Netherlands \n18']"
F550934,10 September 2020,Tadas Tumenas,Unternehmen/Unternehmensverband,Orgalim,klein (10 bis 49 Beschäftigte),20210641335-88,Belgien,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"Orgalim strongly endorses the overall policy objective of ensuring the development and uptake of trustworthy AI across the Single Market. As this inception impact assessment outlines various options, Orgalim would like to affirm its support for Option 1 of the alternative options to the baseline scenario – i.e. the option of an EU ‘soft law’, a non-legislative approach to facilitate and encourage industry-led intervention (with no EU legislative instrument). In general, Orgalim believes that, before choosing any option, existing regulation needs to be carefully analysed, potential gaps precisely formulated, and the right tools adequately proposed, based on a realistic definition of AI. For the manufacturing sector, the most important aspect to keep in mind is that AI is not a product, but a technology embedded in products (applications), which puts all concerns related to AI into another perspective. There are very diverse applications that might be deemed AI-based or AI-operated systems, ranging from a driverless car to a smart-toothbrush, a robot-companion, or a non-embedded expert system for medical diagnosis. New regulation should be introduced only where it is necessary, and where it delivers clear benefits (e.g. helps to uptake the new technologies by creating a level playing field, ensuring safety etc.), and with a reference to industry standards which reflect the state of the art. It is important for policymakers to differentiate between the varying degrees of risk linked to use of AI technologies in their different applications. Clear criteria should be established for identifying critical areas in a way that is legally certain. In Orgalim’s view, the quality of any future regulation will depend on the ability to identify a common, transparent and easily applicable understanding of ‘high-risk’. High-risk situations should be defined in cooperation with industry, based on risk-benefit considerations and adjusted when necessary. Clear definition of criteria for perceived high-risk applications and the degree of autonomy is crucial, in order to avoid over-regulation of completely harmless automation. When something has been identified as a high-risk application (which we believe will be a minority of industrial AI applications) a targeted  approach to risk-management could be the right one. Taking this into account, it can for instance be concluded that most industrial AI application use cases have entirely different ethical implications compared to consumer-oriented AI solutions for end-consumers. It is crucial that the framework for identifying high-risk use cases is predictable and proportionate in order to create a stable environment for investments. From a policy-making perspective, clearly identifying the object to be regulated is essential. In the absence of a precise definition, which is currently the case for AI, the scope of any intended regulation would be uncertain, potentially being either over- or under-inclusive, and triggering litigation. Orgalim would like to highlight a definition of AI, as  outlined in our previous position papers and it is similar to the definition given by the Commission’s High-Level Experts Group on AI. More detailed analysis and suggestions, especially when it comes to the safety and liability of AI, can be found in the attached Orgalim position paper."
F550933,10 September 2020,Constantin Greim,Wirtschaftsverband,Gesamtverband der Deutschen Versicherungswirtschaft e.V.,mittel (50 bis 249 Beschäftigte),6437280268-55,Deutschland,Künstliche Intelligenz – ethische und rechtliche Anforderungen,Please find the complette feedback of the GDV in attachment.
F550925,10 September 2020,Kamila Sotomska,Wirtschaftsverband,Związek Przedsiębiorców i Pracodawców,klein (10 bis 49 Beschäftigte),868073924175-77,Polen,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"First, we would like to highlight the risks arising from the extension of the scope of future AI Regulation to automated decision making. Such inclusion would stand in opposition to the fundamental ideas laid out on the White Paper on Artificial Intelligence, which proposed to adopt a risk-based approach and focus on high-risk IA-based applications. Moreover, an extension of the definition of AI to automatic decision making would lead to an unproportionate regulatory burden for various market operators. This, in turn, would hinder the development and implementation of IA technology as well as automated decision systems.
Secondly, the Union of Entrepreneurs and Employers expresses its concern over the potential introduction of a new category of harm, meaning immaterial harm.
Option “0”
In our view, even in the absence of AI-specific European regulation, the Commission should propose to adapt the existing legislative framework to the challenges raised by the development of new technologies, including AI. Here, it is also important to note that AI solutions do not operate in a legal vacuum, and are subject to an existing regulatory framework such as GDPR or medical device regulations.
Option 1
We support all measures, which engage the industry in setting regulatory norms. Therefore, we would suggest that the Commission takes into consideration the possibility of implementation of such a scheme regardless of the policy option, which would be eventually chosen. 
Option 2
Voluntary labelling scheme in the longer term could become a standard for large companies. However, compliance with such a scheme for SMEs could prove to be a significant regulatory burden, effectively deteriorating their position in the market. Keeping in mind the negative effects of the competition, we consider that the costs of such a voluntary scheme could outweigh its potential benefits.
Option 3
•	Sub-option 1: It assumes that the EU regulation could be limited to a specific category of AI applications only, for instance, biometric identification systems. In our opinion, remote biometric identification systems are a good example of high-risk AI, which should be regulated by mandatory law. 
•	Sub-option 2: In this case, the EU legislative instrument could be limited to high-risk AI applications. As mentioned above, we support the risk-based approach, which was initially outlined in the White Paper on Artificial Intelligence. Sectoral regulation limited only to high-risk AI applications would not hinder innovation and technological development, while safeguarding respect for fundamental rights. We would encourage the Commission to base its’ work on the proposal presented in the White Paper. Such a solution would not only increase the effectiveness of the process but also minimize uncertainty and ensure a smooth transition.
•	Sub-option 3: The last sub-option proposes to regulate all AI-based applications. The Union of Entrepreneurs and Employers strongly advises not to adopt this solution. As mentioned above, various AI-based applications pose different levels of risk. In our opinion, such an approach would lead to over-regulation and result in various costs (administrative burdens, additional costs, hindrance of economic development). Moreover, a one-size-fits-all approach also means that IA-based applications, which pose high risks, could be under-regulated. Consequently, it could lead to the creation of a gap, where some of the negative effects of such high-risk applications could not be easily fit into the legal framework. Effectively, making documentation and obtaining damages for harm could become more difficult. 
We support ex-post enforcement. In case ex-ante regulation is deemed necessary, we encourage the Commission to work with the industry representatives in order to develop transparent and precise risk-assessment guidelines as well as implement a due diligence system.
For a more detailed analysis, please refer to the attached contribution.","['Comment\n \n \n   \n \nof the German Insurance Association (GDV)\n \nID number 6437280268-55\n \n   \n \non the roadmap for\n \na proposal for a legal act of the European Parliament and the\nCouncil laying down requirements for Artificial Intelligence\n \n   \n \n \n \n     \n \n \nUnion of Germans\ninsurance industry e. V\n \nGerman Insurance Association\n \nWilhelmstrasse 43 / 43 G, 10117 Berlin\nP.O. Box 08 02 64, 10002 Berlin\nPhone: +49 30 2020-5000\nFax: +49 30 2020-6000\n \n51, rue Montoyer\nB - 1000 Brussels\nPhone: +32 2 28247-30\nFax: +49 30 2020-6140\nID number 6437280268-55\n \nContact:\ndepartment (or sector/group) XY\n \nEmail: department@gdv.de\n \nwww.gdv.de', '1.  General remarks \n \nThe German insurance industry welcomes the approach of the EU Com-\nmission to make the development and application of artificial intelligence \nlegally secure and trustworthy, thus strengthening its acceptance. AI will \nonly be able to develop its innovative potential and the associated benefits \nfor citizens and the economy if both consumers and companies can trust \nAI.  \n \nFor AI, a European approach is needed. Fragmentation, especially in \nthe digital sector, must be avoided. \n \nWith that said, we believe that the existing EU-legislation is fully applica-\nble to AI-applications. However, it could be reviewed whether the current \nregulatory framework hinders their beneficial use and thus needs to be \nadjusted to facilitate AI dissemination. It is commendable that the Com-\nmission wishes to create the right incentives for the adoption of AI, but in \nmost cases sufficient business incentives for responsible AI use would \nalready be there.  \n \nFor the insurance industry in particular, the existing regulations are suffi-\ncient. The insurance industry, as part of the financial services sector, is \nhighly regulated. Because legislation is generally technology-neutral, nov-\nel technologies or methods such as AI are already captured by the exist-\ning regulatory framework and supervisory authorities are continually refin-\ning their approaches regarding new technologies. \n \nThe German insurance industry is subject to supervision on the European \nand on the national level (EIOPA and BaFin). Both EIOPA and BaFin have \nalready increased their regulatory efforts in this field, and they have the \nexpertise required for this purpose. Already today the regulations of the \nanalogue world in insurance automatically apply to the digital world as \nwell. Double and/or too specific regulation is not necessary and should be \navoided.  \n \nFor these reasons, potential future regulatory measures concerning AI \nshould be considered with utmost care. As a number of surveys in the \npast have revealed, many businesses recognize the significant potential of \nAI and have expressed interest in incorporating such technologies into \ntheir business processes in the coming years. However, these declara-\ntions have not yet been followed up with actual matching implementations. \nThe introduction of AI applications currently progresses slowly and can still \nbe considered to be in its infancy in many areas. In order to promote the \nuptake of AI and prevent innovative technologies from being stifled by \npremature regulation, ethical use of AI should be supported by and rein-\n \npage 2 / 6', 'forced through voluntary and/or non-legislative instruments as far as pos-\nsible. The principles which constitute ethical use of AI as outlined by the \nHLEG are already subject to existing legislation (e.g. anti-discrimination \nlaw and transparency obligations in the GDPR). That is why the EU-\nCommission should primarily focus on a soft law approach that promotes \nindustry initiatives. Recent years have shown that many organisations and \nindustries are aware of the importance of ethical AI and have thus intro-\nduced their own ethical codes that are tailored to the specific characteris-\ntics  of  that  sector.  Additional  industry-specific  initiatives  are  to  be  ex-\npected.  \n \nFor this reason, option 1 appears preferable. In order to enhance con-\nsumer confidence, undertakings can also commit themselves to comply \nwith further voluntary requirements.   \n \nVoluntary certifications have traditionally proven to be effective means of \nensuring high and transparent standards (e.g. in the area of IT security). \nThey enable customers to easily identify trustworthy products and allow \nthe businesses to display the quality of their products and to further pro-\nmote them. Many companies would likely voluntarily opt for certifying their \nAI applications to stay ahead of the competition and to ensure compliance \nwith current standards and regulation and ethical principles. \n \nAn approach that mainly focusses on voluntary instruments does not nec-\nessarily have to exclude all options for legislative instruments containing \nmandatory requirements for certain AI applications. However, such regula-\ntion should be limited to AI applications which possess high risk potential. \nFurthermore, the following points should be considered in such cases: \n \n\uf0b7  The new regulatory framework for AI should achieve its objectives \neffectively and efficiently. In particular, a disproportionate burden \non users of AI - e.g. because of excessively prescriptive provisions \n– should be avoided. \n\uf0b7  A risk based approach is important to help ensure that the regula-\ntory intervention is proportionate. Clear criteria are required to dif-\nferentiate between the different AI applications, in particular in rela-\ntion to the question whether or not they are ‘high-risk’. High re-\nquirements for each AI application hinder innovation, cause high \ncosts and create a protection that is not necessary in view of the \nlow risk. \n\uf0b7  The scope of future regulations must be precisely defined. The \nclarification provided by the HLEG can serve as a good starting \npoint.  Algorithms  that  do  not  incorporate  any  form  of  machine \nlearning or self-improvement should by definition not be subject to \nAI-regulation. In the same spirit, linear models, statistical methods \n \npage 3 / 6', ""and  basic  facilitation  asset  aiming  to  accept  and  explain  more \ncomplex algorithms should also stay out of scope.   \n\uf0b7  For example, including algorithms which determine insurance pre-\nmiums in the scope should only be considered if their decision \nrules are self-learned and if the modeling strategy is beyond the \ntraditional generalized linear models (GLM), which have been ac-\ncepted for decades. The provision of private insurance cover has \nalways required respective providers to analyse respective data \nand apply the results accordingly by means of mathematical algo-\nrithms. Ever since the insurance industry has emerged, insurance \nundertakings have therefore made extensive use of data and algo-\nrithms.  These  classical  algorithms  and  data  analyses  are  well \nknown and authorities are able to supervise them. There is no \nneed for additional regulation in these areas. \n\uf0b7  Any regulatory approach should take into account sectoral charac-\nteristics of different industries. A one-fits-all solution is not appro-\npriate for the regulation of AI.  \n \n \n2.  Liability and Product Safety \n \nExisting principles of liability for damage caused to third parties are fit for \npurpose (adequate and appropriate) to address the risks posed by AI and \nother emerging digital technologies. Existing product safety legislation \nshould be reviewed as to its fitness in the context of AI and other emerg-\ning digital technologies. New legislation should only be undertaken where \na clear evidence base demonstrates a need for action, and should be pro-\nportionate to the concrete risks posed by the AI application in question. \n \n\uf0b7  The Product Liability Directive (PLD) is technology-neutral and \ntherefore applies to AI in the same way as to conventional tech-\nnology. It constitutes a well-balanced system by providing a high \nlevel of protection to injured persons while at the same time taking \ninto account producers' legitimate interests and thereby encourag-\ning technological innovation and promoting economic growth.  \n \nUnder the PLD, a product is defective if it does not provide the \nsafety which a person is legitimately entitled to expect. Product \nsafety and product security legislation act as a filter for liability and \nhelp to define whether a product does not provide the safety which \na person is entitled to expect and is therefore defective under the \nPLD.  Rather  than  altering  the  existing  product  liability  system, \nevaluating current product safety legislation should therefore be \nprioritised. \n \n \npage 4 / 6"", 'Software should be considered a product under the PLD. In case \nof “embedded software”, potential difficulties in determining wheth-\ner a product or a service has caused damage will be mitigated by \nrecognizing software as a product. \nAI systems are controlled by software. Access to data stored by \nthese systems will be key to establishing fault and thereby allocat-\ning responsibility for damage either to the producer (if caused by a \nproduct defect) or to the user (if caused by the circumstances of \nuse). In this sense, it may actually be easier for a person damaged \nby an AI system to establish the responsible party than for a per-\nson injured by a traditional system. Questions of data recording, \nstorage and access are therefore crucial, but should be addressed \noutside of liability legislation (e. g. by product safety and security \nlegislation). \n \nA risk-based approach is highly appropriate for determining prod-\nuct safety and security standards for AI applications as the term \nencompasses a multitude of uses and devices that will require \nspecific solutions. By contrast, because of the PLD’s technology-\nneutral nature, an additional risk-based differentiation prescribing \nmore stringent levels of liability within the PLD for certain products \ndeemed especially dangerous is not required. \n \n\uf0b7  At present, existing national liability regimes addressing the liabil-\nity of a user or deployer of AI systems provide adequate means \nof redress to persons incurring damage from AI-systems, also tak-\ning into account that these persons should have access to the \nsame level of compensation as those incurring damage from tradi-\ntional systems (which is the case under applicable national law). \n \nFully autonomous and self-learning AI systems are not at this time \nmarket-ready and will not become so for the foreseeable future. \nAdditional harmonised rules on deployer liability should only be in-\ntroduced on the basis of clear evidence that existing liability regula-\ntion fails to address specific risks posed by the operation of such AI \nsystems. Legislating on liability for such highly advanced systems \nshould therefore be deferred until their specific risk potential can be \nfully understood. \n \nLegal certainty requires that the coherence of national and EU leg-\nislation be preserved, i. e. in respect of AI systems that fall under \nexisting legislation on motor vehicles and aircraft. For all motor ve-\nhicles and aircraft, strict liability is already in place at the national \nlevel. \n \n \npage 5 / 6', 'Infringements of basic rights (data protection, discrimination, priva-\ncy) should continue to be dealt with exclusively in existing dedicat-\ned EU legislation such as the GDPR. Basic rights infringements \nare alien to existing civil liability concepts (to include the PLD) and \nadding related provisions to liability legislation could only provoke a \nconflict of statutes. Consistency and coherence between the vari-\nous legislative instruments at EU level must be preserved. \n \n\uf0b7  Additional compulsory insurance requirements are unnecessary.  \n \nA free voluntary insurance market is best able to provide tailored \ninsurance solutions that are designed to cover the individual in-\nsured\'s risks and liabilities. ‘AI applications’ covers a wide range of \ndifferent appliances and uses, and effective insurance protection \nmust be geared towards individual risk exposure. Compulsory in-\nsurance, on the other hand, of necessity introduces a ""one size fits \nall"" approach and only works well where a large pool of identical or \nsufficiently similar risks exists, allowing insurers to draw on suffi-\ncient data to quantify and price these risks. \n \nInsurance solutions are readily available to cover the liabilities of \nproducers and deployers of all kinds of AI systems. Their liability is \ncovered as a matter of course in corporate liability insurance con-\ntracts. Although voluntary, corporate liability insurance, including \nproduct liability insurance, is standard for companies of all sizes \nand from all sectors of the economy.  \n \n \nBerlin, September 2020 \n \npage 6 / 6']"
F550924,10 September 2020,-,-,-,-,-,-,Künstliche Intelligenz – ethische und rechtliche Anforderungen,We welcome the opportunity to comment on the European Commission’s Inception Impact Assessment on 'Proposal for a legal act of the European Parliament and the Council laying down requirements for Artificial Intelligence'. Please find attached our contribution.,"['SAP Response to Inception Impact Assessment (IIA) on the EU AI Regulatory \nFramework \n \nWe strongly agree with the Commission’s assessment that Europe needs a harmonized policy \nframework for AI in order to establish a single market for AI products and services, to provide legal \ncertainty for AI developers and users, and to build consumers’ trust in the technology across Europe.  \nAs addressed by the IIA, if the European Commission proceeds with a regulatory intervention for AI, \na clear and precise definition will need to be established for the sake of legal clarity, the effective \nenforcement of an EU regulation and having a common understanding of the technology.  \nWe differentiate between two types of AI systems: \n•  Rule-based AI systems are characterized by the fact that the behaviour of their components is \nfully defined by rules created by human experts. These systems are often described as symbolic \nor expert systems.  \n•  Learning-based AI systems are differentiating themselves by the fact that their initial \nconfiguration made by humans is only the basis for the final form of their functions. With the \nhelp of data, they train how to solve a problem and continuously adapt their function in this \nprocess. For learning-based AI systems, humans define the problem and the goal, but the \nbehaviour rules and relationships required for the solution are learnt in an automized way.  \nAs the IIA points out, the potential harms identified by the Commission are not new and not \nnecessarily related to AI only. SAP is of the opinion that concerns around AI needs to be addressed, \nbut the context and purpose of AI systems will be key to determine the implications and relevance of \nthe ethical and legal challenges that may emerge in specific use cases.  \nTherefore, we believe it is important not to look at solely the specific characteristics of AI systems \n(opaque decision-making) but to clarify concrete risks associated with some use cases of AI based on \nan outcome-based risk assessment. \nWe believe that the concerns addressed in the White Paper and the objectives outlined above can \nbe achieved with the revision of horizontal and sector specific EU legislation outlined in the White \nPaper. As mentioned by IIA, the Commission has already started the revision of the EU legislative \nframework on safety and liability to address AI related concerns. In addition, we can build on our \nexisting comprehensive legal framework for data in the European Union, in particular for the \nprocessing of personal data under the GDPR. Building on the GDPR is a major step to foster a digital \ntrust that is essential for AI acceptance by individuals and other stakeholders.  \nWe recommend the European Commission to complete the review of EU legislation in other areas \nthat are potentially applicable to AI and make them fit for AI (e.g.: employment, anti-discrimination \nand relevant sector specific legislation). This is the first crucial step to have a holistic and \ncomprehensive overview on the identified legislative gaps in order to address them through \nadditional guidance or concrete amendments of existing EU legislation. \nA potential EU regulatory framework for AI should make an emphasis on the protection of \nindividuals and analysing risks from the perspective of the consumer. Given the identified legislative \ngaps, if the European Commission deems that a regulatory intervention for AI is necessary, we \nsupport a principle-based framework limiting the scope to high risks AI systems arising in B2C \ncontext impacting consumers directly. B2B AI applications should be left out of scope with B2B \n1', 'matters handled by the supply chain effect (i.e. B2C players cascade their requirements through the \nsupplier network). Risks related to B2B AI applications can be addressed in private contract ensuring \na fair allocation of risks among developer and deployer. This way, compliance with any AI regulatory \nrequirements will be ensured throughout the supply chain by private contracts.  \nWhen defining the scope of high-risk AI systems, it will be important to tailor mandatory \nrequirements and potential compliance costs to the actual risks imposed by the AI system. The \nEuropean Commission should also take into account companies’ existing business practises on \naddressing AI related ethical concerns.  \nIf the European Commission deems to proceed with a regulatory framework addressed to high-risk \nB2C AI systems, we strongly recommend adopting a legislative instrument consisting of high-level \nprinciples that could be transformed into operational requirements set up in the form of industry-\nled standards/certification schemes. \nSuch a regulatory framework should be enforced through the application of existing self-assessment \ntools for Trustworthy AI systems such as the DPIA under GDPR that is built upon companies’ existing \npractises. \nWe believe that a potential certification scheme should be process -based for high-risk AI systems \ninstead of individual product or algorithm-based certifications in order to avoid “repeated \nassessments over the lifetime of AI systems” as suggested in the White Paper. With process \ncertification, one can provide the required insights into the best practises of AI Ethics applied to the \ndevelopment and deployment activities that each AI system undergoes. Therefore, it would enable \nnew product versions without the need to re-assess AI systems throughout their lifetime each time. \nThis will require defining the criteria and a methodology for evaluation. Following this approach, a \nprocess-based certification scheme could serve as the baseline across industries that will provide a \ntransparent and effective processes to develop and deploy Trustworthy AI systems. \nFinally, we would caution the European Commission against the application of new ex-ante \nconformity assessments that could cause significant delays in releasing AI products and services to \nthe European market. \n \n \n2']"
F550923,10 September 2020,Jethro Schiansky,Wirtschaftsverband,EUnited AISBL,sehr klein (1 bis 9 Beschäftigte),0289344948-82,Belgien,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"EUnited is convinced that artificial intelligence (AI) drives and contributes to the digital and green transformation. It can do so across a broad range of sectors from healthcare to manufacturing and beyond, which in turn will lead to badly needed economic growth and the ability to meet numerous other societal challenges.
EUnited supports the principle and pursuit of trustworthy, human-centric, and ethical AI in Europe. Nevertheless, it reminds the Commission that – contrary to the statements made in the Inception Impact Assessment - the reality today is that uptake and investment in AI is greatest in areas of the world where no regulation yet hampers or stifles its deployment. Europe is currently losing the global AI race. The stipulation of trustworthy AI being the major force for gaining a competitive edge vis-à-vis China and the US is unrealistic. We believe it is therefore of vital importance to strike the right balance between EU intervention on the one hand and ensuring the competitiveness and innovation potential of our businesses, whilst protecting the rights of our citizens, on the other. In general, we believe that more work can be done to boost EU businesses by, for example, open-sourcing datasets and sharing ongoing research. Finally, it should also be pointed out that the compliance costs related to - what might appear to be only minor regulatory interventions - may be decisive for the competitiveness of many European businesses, especially SMEs. 
EUnited favours Policy Option 1 contained in the Inception Impact Assessment (IIA). We believe that this soft law approach could quickly allow industry to harness the potential of ongoing national initiatives thus increasing uptake of AI systems across many industrial sectors and thereby accelerating the green and digital transformation. A good way to ensure that the complex socio-technical systems being created with AI will in practice adhere to ethical standards is to actively involve all people devising and implementing such systems. Option 1 ensures the involvement of those taking responsibility and accountability for the ethical implications. At this stage a soft law approach is preferable to risking a serious disturbance of this transformation. Such disturbance could arise as a result of a lengthy and difficult legislative procedure borne out of the desire to introduce hard law to regulate only a very few contentious applications, already covered by other parts of EU law (e.g. data, privacy, safety and liability). 
Whilst EUnited also believes that there may be some benefit to pursuing regulation for highest risk applications (i.e. Policy Option 3(1)), we think that it is premature given the difficulties we will inevitably face in distinguishing risk levels related to vastly different use-cases of similar or identical applications, all in a rapidly evolving context. To put it another way, an application such a facial recognition could be totally benign or extremely risky depending on the use-case. Sectors such as ours rely heavily on innovation through offering new functionalities to the customer. Indeed, this is the key to remaining competitive. So a one-size-fits-all approach to the regulation of different applications of AI is not in the interest of Europe. EUnited therefore clearly rejects an Option 3(3) approach. It adds no additional benefit while seriously damaging European competitiveness.
The potential for European industry to support the green and digital transformation through uptake of AI shouldn’t be overlooked. The European institutions should keep the competitiveness of Europe at the forefront when considering any legal intervention on AI, particularly given the breadth of coverage of the exiting legal framework. EUnited recommends the approach set out in Policy Option 1 to best strike the balance at this stage.
"
F550917,10 September 2020,Bruno Min,NRO (Nichtregierungsorganisation),Fair Trials,klein (10 bis 49 Beschäftigte),302540016347-29,Vereinigtes Königreich,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"Fair Trials is aware that the Inception Impact Assessment considers the entire and wide array of economic and societal issues in relation to AI across a spectrum of industries and social activities. However, this response is concerned specifically with AI in the context of criminal justice and will consider the legal and policy measures needed for the regulation of AI in criminal justice.

Rapid technological advancements in recent years have made machine learning and algorithmic automated decision-making systems, often referred to as artificial intelligence (‘AI’) a prominent aspect of our lives. There is little doubt that these systems have great capacity to increase human potential and improve the lives of many, but their increasing role in assisting important public functions has also highlighted serious risks and challenges, particularly in the context of criminal justice. If not subject to proper regulation and oversight, AI can threaten fundamental human rights and, far from expanding human potential, it can amplify and worsen harmful aspects of our society, including inequality and injustice.

Fair Trials is grateful for the opportunity to submit feedback on the EU Commission’s AI Inception Impact Assessment. We submitted a response to the EU Commission’s consultation on the White Paper, ‘Regulating Artificial Intelligence for Use in Criminal Justice Systems in the EU’, which provided evidence and analysis about the negative impact that ‘AI’ can have on criminal justice.

We are pleased that the EU Commission recognises that AI represents risks for fundamental rights, including the right to a fair trial, as well as the need for improvements to the EU’s legislative framework on AI, particularly the ‘effective application and enforcement of existing EU and national legislation’, as well as the ‘limitations of scope of existing EU legislation’.

We are also pleased to see that the objectives identified in the Roadmap include preventing or minimising significant risks for fundamental rights, as well as ensuring the effective enforcement of rules of existing EU law to protect fundamental rights and avoid illegal discrimination. However, with regards to the policy options stated in the Roadmap, in order to achieve the above stated objectives to protect and minimise risks to fundamental rights and prevent discrimination, we believe that there is a clear need for a legislative solution under Option 3. For the purposes of protecting rights in criminal justice, any legislative proposal would at the very least need to cover ‘high-risk’ applications (Option 3, second sub-option b), but more comprehensive protection may be offered by a legislative act which covered all AI applications (Option 3, third sub-option c).","['EU Commission \nRoadmap Inception impact assessment: Proposal for a legal act of the European Parliament and \nthe Council laying down requirements for Artificial Intelligence \nFair Trials response \n(10 September) \n \nIntroduction \nFair Trials is aware that the Inception Impact Assessment considers the entire and wide array of \neconomic and societal issues in relation to AI across a spectrum of industries and social activities. \nHowever, this response is concerned specifically with AI in the context of criminal justice and will \nconsider the legal and policy measures needed for the regulation of AI in criminal justice. \nRapid technological advancements in recent years have made machine learning and algorithmic \nautomated decision-making systems, often referred to as artificial intelligence (‘AI’) a prominent \naspect of our lives. There is little doubt that these systems have great capacity to increase human \npotential and improve the lives of many, but their increasing role in assisting important public \nfunctions has also highlighted serious risks and challenges, particularly in the context of criminal \njustice. If not subject to proper regulation and oversight, AI can threaten fundamental human rights \nand, far from expanding human potential, it can amplify and worsen harmful aspects of our society, \nincluding inequality and injustice. \nThere are differences of opinion as to the definition of AI and its true meaning, but for the purposes \nof  this  submission, we  are  referring  broadly  to  automated  decision-making systems  based  on \nalgorithms, including machine-learning, which are used in the criminal justice system. \nFair Trials is grateful for the opportunity to submit feedback on the EU Commission’s AI Inception \nImpact Assessment. We submitted a response to the EU Commission’s consultation on the White \nPaper, ‘Regulating Artificial Intelligence for Use in Criminal Justice Systems in the EU’, which provided \nevidence and analysis about the negative impact that ‘AI’ can have on criminal justice.  \nWe are pleased that the EU Commission recognises that AI represents risks for fundamental rights, \nincluding the right to a fair trial, as well as the need for improvements to the EU’s legislative framework', 'on AI, particularly the ‘effective application and enforcement of existing EU and national legislation’, \nas well as the ‘limitations of scope of existing EU legislation’.1  \nWe are also pleased to see that the objectives identified in the Roadmap include preventing or \nminimising significant risks for fundamental rights, as well as ensuring the effective enforcement of \nrules of existing EU law to protect fundamental rights and avoid illegal discrimination.2 \nHowever, with regards to the policy options stated in the Roadmap, in order to achieve the above \nstated objectives to protect and minimise risks to fundamental rights and prevent discrimination, we \nbelieve that there is a clear need for a legislative solution under Option 3. For the purposes of \nprotecting rights in criminal justice, any legislative proposal would at the very least need to cover \n‘high-risk’ applications (Option 3, second sub-option b), but more comprehensive protection may be \noffered by a legislative act which covered all AI applications (Option 3, third sub-option c). \n \nArtificial intelligence and criminal justice: EU legislative proposals \nIn recent years, AI, comprising machine-learning and other analytical algorithm-based automated \ndecision-making systems, has been increasingly deployed in criminal justice systems across the world, \nplaying an increasingly significant role in the administration of justice in criminal cases. This trend is \noften driven by perceptions about the reliability and impartiality of technological solutions, and \npressures to make cost savings in policing and court services. However, studies in various jurisdictions, \nincluding in Europe, provide substantial evidence that AI and machine-learning systems can have a \nsignificantly negative influence on criminal justice. \nAI systems have been shown to directly generate and reinforce discriminatory and unjust outcomes; \ninfringing fundamental rights, they have been found to have little to no positive influence on the \nquality of human decisions, and they have been criticised for poor design that does not comply with \nhuman rights standards. \nMost AI systems used in criminal justice systems are statistical models, based on data which is \nrepresentative of structural biases and inequalities in the societies which the data represents, and \nwhich is always comprehensively lacking in the kind of detail that is needed to make truly ‘accurate’ \npredictions or decisions. The data used to build and populate these systems is mostly or entirely from \nwithin criminal justice systems, such as law enforcement or crime records. This data does not \nrepresent an accurate record of criminality, but merely a record of law enforcement - the crimes, \nlocations and groups that are policed within that society, rather than the actual occurrence of crime. \nThe data reflects social inequalities and discriminatory policing patterns, and its use in these AI \nsystems  merely  results  in  a  reinforcement  and  re-entrenchment  of  those  inequalities  and \ndiscrimination in criminal justice outcomes.  \n \n1 https://ec.europa.eu/info/sites/info/files/commission-white-paper-artificial-intelligence-feb2020_en.pdf  \n2 https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=PI_COM:Ares(2020)3896535&rid=3', 'Given these extremely serious risks, strong regulatory frameworks are needed to govern the use of \nAI in criminal justice decision-making and, which in some circumstances, may restrict its use entirely. \nWe believe that unless it is subject to robust regulation, it is unlikely that AI can be used in criminal \njustice systems without undermining the right to a fair trial. There is a need for clear and enforceable \nstandards and safeguards to protect the right to a fair trial, as well as preventing discrimination in \nthe criminal justice system. \n \nIssues with the existing EU legal framework \nIn theory, existing EU data protection laws restrict the use of automated decisions, but there are gaps \nand ambiguities that could result in the use of AI systems in ways that undermine human rights. The \ncurrent legal framework governing the use of automated decision-making systems do not sufficiently \nprotect individuals, such as those engaged with the criminal justice system, from the systems or the \ndecisions they produce or influence.  \nExisting EU laws intend to restrict the use of automated decisions, with Article 22 of the General Data \nProtection Regulation (‘GDPR’) providing that data subjects have the right not to be subject to \ndecisions ‘solely’ based on automated processes, where they produce ‘legal effects’ concerning them, \nor where they ‘similarly significantly affect’ them. The Law Enforcement Directive (‘LED’) has a similar \nprovision at Article 11, which requires Member States to prohibit decisions based solely on automated \nprocessing, where they produce ‘adverse legal effects’ on the individual, or effects that are ‘similarly \nsignificant’. \nAs we noted in our submission to the EU Commission’s consultation on the White Paper, there are \nthree issues with the existing legislative framework governing automated decision-making systems \nunder both the GDPR and the LED. These ambiguities and potential loopholes could be exploited in \nways that seriously undermine the general prohibition of automated decision-making processes, and \nadversely impact human rights.  \nFirstly, the provisions in the GDPR and LED only prohibit decisions based ‘solely’ on automated \nprocesses. In other words, the laws regulate the impact of decisions made through automated \nprocessing, but not the AI systems themselves. As discussed later in this paper, the main human rights \nchallenges of AI systems can be attributed to how they are designed and trained, and the types of \ntechnology used, such as machine-learning, so it is crucial that decisions about the design and \ndeployment of AI systems are also regulated. \nSecondly, neither the GDPR nor the LED provide regulatory standards to govern situations where \nautomated processing is not the ‘sole’ basis of a decision, but a primary influencer. In reality, the \ndifference between a fully automated decision and a decision made with a ‘human-in-the-loop’ is not \nalways clear, but because of this strict classification, AI systems are able to be used and have significant \nlegal effects without the corresponding safeguards. Stronger legal standards are needed to make sure \nthat semi-automated decision-making processes do not become de facto automated processes.', 'Thirdly, the prohibition on automated decision-making is subject to two very broad exceptions. \nAutomated decisions are prohibited under the GDPR and LED, ‘unless authorised by Union or Member \nState law’ and there need to be ‘appropriate safeguards for the rights and freedoms of the data \nsubject, at least the right to obtain human intervention’.3 These provisions give extremely wide \ndiscretion to Member States to override the general prohibition. It is significant that EU laws \nemphasise the need for human rights safeguards, and the need to ensure the possibility of human \ninterventions, but neither of these concepts have yet been adequately defined. Although influential \nactors like the EU and the Council of Europe have established principles on the ethical and responsible \nuse of AI, there is currently no authoritative guidance on the practical safeguards that need to be in \nplace.4 Likewise, the meaning of ‘human intervention’ is open to interpretation. LED provides some \nguidance on who should be carrying out the human intervention,5 but there needs to be greater clarity \non what meaningful human intervention entails in different contexts. \nIn recognition of this challenge, the European Data Protection Board has recommended that in order \nfor decisions to be regarded as not ‘based solely’ on automated processing for the purposes of Article \n22 GDPR, there has to be ‘meaningful’ human oversight, rather than just a token gesture.6What \n \nqualifies as ‘meaningful’ intervention is open to interpretation, and it is likely to differ depending on \nthe circumstances and the type of decision being made. In the context of criminal justice procedures, \nwhere decisions often have particularly severe and far-reaching implications for individuals’ rights, \nsafeguards for ensuring meaningful human intervention have to be especially robust. \nThese ambiguities and potential loopholes could be exploited in ways that seriously undermine the \ngeneral prohibition of automated decision-making processes, and adversely impact human rights.  \nIt is necessary, therefore, that the EU provides further guidance on how these provisions should be \ninterpreted, including through legislation (if appropriate) to further clarify the circumstances in \nwhich Member States are allowed to deploy AI systems for criminal justice proceedings.  \nThere is also a need for procedural safeguards that ensure ‘meaningful’ human oversight; no \nindividual should be subject to an automated decision which engages their human rights without \nmeaningful  human  input.  There  must  also  be  greater  clarity  on  what  meaningful  human \nintervention entails in different contexts, to prevent mere administrative (e.g ‘box-ticking’) human \ninvolvement  in  such  automated  decisions,  without  either  any  or  sufficient  engagement  or \nconsideration, from overriding protections in legislation, such as notification to an individual where \nthey have been subject to an automated decision. \nProcedural safeguards which can be put in place to ensure meaningful human input include: \n \n3 Article 11(1), LED; Article 22(2)(c) and (3), GDPR  \n4 On the other hand, civil society organisations, such as the ‘Partnership for AI’ and ‘AI Now’ in the United \nStates have attempted to address this gap through various recommendations and guidelines \n5 Recital 38, Law Enforcement Directive \n6 Article 29 Data Protection Working Party, ‘Guidelines on Automated individual decision-making and profiling \nfor the purposes of Regulation 2016/679’ (3 October 2017)', 'a)  making it a legal requirement for decision-makers to be adequately alerted and informed \nabout the risks associated with AI systems;  \nb)  making AI systems’ assessments intelligible to decision-makers;  \nc)  requiring decision-makers to provide full, individualised reasoning for all decisions \ninfluenced by an AI system; and  \nd)  making it easier for decision-makers to overrule AI assessments that produce unfavourable \noutcomes for defendants.  \n \nPresumption of innocence \nThe right to be presumed innocent in criminal proceedings is a basic human right, and one that is \nexpressly recognised in, and safeguarded by EU law under Directive 2016/343 (the ‘Presumption of \nInnocence Directive’).7The increasing use of AI in the sphere of criminal justice, however, raises \n \nquestions about the scope of this right, and how AI systems should be built and used to protect it. \nConcerns about how AI systems undermine the presumption of innocence have been voiced in the \ncontext of certain types of predictive policing software,8 with a variety of predictive policing tools that \naim to facilitate preventative policing measures and to deter crimes before they have taken place \ndeveloped and deployed across Europe.9  \nThese predictive tools can be regarded as part of a broader trend in law enforcement that moves away \nfrom ‘reactive’ policing, and towards ‘preventative’ or ‘proactive’ policing. These tools intend to \n  \npursue legitimate objectives of preventing, or reducing harm,10but there are serious concerns that \n \nthese systems single-out individuals as ‘pre-criminals’, who are subject to police interventions even \nthough they are not formally suspected of any crime, and there is no evidence that they have done \nanything wrong.11 It is of further concern that these types of predictive policing tools do not \nnecessarily designate individuals’ risk levels on the basis of their past actions, or behaviour that can \nbe regarded as ‘suspicious’ in any way, but on account of factors far beyond their control, and \nimmutable characteristics. In particular, there is strong evidence to suggest that AI systems have a \ntendency to overestimate the risks of criminality of certain ethnic and racial groups. \n \n7 Directive (EU) 2016/343 of the European Parliament and of the Council of 9 March 2016 on the strengthening \nof certain aspects of the presumption of innocence and of the right to be present at the trial in criminal \nproceedings; Article 6(2), ECHR   \n8 Alan Turing Institute, ‘Using analytics in policing: Ethics Advisory Report for West Midlands police’ (2018), \nhttps://www.turing.ac.uk/news/using-analytics-policing-ethics-advisory-report-west-midlands-police     \n9 Fieke Jansen, ‘Data Driven Policing in the Context of Europe’ (2018) https://datajusticeproject.net/wp-\ncontent/uploads/sites/30/2019/05/Report-Data-Driven-Policing-EU.pdf  \n10 Alan Turing Institute, ‘Using analytics in policing: Ethics Advisory Report for West Midlands police’ (2018), \nhttps://www.turing.ac.uk/news/using-analytics-policing-ethics-advisory-report-west-midlands-police  \n11 Hettie O’Brien, ‘The police know what you’ll do next summer’, New Statesman (15 August 2019) \nhttps://www.newstatesman.com/politics/uk/2019/08/police-know-what-you-ll-do-next-summer', 'Many AI and algorithmic systems used in criminal justice systems are statistical models, based on data. \nThe data used to build and populate these systems is mostly or entirely from within criminal justice \nsystems, such as law enforcement or crime records. This criminal justice data is representative of \nstructural  biases  and  inequalities  in  the  societies  which  the  data  represents,  and  is  always \ncomprehensively lacking in the kind of detail that is needed to make truly ‘accurate’ predictions or \ndecisions. This data does not represent an accurate record of criminality, but merely a record of law \nenforcement – the crimes, locations and groups that are policed within that society, rather than the \nactual occurrence of crime. The data reflects social inequalities and discriminatory policing patterns, \nand its use in these AI systems merely results in a reinforcement and re-entrenchment of those \ninequalities and discrimination in criminal justice outcomes.12 \nWhile it is clear that certain types of predictive policing infringe the presumption of innocence from a \nmoral and ethical viewpoint, it is unclear whether these systems are considered to violate the legal \npresumption of innocence under EU law and international human rights law as currently formulated \nand applied. Even if the current language on the presumption of innocence is such that it is not directly \napplicable to the predictive policing context, it must be recognised that these tools nevertheless \ninterfere with human rights.  \nAlthough predictive policing tools do not directly ‘convict’ people, they not only allow the police to \ntreat legally innocent individuals as pseudo-criminals, but they can also result individuals being \ndeprived of their basic rights with regard to education, housing, and other public services – effectively \n‘punishing’ them on account of their profiles. This seriously damages the fundamental human rights \nprinciple that the matter of guilt or innocence can only be determined by means of a fair and lawful \ncriminal justice process.13 These types of high impact, fact-sensitive decisions should never be \ndelegated to automated processes, either wholly or partly, particularly those which ultimately operate \nby identifying correlations rather than causal links between an individual’s characteristics and their \nlikely behaviour.  \nIn order to achieve this, and ensure these protections are respected by law enforcement and \ncriminal  justice  agencies,  strong  and  clear  regulation  is  needed.  There  must  be  clear  legal \nrequirements to ensure that AI systems respect the presumption of innocence and do not operate \nin a way which pre-designates an individual as a criminal before trial, nor allow or assist the police \nto take unjustified, disproportionate measures against individuals without reasonable suspicion.  \n \n \n12 Lum, Kristian, and William Isaac. 2016. ‘To Predict and Serve?’, Significance 13 (5): 14–19, \nhttps://rss.onlinelibrary.wiley.com/doi/full/10.1111/j.1740-9713.2016.00960.x; Bennett Moses, L., & Chan, J. \n(2016). ‘Algorithmic prediction in policing: Assumptions, evaluation, and accountability’. Policing and Society. \nhttps://www.tandfonline.com/doi/10.1080/10439463.2016.1253695; Barocas, S. and Selbst, A.D., 2016. ‘Big \nData’s disparate impact’. California Law Review, 104, 671. \nhttps://papers.ssrn.com/sol3/papers.cfm?abstract_id=2477899  \n13 ECHR, Article 6(2)', 'Discrimination \nOne of the most frequent and well-evidenced criticisms of AI systems and their use in criminal justice \nsystems is that they can lead to discriminatory outcomes, especially along racial and ethnic lines. As \nnoted above, there are fundamental issues with the way AI systems are designed and created which \ncan lead to bias. AI built on data embedded with such biases and used to assist, inform, or make \ndecisions in the criminal justice system, can expand and entrench the biases represented in the data.14 \nThis conclusion has been echoed by the UN Rapporteur on racism, who stated in her most recent \nreport that emerging digital technologies driven by big data and artificial intelligence “are entrenching \nracial inequality, discrimination and intolerance”.15 \nThe dangers of the failure to adequately regulate the use of AI to prevent such discrimination are clear \nand have been witnessed in Europe. The ‘Crime Anticipation System’ (‘CAS’), a predictive policing \nsoftware used across the Netherlands, was initially designed to consider ethnicity as a relevant factor \nfor determining the likelihood of a crime being committed. Amongst the indicators used by CAS to \npredict crimes in a particular area was the number of ‘non-Western allochtones’ in the area – in other \nwords, ‘non-Western’ individuals with at least one foreign-born parent.16 The software not only \n \npresupposed the existence of a correlation between ethnicity and crime, but also singled out a \ncategory of ethnicities to be of particular concern, given that the presence of ‘Western’, ‘autochtone’ \nindividuals were not used as indicators. Furthermore, given that ‘Western’ was defined somewhat \nsubjectively (for example, including individuals of Japanese or Indonesian origin, and including all \nEuropean  nationalities,  apart  from  Turkish),  CAS  incorporated  highly  questionable  societal \ncategorisations and biases.  \nIn  Denmark,  an  automated  algorithmic  assessment  has  been  used  to  classify  different \nneighbourhoods, based on criteria such as unemployment, crime rates, educational attainment, and \nother ‘risk indicators’, as well as whether the levels of first and second-generation migrants in the \npopulation is more than 50%. Neighbourhoods which meet these criteria are classified as ‘ghettos’. \nThese neighbourhoods are then subject to special measures, including higher punishments for \ncrimes.17It is clearly discriminatory, as well as entirely unfair and an affront to equality of arms, for \n \npeople living in certain areas to be punished more severely than others in different areas for the same \ncrimes.  \nThese examples illustrate the need for regulations to ensure that AI systems are designed to be non-\ndiscriminatory, and to exclude categorisations and classifications that deepen and legitimise social \nbiases and stereotypes. \n \n14 Lum, Kristian, and William Isaac. 2016. ‘To Predict and Serve?’ Significance 13 (5): 14–19, \nhttps://rss.onlinelibrary.wiley.com/doi/full/10.1111/j.1740-9713.2016.00960.x  \n15 https://www.ohchr.org/Documents/Issues/Racism/SR/HCR44_Statement_15_July_2020.pdf  \n16 Serena Oosterloo and Gerwin van Shie, ‘The Politics and Biases of the “Crime Anticipation System” of the \nDutch Police’ (2018), http://ceur-ws.org/Vol-2103/paper_6.pdf  \n17 Algorithm Watch, ‘Automating Society’ (2019), https://algorithmwatch.org/en/automating-society-\ndenmark/', 'Fair Trials’ view is that the only effective way in which AI systems can be regarded as non-\ndiscriminatory is if they have been subject to rigorous independent testing for biases. These tests \nmust be mandated by law, must be independently run, have clearly stated aims or objectives, and \nbe  carried  out  pre-deployment  to  reduce  the  likelihood  of  individuals  being  affected  by \ndiscriminatory profiling and decisions. There is, therefore, a clear need for a legislative solution to \nmandate these requirements. \n \nTransparency \nAI systems can have a significant influence over criminal justice decisions, and they should be open to \npublic scrutiny in the same way that all decision-making processes by public entities should be. \nHowever, a common criticism of many AI systems is that they lack transparency, which often makes it \ndifficult, if not outright impossible, to subject them to meaningful impartial analysis and criticism. This \nlack of transparency is both as a result of deliberate efforts to conceal the inner workings of AI systems \nfor legal or profit-driven reasons, and of the nature of the technology used to build AI systems that is \nuninterpretable for most, if not all humans. Criminal procedure should enable the full disclosure of all \naspects of AI systems that are necessary for suspects and accused persons to contest their findings. \nIn addition, AI systems’ decisions, or decisions they have influenced, also need to be contestable by \ncriminal defendants. This is so that they can not only challenge the outcomes of the AI systems’ \ncalculations and analyses, but also scrutinise the legality of their use. In other words, being able to \nchallenge AI systems in criminal proceedings is not only a procedural fairness requirement for \ndefendants, it is also a means by which legal standards governing AI systems and their use can be \nenforced. One of the major issues preventing the sufficient contestability of AI systems in criminal \nproceedings is the lack of notification. If an individual is not notified that they have been subject to an \nautomated decision by an AI system, they will not have the ability to challenge that decision, or the \ninformation that the decision was based on. There must be a requirement for individuals to be \nnotified, not just for “purely automated” decisions, but whenever there has been an automated \ndecision-making system involved, assistive or otherwise, that has or may have impacted a criminal \njustice decision. \nIn order to ensure the necessary transparency and notification in relation to AI systems in criminal \njustice, regulation is needed, so that individuals are notified of AI decisions, and the decisions can \nbe understood, scrutinised and challenged by their primary users, suspects and accused persons, as \nwell as the general public. \n \nRoadmap policy options \nIn order to regulate the current and future use of AI in criminal justice proceedings, as well as closing \nthe gaps in existing data protection laws, the EU must, at a minimum, set the following standards:', '1)  to govern the design and deployment of AI systems in criminal justice systems;  \n2)  to make sure that AI systems are used in accordance with human rights standards and prevent \ndiscrimination in criminal justice proceedings; and  \n3)  to guide Member States in governing the deployment of AI systems and monitor their \nsubsequent use.  \nFair Trials believes that these requirements can only be achieved by Option 3 in the Roadmap, an EU \nlegislative instrument establishing mandatory requirements in relation to AI applications. \nThe ‘Baseline’ of no EU policy change, the current status quo, is clearly not acceptable in this context, \ngiven the very real and increasing threats to the right to a fair trial posed by AI and algorithms in the \ncriminal justice system. \nSimilarly, Option 1 and Option 2 are not sufficiently robust to provide the protections needed, and \nultimately,  there is too much at stake in the criminal justice context – and indeed others – to leave it \nup to self-regulation by private companies. \nA soft-law approach as per Option 1, relying on industry to take the necessary action will clearly not \nbe appropriate in the context of criminal justice, in as much as it would rely on industry to set \nstandards or take a leading role in criminal justice systems. Nor would a ‘voluntary labelling scheme’ \nas under Option 2 be sufficient, as the protections and safeguards needed in the criminal justice \nsystem cannot be on a ‘voluntary’ basis.  \nIndustry has proven in the context of criminal justice that they are not able to meet even the minimum \nrequirements needed to protect and safeguard individuals subjected to AI systems. AI systems need \nto be transparent and explainable, so they can be understood and scrutinised by their primary users, \nsuspects and accused persons, and the general public. Due to commercial or proprietary interests, \nthese  minimum  requirements  are  often  not  met,  with  companies  selling  AI  systems  to  law \nenforcement and criminal justice agencies citing proprietary interests when refusing to be transparent \nabout the system or how it reaches decisions.18 Ultimately, there is too much at stake in the criminal \njustice context – and indeed others – to leave it up to self-regulation by private companies. \nAs the Roadmap notes, soft-law may only “marginally facilitate implementation of fundamental rights \nlegislation”, whereas “binding requirements will strengthen the respect of existing fundamental rights \nfor all AI systems covered”.19 \nAs a result, Option 3, an EU legislative instrument establishing mandatory requirements in relation to \nAI applications, is clearly the most appropriate option within those proposed in the Roadmap. Of the \nsub-options given, sub option a is potentially too narrow and restrictive to deal sufficiently with the \n \n18 Taylor R Moore, ‘Trade Secrets and Algorithms as Barriers to Social Justice’, Center for Democracy & \nTechnology (2017), https://cdt.org/files/2017/08/2017-07-31-Trade-Secret-Algorithms-as-Barriers-to-Social-\nJustice.pdf  \n19 https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=PI_COM:Ares(2020)3896535&rid=3', 'range of AI systems being used in the criminal justice system beyond such specific categories as \nbiometric  identification  systems,  though  such  systems  do  represent  a  significant  threat  to \nfundamental rights. The framing of sub-option b is too vague for us to be able to consider whether all \nthose AI and algorithmic systems in the criminal justice system would definitively be classified as ‘high-\nrisk’, although it is likely, but it is possible that a narrower definition with subjective interpretation \nmay not sufficiently capture all those systems which could pose a threat to the right to a fair trial. A \nlegislative act which covers all AI such as that proposed by sub-option c may be able to ensure proper \nand comprehensive protection, encompassing the range of AI and algorithmic automated decision-\nmaking systems, and provide a clear and sufficient solution to the challenges and problems posed by \nAI in criminal justice.']"
F550910,10 September 2020,José Antonio Parrilla,EU-Bürger/-in,-,-,-,Spanien,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"Since February 2020 I am part of PANELFIT, an EU 2020 research project whose objectives and outcomes are: 
- To facilitate the implementation of European data protection new regulation by producing a set of outcomes that serve as operational standards and practical guidelines able to reduce the ethical and legal issues (ELI henceforth) posed by ICT technologies while promoting innovation and market growth, enabling high-quality job creation and ensuring a high level of privacy and security/cybersecurity.
- To suggest possible concrete improvements to the current regulatory and governance framework for the same purpose
- To create mutual learning and support tools and to promote networking among stakeholders and policymakers
- To increase the quantity and quality of the information available to policymakers, professionals, journalists and the general public.

As part of these objectives, I submit comments on the EU Proposal for a legal act laying down requirements for Artificial Intelligence. The feedback focuses on the convenience of the different levels of intervention explained in the Proposal and two specific issues: the black box phenomenon as a problem for developing rules on AI-tools liability and the accountability principle (art. 5.2 GDPR) as a current rule to guarantee AI-tools transparency.","['Feedback  \non \nProposal for a legal act of the European Parliament and the Council laying down \nrequirements for Artificial Intelligence \nJosé Antonio Castillo Parrilla \nInvestigador Posdoctoral UPV – Cátedra Derecho y Genoma Humano UPV \n \nMember of the PANELFIT PROJECT, funded by the EC, GA number: No: 788039 \n \nContext of the comment \nAs highlighted in the proposal, AI is a rapidly evolving technology whose use can \ncontribute  to  economic  development  in  several  fields  such  as  health,  education  or \ntransport. Equally, AI evolution cannot be separated from the legal situation of data \nprocessing1 since AI tools use big amounts of data to obtain useful knowledge. \nProblems to be tackled and options \nThe  starting  point  of  the  proposal  seems  correct:  the  development  of  AI  (as  has \nhappened with other innovative technologies) needs a clear legal and ethical framework \nin order to guarantee legal certainty2. Legal certainty must be achieved, at least, at the \nlevel of EU law since this is a global legal challenge (EU law principle of subsidiarity). \nProtection  against  damages  is  highlighted  as  one  of  the  aims  of  future  regulatory \ndevelopment: (1) material harm (such as damages to health or individuals´ properties \nbecause of autonomous vehicles or AI-driven tools); (2) and immaterial harm (such as \nloss of privacy, limitations to the right of freedom of expression, and discriminatory \nsituations). \nMost of these damages, as stated correctly in the text, will not necessarily be linked to \nAI, but can be caused by poor quality data (either unreliable or poorly collected data), \nwhich confirms the importance of data processing techniques for the development of AI \nas well as their applicable law.  \nThe proposal presents four levels of intervention: 0 (no intervention), 1 (EU soft law), 2 \n(EU law on voluntary labelling scheme), 3 (compulsory law, either limited to specific \nareas – 3.1 and 3.2 – or general – 3.3-), 4 (a combination of options 1, 2 and 3). \n                                                           \n1 Vid. European Parliamentary Research Service, The impact of the General Data Protection Regulation \n(GDPR) on artificial intelligence, Junio de 2020. Disponible en: \nhttps://www.europarl.europa.eu/RegData/etudes/STUD/2020/641530/EPRS_STU(2020)641530_EN.pdf.  \n2 Vid. “Personal autonomy and the digital revolution”, in DE FRANCESCHI, A., SCHULZE, R., Digital \nRevolution – New Challenges for Law, C.H. Beck & Nomos, München, 2019, p. 17: “A robust', 'Feedback on the different levels of intervention \n-  General comment \nOption 4 seems the most advisable one: compulsory law on specific sectors (option 3.2) \ncombined with rules on a voluntary labelling scheme and soft law for low-risk areas or \nareas where political agreements have not been reached. \nPolitical agreements are key to develop rules to provide with legal certainty on AI. To \nget them, industry and civil society (and the academia) must collaborate as soon as \npossible to develop drafts to be submitted for consideration by the EU institutions. \n-  On option 0, explanatory notes and authentic interpretations \nNo legal intervention (option 0) would not solve legal uncertainty problems, but it \nwould  even  increase  them  according  to  the  proposal.  Nonetheless,  some  problems \nrelated to AI do not need a complete regulation nor even legal modifications: some of \nthe  problems  related  to  the  development  or  use  of  AI  only  need  to  adapt  the \ninterpretation of the current law to the new situation. This goal can be achieved through \nGuidelines or explanatory notes as, for instance, those issued by the EDPB on data \nprotection to favour a common and updated interpretation of the GDPR as well as solve \nspecific problems.  \nFor  instance,  an  explanatory  note  by  the  European  Commission  on  the  black  box \nphenomenon  and  EDPB  Guidelines  on  accountability  applied  to  AI  tools  would \ncontribute to diminish the problem of algorithms opacity regardless of the needed future \nlegal development to get major legal certainty (see below). \nThese documents can be doubly useful: they can help to mitigate a problem while a new \nlegal development is being discussed or eliminate the need for a legal development. \n-  On soft law (option 1) and voluntary labelling scheme (option 2) \nOptions 1 and 2 are a great complement for the necessary binding law on certain areas \n(option 3.2). What  areas should use soft law or voluntary labelling schemes? The \ncriteria, from our point of view, should be: (1) secondary issues of areas regulated by \nbinding  rules,  and  (2)  areas  where  political  agreements  have  not  been  possible  or \nsufficiently strong to develop binding rules. \n-  On the possibilities for binding rules (option 3 and sub-options) \nOption  3  offers  three  sub-options.  It  does  not  seem  reasonable  reducing  legal \ndevelopment to a single area of AI, nor trying to develop a generally comprehensive \nlegal development or single legal developments for each area of AI (sub-options 1 and \n3).  The  best  choice  is  to  identify  those  areas  that  represent  higher  risk  for  rights \ndamages (option 2). \nOption 3.2 needs, however, some considerations. Previously it would be advisable to \ndevelop a double analysis on the potential of option 0: first analysis, to identify what AI \nproblems could be solved or diminished by explanatory notes or guidelines issued by', 'EU institutions as authorized interpretations of the current law; and second analysis to \nidentify minor legal changes where needed. \nThis analysis would allow us to: \n-  (1) focus efforts on genuinely necessary regulatory developments, \n-  (2) increase the efficiency of these efforts, and \n-  (3)avoid  normative  hypertrophy,  which  would,  paradoxically,  increase  legal \ncertainty. \nSo,  after  discarding  subjects  related  to  AI  that  would  only  need  clarifying  notes, \nguidelines  or  minor  legal  changes,  a  list  of  AI-related  subjects  that  need  legal \ndevelopment can be elaborated. The Commission White Paper on artificial intelligence \nof 19 February 2020 (pp. 21 et seq.) provides us with criteria on the risk level.  \n-  On industry intervention \nThe involvement of industry is essential in the regulatory development of AI, as in the \ncase  of  any  other  technology,  since  industry  is  directly  involved  in  research  and \ndevelopment  of  AI.  No  other  actor  could  explain  and  highlight  better  problems, \nchallenges, limitations, legal needs or convenient options for regulatory frameworks. \nNonetheless, this is not a statement in favour of industry-led intervention, nor in favour \nof co-leadership between public institutions and the industry. As explained before, soft \nlaw and voluntary labelling schemes should be complementary. Moreover, it does not \nseem reasonable that one of the parties most affected by this future regulation would be \nthe one that develops and/or monitors its compliance. \nNevertheless, for those areas where political agreements are not possible by soft law, \nvoluntary labelling schemes or even self-regulatory codes developed and monitored by \nindustry are better than nothing.  \nFeedback on specific proposals \n-  Black box as a problem? Objective liability as an answer \nThe black box phenomenon refers to the difficulty (or even impossibility) of knowing \nentirely the reasoning process of the AI tool, that is, the path between entering big \namounts of raw data and obtaining useful knowledge. This phenomenon is argued either \nto (1) elude liability or to (2) dissuade of trespassing certain limits when developing \nrules to boost algorithm transparency.  \nThe  black  box  phenomenon  as  an  argument  to  elude  liability  (e.g.,  liability  for  a \ndefective product) works as follows: \n-  (1) it is impossible to know the reasoning process by which the algorithm takes a \ndecision or makes a suggestion (e.g., focus on certain people to investigate if there \nis a social security system fraud – SyRI case – select overwhelmingly white men \nfor certain jobs – Amazon recruitment algorithm, already retired -); \n-  (2) this makes it difficult (or impossible) to detect who beaks a rule, and when a \nrule is broken;', '-  (3) this situation renders it impossible to claim for liability due to bad development \nof the algorithm when it is not possible to know with certainty (a) that this is the \nproblem and/or (b) who introduced the wrong instructions or what instructions were \nrevealed as wrong in certain situations. \nThis argument may have been successful because we are faced with a relatively new \nchallenge. It is also impossible to know the human reasoning process, that is, the path \nbetween observing big amounts of information and making certain decisions based on \nour thoughts (e.g., a driver who overtakes recklessly). In certain situations, it can also be \nvery difficult to know exactly what went wrong in a car accident. Despite all this, \nliability rules have mechanisms to designate a party responsible for compensating the \ndamage,  even  if  it  has  been  impossible  to  know  exactly  who  or  what  caused  the \ndamage: that is, the objective liability paradigm. Objective liability rules for vehicles are \none of many examples.  \nObjective liability  is the current paradigm: subjective liability (or liability based on \nculpability) was demonstrated to be slightly useful in the XIX century, giving way to \nobjective  liability,  that  is,  liability  not  linked  to  culpability  but  based  on  the  risk \ngenerated by certain activities3 (such as driving a car). This (not so) new paradigm is \nbased on two assumptions: since (1) certain activities are beneficial or even necessary \nfor society, (2) society then must accept the risk generated by those activities and \ndevelop systems (objective or risk-based liability) to compensate unavoidable future \ndamages related to them. Most of the times, liability rules combine both culpability and \nrisk/benefit criteria. \nThere is no reason to exclude AI tools and their depending and related activities from \nthis liability paradigm. First, it would be necessary to accept that AI is a risky activity \n(as explained in the Commission White paper on artificial intelligence 2020) and, so, \nthat risk-based liability is the appropriate inspiring paradigm to develop liability rules \non the topic. Hence, the black box phenomenon is not a serious impediment to claim \ncompensation for damages or develop a system in the future that guarantees it. \nFor all that has been said, we must warn of the basic error that could be made if a future \ndevelopment of an AI civil liability system had, as its main reference, the subjective \ncivil liability paradigm, as it seems when reading the following statement: “if safety \nrisks materialise, the characteristics of AI technologies mentioned above may make it \ndifficult for persons having suffered harm to obtain compensation under the current EU \nproduct liability legislation because those characteristics make it difficult to obtain \ndocumentation that would allow to identify a person responsible for the damage and to \ntrace back the damaging outcome to a particular human action or omission”.  \nBlack box phenomenon is also argued when trying to dissuade the development of \ntransparency  binding  rules  for  algorithms  in  order  to  protect  fundamental  rights. \n                                                           \n3 Ulrich BECK, Risiko-Gesellschaft. Auf dem Weg in eine andere Moderne, Suhrkamp, Frankfurt am Mein, \n1986, Ulrich BECK, Risk Society, Towards a New Modernity, Sage Publication, London, 1992, (trad. por \nMark RITTER), Jonathan SIMON, “The Emergence of a Risk Society. Insurance, Law and the State”, \nSocialist Law Review, núm. 95, 1987, pp.61-89.', 'Nonetheless,  what  matters  is  developing  rules  to  obtain  a  level  of  algorithm \ntransparency that prevents discriminatory situations or violations of fundamental rights. \nThis  level  of  transparency  does  not  refer  to  “technical  transparency”  but  “legally \nsufficient  transparency”:  that  is,  as  much  transparency  as  necessary  to  avoid \ndiscrimination or violation of fundamental rights (or presumptions that make up for the \nlack of transparency), together with sanctioning criteria. Sanctioning criteria should be \nenough to dissuade AI developers and AI beneficiaries from violating fundamental \nrights or discrimination. None of these (legal) aims are contradictory to the existence of \nthe (technical) black box phenomenon. \n-  Accountability as an immediate tool to get algorithm transparency \nEDPB Guidelines and EDPS opinions have been revealed as very useful tools to get a \nmore  uniform  interpretation  and  application  of  GDPR  as  well  as  to  increase  its \nfulfilment. The last of these Guidelines clarifies the concepts of data controller and data \nprocessor and is open to feedback until 19th October 20204. \nAI tools are fed by massive data processing. In other words, whenever obvious, an AI \ntool implies multiple data processing situations. \nArt. 5.2 GDPR (accountability principle) states that a data controller will be responsible \nfor,  and  able  to  demonstrate  compliance  with  paragraph  1  (principles  relating  to \nprocessing of personal data). \nIf an AI tool process data, EDPB would be fully entitled to issue Guidelines on data \nprocessing  by  AI  tools  in  order  to  solve,  among  other  problems,  who  is  the  data \ncontroller and data processor or, to summarise, specify criteria for the fulfilment of \nGDPR when developing and using AI tools (including, for instance, (1) if/when and (2) \nhow the data controller shall carry out the assessment of the impact of the envisaged \nprocessing operations on the protection of personal data ex art. 35 GDPR)5. \nAmong the principles relating to the processing of personal data, we would like to \nhighlight the following: data shall be processed lawfully, fairly and in a transparent \nmanner (art. 5.1.a); data shall be collected for specified, explicit and legitimate purposes \nand not further processed in a manner that is incompatible with those purposes (purpose \nlimitation principle, art. 5.1.b); data shall be adequate, relevant and limited to what is \nnecessary in relation to the purposes for which they are processed (data minimisation \nprinciple, art. 5.1.c); or, data shall be processed in a manner that ensures appropriate \nsecurity of the personal data (art. 5.1.e). \nThe  data  controller  is,  according  to  art.  5.2  GDPR,  not  only  responsible  for  the \nfulfilment of these principles but also shall also be able to demonstrate it. There is no \n                                                           \n4 https://edpb.europa.eu/our-work-tools/public-consultations-art-704/2020/guidelines-072020-\nconcepts-controller-and-processor_en.  \n5 Article 35.1 GDPR states that “where a type of processing in particular using new technologies, and \ntaking into account the nature, scope, context and purposes of the processing, is likely to result in a high \nrisk to the rights and freedoms of natural persons, the controller shall, prior to the processing, carry out \nan assessment of the impact of the envisaged processing operation on the protection of personal data”.', 'other way for the AI-tool data controller to comply with this rule than guaranteeing \ntransparency of the algorithm and the AI-tool. Also, according to art. 5.2, the AI-tool \ndata controller will be responsible for this transparency.  \nTo summarise: \n-  (1) using AI tools implies data processing; hence, this activity shall be compliant \nwith GDPR; \n-  (2) there is an obligation to designate an AI-tool data controller, who will be \nresponsible for, and able to demonstrate, compliance with art. 5.1 GDPR (due to \nart. 5.2 GDPR); \n-  (3) to comply with the accountability principle, the AI-tool data controller will also \nbe responsible to guarantee (and show) an adequate level of AI-tool transparency, \nin order to be able to demonstrate compliance with art. 5.1 GDPR as required by \nart. 5.2 GDPR. \nThis reasoning or a similar one can be independently issued or be part of a broader \ndocument such as, for instance, the future EDPB Guidelines for using AI tools.']"
F550909,10 September 2020,Jan Rempala,Wirtschaftsverband,European Tech Alliance,sehr klein (1 bis 9 Beschäftigte),189607519323-76,Belgien,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"
In February 2020, members of the European Tech Alliance (EUTA) joined forces to publish the EUTA High Level Principles on AI ahead of the publication of the European Commission’s White Paper on AI. EUTA members strongly believe the EU has the potential to become a world leader in AI. Europe benefits from a vibrant ecosystem of top academic talent, leading AI research labs and an ever growing number of AI-driven start-ups. This fruitful ecosystem is supported by industry best practices and the strong fundamentals of the EU's regulatory architecture. 

 Against this backdrop and our contribution to the public consultation on the AI White Paper last Spring, we welcome the opportunity to share the following comments, attached,  on the European Commission’s inception impact assessment “Artificial Intelligence – Ethical and Legal requirements” 
"
F550908,10 September 2020,Zsolt BARTFAI,EU-Bürger/-in,-,-,-,Ungarn,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"I am in favour of a piece of sectoral legislation that provides a comprehensive legal framework of the privacy related aspects of the AI.  This legal act should – while preserving the principles of the privacy (e.g. GDPR) –, surely, deter from the rules of the GDPR to the necessary extent in order to provide AI-tailored privacy provisions. Therefore, Option 3 (as presented in the Inception Impact Assessment) seems to be the best option. 
The followings could be – from privacy perspective – topics of an AI-related sectoral legislative act (taking also into account the principle laid down in recital 4  of the GDPR):
a)	Declaration of the possibility of use AI solutions that meet the requirements of the given legislative act – this would create the legal ground (different from the GDPR) for the use of AI;
b)	Limitation on the potential use (i.e. the purposes) of AI solutions (either “blacklist” – i.e. forbidden use, or “white list” – i.e. allowed use), including the special conditions to be met. On the other hand, the new legislative act should adapt specific rules “to permit organizations to repurpose data already collected, as long as doing so poses only minimal risk of harm to individuals and does not involve the transfer of data from one controller to another”; 
c)	Special rules for some data processing activities, e.g. general permission for use of personal data as training data – with no or limited possibility to opt out. In this way, the future legislation might ease the strict interpretation of “data minimization” principle as well;
d)	Specific rules on transparency, inc. the minimum set of information to be shared with data subjects on the “logic” of AI;
e)	If necessary, some limitations on the data subject rights (e.g. on erasure or objection to ensure the availability of the data for the purpose of training of AI); 
f)	AI-specific data security measures (e.g. compulsory pseudonymization or anonymization, other rules stricter than the general ones);
g)	Allocation of obligations on those actors who are “best placed to address any potential risks”. It means that the personal scope of the new legislative act should not be limited to data controller and data processors, but should “include the developer, the deployer (the person who uses an AI-equipped product or service) and potentially others (producer, distributor or importer, service provider, professional or private user)” – with carefully allocated adequate responsibilities.  
  ","['Comments to Inception Impact Assessment regarding a potential proposal for a \nlegal act of the European Parliament and the Council laying down requirements for \nArtificial Intelligence \n(from privacy/data protection perspective) \nThis opinion only reflects the views of its author. \n1. The European Commission published an Inception Impact Assessment,1 in which – after a \nWhite Paper2 on similar issue – outlines options of regulating the AI. These options are the \nfollowings (according to the Inception Impact Assessment): \na)  Option 1: EU soft-law (non-legislative approach) to facilitate and spur industry-led \nintervention, \nb)  Option 2: EU legislative instrument setting up a voluntary labelling scheme, \nc)  Option 3: EU legislative instrument establishing mandatory requirements for all or \ncertain type of AI application, \nd)  Option 4: Combination of any of the option above taking into account the different \nlevels of risk that could be generated by a particular AI application.  \n \n2. Although there are considerable pros and cos regarding any of the options above, one thing \ncannot be forgotten: as the White Paper did, this Inception Impact Assessment also just \nrepeats – among others – that there is legal uncertainty around AI but refrains from stating \nclearly that the most considerable legal uncertainty around the AI is the data protection \nrelated uncertainty, i.e. whether current EU data protection legislation (GDPR and Law \nEnforcement Data Protection Directive) allows the use of AI at all.3  \n \nUnfortunately, the current text of the GDPR, as well as its interpretation represent a constant \nrisk of non-compliance with the GDPR: the necessity of use of AI can always be challenged, \nas  well  as  its  proportionality,  or  the  compliance  with  any  other  principles  (e.g.  data \nminimization, storage limitation etc.).4 In order to support the use of AI (and to invest in AI \nsolutions) such uncertainties should be eliminated. It might happen in two ways: \na)  interpretation of the current legal framework (GDPR, Law Enforcement Data Protection \nDirective) but neither the provision of the said legal instruments, nor the restrictive \ninterpretation is promising;5 \nb)  sectoral  EU  legislation.  The  GDPR  [Art.  22(2)(b)  and  23(1)(e)]  refers  to  such \npossibility. \n \nIf the EU intends to regulate AI, that would be a good opportunity to regulate these privacy \nrelated issues as well. The form of such provisions – because the privacy rules now are in an \nEU regulation – cannot be else but another EU regulation, i.e. a piece of sectoral legislation \nthat provides a comprehensive legal framework of the privacy related aspects of the AI.6 This \nlegal act should – while preserving the principles of the privacy (e.g. GDPR) –, surely, deter \n \n1  Ref. Ares(2020)3896535 - 23/07/2020  –  downloadable  from  https://ec.europa.eu/info/law/better-\nregulation/have-your-say/initiatives/12527-Artificial-intelligence-ethical-and-legal-requirements  \n2 White Paper on Artificial Intelligence – A European approach to excellence and trust (COM(2020) 65 final) - \nhttps://ec.europa.eu/info/sites/info/files/commission-white-paper-artificial-intelligence-feb2020_en.pdf  \n3 See e.g. Center for Data Innovation Report: The EU Needs to Reform the GDPR to Remain Competitive in the \nAlgorithmic  Economy  –  https://www.datainnovation.org/2019/05/the-eu-needs-to-reform-the-gdpr-to-remain-\ncompetitive-in-the-algorithmic-economy/  \n4 “[M]any EU policymakers are resistant to [the] idea [of mending, not ending, the GDPR] because they consider the \nGDPR to be an ethical commitment they cannot walk away from, and any attempt at improvement would be seen as \nan admission of a mistake.” – Center for Data Innovation Report \n5 This is, partially, due to the fact that the GDPR is, from technological point of view, a bit outdated. \n6 As there are other sectoral legislative acts, e.g. the draft e-Privacy Regulation, and many references to possible EU \nlegislation all around in the GDPR.', 'from the rules of the GDPR to the necessary extent in order to provide AI-tailored privacy \nprovisions. Therefore, Option 3 above seems to be the best option.7 A separate piece of \nsectoral legislation could cover other aspects of AI (product safety and product liability, \nconsumer protection etc.) as well. \n \n3. The followings could be – from privacy perspective – topics of an AI-related sectoral \nlegislative act (taking also into account the principle laid down in recital 48 of the GDPR): \na)  Declaration of the possibility of use AI solutions that meet the requirements of the \ngiven legislative act – this would create the legal ground (different from the GDPR) for \nthe use of AI; \nb)  Limitation on the potential use (i.e. the purposes) of AI solutions (either “blacklist” – \ni.e. forbidden use, or “white list” – i.e. allowed use), including the special conditions \nto be met. On the other hand, the new legislative act should adapt specific rules “to \npermit organizations to repurpose data already collected, as long as doing so poses \nonly minimal risk of harm to individuals and does not involve the transfer of data from \none controller to another”;9 \nc)  Special rules for some data processing activities, e.g. general permission for use of \npersonal data as training data – with no or limited possibility to opt out. In this way, \nthe  future  legislation  might  ease  the  strict  interpretation  of  “data  minimization” \nprinciple as well; \nd)  Specific rules on transparency, inc. the minimum set of information to be shared with \ndata subjects on the “logic” of AI; \ne)  If necessary, some limitations on the data subject rights (e.g. on erasure or objection \nto ensure the availability of the data for the purpose of training of AI);  \nf)  AI-specific  data  security  measures  (e.g.  compulsory  pseudonymization  or \nanonymization, other rules stricter than the general ones); \ng)  Allocation of obligations on those actors who are “best placed to address any potential \nrisks”. It means that the personal scope of the new legislative act should not be limited \nto data controller and data processors, but should “include the developer, the deployer \n(the person who uses an AI-equipped product or service) and potentially others \n(producer, distributor or importer, service provider, professional or private user)” – \nwith carefully allocated adequate responsibilities. 10 \n \nBy Zsolt Bartfai \n \n7 It is worth mentioning that even the Inception Impact Assessment seems to prefer this option by outlining sub-\noptions (according to the scope of the to-be legal instrument). \n8 “The processing of personal data should be designed to serve mankind. The right to the protection of personal data \nis not an absolute right; it must be considered in relation to its function in society and be balanced against other \nfundamental rights, in accordance with the principle of proportionality. This Regulation respects all fundamental \nrights and observes the freedoms and principles recognised in the Charter as enshrined in the Treaties, in particular \nthe respect for private and family life, home and communications, the protection of personal data, freedom of \nthought, conscience and religion, freedom of expression and information, freedom to conduct a business, the \nright to an effective remedy and to a fair trial, and cultural, religious and linguistic diversity.” \n9 Center for Data Innovation Report \n10 White Paper, p. 22']"
F550907,10 September 2020,Elżbieta Dziuba,NRO (Nichtregierungsorganisation),The Polish Confederation Lewiatan,mittel (50 bis 249 Beschäftigte),-,Polen,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"Jasna i szeroko rozumiana definicja sztucznej inteligencji będzie miała kluczowe znaczenie dla skuteczności wszelkich przyszłych ram regulacyjnych. Dlatego przestrzegamy przed zdefiniowaniem Sztucznej Inteligencji, jako tzw. automatycznego system podejmowania decyzji. Tak rozumiana definicja Sztucznej Inteligencji byłaby sprzeczna z kierunkiem regulacji zaproponowanym przez KE w Białej Księdze. Co więcej, mogłaby doprowadzić do powstania nieuzasadnionych obowiązków regulacyjnych, z których wiele może znacznie utrudnić rozwój i rozpowszechnienie korzystnych zastosowań Sztucznej Inteligencji w Europie. 
Zwracamy uwagę na nieprecyzyjny charakter pojęcia ""szkody niematerialnej"". Niestety, definicja ta może oznaczać wszystko, od strat gospodarczych do bolesnych emocji, i może prowadzić do niepewności prawnej, zniechęcając do inwestycji i innowacji. W zamian proponujemy, aby przyszłe regulacje odnosiły się do pojęcia ""znacznego ograniczenia korzystania z praw podstawowych"". Pojęcie to jest zbliżone do istniejących ram prawnych.
Przede wszystkim, jesteśmy przeciwni jakimkolwiek nowym regulacjom. Stoimy na stanowisku, że już obecnie Sztuczna Inteligencja podlega wielu istniejącym przepisom, takim jak choćby RODO. Dlatego też przed stworzeniem jakichkolwiek nowych regulacji, należy zapewnić prawidłowe wdrożenie obecnych przepisów prawnych UE w tym zakresie. 
Niezależnie od tego jakie podejście regulacyjne jest stosowane, należy zapewnić wsparcie przemysłu we wdrażaniu norm w zakresie odpowiednich praktyk w zastosowaniu Sztucznej Inteligencji. 
Ponadto, jesteśmy przeciwni tzw. systemowi etykietowania. Uważamy, że nie spełni on zakładanej funkcji, a przyczyni się z znacznym stopniu do jeszcze większego obciążenia administracyjnego MŚP, a także nałoży na tych przedsiębiorców nowe obowiązki dotyczące oznakowania. Koszty dostosowania się do tego typu systemu mogłyby znacznie przewyższyć korzyści z niego płynące. 
Ważne jest, aby przy konstruowaniu jakichkolwiek nowych regulacji zachować elastyczność interpretacji prawnej i współpracować z osobami zajmującymi się rozwojem AI w celu opracowania przepisów, które będą wykonalne z technicznego punktu widzenia. AI to zestaw technologii zdolnych do uczenia się, wnioskowania, dostosowywania i wykonywania zadań  w sposób inspirowany przez umysł ludzki. Technologia ta stale się rozwija i ulepsza. Potencjalne korzyści z rozwoju AI są olbrzymie. Przepisy powinny uwzględniać szybkie tempo postępu technologicznego.
Komisja słusznie stwierdziła potrzebę dobrze zdefiniowanego, opartego na ryzyku podejścia do regulacji AI, które nie stosuje logiki „jeden rozmiar dla wszystkich” w niezliczonych aplikacjach AI. Dobrym przykładem rozwiązania, do którego można by zastosować obowiązkowe wymogi w oparciu o analizę ryzyka jest chociażby system zdalnej identyfikacji biometrycznej. Należy pamiętać, że każda ocena ryzyka powinna mieć charakter holistyczny, odzwierciedlający nie tylko potencjalne szkody, ale również możliwości społeczne. Sztuczna inteligencja to zestaw technologii zdolnych do uczenia się, rozumowania, dostosowywania i wykonywania zadań w sposób zainspirowany przez ludzki umysł. Technologia ta stale się rozwija i doskonali. Potencjalne korzyści płynące z jej rozwoju są ogromne. Przepisy powinny uwzględniać szybkie tempo postępu technologicznego. Mając to na uwadze, należy wprowadzić szereg dostosowań, aby zapewnić, że wszelkie potencjalne regulacje będą ukierunkowane na właściwe przypadki użycia, zwiększą pewność prawną i nie zniechęcą do rozwoju i rozpowszechniania AI. 
Uważamy, że najlepsze podejście do aplikacji AI wysokiego ryzyka to ocena rozwiązań ex-post. Rozumiemy oczywiście, że są dziedziny, w których ocena ex-ante stanowi utrwaloną praktykę, np. medycyna, jednakże w tych dziedzinach postulujemy, aby ocenę ex-ante połączyć z oceną stosowanych praktyk sektorowych. 
","['Warsaw, September 10, 2020\nKL / 420/300 / ED / 2020\n \n \nEuropean Commission consultation on ethical and legal requirements for artificial intelligence\n \n \n1. Scope:\n○ A clear and broad definition of artificial intelligence will be crucial for\nthe effectiveness of any future regulatory framework. Therefore, we warn against\ndefining Artificial Intelligence as the so-called automatic decision-making system.\nA definition of Artificial Intelligence understood in this way would be contrary to the direction of regulation\nproposed by the EC in the White Paper. Moreover, it could lead to an uprising\nunjustified regulatory obligations, many of which can significantly hamper development\nand spreading the beneficial uses of Artificial Intelligence in Europe.\n○ Please note that the concept of ""non-pecuniary damage"" is imprecise. Unfortunately, the definition\nthis can mean anything from economic losses to painful emotions, and it can lead to\nlegal uncertainty, discouraging investment and innovation. Instead, we suggest that\nfuture regulations referred to the notion of ""significant limitation of the enjoyment of rights\nbasic ""This concept is similar to the existing legal framework.\n \n2. Approach to regulation of Artificial Intelligence:\n○ Above all, we are against any new regulation. We stand on\nthe position that Artificial Intelligence is already subject to many existing regulations, such as\nsuch as the GDPR. Therefore, before making any new regulations, you should\nensure the correct implementation of current EU legal provisions in this area.\n○ Whichever regulatory approach is used, support must be provided\nindustry in the implementation of standards for appropriate practices in the use of Artificial\nIntelligence.\n○ Moreover, we are against the so-called the labeling system. We believe that it will not meet\nintended function, and will significantly contribute to an even greater load\nadministration of SMEs, and will impose new obligations on these entrepreneurs regarding\nmarkings. The costs of adapting to such a system could far exceed\nits benefits.\n○ It is important to remain flexible when designing any new regulations\nlegal interpretation and collaborate with people involved in the development of AI in order to\ndevelop technically feasible rules. AI is a set\ntechnologies capable of learning, inferring, adapting, and performing tasks\n \n- 1 -', 'in a way inspired by the human mind. This technology is constantly evolving and improving.\nThe potential benefits of AI development are enormous. The rules should be fast-paced\ntechnological progress.\n■ The Commission has rightly identified the need for a well-defined, risk-based approach\napproaches to AI regulation that do not follow the ""one size fits all"" logic\nin countless AI applications. A good example of a solution to which you could\nto apply mandatory requirements based on a risk analysis, for example, is a system\nremote biometric identification. Keep in mind that any risk assessment should\nbe holistic, reflecting not only potential damage,\nbut also social opportunities. Artificial Intelligence is a set of technologies\ncapable of learning, reasoning, adapting and performing tasks\nin a way inspired by the human mind. This technology is constantly evolving\nand perfect. The potential benefits of its development are enormous. The rules should\ntake into account the rapid pace of technological progress. With that in mind, you should\nmake a number of adjustments to ensure that any potential adjustments will be made\ntargeted at the right use cases, will increase legal certainty, and no\ndiscourage the development and diffusion of AI.\n \n3. Law enforcement:\n○ We believe the best approach for high-risk AI applications is to evaluate ex-post solutions.\nWe understand, of course, that there are areas in which ex-ante evaluation is an established practice,\neg medicine, however in these areas we propose to combine ex-ante evaluation with evaluation\napplied sectoral practices.\n○ Where the EC plans to introduce an ex-ante enforcement mechanism,\nwe recommend that it is not third parties but solution developers that make the self-assessment. Organs\nregulators should develop guidelines for the concept of “sound\ndiligence ”. It is also worth paying attention to the fact that the self-certification process is not too burdensome,\nespecially in terms of documentation requirements. Otherwise there is a risk that due\nexcessive legal requirements, this process may discourage SMEs from taking action on\ninnovation.\n \n \nKind regards,\n \n \nMaciej Witucki\nPresident of the Lewiatan Confederation\n \n \n- 2 -']"
F550904,10 September 2020,Takenobu Kurihara,Wirtschaftsverband,Japan Business Council in Europe,sehr klein (1 bis 9 Beschäftigte),68368571120-55,Belgien,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"JBCE welcomes the European Commission’s efforts to establish a common European approach to AI. This will help the EU’s AI market reach scale and avoid legal uncertainty or fragmentation of multiple policies among EU Member States, through protecting our safety, consumer rights and fundamental rights when we use AI applications.

1.Our recommendation

Considering the different levels of risk that could be generated by AI applications, we support the Commission’s proposal of a risk-based approach and would recommend Option 4. This means that the EU’s consideration on regulation should be limited to high-risk AI only, with detailed requirements being determined by industry via a soft law approach. The EU must also help avoid duplications between future regulations and current laws and ensure harmonization with international rules and existing regulatory schemes, minimizing the burden on businesses.

2.Scope of high-risk AI

2.1.We welcome the preliminary ideas proposed by the Commission in the White Paper (p. 17 – sector and specific use), but it is essential to develop clear criteria and definitions through dialogues with industry to ensure legal certainty and to distinguish between high-risk and low-risk AI. In our view, some examples of non-high-risk AI include, but are not limited to, the following types of applications:

Mobility
Driver assistant safety and monitoring systems, biometric ID to unlock and start automobiles, vehicles navigation systems with voice recognition and telematic services.

Manufacturing
Fiber laser processing machines, intellectualization of industrial robots, real-time data analyzers.

Home, offices & shops
Air conditioning systems, digital still cameras/camcorders (automatic focus, smile detection), emotion visualization, ID card.

3.Requirements on high-risk AI

3.1.We welcome the scenario outlined in the IIA p.5 in which the principles and basic regulatory framework are legislated by the EU institutions, but the details of each requirement should be determined by industry (soft law approach). AI is constantly evolving, and it is difficult to pre-determine detailed regulations or to amend them in a timely manner. Therefore a future-proof regulatory framework, rather than specific and sectorial regulations that could stifle the EU’s competitiveness in this field, has to be preferred.

3.2.New ex-ante conformity assessments such as testing and algorithms verifications or data sets, inspections and certifications could be disproportionate for certain applications, but we would support further discussion on how to build a credible assessment mechanism that is capable of checking the trustworthiness of AI applications throughout their life time.

3.3.It is important to ensure that there is no duplication between the future regulatory framework for high-risk AI and existing EU legislation. We also believe that the establishment of a new independent AI authority could bring low added value: some of the potentially high-risk sectors or AI applications are already subject to strict ex-ante rules, which should continue to be covered under existing law and amended if necessary.

4.Facial recognition and biometric data

4.1.There is a broad range of facial recognition technologies and applications that process biometric data. As such, we would warn against adopting a one-size-fits-all approach when considering regulation for these applications. A general ban would compromise innovation and competitiveness for the development of new technologies, products and services.

4.2.For AI solutions based on biometric parameters, we encourage the development of a framework that excludes non high-risk applications.

4.3.Some remote biometric identification solutions used in public spaces could be identified as high-risk applications. They should be regulated under the EU’s future framework on AI, but as long as such solutions are complying with future requirements, they should be allowed in publicly accessible spaces.","['Japan Business Council in Europe \nReply to Inception Impact Assessment on AI \n \n10/September/2020 \n \nJBCE welcomes the European Commission’s efforts to establish a common European approach \nto AI. This will help the EU’s AI market reach scale and avoid legal uncertainty or fragmentation of \nmultiple policies among EU Member States, through protecting our safety, consumer rights and \nfundamental rights when we use AI applications. \n \n1.  Our recommendation \nConsidering the different levels of risk that could be generated by AI applications, we support \nthe Commission’s proposal of a risk-based approach and would recommend Option 4. This \nmeans that the EU’s consideration on regulation should be limited to high-risk AI only, \nwith detailed requirements being determined by industry via a soft law approach. The \nEU must also help avoid duplications between future regulations and current laws and \nensure harmonization with international rules and existing regulatory schemes, minimizing \nthe burden on businesses.  \n2.  Scope of high-risk AI \n2.1.  We welcome the preliminary ideas proposed by the Commission in the White Paper (p. 17 – \nsector and specific use), but it is essential to develop clear criteria and definitions through \ndialogues with industry to ensure legal certainty and to distinguish between high-risk and \nlow-risk AI. In our view, some examples of non-high-risk AI include, but are not limited to, the \nfollowing types of applications: \n✓  Mobility \nDriver assistant safety and monitoring systems, biometric ID to unlock and start \nautomobiles, vehicles navigation systems with voice recognition and telematic \nservices. \n✓  Manufacturing \nFiber laser processing machines, intellectualization of industrial robots, real-time data \nanalyzers. \n✓  Home, offices & shops \nAir conditioning systems, digital still cameras/camcorders (automatic focus, smile \ndetection), emotion visualization, ID card. \n \n \n \n1', '3.  Requirements on high-risk AI \n3.1.  We welcome the scenario outlined in the IIA p.5 in which the principles and basic regulatory \nframework are legislated by the EU institutions, but the details of each requirement should \nbe determined by industry (soft law approach). AI is constantly evolving, and it is difficult \nto pre-determine detailed regulations or to amend them in a timely manner. Therefore a future-\nproof regulatory framework, rather than specific and sectorial regulations that could stifle the \nEU’s competitiveness in this field, has to be preferred. \n3.2.  New ex-ante conformity assessments such as testing and algorithms verifications or \ndata  sets,  inspections  and  certifications  could  be  disproportionate  for  certain \napplications, but we would support further discussion on how to build a credible assessment \nmechanism that is capable of checking the trustworthiness of AI applications throughout their \nlife time. \n3.3.  It  is  important  to  ensure  that  there  is  no  duplication  between  the  future  regulatory \nframework  for  high-risk  AI  and  existing  EU  legislation.  We  also  believe  that  the \nestablishment of a new independent AI authority could bring low added value: some of the \npotentially high-risk sectors or AI applications are already subject to strict ex-ante rules, which \nshould continue to be covered under existing law and amended if necessary.  \n4.  Facial recognition and biometric data \n4.1.  There is a broad range of facial recognition technologies and applications that process \nbiometric data. As such, we would warn against adopting a one-size-fits-all approach when \nconsidering regulation for these applications. A general ban would compromise innovation and \ncompetitiveness for the development of new technologies, products and services. \n4.2.  For  AI  solutions  based  on  biometric  parameters,  we  encourage  the  development  of  a \nframework that excludes non high-risk applications. \n4.3.  Some remote biometric identification solutions used in public spaces could be identified as \nhigh-risk applications. They should be regulated under the EU’s future framework on AI, but \nas long as such solutions are complying with future requirements, they should be allowed \nin publicly accessible spaces. \n \n \n \nAbout JBCE \nFounded in 1999, the Japan Business Council in Europe (JBCE) is a leading European organization representing the \ninterests of about 90 multinational companies of Japanese parentage active in Europe. Our members operate across a \nwide range of sectors, including information and communication technology, electronics, chemicals, automotive, machinery, \nwholesale trade, precision instruments, pharmaceutical, textiles and glass products.  \nFor more information: https://www.jbce.org / E-mail: info@jbce.org \n2']"
F550900,10 September 2020,Clemens Otte,Wirtschaftsverband,Bundesverband der Deutschen Industrie,mittel (50 bis 249 Beschäftigte),1771817758-48,Deutschland,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"The BDI welcomes the efforts of the European Commission to develop an appropriate ethical and legal framework based on the Union's values with the overriding goal to take on a global leading role in AI. A selective adaptation of the existing legal framework for AI can in-crease legal certainty for companies and foster trust in AI applica-tions. However, it must always be borne in mind that too high regulatory re-quirements can hinder the development and thus the uptake of AI. We therefore expressly welcome the fact that the EU Commission not on-ly considers the effects on social and ethical aspects of AI regulation but al-so the compliance costs and the impact on the innovative and competitive capacity of Europe. It is of uttermost importance to balance innovation and consumer protection appropriately. 
 
The BDI is of the opinion that a selective adjustment of the legal framework is generally sufficient to meet the challenges posed by AI. Should the Commission decide to introduce new measures on AI, BDI prefers option 4, a combination between option 1 and an adapted option 3b. This approach would be based on horizontal EU-wide AI principles, which serve as a European standard. The principles could build on the work of the High Level Expert Group. The EU would provide guidance and recommendations on how to best implement the AI principles. Industry standards could supplement as needed (option 1). 
 
Furthermore, consideration could be given to additional requirements for high-risk applications. This requires a clear definition of high risk and a uni-form framework for the risk assessment. However, it does not seem to be suitable to define the sector as one of two main criteria in order to classify AI as high risk. A sectoral differentiation would only make sense if the vast majority of AI applications used in a given sector pose a significant risk or if the vast majority of AI applications from a given sector are completely risk-free. The BDI doubts that such a clear distinction is possible. More im-portant than the sector affiliation is the concrete field of application of the AI system. In addition, the criterion of sector affiliation is softened by the considered exceptions in the white paper on AI and therefore appears to be inconsistent. In order to achieve sector-specific differentiation, it is more purposeful to transfer general horizontal requirements for high-risk applica-tions to existing sector-specific regulation. 

Regarding the liability regime for AI applications, BDI is of the opinion, that the currently existing national and European legal provisions on safety and liability issues, which are based on a technology-neutral approach, are largely fit for purpose and provide an adequate legal framework that also co-vers AI as a source of risk. Only if actual liability gaps can be proven, should cautious adjustments be made. However, a fundamental readjustment of the liability framework is not necessary.  
 
BDI outlines its position to the Commission’s ongoing consultation in the document attached. 
","['Statement \n \n \n \n \n \n \n \nStatement on Section 5 ""An \n \necosystem of trust"" within the \n   \nWhite Paper on AI of the European \nCommission \n \n \nFederation of German Industries  \n \n \n \n \nVersion: 19.06.2020 \n \n \n \nStand: TT.MM.JJJJ', 'Statement on Section 5 ""An ecosystem of trust"" within the White Paper on AI of the European Commission \n \nTable of contents \nManagement Summary ...................................................................................... 3 \nDetailed statement on section 5 of the Commission\'s White Paper on AI .... 4 \nA   Problem definition ................................................................................... 5 \nB   Possible adjustments to existing EU legislative framework relating to AI 5 \nC  Scope of a future EU regulatory framework ............................................ 7 \nD  Types of requirements ............................................................................ 9 \nE  Addressees ............................................................................................. 9 \nF  Compliance and enforcement ............................................................... 10 \nG  Voluntary labelling for no-high risk applications .................................... 11 \nAbout BDI .......................................................................................................... 12 \nImprint ............................................................................................................... 12', 'Statement on Section 5 ""An ecosystem of trust"" within the White Paper on AI of the European \nCommission \n \nManagement Summary   \n▪  The BDI welcomes the efforts of the European Commission (Commis-\nsion) to create an ecosystem for excellence and trust. A selective adapta-\ntion of the existing legal framework for AI can increase legal certainty \nfor companies. \n▪  Up to now known high-risk AI applications can already be effectively \nregulated through existing national or European law. Additional legisla-\ntion going beyond a selective adaptation of the existing legal framework \nmay only be considered if there is a proven need. Should the Commission \ndecide to introduce a new legislation specifically on AI, it should be based \non a horizontal approach, which should be specified sector-specifically \nor transferred to already existing sector-specific legislation. \n▪  In the view of the BDI, the currently existing national and European legal \nprovisions on safety and liability issues are largely fit for purpose and \nprovide generally an adequate legal framework that also covers AI as a \nsource of risk in principle. A cautious adjustment is conceivable, but a \nfundamental readjustment of the liability framework is not necessary. \n▪  In the event of any AI-specific legislation, a differentiated and risk-based \napproach is the right way forward. However, too much emphasis is put \non the sectoral nature of an AI application. Sector affiliation will indeed \nplay a role in assessing the risk, nevertheless it does not seem to be very \nuseful to define it as one of two main criteria. More important than the \nsector is the specific field of application in which the AI application is \nused. \n▪  When defining requirements for AI systems, a distinction should be made \nbetween B2C and B2B applications. The requirements in the B2C area  Federation of German Industries \nMember Association of   \nare different from those in the B2B area, since consumer rights must be  BUSINESSEUROPE \nspecially protected by law (for example, via product liability). In the B2B \nAddress  \narea, companies can negotiate legal aspects among themselves in con- Breite Straße 29 \n10178 Berlin \ntracts. \n \nContact person \n▪  As described in the White Paper on AI, conformity assessments for AI  Clemens Otte \nT: +49-30-2028-1614 \nshould primarily be part of the existing conformity assessment mecha-\nE-Mail: c.otte@bdi.eu \nnisms. Conformity assessments should be designed in a user-friendly way \n \nand should take the form of a self-assessment with clearly defined stand-\nInternet \nards.  www.bdi.eu', 'Statement on Section 5 ""An ecosystem of trust"" within the White Paper on AI of the European \nCommission \n \n \nDetailed statement on section 5 of the Commission\'s White Paper \non AI \nThe Commission should critically examine whether lack of trust is really a \nmain factor holding back a broader uptake of AI \nThe White Paper on AI, on page 9, identifies lack of trust as a ""main factor \nholding back a broader uptake of AI."" At the European level, it is often argued \nthat a trustworthy AI can promote social acceptance of the technology and \nhelp the European economy to differentiate itself in the international compe-\ntition for AI. Generally, the BDI also sees this as a valid strategy. However, \nthe Commission should at least critically examine whether lack of trust is \nreally a main factor holding back a broader uptake on AI. Analogous to the \nso-called privacy paradox, a discrepancy between concerns about the trust-\nworthiness of an application and the actual user behaviour is also apparent in \nthe case of AI. Thus, even a minimal additional benefit or cost saving could \nbe sufficient to induce consumers to use less trustworthy AI applications. In \nthis respect, it is questionable whether supposedly more trustworthy applica-\ntions from Europe can compete better on the international market. \nThe Commission should use concrete positive examples to show how high-\nrisk AI applications can already be addressed by the existing legislative \nframework \nThe public debate often focuses on AI applications that are not wanted by \nEuropean society or the European economy, such as applications that can \ndetect the sexual orientation of people or applications used in other regions \nof the world for systematic surveillance of citizens. Such examples stir up \nfears in society and increase mistrust in AI. However, not everyone knows \nthat such applications are generally not compatible with the existing Euro-\npean legal framework. Therefore, broader knowledge of the existing applica-\ntions and legal limits of AI is necessary to counteract fears and objections of \nsociety. The Commission should use concrete examples to show that partic-\nularly critical AI applications can already be adequately addressed by the ex-\nisting legal framework. Furthermore, this could also reveal concrete gaps in \nlaw enforcement that need to be closed as part of the Commission\'s regulatory \nefforts. \n \nPage \n4 of 12 \nwww.bdi.eu', 'Statement on Section 5 ""An ecosystem of trust"" within the White Paper on AI of the European \nCommission \n \nA   Problem definition \nEnabling individualisation \nPossible new requirements for non-discrimination must take account of the \nfact that non-discriminatory individualisation of offers also has a strong \npositive impact and must remain possible.  \nB   Possible adjustments to existing EU legislative framework re-\nlating to AI \nThere are currently no fundamental legislative gaps \nThe paper rightly points out that many challenges related to the use of AI can \nbe addressed by the existing regulatory framework. As described in the intro-\nduction to section five, the EU Commission should therefore, before intro-\nducing new legislation, ""examine whether current legislation is able to ad-\ndress the risks of AI and can be effectively enforced "". In the view of the BDI, \nthere are currently no fundamental regulatory gaps that cannot be closed by \na selective adjustment of the existing regulatory framework. \nThe Commission should choose a combination of a horizontal approach and \nsectoral specification when introducing new legislation \nIf the Commission considers it necessary to introduce new legislation, a hor-\nizontal approach with general requirements should be taken, which should be \nspecified sector-specifically or transferred to already existing sector-specific \nlegislation. In this way, the Commission could, on the one hand, achieve a \nminimum degree of cross-sectoral coherence and, at the same time, take into \naccount that AI is a basic technology that poses different challenges depend-\ning on the context of application, the AI technology used and the overall so-\ncio-economic system. \nA fundamental readjustment of the liability framework is not necessary \nFrom the point of view of the BDI, the extensive body of existing national \nand European legal provisions on safety and liability issues is largely appro-\npriate, sufficient and balanced and provides an adequate legal framework that \ngenerally also covers AI applications as a source of risk. A cautious adjust-\nment could possibly be considered, this however does not justify a funda-\nmental readjustment of the liability framework. Due to the special character-\nistics of AI applications, further investigations should be carried out for the \ntime being. In particular, the existing regulations should be examined in de-\ntail in order to finally identify actual liability gaps and problems of proof. \n \nPage \n5 of 12 \nwww.bdi.eu', 'Statement on Section 5 ""An ecosystem of trust"" within the White Paper on AI of the European \nCommission \n \nOnly thereafter and if necessary, legal adjustments could be made accord-\ningly, taking into account the specific risk factors. The decision to introduce \nnew regulations must not be an end in itself, but must ensure both effective \nprotection against AI and sufficient scope for technological innovation. It is \nimportant to balance innovation and consumer protection appropriately. \nThe possibilities of the existing product liability directive (85/374/EEC) \nshould be reviewed in combination with the safety legislation in order to pro-\nvide clarity on the core concept of ""product"". For example, in line with the \nview of the Commission, it should be made clear to what extent independent \nsoftware applications or digital services are also covered by product safety \nlegislation.  \nIn any case, the Commission should take into account differences between \nB2C and B2B applications when adapting existing legislation. In the B2C \narea, the requirements are higher than in the B2B area. Consumer rights are \nto be protected under the Product Liability Directive, while companies can \neffectively regulate liability and other legal aspects through contracts, espe-\ncially in the ""high-risk"" sectors. In addition, special rules in the B2C sector \ncould be useful, as there might be situations in the future where AI applica-\ntions learn from the consumer\'s input and responsibility may shift to the con-\nsumer accordingly. \nAs a matter of principle, it must be guaranteed that all parties involved along \nthe production chain should be obliged according to their individual casual \ncontribution. Ultimately, the aim is to fill unacceptable liability gaps in such \na way that no party is unfairly burdened. \n   \n \nPage \n6 of 12 \nwww.bdi.eu', 'Statement on Section 5 ""An ecosystem of trust"" within the White Paper on AI of the European \nCommission \n \nC  Scope of a future EU regulatory framework \nAdditional requirements should apply to all technologies concerned \nMany of the issues and challenges raised in the White Paper on AI are not \nspecific to AI, such as changing functionality of systems or uncertainties in \nthe allocation of responsibilities. In the light of the above, generally there \nshould not be laid down different requirements for AI than for those ""tradi-\ntional"" algorithmic or numerical systems that are also affected by the chal-\nlenges. AI functionalities can rarely be clearly distinguished from non-AI-\nbased functionalities. Furthermore, there will be a multitude of hybrid sys-\ntems. New requirements must always keep up with the state of the art, be \nupdated and then be applied to all systems. \nSectoral affiliation is unsuitable as a main criterion \nA differentiated and risk-based approach is the right way forward for any \nregulation. However, too much weight is attached to the sector of an AI ap-\nplication. Sector affiliation will indeed play a role in risk assessment. How-\never, it does not seem to be suitable to define it as one of two main criteria. \nA sectoral differentiation would only make sense if the vast majority of AI \napplications used in a given sector pose a significant risk or if the vast major-\nity of AI applications from a given sector are completely risk-free. The BDI \ndoubts that such a clear distinction is possible. More important than the sector \naffiliation is the concrete field of application of the AI system. In addition, \nthe criterion of sector affiliation is softened by the considered exceptions and \ntherefore appears to be inconsistent. If sector affiliation is nevertheless cho-\nsen as one of the main criteria, it must be clearly visible according to which \ncriteria the sectors are chosen. \nA more precise definition of \'significant risk\' is necessary \nClear criteria must be determined to exactly define what is meant by a ""sig-\nnificant risk"". The paper is still very vague in this respect and leaves too much \nroom for interpretation. Risk matrices which are also used in the area of prod-\nuct safety can serve as a guide. Here the probability of occurrence is multi-\nplied by the possible extent of damage and threshold values for classification \nare defined. \n \n \n \nPage \n7 of 12 \nwww.bdi.eu', 'Statement on Section 5 ""An ecosystem of trust"" within the White Paper on AI of the European \nCommission \n \nLevel of decision autonomy and level of learning autonomy should be in-\ncluded as criteria in the risk assessment \nThe level of decision autonomy and the level of learning autonomy should \nbe taken into account both in assessing the risk and in defining appropriate \nrequirements for AI applications. The level of decision autonomy depends on \nwhether an AI application is merely a source of information, provides a sup-\nport function for human decision making, or can decide completely autono-\nmously without human involvement. The level of learning autonomy de-\npends on whether a re-training of an AI system is a) completely possible b) \nonly for limited, non-safety relevant parameters or c) not possible at all. Ac-\ncording to this classification, limits of action could be defined for the respec-\ntive AI systems. For example, safety-relevant parameters should be defined, \nwhich may not be re-trained in the field without a new conformity assess-\nment. \nIt must be clearly defined who asseses whether an application poses a high \nrisk \nThe paper does not clarify who assesses whether an application poses a ""high \nrisk"" and how this assessment should be made in a legally secure manner. \nThe assessment could lead to a considerable amount of bureaucracy for com-\npanies and government or notified bodies. Practical mechanisms need to be \ndeveloped which, on the one hand, ensure rapid market introduction and, on \nthe other, provide sufficient legal certainty for companies. In addition, the \nassessment must be carried out uniformly throughout Europe and coordinated \nacross all EU countries. \n   \n \nPage \n8 of 12 \nwww.bdi.eu', 'Statement on Section 5 ""An ecosystem of trust"" within the White Paper on AI of the European \nCommission \n \nD  Types of requirements \nPractical challenges in the data and record-keeping must be considered \nThe requirement (b) ""keeping of records and data"" may be appropriate in cer-\ntain cases. However, it poses practical challenges for companies that need to \nbe considered. Cataloguing and managing data and data models requires a \nconsiderable administrative and financial effort. Excessive requirements for \nthe data and record-keeping could result in certain AI applications no longer \nbeing economically viable. The reference on page 23 that the data sets them-\nselves should only be kept in ""certain justified cases"" is to be welcomed. It is \nimportant that the Commission adheres to this restriction. After all, it is the \nAI model that is decisive for the evaluation of an AI system. The model can-\nnot be reconstructed from the data sets alone, as the chosen parameterisation \nis an important factor. Furthermore, the long-term storage of data sets must \nremain compatible with the requirements of the GDPR, in particular the \n""right to erasure"". Particular attention must be paid to IoT systems or edge \ndevices that learn during operation and have limited storage capacity. For \ncapacity reasons, comprehensive data storage is difficult or technically im-\npossible. If necessary, periodic spot-checks can be considered to verify com-\npliance with the requirements. In this way, the amount of data stored could \nbe kept to a minimum.  \nE  Addressees \nThe ""cheapest cost avoiders"" principle proposed by the Commission may \nlead to legal uncertainties \nBDI takes a critical view of the Commission\'s proposal that any obligations \nshould be distributed to those players who are best placed to address potential \nrisks. Those companies that are actually most responsible for causing a risk \nmust not be allowed to shirk their producer responsibility and should be in-\ncluded in the obligations to minimise the risk (""polluter pays principle"" vs. \n""cheapest cost avoider""). Moreover, it can be highly open to interpretation \nand explanation as to which actor is actually best able to address the potential \nrisks. This could lead to great legal uncertainty and a high level of bureau-\ncracy for the bodies involved. The detailed case-by-case consideration re-\nquired in cases of doubt would collide with the character of a generally valid \nregulation. \n \nPage \n9 of 12 \nwww.bdi.eu', 'Statement on Section 5 ""An ecosystem of trust"" within the White Paper on AI of the European \nCommission \n \nF  Compliance and enforcement \nThe conformity assessments should primarily be carried out by the compa-\nnies themselves and should be designed in a user-friendly manner \nThe current market entry regulations are already very far-reaching and time-\nconsuming. The introduction of additional, AI-specific mechanisms for con-\nformity assessment must be very well justified and designed in a user-friendly \nway. First and foremost, conformity assessment by the manufacturer - anal-\nogous to the CE mark - should take the form of self-assessment with clearly \ndefined standards and effective market surveillance. If companies have to put \ntoo much effort in a conformity assessment, they could tend to abandon AI \napplications in case of doubt and use ""traditional"" systems instead. This \nwould lead to less uptake of AI in ""high risk"" applications. Moreover, the \nadditional work involved in conformity assessment could have a negative im-\npact especially for small and medium-sized enterprises. The fixed regulatory \ncosts are disproportionate to their small size in comparison to larger compa-\nnies. \nIt must be clearly defined when a new conformity assessment is necessary \nThe White Paper on AI points out that the functionality of AI systems can \nchange over time. This raises the question of the cycles in which a conformity \nassessment should be carried out. Conformity assessment at each update \nwould not be practical and would significantly limit the uptake of AI. Ac-\ncording to current regulation, a renewed conformity assessment is only nec-\nessary if fundamental safety-relevant parameters of the product change. A \ncomparable scheme can also be applied with regard to self-learning systems \nor software updates. At this point, a clear definition of the risk-relevant pa-\nrameters is required, whose change makes a renewed conformity assessment \nnecessary.  \nStrengthening audit and assessment capacity \nIn order to be able to introduce and carry out proper and independent audits, \nthe existing authorities, institutions and agencies must be given the necessary \nhuman, technical and financial resources. The assessment of many AI appli-\ncations will require the cooperation of different bodies, e.g. in cancer detec-\ntion, where image evaluation algorithms, mathematical models, laboratory \ntests and data from the operating theatre will merge. In addition, test scenar-\nios, methods and standards must be developed to ensure the security of the \n \nPage \n10 of 12 \nwww.bdi.eu', 'Statement on Section 5 ""An ecosystem of trust"" within the White Paper on AI of the European \nCommission \n \nalgorithms and self-learning systems used throughout the product life cycle. \nThis also includes developing methods and standards that counteract the use \nof ""biased data"". Test scenarios should focus on compliance with prescribed \nprocesses and methods, such as ISO 26262 (Functional Safety). \nG  Voluntary labelling for no-high risk applications \nThe added value of voluntary labelling is not apparent \nWe are critical of voluntary labelling in addition to proven markings such as \nthe CE mark. This could rather confuse the user. Moreover, if an AI applica-\ntion is not regarded as high risk, voluntary labelling does not create any ad-\nditional added value, but only additional work for companies. It is also ques-\ntionable whether ""horizontal"" certification can meet the needs of the large \nnumber of different AI applications. More important than voluntary labelling \nare clear and transparent set of rules based on international standards. \n \n \n   \n \nPage \n11 of 12 \nwww.bdi.eu', 'Statement on Section 5 ""An ecosystem of trust"" within the White Paper on AI of the European \nCommission \n \nAbout BDI \nThe Federation of German Industries (BDI) communicates German indus-\ntries’ interests to the political authorities concerned. She offers strong support \nfor companies in global competition. The BDI has access to a wide-spread \nnetwork both within Germany and Europe, to all the important markets and \nto international organizations. The BDI accompanies the capturing of inter-\nnational markets politically. Also, she offers information and politico-eco-\nnomic guidance on all issues relevant to industries. The BDI is the leading \norganization of German industries and related service providers. She repre-\nsents 36 inter-trade organizations and more than 100.000 companies with \ntheir approximately 8 million employees. Membership is optional. 15 federal \nrepresentations are advocating industries’ interests on a regional level.  \nImprint \nBundesverband der Deutschen Industrie e.V. (BDI) \nBreite Straße 29, 10178 Berlin \nwww.bdi.eu \nT: +49 30 2028-0 \n \nContact persons \nClemens Otte \nDeputy Head of Department \nDigitalisation and Innovation \nT: +49 30 2028-1614 \nc.otte@bdi.eu \nKathrin Hintner \nSenior Manager \nLaw, Competition and Consumer Policy \nT: +32 2 792 1008 \nk.hintner@bdi.eu \n \nBDI Dokumentennummer: D 1203 \n \n \nPage \n12 of 12 \nwww.bdi.eu']"
F550893,10 September 2020,Marco Leto Barone,Wirtschaftsverband,Information Technology Industry Council ITI,klein (10 bis 49 Beschäftigte),061601915428-87,Belgien,Künstliche Intelligenz – ethische und rechtliche Anforderungen,Please refer to the attached position paper for our detailed feedback.,"['10 September 2020 \n \nRequirements for Artificial Intelligence \nITI views on the European Commission Inception Impact Assessment\n \n \nThe Information Technology Industry Council (ITI) welcomes the publication of the Inception Impact \nAssessment (IIA) on Requirements for Artificial Intelligence, and appreciates the opportunity to \nprovide comments building on our contribution to the February 2020 White Paper on Artificial \nIntelligence. \n \nAs the premier advocate and thought leader for the global technology industry, ITI and its members \nshare the firm belief that building trust in the era of digital transformation is essential. At the same \ntime, it is important to promote innovation to ensure Europe’s global competitiveness and security. \nFor this reason, we welcome the IIA’s goal to foster the development and uptake of safe and lawful AI \nthat respects fundamental rights and ensures inclusive societal outcomes, all while preserving an \nenabling environment for innovation.  \n \nWe firmly believe that AI can bring about significant benefits to our society. AI-driven medical \ndiagnostics can alert doctors to early warning signs to help them treat patients more capably. \nIncreasingly intelligent systems are able to monitor large volumes of financial transactions to identify \nfraud more efficiently. Small and medium-sized enterprises (SMEs) can gather new insights and \nimprove their businesses by using AI and data analytics made available to them through online \nservices.  \n \nTherefore, it is crucial for Europe to not only look at the potential harms of using AI, but also consider \nthe potential economic and social harms of limiting the use of AI, which may decrease its positive \nimpact on our communities. Technological innovations bring innumerable benefits to the European \neconomy and society. We are already experiencing the benefits of AI in an array of fields. Promoting \nthese advances is no less important than managing any potential challenges. \n \nWe thus urge the Commission to take into consideration the wide array of possible applications of AI \ntechnology, and their different use cases and risk factors when approaching the development of \npolicies  related  to  AI.  We  believe  that,  through  a  context-specific  and  risk-based  approach, \npolicymakers can develop a framework that adequately addresses any unintended risks that AI may \npose while simultaneously promoting technological innovation.  \n \nProblem definition \n \n1.  Fundamental rights \n \nWhile we are aware of the potential risks that may arise from some applications of AI, we believe that \ncontext is key in identifying appropriate policies to mitigate this risk. The IIA mentions for instance \nissues related to fundamental rights and inclusion. The technology industry recognises the need to \nmitigate bias, inequity, and other potential harms in automated decision-making systems. We share \nthe goal of responsible AI adoption and development. As technology evolves, we take seriously our \nresponsibility as enablers of a world with AI, including through seeking solutions to address potential \nrisks.', 'Our industry is committed to partnering with relevant stakeholders to develop a reasonable, effective, \nand balanced accountability framework that takes into account the different actors and phases of \ndeveloping and deploying AI systems. As leaders in the AI field, ITI members recognise their important \nrole in making sure technology is built and applied for the benefit of everyone. Approaches must be \ncontext-  and  risk-specific  and  should  consider  that  not  all  AI  applications  affect  individuals’ \nfundamental rights. Such AI applications would not require an all-encompassing fundamental rights-\nbased approach and may not warrant any additional regulatory intervention. \n \nIn fact, many AI uses have little or no impact on individuals’ rights, such as in the context of industrial \nautomation and the use of analytics to streamline automobile manufacturing, to improve baggage \nhandling and tracking at busy European airports. This is also true for consumer products, where \nmachine learning can support users to optimise a device’s battery usage or reduce wait time linked \ncustomer service or technical support. AI development should not be disrupted with new stringent \nobligations that could significantly slow the adoption of AI and hamper innovation.  \n \nMoreover, where fundamental rights are affected, AI applications used in specific sectors (e.g. \nhealthcare, financial services, transportation) are already subject to sectoral regulation that is often \ngeared towards addressing risks to fundamental rights of individuals (e.g. Medical Device Regulation \n(EU) 2017/745, Payment Services Directive (EU) 2015/2366). In addition, the Commission is currently \ncontemplating relevant updates or revisions of existing legislation, including initiatives noted by the \nIIA. While it is important to assess if existing, domain-specific EU regulations are exhaustive, it is also \nimportant to further underline that they already cover many of the most common concerns, including \nby providing sufficient assurances regarding the safety of connected and AI-embedded devices. Any \nfuture regulatory activities should therefore be limited to address discrete and specific issues not \ncovered by existing rules. \n \nWhen considering new rules tackling AI’s impact on fundamental rights, the European Commission \nshould not seek to duplicate the existing regulatory framework – GDPR in particular. It should also \nconsider  whether  market  access-related  conformity  assessment  approaches  are  appropriate  to \nensure the enforcement of fundamental rights. First, impacts to fundamental rights are more likely to \narise during the technology’s deployment or usage, rather than at manufacturing stage. Secondly, \nimposing such pre-market assessment on standalone software, which are easy to distribute, should \nbe carefully assessed in terms of proportionality so as to not limit innovation. Thirdly, this kind of \nregulatory approach requires clear ways of measuring and demonstrating compliance, such as the \navailability and use of relevant technical standards, the availability of testing protocols to test \nimplementations against those standards, and, even if uses on a voluntary basis, notified body with \nthe technical experience and bandwidth. \n \nWe also take note of the specific concern that the IIA puts forward on surveillance by biometric facial \nrecognition. Our industry takes this issue seriously and recognises our important role in making sure \nAI technologies, like facial recognition technology, are built and applied in a way that benefits \neveryone. It is critical that society, governments, and the technology sector work together to begin to \nsolve some of the most complex issues, including this one. New regulations and policies should be \ncompatible with existing rules like GDPR to protect users without causing harm or unintended \nconsequences. \n \n2.  Product Safety \n \nThe IIA mentions product safety as one of the potential areas of focus for a potential forthcoming \nlegislative initiative on AI. We recognise and share the crucial aim of the European Commission of \n2', 'ensuring that all goods marketed in the EU, whether or not governed by sector-specific legislation, are \nsafe and effective. As alluded to above and in the IIA, the EU is leading several discussions on how to \nensure product safety while taking into account new technologies, including through the review \nexercises of the General Product Safety Directive (GPSD) and Product Liability Directive (PLD), as well \nas sector-specific initiatives including the revision of the Machinery Directive (MD) and updates to the \nRadio  Equipment  Directive  (RED). Each of  these  potential legislative  processes bear  significant \nimplications for the manner in which technology firms across a wide spectrum of business models \nmarket safe and effective products and services in the EU. As the Commission contemplates potential \nfurther regulatory action governing AI alongside these initiatives, we invite policymakers to see all of \nthese exercises and existing laws in connection with each other to ensure a coherent approach that \npromotes new technologies while managing any potential challenges. Considering these efforts in \nparallel will help understand what, if any, gaps remain, and how to best resolve any remaining \nchallenges. In addition, such coordination will prevent the inadvertent development of any technical \nbarriers to trade, which might limit access to productivity-enhancing ICT goods and services. \n \nAs a principle, should the Commission choose to take further action in this field on the basis of the \nresults of the present IIA, we believe that such action should be technology neutral, meaning that the \ntechnology itself is not regulated but rather that the law sets forth more general pillars that can remain \nfit for purpose for future innovations regardless of  the technology in question. Given that AI is a \nrapidly evolving technology, regulation which targets AI as such would likely become obsolete and \npossibly disproportionate as the technology, our understanding of the underlying science, and use \ncases evolve. This is also why we caution against the introduction of new product safety requirements \nspecifically targeting AI. We encourage the European Commission to focus on specific applications of \nthe technology, outcomes, and governance approaches for the use of AI technology, instead of \nregulating the technology itself, which will allow for flexibility. \n \nAs we have pointed out in the context of the review of the GPSD, stand-alone software typically does \nnot pose the same type of heightened safety risks associated with traditional physical products. As the \nIIA notes, with limited exceptions, liability and product-specific legislation does not provide for the \nregulation of stand-alone software, and it is unclear that assurance mechanisms associated with such \nlegislation – such as testing requirements – would be proportionate or fit-to-purpose in this context. \nWe therefore urge the Commission to carefully consider whether legislative tools aimed at ensuring \nproduct safety are necessary or appropriate when assessing any potentially relevant risks associated \nwith stand-alone software.  \n \n3.  Liability \n \nAI presents  great  opportunities  for  society  in  different  fields  yet  raises valid concerns  around \nresponsible and safe deployment. We believe that the clarification of rules around liability, currently \ndesigned  for  physical  products,  is  an  appropriate  area  of  focus.  There  are  also  important \nconsiderations about  finding the appropriate balance of ex-ante, preventive rules, and ex-post \nmeasures, including remedies. We support an effective and balanced liability regime that fosters trust \nin the use of AI, provides a clear path for redress and adequately compensates victims for damages, \nwhile allowing for incremental improvements and innovations that come with placing AI systems on \nthe market.  \n \nIn many cases the existing liability framework will be easily applied in an AI context and we suggest \nthat the EU maintain a strong presumption against altering it except in response to significant and \ndemonstrable shortcomings.  Should a need for future legislative or administrative action be identified \nin areas that involve increased risks for end-users of AI applications, it should be addressed in a sector-\nspecific manner, with new regulation or suggested legislation addressing clearly identified issues, \n3', 'based  on  evidence  and  data,  and  designed  to  avoid  overreach.  Sector-specific  safe  harbour \nframeworks or liability caps are also worth considering in domains where there is a concern that \nliability laws may otherwise discourage socially beneficial innovation. Updating such sector-specific \nregulation, rather than adopting sweeping changes to general product liability frameworks, would \nallow for more precise targeting of remedies for identified gaps in liability coverage.  \n \nIf the existing liability regime falls short of addressing new challenges arising from specific applications, \nlegislative intervention should be limited to filling in the gaps or addressing clear shortcomings, while \navoiding an overhaul of the existing framework, which has proven to provide an adequate balance in \nprotecting consumers while encouraging the launch of innovative products in the market. It is also \nimportant that due consideration is given to the diversity of the actors in the supply chain, to avoid \nthat liability is disproportionally spread to actors that could not reasonably be expected to bear \nresponsibility for situations beyond their control. \n \nWe also consider that the business/individual can fulfil the requirements of the Product Liability \nDirective (PLD) and recover damages if a product containing AI technologies causes harm to a business \nor individual. In this sense, amendments to the PLD to cover embedded AI are unnecessary, since \nthe  directive  is  technology-neutral  and  strikes  the  right  balance  between  the  obligations  of \nconsumers and producers, thereby creating legal certainty. Still, it is crucial to recognise that there is \na fundamental difference between on the one hand a hypothetical, undefined risk, and on the other \nthe danger based on the product’s fault or its use in a specific context. Strict liability frameworks like \nthe one set up by the Product Liability Directive (PLD) remove any consideration of intent or \nnegligence. Therefore, strict liability should only apply for high-risk AI applications defined as sector, \nuse case, complexity of the AI system, probability of worst-case occurrence, irreversibility and scope \nof harm in worst case scenarios. Further comments on this definition are further down in this paper \nin the section about high-risk AI applications (pages 8,9). Manufacturers should instead be equipped \nwith a right to cure or correct identified violations with consumers directly. This would also be in the \ninterest of fostering consumer trust in AI applications.  \n \nPolicy options \n \nWe believe that in order to avoid overregulation, any potential future initiative targeting AI will need \na clear, targeted scope focusing on those high-risk AI applications where issues are most likely to arise.  \n \nSince there is no single widely agreed-upon definition of AI, it will be important for policymakers to \nprovide greater clarity if they plan to seek specific rules for AI functions. An essential factor is to \nproperly identify AI and its different categories, including the component parts of AI systems beyond \nalgorithms, as well as to define related key terms such as machine learning. Some algorithms have \nbeen applied for decades but do not constitute “Artificial Intelligence” or ""machine learning"" systems. \nThe first task is to determine what is AI and what is not. There is a difference between the latest wave \nof AI systems that learn from data and experience, and traditional software and control systems that \noperate according to predictable rules, which have long been embedded in a wide variety of high-risk \nsystems from flight control to pacemakers to industrial settings. As mentioned above, we believe that \nthe risks associated with traditional software and control systems that make probability predictions \nare already adequately addressed by existing regulation. \n \nWe caution the Commission about considering to significantly expand the scope of possible future AI \nregulation to the open-ended category of “automated decision making.” This would go against the \ninitial, thoughtful direction proposed in the AI White Paper that proposes to focus on the risk-based, \ndouble-criterion for  sectorial and  application/use-based  AI technologies. If  AI  were  defined  as \n“automated decision making” for the purpose of possible future AI regulation, it would create \n4', 'disproportional, unjustified  regulatory  obligations  that  would not  only  deter  development  and \ndeployment of AI-based applications in Europe, but also automated systems that do not pose any \nsignificant risk of harm. \n \n \nOption 1: EU “soft law” (non-legislative) approach to facilitate and spur industry-led intervention \n(no EU legislative instrument)  \n \nStandardisation  is  a  necessary  tool  to  bridge  any  potential  AI  regulations  and  practical \nimplementation. The EU should support global, voluntary, industry-led standardisation, and safeguard \nthe work and processes of bodies developing international standards. Global AI standards reflect \nbroad consensus around technical aspects, and work is currently underway on standards to address \nmanagement, and governance of the technology. Building on decades of experience and lessons \nlearned, technical standards development is underway to help frame concepts and recommended \npractices to establish trustworthiness of AI inclusive of privacy, cybersecurity, safety, reliability, and \ninteroperability. Standards and their use in regulations must not be done in a manner that creates \nmarket access barriers or preferential treatment; rather, they should work for the benefit of the \ninternational community and be applicable without prejudice to cultural norms and without imposing \nthe culture of any one nation in evaluating the outcomes/use of AI. \n \nBeyond the support of international standards development, the Commission should use its approach \nto AI as a way to incorporate greater flexibility into its approach to standardisation, thus recognising \nthe value of standardisation for new technology and enabling regulators alike to draw upon the \nbroadest range of fit-to-purpose solutions in determining the most appropriate global, industry-\ndriven, voluntary standards. A greater degree of flexibility with respect to the standards that may be \nused  to  demonstrate  compliance  with  relevant  AI-specific  requirements  would  yield  positive \noutcomes for both domestic and global innovation, consumer protections, and market openness. \n \nOption 2: EU legislative instrument setting up a voluntary labelling scheme  \n \nIn general, voluntary labelling schemes can play a role in promoting consumer trust, if they are well \ndesigned, recognised and sufficiently specific. It remains unclear how this could play a role for AI \npowered services and products, given the limited amount of information provided in the IIA. Indeed, \nits usefulness would very much depend on the scope of the products considered, their applications, \nand who the end-user is. A very general voluntary label may have limited impact, be misleading or \ncould even lead to adverse outcomes if poorly designed. The use of AI may not be immediately obvious \nor important to the user. Some products may involve very different AI-powered features, which may \nbe hard to reflect on a single label, while multiple labels will impact the user experience and lead to \nlabel fatigue, desensitizing the user to the intended message. It is also unclear whether the label will \nbe product specific or could apply to an entire entity.  \n \nSimilarly, voluntary labelling would also require extensive both pre-labelling assessment and post \nmarket surveillance schemes, which raises questions linked to proportionality. Voluntary labeling \nshould therefore be considered in specific contexts, focusing on areas where its user trust has been a \nproven deterrent to adoption.  \n \nIn conclusion, given that AI systems themselves do not constitute a physical product or service, we \nhave questions about how such labelling could be applied in a manner that achieves the objective of \nfacilitating information to consumers. Broadly speaking, any voluntary labelling approach should not \nbecome a de-facto market entry requirement for AI products and services in Europe. Further, technical \nchallenges around electronic labelling (e-labelling) would need to be broadly addressed for products \n5', 'and services that do not have a physical shape on which to affix a label; therefore, flexibility should be \nconsidered in those scenarios. As a general matter, we strongly encourage the Commission to adopt \ninternational best practices for e-labelling in allowing the display of regulatory and other required \ninformation via electronic means.  Additionally, voluntary labels should be based on international \nstandards and recognised by all EU Member States. \n \nOption 3: EU legislative instrument establishing mandatory requirements for all or certain types of \nAI applications \n \nThe  IIA  mentions  that,  under  option  3,  an  EU  legislative  framework  would  “establish  certain \nmandatory  requirements  on  issues  such  as  training  data,  record-keeping  about  datasets  and \nalgorithms, information to be provided, robustness and accuracy and human oversight.” (page 5) \nBefore commenting on the sub-options proposed by the IIA, we would like to provide additional \ncomments on these aspects, as we believe setting mandatory legal requirements may in some cases \nhinder the development of beneficial AI applications. In general, the potential burden linked to these \nrequirements calls for a risk-based approach to regulatory requirements.  \n \no  Training data \n \nWe fully acknowledge the importance of robust training data sets in the development and deployment \nof AI. However, rather than focusing on the data sets themselves, which often will reflect biases that \nexist in the real world, we suggest focusing on testing outcomes of the AI systems before deployment \nor applying safeguards against biased outcomes after deployment. Stereotypes can be perpetuated \neither in recommendations, searches, or quality of tool so considered quality-control and review \nprocesses should be in place and outputs tested to protect against this. This requires testing and \nhuman involvement in the development of AI with diverse teams that are continually evaluating in the \ndevelopment and innovation of AI. \n \nSome AI models may require less strict requirements to data sets if they are designed with the \nappropriate caveats and caution – requirements should be set in connection to the purpose and what \nis required to ensure non-discrimination in relation to that purpose. We see an urgent need for the \nCommission to clarify how training data requirements will interact with GDPR (including the right to \nbe forgotten and data minimisation) and clarify the requirements addressed to the party that is best \npositioned to perform the quality control of the data. We also urge the concept of ‘sufficiently \nrepresentative’ to be defined more clearly. \n \no  Keeping of records and data \n \nWe caution against the introduction of mandatory record-keeping requirements for datasets used to \ntest, train, or operate AI systems on an ongoing basis. If keeping of data could lead to revealing details \nof AI systems or underlying code, this could risk undermining privacy, copyright, and trade secrets, \ninfringe on IP rights, and heighten cybersecurity risks, privacy, and data manipulation risks. Instead, \nwe urge the Commission to assume an approach that looks at outcomes rather than process, and \nwhich is compatible with the GDPR’s data minimisation principle.  \n \nMoreover, keeping vast amounts of data would be unworkable for many companies given how AI is \ndeveloped in a constantly iterative way. \n \nLastly, it is important to note that there are no common widely-used data naming conventions, no \nformatting standards or concurrent versioning systems used for data, with efforts to address these \n6', 'currently underway; these factors would further complicate mandatory sharing requirements in this \nfield. \n \no  Information provision \n \nWe believe that Transparency does not automatically equate to better control or decisions of AI \nsystems. For example, the driver of a car does not need to fully understand the systems in a vehicle to \nbe able to drive the vehicle safely. Similarly, users of AI would in most cases not need to have a detailed \nunderstanding of the workings of the technology to use it responsibly. In any case, GDPR already \nprovides for a general obligation of transparency including an obligation to inform the individual about \ntheir rights and enable them to contest their decision and even seek redress. \n \nTransparency, in our view, is best achieved through ensuring understandability and interpretability. \nUnderstandability should allow users of AI to understand broadly how an AI application works and \nhow their data is being used to create a better user experience for them individually. Rather than \nintroducing  obligations  to  disclose  technical  features,  we  recommend  an  approach  in  which \nunderstandability is prioritised to build consumer trust. Interpretability on the other hand is geared \ntowards allowing technical experts to understand the rationale behind an AI’s decision/outcome. Both \naspects are important, and we encourage policymakers to think of transparency in these terms to \nmake explicit the objective of any potential transparency requirements.  \n \nWe further urge that there should be a differentiation for transparency requirements for AI in high-\nrisk applications being used in consumer-facing v. B2B products and services. For B2B scenarios, we \ndo not see reason to share such information unless the information in question is deemed to be critical \nfor public interests including safety. This is because excessive sharing obligations might risk IP rights \nand contractual arrangements between business partners. Further, an organization that develops AI \ncannot proactively monitor the way its customers are using AI.  \n \nAs a general principle, if AI is playing a substantive role within a high-risk AI application, that fact \nshould be easily discoverable along with some insight into the nature of the role AI is playing by those \nwho have a legitimate interest.  \n \nPublic disclosure will typically be appropriate for applications designed for or affecting consumers in \nareas of public interest (e.g. government services or healthcare). However, public information about \nB2B use of AI should not be required except in case of clear public interest. \n \no  Robustness and accuracy \n \nA  safety-by-design  approach  should  be  implemented  for  all  high-risk  AI  applications.  Internal \ndocumentation and monitoring will be key for companies developing high-risk AI applications to \nassure their customers of the product’s quality and security. Mandating additional, far-reaching \nreporting or documentation requirements in addition to existing legislation such as, for example, the \nGeneral Product Safety Directive (GPSD) would be premature and could hinder industry from finding \nbest-suited solutions to challenging, complex processes. Due consideration should be given to already \nexisting laws so as not to create a rigid system that could risk longer term safety of products and \naccuracy of AI systems if innovation is not incentivised.  \n \no  Human Oversight \n \nWe need to be mindful of different AI application areas and to what extent humans need to be \ninvolved throughout the lifecycle of an AI application. For example, it is useful to have a human \n7', 'monitor an automated decision in an air traffic control tower and override decisions made by the AI if \nnecessary (for example in an emergency). In such a case, the AI de facto replaces the human and \ntherefore, human oversight is needed continuously. However, for other, less critical situations, we \nmay not require detailed human involvement e.g. for handling baggage at an airport.  \n \nIndividual use cases and the risk of adverse outcomes associated with an application should determine \nthe degree of involvement of humans in reviewing machine-generated decisions. In some cases, \nhuman oversight can not only lead to delays, in others, accuracy of outputs or even human safety \ncould even be undermined by human interventions (for example for mathematical calculations).  \n \n3a. Sub-option 1 – Regulating Biometric Surveillance \n \nAs already mentioned above, our industry takes the issue of biometric surveillance seriously and \nrecognises our important role in making sure AI technologies, like facial recognition technology, are \nbuilt and applied in a way that benefits everyone. It is critical that society, governments, and the \ntechnology sector work together to begin to solve some of the most complex issues, including this \none. Possible new regulations and policies should be compatible with existing rules like GDPR to \nprotect users without causing harm or unintended consequences. \n \n3b. Sub option 2 – Regulation limited to High-Risk AI applications \n \nOur industry welcomes the IIA’s intention to lay out a differentiated, risk-based approach with a \ndistinction between high- and low-risk AI applications, based on a number of criteria. However, as we \nhad previously pointed out in our comments to the Commission’s White Paper on AI, it is important \nthat the Commission carefully considers its definition of high-risk AI applications. The use of “sectors”, \nfor instance, may lead to a too broad categorisation. We thus encourage developing a categorisation \nthat takes into account sector, use case, complexity of the AI system, probability of worst-case \noccurrence, irreversibility and scope of harm in worst case scenarios e.g. individual v. larger groups of \npeople, and other criteria. Those criteria should be clearly defined to ensure legal certainty for AI \ndevelopers and be translated into high-level principles that could be transformed into operational \nrequirements set up in the form of industry-led standards, certification schemes or codes of conduct. \n \nMore specifically, we urge policymakers to consider the following specifications for high-risk AI \napplications in order to build on the White Paper’s differentiation also mentioned in the IIA, and \nensure for the development of principles-based rules:  \n \n-  Specify what constitutes high-risk AI applications based on probability and irreversibility: To \nensure proportionality, the definition should be augmented to better reflect well-established \ninterpretations of risk as a function of severity and likelihood. For example, high-risk could be \ndefined as AI systems that either (a) may cause catastrophic irreversible harm and there is a \npossibility that such harm may occasionally occur, and (b) may cause serious harm and such \nharm is probable. More clearly reflecting a nuanced understanding of high-risk within the \nframework would make clear that the objective of the framework is to mitigate harm for (a) \nand reduce the likelihood for (b).  \n \n-  Acknowledge and define AI’s opportunity costs: In several instances, using automated \nsystems can greatly reduce risk. In air traffic control, using an automated tool paired with \nhuman oversight is an example of how AI can reduce risk as opposed to a situation in which \nair traffic controllers could make mistakes due to fatigue or distraction - factors that do not \naffect a machine. Analysis about the spread of pandemics is another example where limiting \n8', 'the use of AI is likely to lead to potentially bigger risks than possible negative consequences \nof the AI system being deployed for this purpose.  \n \n-  Remove “exceptional instances” clause: We support the notion of clear and predictable \ncumulative criteria, as well as clarity over what constitutes a high-risk use of AI, and ideally \nthe negative impact or concrete consequences that can reasonably be expected on affected \nparties. However, the “exceptional instances” clause in the AI White Paper, which the IIA also \nrefers to, is too open-ended and should be removed to avoid legal uncertainty. For example, \nthe notion that applications affecting consumer rights could potentially fall in the high-risk \ncategory seem overbroad, unjustified and against the objective of focusing only on well-\ndefined areas of risk. In addition, the instances to which the White Paper appears to refer \nseems to be appropriately covered by existing legislation (non-discrimination provisions in \nlabour law and consumer laws).  \n \n-  Align  references  to damages  with  the  PLD:  The  1985  Product  Liability  Directive  (PLD) \nempowers European consumers to receive compensation for damage caused by defective \nproducts. The PLD applies to any product sold in the European Economic Area with a 3-year \nlimit for the recovery of damages. The PLD defines damage as death, personal injury, or \ndamage to the product in questions or other products of a consumer. This definition could be \na good basis to support the definition of what constitutes high-risk AI. We would advise \navoiding  references  to  immaterial  damages  that  could  potentially  lead  to  waves  of \ncompensation claims for producers on illegitimate grounds and lead to a backlog in assessing \ncases all while mostly being covered already by existing legislation in the fields of data \nprotection, non-discrimination and freedom of expression.  \n \n3c. Sub-option 3 – Regulation Covering all AI Applications \n \nGiven the variety of applications that benefit or may benefit from AI, context is key in identifying \nappropriate regulatory options. As we have noted, not all AI applications pose the same level of, or \nany, risk to safety or fundamental rights and not all potential harms are new or related to AI only. For \ninstance, many AI uses have little or no impact on individuals’ rights, such as in the context of industrial \nautomation and the use of analytics to streamline automobile manufacturing, to improve baggage \nhandling and tracking at busy European airports. This is also true for consumer products, where \nmachine learning can support users with optimize a device’s battery usage, or reduce wait time linked \ncustomer  service  or  technical  support.  Finally,  in  business-to-business  contexts  the  same  AI \napplication can be used in different ways by the customers which increases the need for a context-\nspecific assessment of each use case to determine the risk incurred.   \n \nMoreover, as mentioned above, we believe that many AI applications are already covered by existing \nlegislation in the field of fundamental rights, data protection, product safety and liability. Potential \nnew legislation should only be considered in instances where regulatory gaps are identified that \ncannot be addressed through self- or co-regulatory approaches. Completing a revision of the current \nEU legislative framework is the first crucial step to have a holistic and comprehensive overview on the \nidentified  legislative  gaps  in  order  to  address  them  through  additional  guidance  or  concrete \namendments of existing EU legislation \n \nWe strongly caution against considering a one-size fits all regulatory approach. It would in fact risk \ndisrupting AI development with new stringent obligations that could slow down the adoption of AI \nand hamper innovation to the detriment of consumers and businesses in Europe alike, particularly if \nthe Commission chooses to regulate AI technology itself instead of focusing on the governance \n9', 'approaches or use cases. This approach would thus ultimately run counter to this IIA’s very objective \nof promoting the development and uptake of AI in Europe.  \n \nOption 4: Combination of any of the options above taking into account the different levels of risk \nthat could be generated by a particular AI application.  \n \nAs detailed above, we would welcome a context-specific and risk-based approach. We believe that \nregulatory measures should target those high-risk AI applications for which specific regulatory gaps \nhave been identified, while avoiding overly precautionary approaches that may stifle innovation. \nMoreover, legislative approaches should be flexible enough to account for the rapidly changing and \nfast-paced technological advancement in this sector. \n \nAs mentioned above, context is key in identifying appropriate measures concerning AI. Many uses – \ne.g. in medicine, financial services, or transport – are already subject to sectoral regulation. In many \ninstances, the essential requirements already contained in harmonized legislation may be sufficient in \ncovering risks presented by applications of AI.  \n \nA proper assessment of applicable laws should precede new legislation, with a view toward evaluating \nwhether new rules are actually needed, avoiding conflicts of law, and ensuring that both existing and \nforthcoming regulatory requirements prioritize international compatibility and reliance on global, \nindustry-driven, voluntary, consensus-based standards. In cases where regulatory shortcomings are \nidentified, adapting existing laws would be the appropriate way forward.  \n \nEnforcement mechanisms \n \nAs mentioned in our response to the White Paper, we believe that a combination of ex-ante risk self-\nassessment and ex-post enforcement for high risk AI applications would be the most appropriate \nenforcement  mechanism  for  any  future  regulatory  scenario. This  solution would likely  achieve \nintended results within much faster timeframes and without hampering innovation or creating \nunnecessary burdens. For instance, requiring companies to carry out and document risk assessments \nwould be analogous to the data protection impact assessment under GDPR. Such an approach would \nalso build on existing industry practices, including the ethical, legal, and due diligence practices that \nguide the responsible and trustworthy development  of AI. To clarify compliance and facilitate \naccountability, regulators should assess what actors are best to act at what stage in the AI lifecycle. \nFor example, the developer of AI is responsible for conceptualising and training the AI, whereas \ndeployers have the best visibility of the use case for the AI. \n \nThis solution would address the many concerns that an ex-ante regulatory approach would pose. For \ninstance, the existing conformity assessment infrastructure may lack resources or expertise to \neffectively and efficiently carry out testing of AI.  In addition, we are concerned that stringent ex-ante \nrequirements may create problems for products already in the market, R&D of early stages products \nand software updates throughout a product’s lifecycle.  \n \nConclusion \n \nTo sum up, we believe that there cannot be a “one-size-fits-all” solution to properly address all the \nissues related to Artificial Intelligence. In this sense, a mix of solutions as proposed by Option 4 of this \nIIA seems to be the most balanced approach. However, we urge the Commission to consider our \nrecommendations when developing its approach. We see great value in the EU encouraging and \npromoting industry-led initiatives as proposed in Option 1 of the IIA. We believe that industry can \nsolve many issues related to technical aspects, management, and governance of AI technology, as well \n10', 'as  frame  concepts  and  recommended  practices  to develop  trustworthy  AI  applications  that \nconsider privacy, cybersecurity, safety, reliability, and interoperability through processes of voluntary, \nglobal and industry-led standardisation. When it comes to regulation, we believe that new legislation \nshould be considered only where legislative gaps are clearly identified. For this reason, we caution \nagainst horizontal regulatory solutions, as proposed in Sub-option 3c, which would result in overly \nstringent requirements and ultimately slow down the adoption of AI and possibly hinder innovation. \nAt the same time, we welcome the consideration of risk as the key factor in defining the scope as \nproposed by Sub-options 3a and 3b. However, we urge the Commission to carefully consider the \ndefinition of high-risk considering use case, complexity of the AI system, probability of worst-case \noccurrence, irreversibility, scope of harm in worst case scenario and sector. \n \n*** \n11']"
F550892,10 September 2020,-,-,-,-,-,-,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"The German Federal Association of Interpreters and Translators (BDÜ) welcomes the opportunity to comment on this roadmap, and on developments related to artificial intelligence in general.

Translators and interpreters are affected by technological developments, as epitomised by the progress achieved by neural machine translation (NMT). We embrace change and are ready to adapt our skills and processes, but want to raise awareness concerning two major aspects:

Technology must protect citizens' interests and privacy, not just in the EU. There are certain areas where human beings must obtain (or retain) ultimate responsibility for translation and interpretation - including, in particular, the areas of law/legal practice, medicine/healthcare, integration and immigration. In this respect, translators and interpreters also protect society through their services.

This has also been recognised by the Commission:
https://ec.europa.eu/info/sites/info/files/about_the_european_commission/get_involved/documents/discovertranslation-info-sheet-world-without-translation.pdf

Skilled translators and interpreters and their professional assocations must be actively involved in the assessment of technological trends concerning and affecting their professions (and livelihoods). We are ready, and will be happy, to contribute our skills and expertise."
F550891,10 September 2020,Christian BORGGREEN,Wirtschaftsverband,Computer & Communications Industry Association (CCIA),klein (10 bis 49 Beschäftigte),15987896534-82,Vereinigten Staaten,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"The Computer & Communications Industry Association (CCIA) welcomes this opportunity to respond to the European Commission’s inception impact assessment on Artificial Intelligence (AI). We support the Commission’s aim to “create trust and incentivise the use of such AI systems by citizens and businesses”. We agree that AI “can contribute to a wide array of economic and societal benefits”.

Like any other technology, risks lie not in the AI application itself, but in its usage. We therefore support a regulatory focus on “high risk” use cases.

Comments on policy options:

Option 0: No EU policy change:
AI is already subject to EU legislation and we therefore encourage EU lawmakers to ensure that existing law is properly implemented before proposing any new AI-specific regulation.

Option 1: EU “soft law”:
We support this approach and agree that it can “facilitate and spur industry-led intervention”.

Option 2: EU legislative instrument setting up a voluntary labelling scheme:
While in principle in favor of such voluntary measures, we would caution that any such scheme would have to; remain voluntary; focus only on low risk applications, and; must not place a costly administrative burden nor delay time to market. We must avoid overburdening developers and startups given their minimal resources.

Option 3a: EU legislation limited to a specific category of AI applications only, e.g. remote biometric identification (RBI) systems:
We agree that some use cases of RBI systems could be considered high risk, and therefore warrant mandatory requirements. However, facial recognition technologies can also be used to reduce risks, e.g. a search for missing persons on the Internet or secure login on a smartphone.
We would caution against focusing on regulating a specific technology, given that there is no risk inherent to a technology, and instead recommend a clear emphasis on specific “high risk” use cases to address specific and known risks and harms. The Commission can build on, without duplicating, existing specific vertical legislations that already identify specific risks, e.g. in the areas of medical devices and transportation.

3b: EU legislation limited to “high-risk” AI applications:
We support the Commission’s risk-based approach and the distinction between high and low risk AI systems. We would suggest a clear and narrow definition of “high risk” applications, with emphasis on specific (high risk) use cases rather than blanket categories of technologies. Inspiration can be drawn from existing EU sectoral legislation as mentioned above.

3c: EU legislative act covering all AI applications:
We would oppose this option which would be disproportionate, overburdensome, and run counter to stated goals to make the EU a leader in the data economy. This would also ignore the necessary risk-based approach of linked EU policies, e.g. GDPR.
We finally would caution against considering significantly expanding the scope of the future AI regulation to the open ended category of “automated decision making.” This would go against the initial, thoughtful direction proposed in the AI White Paper that proposes to focus on the risk-based, double-criterion for sectorial and application/use-based AI technologies.

Option 4: Combination of any of the options above taking into account the different levels of risk:
We support a combination of policy options, notably we favor the main elements of options 0, 1, 3a, and 3b.

Enforcement: Ex ante and/or ex post:
We support a combination of ex ante and ex post enforcement. Ex ante self-assessment and testing procedures should be based on clear due diligence guidance from regulators made against recognised global standards. Authorities can moreover conduct ex post intervention when required and in limited circumstances. We would finally caution against any bureaucratic, lengthy, ex-ante assessments. 

Thank you for this opportunity to comment. We stand ready to provide further information."
F550883,10 September 2020,Elise Cachin,Unternehmen/Unternehmensverband,IKEA / Ingka Group,groß (250 oder mehr Beschäftigte),1095068839-59,Niederlande,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"Since 2018, Ingka Group, the strategic partner in the IKEA franchise system, has embarked on a journey to transform our company into a retailer fit for the 21st century. We are becoming data-driven, using digital tools such as Artificial Intelligence (AI) to meet customers wherever and whenever they choose, with the range and services they want, always at prices they can afford. We are developing Data Ethics Principles, stemming from our company culture and values to put people first, and put transparency and accountability at the core of our personalised relationships with our customers. 

We welcome the commitment of the European Commission to the digital transformation of the European economy. A future-proof European framework for Artificial Intelligence should rely on a human-centric approach to AI that creates a positive stance towards AI technologies, supports innovation and creates a level-playing field to the benefit of consumers. 

An EU legislative instrument establishing mandatory requirements limited to ‘high-risk’ AI applications (option 3.b) should highlight the benefits of AI and provide risk management mechanisms where needed. We have the following recommendations to secure a future-proof European framework for Artificial Intelligence:
> EU rules should focus on achieving desirable outcomes rather than regulating tools, as it is already the case for other technologies (e.g. software updates). Introducing new obligations such as ‘non-discrimination by design’ would secure positive outcomes in an innovation-friendly manner.
> The future EU rules for AI should rely on a more precise definition of ‘Artificial Intelligence’. A specific and targeted definition of Artificial Intelligence will foster trust among society, and secure the legal certainty needed for businesses to innovate in the European Union. 
> Whether Artificial Intelligence is deemed ‘high risk’ should be equally based on the type of AI application being used (i.e., likelihood and magnitude of adverse outcomes) and on sectoral use. 
> ‘High-risk’ applications should be assessed based on their intended use, i.e., on whether they will be used for (i) internal operational processes, (ii) consumer-facing use, (iii) decision tools, and (iv) physical use that presents safety risks including harm to human body. 
> EU rules for remote biometric identification systems should balance privacy concerns with opportunities for consumer experience improvement. The General Data Protection Regulation (GDPR) has already created a clear framework for remote biometric identification systems in which biometric data processing should be a last resort option. Retailers would welcome the opportunity to explore innovative biometric-based services for our customers and visitors, such as to cashier-less check-out processes. 

A voluntary labelling scheme (option 2) would likely overflow consumers with information and refrain them from using AI technologies by putting forwards the notion of risks rather than benefits. Such a scheme would also increase administrative burdens for developers and reduce the attractiveness of the European Union for AI innovation and development.

Artificial Intelligence is driving the ability of the European economy to grow, compete and become greener tomorrow. Agile innovation-focused human-centric rules for AI will help us innovate and develop new services that will provide consumers with more and better choices. We stand ready to support building a responsible AI-powered economy ensuring a future for Europe that is fair and equal for all, climate neutral, and digital.
","['10 September 2020 \nBrussels \nContribution from Ingka Group / IKEA  \n©\nIngka Holding B.V. and its controlled entities   Ing\nk\n  a H\n  old\nin\ng\n B\nFeedback to the Inception Impact Assessment with  .V\n. 2\n0\n2\n0\nregards to ethical and legal requirements for Artificial   \nIntelligence \n \n \n \nSince 2018, Ingka Group, the strategic partner in the IKEA franchise system, has embarked \non a journey to transform our company into a retailer fit for the 21st century. We are \nbecoming  data-driven,  using  digital  tools  such  as  Artificial  Intelligence  (AI)  to  meet \ncustomers wherever and whenever they choose, with the range and services they want, \nalways at prices they can afford.  \n \nThe recent COVID-19 outbreak is speeding up our digital transformation that is still guided \nby our vision: to create a better everyday life for the many people. We want to create a \nnew IKEA fit for the future, integrate sustainability into everything we do and achieve true \ninclusivity based on equality.  \n \nArtificial Intelligence is the cornerstone to achieving this vision. AI enables responsible \nretailers of all sizes to tailor their services to customers’ needs and improve internal \nefficiency. We are developing a Data Ethics Principles, stemming from our company culture \nand values to put people first, and put transparency and accountability at the core of our \npersonalised relationships with our customers.  These Principles will guide our company \ngoing forward, empowering people – whether our customers, visitors or employees – to \nunderstand why, how and when we use data. Developing a European human-centric \napproach to AI that supports innovation and digital skills will secure a level-playing field to \nthe benefit of consumers.  \n \nThe coronavirus crisis has emphasised the need for a fair and ethical European AI \necosystem. Artificial Intelligence can help us to improve our services, but also to innovate \nand  find  new  opportunities  that  will  support  employment  and  speed  up  recovery. \nNevertheless, responses to the crisis can bias competition and open the door to more \nintrusive practices. We need to stay true to our European values and create a framework \nthat will put people first, and balance business interests to benefit people and planet. \n \nWe welcome the commitment of the European Commission to the digital transformation of \nthe European economy. A future-proof European framework for Artificial Intelligence should \nrely  on  a  human-centric  approach  to  AI  that  creates  a  positive  stance  towards  AI \ntechnologies,  supports  innovation  and  creates  a  level-playing  field  to  the  benefit  of \nconsumers.  \n \nAbout Ingka Group \nIngka Group (Ingka Holding B.V. and its controlled entities) is one of 11 different groups of companies that own and operate IKEA \nretail under franchise agreements with Inter IKEA Systems B.V. Ingka Group has three business areas: IKEA Retail, Ingka Investments \nand Ingka Centres. Ingka Group is a strategic partner in the IKEA franchise system, operating 367 IKEA stores in 30 countries. These \nIKEA stores had 838 million visits during FY18 and 2.35 billion visits to www.IKEA.com. Ingka Group operates business under the IKEA \nvision – to create a better everyday life for the many people by offering a wide range of well-designed, functional home furnishing \nproducts at prices so low that as many people as possible can afford it. \nIKEA Service Centre SA/NV | Part of Ingka Group \nIkaroslaan 28, 1930 Zaventem, Belgium \nT VA No: BE 0437.785.744 – Registered Office: Brussels  1\nEU Interest Representative Register ID number: 1095068839-59', '10 September 2020 \nBrussels \nAn EU legislative instrument establishing mandatory requirements limited to ‘high-\n©\nrisk’ AI Applications (option 3.b) should highlight the benefits of AI and provide risk   Ing\nk\na\nmanagement mechanisms where needed. We have the following recommendations to   H\no\nld\nsecure a future-proof European framework for Artificial Intelligence:  in\ng\n   B.V\n\uf034 EU rules should focus on achieving desirable outcomes rather than regulating  . 202\n0\n \ntools, as it is already the case for other technologies (e.g. software updates). \nArtificial Intelligence is first and foremost a tool used to analyse data and environment. \nDevelopers should have the freedom to innovate while always respecting human rights \nand promoting dignity, diversity and inclusivity, unlocking value for people. Introducing \nnew obligations such as ‘non-discrimination by design’ would secure positive outcomes \nin an innovation-friendly manner. \n\uf034 The future EU rules for AI should rely on a more precise definition of ‘Artificial \nIntelligence’. AI can be understood as a specific type of algorithms, a set of models \nimplemented  in  computers  that  learn  from  data  and  interactions  with  their \nenvironment to make decisions and inferences. It excludes traditional statistical models \nand  human-defined  logic  flows.  A  specific  and  targeted  definition  of  Artificial \nIntelligence will foster trust among society, and secure the legal certainty needed for \nbusinesses to innovate in the European Union.  \n\uf034 Whether Artificial Intelligence is deemed ‘high risk’ should be equally based on \nthe type of AI application being used (i.e., likelihood and magnitude of adverse \noutcomes) and on sectoral use. Applications only aiming at improving business \nprocesses do not have the same implications for people and society than AI-fuelled \nautonomous driving solutions for instance. A balanced application-and-sector-based \napproach would enable businesses to pay particular attention to potentially vulnerable \npeople (those traditionally at risk of exclusion), secure proportional requirements \nacross sectors and support innovation.  \n\uf034 ‘High-risk’ applications should be assessed based on their intended use, i.e., on \nwhether they will be used for (i) internal operational processes, (ii) consumer-facing \nuse, (iii) decision tools, and (iv) physical use that presents safety risks including harm to \nhuman body.  \n\uf034 EU rules for remote biometric identification systems should balance privacy \nconcerns with opportunities for consumer experience improvement. The General \nData Protection Regulation (GDPR) has already created a clear framework for remote \nbiometric identification systems in which biometric data processing should be a last \nresort  option.  Retailers  would  welcome  the  opportunity  to  explore  innovative \nbiometric-based services for our customers and visitors, such as to cashier-less check-\nout processes.  \n \nA  voluntary  labelling  scheme  (option  2)  would  likely  overflow  consumers  with \ninformation and refrain them from using AI technologies by putting forwards the \nnotion of risks rather than benefits. Such a scheme would also increase administrative \nAbout Ingka Group \nIngka Group (Ingka Holding B.V. and its controlled entities) is one of 11 different groups of companies that own and operate IKEA \nretail under franchise agreements with Inter IKEA Systems B.V. Ingka Group has three business areas: IKEA Retail, Ingka Investments \nand Ingka Centres. Ingka Group is a strategic partner in the IKEA franchise system, operating 367 IKEA stores in 30 countries. These \nIKEA stores had 838 million visits during FY18 and 2.35 billion visits to www.IKEA.com. Ingka Group operates business under the IKEA \nvision – to create a better everyday life for the many people by offering a wide range of well-designed, functional home furnishing \nproducts at prices so low that as many people as possible can afford it. \nIKEA Service Centre SA/NV | Part of Ingka Group \nIkaroslaan 28, 1930 Zaventem, Belgium \nT VA No: BE 0437.785.744 – Registered Office: Brussels  2\nEU Interest Representative Register ID number: 1095068839-59', '10 September 2020 \nBrussels \nburdens for developers and reduce the attractiveness of the European Union for AI \n©\ninnovation and development.   Ing\nk\na\n   H\no\n  ld\nin\nArtificial Intelligence is driving the ability of the European economy to grow, compete and  g B\n.V\nbecome greener tomorrow. Agile innovation-focused human-centric rules for AI will help us  . 2\n0\n2\ninnovate and develop new services that will provide consumers with more and better  0 \nchoices. We stand ready to support building a responsible AI-powered economy ensuring a \nfuture for Europe that is fair and equal for all, climate neutral, and digital. \n \n \n \nFor further information, please contact: \nIngka Group European Affairs \nelise.cachin@ingka.ikea.com \nAbout Ingka Group \nIngka Group (Ingka Holding B.V. and its controlled entities) is one of 11 different groups of companies that own and operate IKEA \nretail under franchise agreements with Inter IKEA Systems B.V. Ingka Group has three business areas: IKEA Retail, Ingka Investments \nand Ingka Centres. Ingka Group is a strategic partner in the IKEA franchise system, operating 367 IKEA stores in 30 countries. These \nIKEA stores had 838 million visits during FY18 and 2.35 billion visits to www.IKEA.com. Ingka Group operates business under the IKEA \nvision – to create a better everyday life for the many people by offering a wide range of well-designed, functional home furnishing \nproducts at prices so low that as many people as possible can afford it. \nIKEA Service Centre SA/NV | Part of Ingka Group \nIkaroslaan 28, 1930 Zaventem, Belgium \nT VA No: BE 0437.785.744 – Registered Office: Brussels  3\nEU Interest Representative Register ID number: 1095068839-59']"
F550881,10 September 2020,-,-,-,-,-,-,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"First of all, as a private sector stakeholder, we are pleased to submit our feedback to the Commission on the proposal for a legal act laying down requirements for Artificial Intelligence. 
 
In our opinion, the objectives defined by the Commission are adequate and necessary considering the current legal issues raised by AI. Especially, achieving the objectives (d) and (f) will answer the private sector's needs. To begin with, it is crucial to have a harmonized framework to reduce compliance costs derived from legal fragmentation mostly in cross-border activities. It is known that some Member States like Germany, Denmark, and Malta, have implemented separate regulatory initiatives. Alongside the burdensome compliance costs, these fragmented practices limit private sector stakeholders to extend their solutions and products to the Single Market. Ensuring a level playing field for trustworthy AI is another important aspect and also the reason why we do not agree with the approach of Option 1.
 
Option 1 purposes to promote industry initiatives for AI and suggests implementing monitoring and reporting on voluntary compliance. In our view, this is not applicable because it will lead to further fragmentation first among the Member States and secondly among different industries. An industry-led initiative does not guarantee a harmonized cross-border approach since not all industries have an EU level umbrella organization. Therefore, it could bring some practices, which contradict the principles and goals of the European Single Market. Moreover, it can cause diversified practices among different industries, and maybe also among those usually work together and interact, ruling out the level playing field for trustworthy AI. 
 
We express some doubts also on Option 2. Although a voluntary labeling scheme is not a non-preferable solution per se, it is not clear according to which criteria AI applications will be identified and labeled. The Ethical guidelines piloted by the HLEG lay the foundation and counter the applicable criteria nevertheless the Guidelines are not specific enough to follow in a labeling process. Thus, if the Commission decides to pursue this option, it is necessary that the authorities identify those criteria.
 
Taking into consideration the arguments developed above, we assume that, in a general manner, Option 3 can work best for the private-sector stakeholders, in particular for service providers. Establishing mandatory requirements for AI applications can reduce fragmented practices and enhance cross-border activities. However, it should be noted that limiting these requirements to a specific category of AI applications only, as described in Option 3a, can leave some risky application out of the regulatory framework. Therefore, requirements being limited to ""high risk"" AI applications, as it is described in Option 3b, on basis of two criteria as set out in the White Paper (sector and specific use/impact on rights or safety) would be optimal. The Commission's newly published short summary report from the public consultation on Artificial Intelligence indicates a divided opinion on the issue but we think that some sectors are fragile or, in other words, risky, in nature due to the information they use or to the scope. For example, regulating facial recognition does not prevent risks raised by other AI applications in healthcare. However, this approach requires constant and periodical reviews. A periodically reviewed public list can be a possible solution with particular attention to the delisting activities since compliance requires providers to spend time and resources. 
 
Finally, in our view, a ""trustworthy-by-design or default"" system, like the successful application introduced by the GDPR, can be an alternative option. This would prevent incorrect application at the development phase and assure the trustworthiness and explainability of an AI application. 
 
Sincerely
"
F550875,10 September 2020,APDSI Secretariado,NRO (Nichtregierungsorganisation),APDSI - Associação para a Promoção e Desenvolvimento da Sociedade da Informação,sehr klein (1 bis 9 Beschäftigte),TR 435520331024-09,Portugal,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"●	Âmbito: 
○	Não nos parece adequado expandir o âmbito do futuro regulamento de IA para a categoria aberta de ""tomada de decisão automatizada"". Isso iria contra a ideia inicial refletida no Livro Branco da IA, que propõe concentrar-se e basear-se no risco e no duplo critério para as tecnologias de IA setoriais e de aplicação/utilização. Se a IA fosse definida como ""tomada de decisão automatizada"" para efeitos da futura regulamentação, este conceito criaria obrigações regulamentares que dificultariam o desenvolvimento e a implantação de aplicações baseadas em IA na Europa bem como de sistemas automatizados que não representam qualquer risco ou dano.

●	Opções políticas:
○	Opção 0 (base): Acreditamos que há mérito em assegurar que o regulamento existente da UE é devidamente implementado no que diz respeito à IA, antes de se estabelecerem quaisquer novas regras prescritivas específicas para a IA. 
Atualmente, a IA não funciona no vácuo legislativo, está sujeita a uma série de regras existentes, incluindo o RGPD, o regulamento sobre os dispositivos médicos, e o acervo dos direitos fundamentais.

○	Opção 1 (intervenção liderada pela indústria): Independentemente das opções de políticas públicas que sejam seguidas, é útil dar apoio à indústria na definição e implementação de normas que estabeleçam práticas responsáveis e partilha das mesmas. 

○	Opção 2 (legislação sobre rotulagem voluntária): Parece-nos uma ideia interessante mas somos céticos em relação à eficácia da adesão a este instrumento que pode tornar-se um encargo administrativo para as PME que poderia pesar mais e prevalecer significativamente, em relação aos benefícios que traria.

○	Opção 3 (legislação com requisitos obrigatórios): No geral, o custo de oportunidade da não utilização da IA deve fazer parte da avaliação quando se considera qualquer legislação destinada a reduzir o risco e os danos decorrentes da utilização de aplicações de IA. A legislação deve garantir a segurança jurídica, ser proporcional e aumentar a confiança na IA, sem prejudicar, indevidamente, a inovação conduzida pela mesma. 

■	3a: Os sistemas de identificação biométrica remota podem ser um bom exemplo de uma aplicação à qual poderiam ser aplicados requisitos obrigatórios numa abordagem baseada no risco.

■	3b: Apoiamos uma abordagem bem definida, baseada no risco, ao regulamento sobre IA, que tenha em conta tanto a gravidade como a probabilidade de dano. Critérios de base de utilização/aplicação e setoriais - tal como proposto pelo Livro Branco da Comissão Europeia - parecem em geral ser um bom ponto de partida.

■	3c: Advertimos contra um ato legislativo da UE para todas as aplicações possíveis de IA, que não faça distinção entre as aplicações de IA que possam representar um risco/perigo significativo e as que não comportam riscos ou têm um perfil de risco inferior. Tal instrumento legislativo seria significativamente desproporcional aos problemas até agora identificados pela CE, criaria barreiras significativas à adoção de IA na Europa, resultaria em custos de oportunidade em algumas aplicações devido à falta de implementação de IA, e arriscar-se-ia a baixar a fasquia para aquelas aplicações de IA que são muito suscetíveis de levantar riscos significativos.

●	Aplicação da regulamentação:
○	Apoiamos a aplicação ex post, para quando os problemas surgirem, como sendo o mecanismo mais apropriado e proporcional, exceto em campos onde as avaliações ex ante já são prática estabelecida. Nestas situações, recomendamos o alinhamento de qualquer avaliação ex-ante com os procedimentos existentes.

○	Uma abordagem prática, mais eficiente que a avaliação ex-ante por terceiros, consistiria em que os reguladores fornecessem modelos detalhados e orientações sobre como realizar e documentar a avaliação de risco, delegando porém, a responsabilidade àqueles que utilizam o sistema de IA e estão mais familiarizados com este, a fim de realizar uma avaliação precisa.
"
F550864,10 September 2020,Fiona WILLIS,Unternehmen/Unternehmensverband,Association for Financial Markets in Europe (AFME),mittel (50 bis 249 Beschäftigte),65110063986-76,Belgien,Künstliche Intelligenz – ethische und rechtliche Anforderungen,Please see the attached submission from the Association for Financial Markets in Europe (AFME),"['Association for Financial Markets in Europe \nConsultation Response    \nInception Impact Assessment - Proposal for a Legal Act of the European \nParliament and the Council Laying Down Requirements for Artificial \nIntelligence \n10 September 2020 \n \n \nThe Association for Financial Markets in Europe (AFME) welcomes the opportunity to comment on INCEPTION \nIMPACT ASSESSMENT - PROPOSAL FOR A LEGAL ACT OF THE EUROPEAN PARLIAMENT AND THE COUNCIL \nLAYING  DOWN  REQUIREMENTS  FOR  ARTIFICIAL  INTELLIGENCE  (the  “Impact  Assessment”).    AFME \nrepresents a broad array of European and global participants in the wholesale financial markets. Its members \ncomprise pan-EU and global banks as well as key regional banks, brokers, law firms, investors and other financial \nmarket participants. We advocate stable, competitive, sustainable European financial markets that support \neconomic growth and benefit society. \nAFME is the European member of the Global Financial Markets Association (GFMA) a global alliance with the \nSecurities Industry and Financial Markets Association (SIFMA) in the US, and the Asia Securities Industry and \nFinancial Markets Association (ASIFMA) in Asia.  \nAFME is registered on the EU Transparency Register, registration number 65110063986-76. \nWe summarise below our high-level response to the Impact Assessment, which is followed by answers to the \nindividual options proposed.  \n \nExecutive Summary \nThe capital markets industry is keen to engage with authorities to develop AI in Europe, building understanding \nand skills and maintaining high standards for its use. Increased public-private partnership should be a key focus.  \nRegulation should remain technology-neutral and principles-based. Regulation should also avoid creating \nunnecessary barriers to innovation and the development and adoption of AI by firms. Regulation which focuses \non a particular technology (such as a specific AI application) is less effective as it does not address underlying \nbehaviours or practices, for which a technology is simply a tool to perform. Given the speed of technological \nadvances, technology-specific regulation will struggle to maintain pace with developments in its use and risks \ncreating barriers to the adoption of new and innovative technologies.  \nIn relation to the options discussed in the Impact Assessment, we support Option 1, “Soft Law” Approach, since \nit could effectively take into account existing regulations and developments in sectors that are comparatively \nmature in their use of AI, rather than imposing a one-size of its all regulatory approach.  \nAs general principles, AFME believes that any additional requirements for AI within the EU should: \n•  Take into account existing regulation, particularly for highly regulated sectors such as financial services. \nThis would reduce the risk of duplication with existing regulatory requirements; \nPage 1 of 7 \nAssociation for Financial Markets in Europe \nLondon Office:   39th Floor, 25 Canada Square, London E14 5LQ, United Kingdom   T: +44 (0)20 3828 2700 \nBrussels Office: Rue de la Loi 82, 1040 Brussels, Belgium   T: +32 (0)2 788 3971   \nFrankfurt Office: Bürohaus an der Alten Oper, Neue Mainzer Straße 75, 60311 Frankfurt am Main, Germany \n T: +49 (0)69 153 258 967 \nwww.afme.eu \n \nCompany Registration No: 6996678   Registered Office: 39th Floor, 25 Canada Square, London E14 5LQ \nAFME is registered on the EU Transparency Register, registration number 65110063986-76', '•  Be tailored to the level of risk of particular uses of AI, such as by being limited to ‘high-risk’ activities that \nare not covered by existing regulation; and \n•  Be applied to the activity being undertaken, regardless of whether the use of the AI application is by a \nregulated entity in a particular sector, to ensure a level playing field across all firms. \nFinally, we encourage the Commission to continue consultation with all industry sectors on its proposals, beyond \nthose which are designated as high risk, to ensure that the resulting framework is suitable and draws on \nexperiences of working successfully with AI.  \n \nComments on the Policy Options  \nWe draw the Commission’s attention to our response to its consultation on its white paper ‘Artificial Intelligence \n– A European Approach to Excellence and Trust’ (the “White Paper”) earlier in 2020.1 In an appendix to our \nresponse, we set out our views, which are also included below in respect to each of the policy option presented \nin this Impact Assessment.  \nIn that appendix, we also suggested a revised definition of AI, based on the definition2 provided by the \nCommission’s High Level Expert Group (HLEG) on AI: \n“Artificial intelligence (AI) systems are systems that act in the physical or digital world by perceiving \ntheir  environment  through  data  acquisition,  interpreting  the  collected  data,  reasoning  on  the \nknowledge, or processing the information, derived from this data and identifying the best action(s) to \ntake to achieve the given goal. AI systems may adapt themselves or their own algorithms by analysing \nhow the environment is affected by previous actions, knowledge or data.” \nOption 0 – No EU Policy Change  \nRegulation should remain technology-neutral and principles-based, recognising that technologies such as AI are \nmerely tools. Regulation should also avoid creating unnecessary barriers to innovation and the development and \nadoption of AI by firms. Regulation which focuses on a particular technology (such as a specific AI technique) is \nless effective as it does not address underlying behaviours or practices, for which a technology is simply a tool to \nperform. Given the speed of technological advances technology-specific regulation will struggle to maintain pace \nwith developments in its use and risks creating barriers to the adoption of new and innovative technologies.  \nHowever, we acknowledge that the Commission may consider that there are gaps in the existing regulatory \nframework which it wishes to address.  \nOption 1 – EU “Soft Law” (Non-legislative) Approach  \nCapital markets is a highly regulated industry and from that perspective AFME would support Option 1.  This \nwould limit the creation of unnecessary barriers to innovation and the development and adoption of AI by firms. \nThose sectors that are highly regulated have already had to develop processes and safeguards for identifying and \nmitigating a variety of risks.  For example, in financial services, firms are under obligations relating to areas such \nas consumer protection, conduct risk, duty to clients, internal governance, third-party risk management, \ntechnology, cloud, outsourcing, operational resilience and data privacy and model risk management. Many of \nthese will already mitigate the risks related to firms’ AI use, throughout the lifecycle of any AI application, and \nwill also drive firms’ own initiatives to nurture the safe development of AI. \n \n1 \nhttps://www.afme.eu/Portals/0/DispatchFeaturedImages/20200612%20AFME%20EC%20AI%20CP%20Response%20-\n%20Final_.pdf  \n2  https://ec.europa.eu/digital-single-market/en/news/definition-artificial-intelligence-main-capabilities-and-scientific-\ndisciplines', 'These processes will be tailored to the needs and specificities of that sector, and (in relation to AI) to the different \nlevels of risk of the AI applications. The imposition of an additional regulatory framework may conflict with the \nwork done to date and would be less effective as a risk mitigation tool.  \nFor these highly regulated sectors, if, following an analysis of existing sectoral regulation (which should also \ninclude relevant data and privacy laws/regulations) residual concerns are identified, the Commission should \nconsider its full toolkit of regulatory and non-regulatory measures to determine the most effective solution.  This \ncould range from cross-sectoral principles, targeted guidance, and supervisory adjustments, among others, not \nonly legislative amendments.  \nFor example, it may be more helpful in many sectors, where applicable regulation already exists, for existing rules \nto be applied in a technology-neutral way and for authorities to assist firms in how to meet supervisor’s \nexpectations (for example to avoid unjustifiable discrimination or achieve suitable levels of interpretability). A \nstrong public-private partnership, as highlighted in the actions to build an Ecosystem of Excellence, will aid both \ninstitutions and public authorities to share information and further understanding on AI techniques and \nmethodologies to address concerns outlined in the white paper.   \nThis may be done in conjunction with a review of the regulatory perimeter, where activity by non-regulated firms \nmay bring new risks. This could assist in creating a harmonized framework of regulators and supervisors’ \nexpectations on AI applications depending on the level of risk they pose. \nOption 2 – EU Legislative Instrument Setting Up a Voluntary Labelling Scheme \nAFME has concerns relating to the proposal for a voluntary labelling system for AI applications and would not \nsupport option 2.  We discuss this in more detail below.  \n \nThe awarding of a quality label introduces an element of market discipline; it would be difficult to ensure that \nsuch systems would remain market-driven, and therefore truly voluntary, and not become effectively mandatory.  \nA voluntary labelling scheme also cuts across the principle that the object of trust should not be AI as a \ntechnology, but firm using a given AI application. This should be a focus of education about the use of AI in the \nEU; clearly explaining the benefits and dispelling the myths that may have arisen about the technology. \nPracticality  \nIn relation to how such a system would work in practice, it is likely to be disruptive and even a voluntary system \nimposes disproportionate obligations, such as audit costs. This is also a potential consideration for the High Level \nExpert Group Guidelines for Trustworthy AI (HLEG Guidelines), for which the Assessment List runs to several \npages and more than 50 items; significantly more than is necessary for many low-risk uses of AI. Any firm which \ndecided to certify certain AI applications would need to be assured that it was clearly beneficial for themselves \nand for their clients.   \nA voluntary labelling system would also be very difficult to tailor to a specific application or use case. This \nspecificity would be necessary for a truly accurate assessment to be made. This lack of specificity would become \napparent in the applicability of the criteria to each AI application, in the definition of each criterion across \ndifferent participating firms (e.g. how would you define ‘accuracy’ in a comparable sense), and in that such a \nsystem would not be able to take into account the individual risk thresholds and management framework of any \nparticipating firm.  \nConsumer Benefit  \nFurthermore, it is not clear that enough consideration has been given to how a voluntary labelling system could \nbe useful to consumers, or how it would be presented. For instance, there would be no commentary to explain \nwhy any given AI application had, or did not have, the label applied (which would be key in a voluntary, market-\ndriven system). This would be particularly detrimental to those applications for which the labelling system was', 'inappropriate  or  insufficiently  accurate/specific,  or  for  which  it  would  create  disproportionate  cost  or \ndisruption. It is also unclear what the label would convey to the consumer; there is therefore a risk that an \ninsufficiently specific voluntary labelling system (that under the current proposals would then in effect become \nmandatory and set the market benchmark) would actually cut across the principle of human autonomy, by \nmisleading or confusing customers with incomplete information.        \nIt is worth noting that the above considerations are in respect of a voluntary labelling scheme as proposed in the \nWhite Paper and Impact Assessment.  There can be instances where voluntary disclosure on a more general basis \nis useful for consumers, particularly in instances where disclosing to consumers that they are interacting with an \nAI system rather than a human.  \nOption 3 – EU Legislative Instrument Establishing Mandatory Requirements for All or Certain Types of AI \nApplications \nAs noted in our comments above under Option 0, we strongly believe that regulation should remain technology-\nneutral and principles-based, recognising (i) that the risks applicable to the use of AI are also applicable to the \nuse of many other types of technology and (ii) that the object of trust should be the user of the technology (i.e. a \nfirm), not an individual type of technology itself. Regulation or policy which focuses on the specifics of a \ntechnology, as opposed to the core obligations and principles to ensure the trustworthiness of the technology \nprovider or user, could negatively and unnecessarily impact the adoption of AI within Europe. \nConsideration of the sub-options \n•  Sub-option 1, to focus on a limited category of AI applications \nFocusing on a limited category of AI applications, , such as biometric identification systems, could be a suitable \napproach, as the legislative instrument could be tailored to systems that present specific risks. \nThis would help to narrow the criteria and scope of AI applications to those with the greatest perceived risk to \nsafety, fundamental rights, and discrimination. However, further thought and consultation would be needed on \nthe criteria for determining the category and how these would be kept up to date.  \n \n•  Sub-option 2, to focus on a limited set of ‘high-risk’ AI applications \nFocusing on ‘high risk’ AI applications could be a suitable approach, provided that the definition of ‘high-risk’ is \nset appropriately within the legislative instrument. We discuss this in more detail below.  \n \nFocus on High Risk Applications \nAFME supports the Commission’s focus on safety and fundamental rights. Limiting any new requirements to \nthose applications that may pose a real and relevant risk to citizens (e.g. on their safety or physical integrity) will \nprevent these requirements from creating barriers to innovation and inhibiting the broader development of AI \nin Europe. This focus on safety should be the foundation, to ensure a consistent application of the requirements \nacross any activities to which they are applied, based on comparable risks. \nOn this basis, and given the highly-regulated nature of financial services noted above in our comments on Option \n1, AFME is of the view that financial services is not a high-risk sector, particularly noting the comments below. \nHowever, we also feel that the EU is a leader in protecting consumers and fundamental rights. It is crucial that \nthe regulatory landscape for AI in Europe appropriately balances risk reduction with fostering innovation, \nallowing firms in Europe to harness the benefits of AI and maintain their competitive position globally; \nparticularly in key sectors for the European economy. In assessing the approach to high risk applications, we \nrecommend the Commission consider the comments made on highly regulated sectors under Option 1 above, as \nwell as the following on unintended consequences.', 'We note that the designation of a sector as ‘high-risk’ may also create unintended opportunities for regulatory \narbitrage. Where an activity can be performed by a firm from outside the designated sector (or from outside the \nEU), this firm would be in a position to provide products or services which utilise in-scope high-risk AI \napplications to consumers, without adhering to the new regulatory obligations intended to ensure trustworthy \nAI. To ensure that citizens’ safety and fundamental rights are well protected, and to avoid creating an unlevel \nplaying field among firms, once an application has been identified as high-risk, the Commission should ensure it \nis appropriately covered by the regulatory framework regardless of sector, following the principle of “same \nactivity, same risks, same rules.” \nIn addition, we also request clarity on the criteria to be considered to designate high-risk activities (specifically \non how to evaluate “that significant risks are likely to arise”),  the procedure to follow and the body responsible \nfor making the decision. This assessment would presumably have to be performed by a central authority, rather \nthan on a firm-by-firm basis. It would therefore fall either to the Commission or, given their specialist expertise, \nto sectoral authorities. The latter would, however, be limited by their own regulatory perimeter in terms of the \nactivities or entities they supervise.  We would like to emphasize again that in order to ensure a consistent \nframework the focus on safety and citizens’ fundamental rights should be the foundation to evaluate, including a \nnew activity in this framework, ensuring that the same requirements are applied based on comparable risks. \nFurther Consultation  \nFinally, we note that the regulatory approach for high-risk activities as proposed in the Impact Assessment will \nneed refinement via further consultation once the scope has been defined. This will be important not just for any \nsectors categorised as high-risk, but also more broadly to allow all industry sectors to comment on how the \nregulatory approach will apply to individual high-risk applications. Furthermore, it will allow the sharing of \nexperiences from sectors that are already highly regulated, such as financial services. \n•  Sub-option 3, to focus on all AI applications \nWe believe that focusing on all AI applications would not be a suitable approach.  As noted in our comments \nunder Option 0 above, this would create unnecessary barriers to innovation and the development and adoption \nof AI by firms.  We also note that capital markets is a highly regulated industry, and our comments are made from \nthis perspective.  \nOption 4 – Combination of any of the options above taking into account the different levels of risk that could be \ngenerated by a particular AI application \nAs outlined in our comments under Option 1 above, the Commission should consider its full toolkit of regulatory \nand non-regulatory measures to determine the most effective solution.  This could range from targeted guidance, \nand supervisory adjustments, among others, not only legislative amendments.  \nComments on the Preliminary Assessment of Expected Impacts \nFurther to our comments above, we strongly support that the following are taken into account in any cost-benefit \nanalysis of a new approach to AI: \n•  That any new requirements should be risk-based, rather than broadly applied, to prevent unnecessary \nbarriers being placed on the development of AI applications; \n•  That existing regulatory requirements should be taken into account, to avoid duplication or conflicting \nobligations being placed upon firms; \n•  That a level playing field should be created and maintained, i.e. that the requirements should apply to \nspecific activities, regardless of the size, or regulatory status of the firm performing the activity', 'AFME contacts \nDavid Ostojitsch, david.ostojitsch@afme.eu       +44 (0)20 3828 2761 \nFiona Willis, fiona.willis @afme.eu         +44 (0)20 3828 2739', '']"
F550863,10 September 2020,Jelle Hoedemaekers,Wirtschaftsverband,Agoria vzw/asbl,mittel (50 bis 249 Beschäftigte),68004524380-10,Belgien,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"Agoria is the Belgian federation for the technology industry. We are paving the way for all technology-inspired companies in Belgium pursuing progress internationally through the development or application of innovations and which, together, represent some 300,000 employees. We are proud that more than 1,900 member companies trust in the three pillars of our services: consulting, business development and the creation of an optimal business environment.

Agoria thanks the European Commission for its efforts to foster Artificial Intelligence and for the opportunity to provide inputs regarding its Proposal for a legal act laying down requirements for Artificial Intelligence, which presents several options for future legislation. 

Firstly, Agoria is in favour of Option 1 of the alternative options to the baseline scenario, i.e. EU “soft law”, implying a non-legislative approach to facilitate and spur industry-led intervention without putting forward an EU legislative instrument. In addition, we believe that a “soft law” approach could build upon existing national initiatives and encourage a quicker industrial transformation using AI-driven systems to automate and reinvent fundamental industrial processes. Secondly, we acknowledge that others may see specific risks relating to the usage of AI in certain situations, justifying a legislative intervention. However, we must stress that the possibility of such risks arising at some point in the future does not imply that this is the case today, neither sporadically nor systematically. We urge the European Commission to analyse what is wrong in practice, before considering the optimal means of resolving these issues. Once this is clear, requirements for these specific high-risk situations will become evident and remedies may be needed. Where remedies are needed, enforcement methods should be considered, but not beforehand. Suggesting ex-ante enforcement mechanisms without any background is causing a lot of uncertainty, and SME’s specifically will be put at a disadvantage by this. 

Agoria believes that before choosing any option for a legislative intervention, existing regulation must be analysed carefully, identifying potential gaps precisely. This analysis should help to propose the adequate tools to fill those gaps and to establish a legally sound definition of AI. Given that AI has many meanings for different people and that many definitions of this term exist already, it is vital for the success of any AI related legislative action to define AI in a robust way. The most important aspect to keep in mind is that AI is not a product in itself, but rather a technology embedded in products, services and applications, which may be used in an extensive array of sectors, which puts all concerns related to AI in another perspective. For example, the health sector is already heavily regulated, and additional requirements should fit into the existing mechanisms. Additional unnecessary AI-related requirements within the EU product legislation should be avoided.

"
F550862,10 September 2020,Pedro Simoes,Unternehmen/Unternehmensverband,Visa Europe,groß (250 oder mehr Beschäftigte),61954192201-58,Vereinigtes Königreich,Künstliche Intelligenz – ethische und rechtliche Anforderungen,Please see attached file.,"['Visa Response to the European Commission \nIIA on Requirements for Artificial Intelligence\n \n \nSeptember 2020 \n   \n  1', 'About Visa \n \nVisa welcomes the opportunity to respond to the European Commission’s Artificial Intelligence White Paper. \nOur mission is to connect the world through the most innovative, reliable, and secure payments network – \nenabling individuals, businesses, and economies to thrive. Visa’s relentless focus on innovation is a catalyst \nfor the rapid growth of connected commerce on any device, a driving force for increased digital acceptance, \nand a cornerstone of safety and security across the digital economy.   \n  \nVisa currently facilitates commerce across more than 200 countries and territories. As a leading global \npayments technology company, Visa understands what it means to function, innovate, and invest in a \ncontinuously evolving and interconnected digital world. The global nature of our business also gives us a \nholistic perspective on regulation, which is particularly useful when considering regulatory approaches to \n‘borderless’ markets and technologies, such as data and artificial intelligence (‘AI’).  \n  \nOur ‘north star’ for data use (including data-driven technologies such as AI) is that it should first, and foremost, \nbenefit individuals, businesses, and economies. This overarching approach is underpinned by our commitment \nto those who use our products and services that we will be accountable stewards of their data, will uphold \ntheir privacy, and will promote high standards of responsible, ethical practice in every market in which we \noperate.   \n  \nVisa pioneered the use of AI in payments in 1993, becoming the first payments network to use neural networks \nfor real-time, risk-based fraud analytics. Visa Advanced Authorisation (‘VAA’) now prevents approximately \nUSD 25 billion of fraud annually. Today, we are increasingly leveraging the power of data and data-driven \ntechnologies, such as AI, for a wide variety of purposes across our business, including security, product and \nservice delivery, operational efficiency, and network reliability. We are confident the next generation of \npayments  experiences  (such  as  ‘conversational  payments’, behavioural biometrics,  and  automation  of \nbespoke ‘point of sale’ shopping experiences) will be powered by AI.   \n \n \n \n   \n  2', '1.  Views on the impact assessment \nVisa is keen to engage with the European Commission in supporting its vision to create a human-\ncentric  and  trustworthy  digital  environment  in  Europe  for  the  use  of  data  and  data-driven \ntechnologies, such as Artificial Intelligence (AI). Visa is also aligned with the Commission’s aim to \ncreate a common regulatory framework in Europe, and we are encouraged by the AI White Paper’s \nmeasured approach to regulation. We welcome the publication of the Inception Impact Assessment \n(IIA) on the requirements for AI and appreciate the opportunity to provide comments, building on \nour contribution to the Commission’s EU AI White Paper public consultation. \nVisa agrees with the Commission’s stance that there are potential ethical and legal clarifications \nneeded to safely develop AI powered services and applications. However, we would highlight the \nCommission’s own assessment noting that developers and deployers of AI are already subject to a \nwide body of EU legislation on fundamental rights, consumer protection, unfair commercial practices, \ncompetition law, and product and safety liability and would urge the Commission to avoid over-\nprescriptive regulation which would place the EU at a competitive disadvantage in the development \nand innovation of AI. \n2.  Policy options \nA holistic review of sectoral regulation should be prioritised before the introduction of any new rules, \nin full and open consultation with industry, and close co-ordination between vertical and horizontal \nregulators, to ensure consistency of approaches and avoid unnecessary cumulative or duplicative \nregulation. Industry and private-sector alignment will be critical to the success of the European AI \nstrategy and any efforts to facilitate and spur industry-led action (Option 1 of the IIA) should be done \nin close alignment with international standards and best-practices. \nThis approach would provide the most efficient route to effective outcomes in many cases, supporting \nEuropean competitiveness by avoiding the imposition of undue regulatory burden for businesses \ninvesting and innovating in Europe. \nThere are two critical elements that must be outlined by the Commission in order to deliver a practical \nand  effective  framework  for  the  application  of  AI,  namely,  providing  a  clear  criteria  for  the \nclassification of high-risk and low-risk AI applications, and the definition of high-risk AI applications. \nUnder the current iteration, high-risk AI applications could include any AI application and is very open \nto interpretation. We would urge the EC to further engage with industry and stakeholders to ensure \na workable and practical definition. \nVisa agrees that risk should be a critical delineator for regulators in defining the scope of new \nregulation and that high-risk applications merit a higher degree of scrutiny than low-risk use cases. \nWe believe the proposals in the White Paper, which are further fleshed out in the IIA, go some way \ntoward establishing a sensible approach in this respect; however, we would urge caution in several \nrespects, namely on the ability to assign risk in an accurate and effective manner, and the broad \n  3', 'categorisation  of  high-risk  applications.    As  noted  in  Visa’s  response  to  the  AI  White  Paper \nconsultation, we would submit to consideration the addition of a third criteria to evaluate the \n‘likelihood’ of risk. The three criteria would thus be: a) sector; b) significance or severity; c) likelihood \nof risk. It should be noted that there are several categories of risk used by risk professionals and it will \nbe significantly easier for both companies are regulators if there is alignment between new legal \nlanguage and established industry terminology \nSetting an overly rigid and onerous new requirements, could act as a headwind to investment and \ninnovation, and potentially place the EU at a global disadvantage. This is particularly the case with a \nstringent ex-ante oversight and approval mechanism such as that proposed in [Option 3 and 4 of] the \nIIA, which would consume significant time and resources for both regulators and businesses, and \ncould be especially detrimental for the competitiveness of SMEs in particular, jeopardising potential \nbusiness models and possibly having a significant business impact on existing ones, ranging from \nadditional costs, less revenue to reliance upon third parties for product launches and potentially \nundermining  copyright, and trade secrets, infringe on IP rights, and heighten cybersecurity risks. Visa \nis optimistic about the potential of a voluntary certification and labelling scheme for AI applications \n(suggested in Option 2 of the IIA). Such a scheme (applied in the appropriate cases) could help drive \nbest  practice,  enhance  consumer  trust  in  new  products  in  the  marketplace,  and  familiarize \norganisations with the sorts of principles and processes required to build trustworthy AI. However, \nsuch scheme would require further clarification on how it would apply to the myriad of AI use cases \nand  application  areas.  A  balance  must  be  struck  between  a  regime  robust  enough  to  merit \naccreditation (and thus trust) and the commensurate level of risk attributed to these applications. It \nwould be useful to explore similar industry-driven certification schemes, for example those available \nfor IoT device security. These schemes exist to serve different industry needs, depending on the \nrespective risk level and risk appetite. This could potentially be an alternative to one single scheme.   \nAs noted in the IIA, the HLEG Trustworthy AI Guidelines may serve as a template, as they address the \npotential  for  harm  through  establishing  strong  ethical  foundations  for  AI,  anchored  in  good \ngovernance, best practice and international standards, embedding of principles and practices which \ngo beyond legal compliance.  \n3.  Enforcement mechanisms \nThe IIA introduces the discussion on enforcement mechanisms to ensure effective compliance with \nany applicable requirements for AI applications, which could be ex-ante or/and ex-post. Again, we \nwould reiterate that although Visa supports the need for new governance to mitigate risks posed by \nAI, this is appropriate only where other existing regulation cannot adequately do so.   \nThe Commission must ensure that any proposed process of prior conformity assessment does not \nduplicate existing self-assessment frameworks and is designed in a way that is practical and effective \nfor both regulators and industry participants, without restricting and slowing down innovation in \nEurope and whilst being compliant with existing use-case and industry specific laws. Furthermore, we \n  4', 'would strongly encourage that any suggested framework be closely aligned with internationally \nrecognised, voluntary industry-driven standards and best practices.  \nAddressing potential ex post controls, these mechanisms (such as scrutiny or auditing of risk impact \nassessments) could potentially limit the possibility of bottlenecks in the prior-conformity process, \nensuring resources are spent where need for prior conformity assessments to the most high-risk \ncases. This could reinforce the expectation of reasonable scrutiny across the wider market, while \navoiding impeding fast-moving development where this is not strictly necessary.  We would urge the \nCommission  to  stride  for  a  proportionate,  non-discriminatory,  transparent  objective  criteria  in \ncompliance with international obligations. \n4.  Clarifications needed \nWe would encourage the European Commission to provide further clarity on the definition of what \nconstitutes a “remote AI biometric identification”. It is important to 1) target applications that are of \nconcern, 2) specifying what applications are out of scope, if possible and as already seen in other \nexisting legislation, to help provide certainty to industry and consumers.  \n \n \n \nWe would refer to Visa’s response to the AI White Paper consultation where we elaborate on many \nof these aspects and considerations in more detail. \n   \n  5']"
F550853,10 September 2020,Jens-Henrik Jeppesen,Unternehmen/Unternehmensverband,Workday,groß (250 oder mehr Beschäftigte),021253717373-58,Vereinigten Staaten,-,"Please find attached Workday's comments on the Inception Impact Assessment

Best regards,

Jens-Henrik Jeppesen
Director, Public Policy, EMEA
","['Workday Comments on the European Commission \nInception Impact Assessment for a  \n“Proposal for a legal act of the European Parliament \nand the Council laying down requirements for \nArtificial Intelligence” \nSeptember 10, 2020 \n \nIntroduction \nWorkday is pleased to submit comments on the Inception Impact Assessment (IIA) for a “Proposal for a \nlegal act of the European Parliament and the Council laying down requirements for Artificial Intelligence”.  \nWorkday is a leading provider of enterprise cloud applications for finance and human resources, helping \ncustomers adapt and thrive in a changing world. Workday applications for financial management, human \nresources, planning, spend management, and analytics have been adopted by thousands of organizations \naround the world and across industries—from medium-sized businesses to more than 60 percent of the \nFortune 50. Headquartered in Pleasanton, California, Workday has more than 12,300 employees worldwide \nand 21 offices across Europe. Workday’s European-based customers include Airbus, BlablaCar, Deutsche \nBank, Primark, Siemens, Sanofi, and ThyssenKrupp.  \nWithin its applications, Workday incorporates machine learning technologies that enable its customers to \nmake more informed decisions and accelerate operations, as well as assist workers with data-driven \npredictions that lead to better outcomes. Workday believes these technologies have the potential to impact \nenterprises in the near term by making operations more efficient. In the longer term, enterprises will be able \nto reorganize operations around machine learning’s unique possibilities. Promoting the thoughtful and \nresponsible adoption of machine learning is a fundamental component of Workday’s public policy agenda.  \nWe have contributed actively and constructively to the Commission’s preparatory work in this area. We \nprovided  comments  to  the  Consultation  on  the  Artificial  Intelligence  White  Paper,  and  piloted  the \nTrustworthy AI Assessment List developed by the High-Level Expert Group on Artificial Intelligence, \nmapping its questions against our machine learning ethics controls. In addition, we contributed to national \ninitiatives, including the UK ICO Guidance on AI and data protection, to ensure emerging AI governance \nschemes appropriately account for existing law.', 'Comments  \nBuilding on the material we have already contributed, we would like to offer the following observations for \nthe Inception Impact Assessment. Workday supports the European Commission’s stated objective: “…to \nfoster the development and uptake of safe and lawful AI that respects fundamental rights across the Single \nMarket by both private and public actors while ensuring inclusive societal outcomes.” \n1.  Legislation is likely to be warranted for high-risk applications/use scenarios \nWorkday recognizes that maintaining the status quo (baseline / option “0”) is unlikely to bring about the \nlegal certainty and trust necessary to ensure the development and uptake of lawful and trustworthy AI. We \nalso consider that certain use scenarios and AI applications can entail risks to fundamental rights that \nshould be addressed by legislation. Among the issues at stake are risks of unfair or illegal discrimination. \nAt the same time it is important to acknowledge that AI applications, properly developed and deployed, \nhave immense potential to improve and augment human decision-making, and to detect and help address \nproblems with bias that already exist. Further, it should be noted that existing legal  protections of \nfundamental rights apply fully to any organization, whether or not it uses AI applications. It is essential that \nany new legislation does not duplicate current rules, and this will require careful mapping of the legislative \nenvironment before proceeding.  \n2.  Risk assessment requires a two-pronged approach \nA legislative instrument should be carefully and narrowly targeted at high-risk applications. It would be \nextremely important to define the concept of ‘high risk’ in a way that provides clarity, predictability and legal \ncertainty both for developers (organizations that develop and provide applications that embed AI), deployers \n(organizations that purchase and use applications that embed AI), and the public. Careful risk assessment \nis fundamentally important because of the large number of types of AI applications and the many different \nways they can be used. We agree with the two-pronged approach set out in the White Paper on AI: that \nregulation should be focused on those applications that are deployed in high-risk sectors, and in a way that \ninvolves high-risk use. However, it would be a mistake to attempt to designate classes of applications as \nhigh risk per se. This would fail to encompass the particular context in which an application is deployed and \nused. The results of an overly broad risk assessment would be imposition of regulatory burdens on \napplications that carry little or no risk, and less effective oversight of those that do carry high risk.  \nIt is also important to recognise that most AI applications are designed to aid, not replace, human judgment \nand decision-making. AI applications can provide valuable predictions and recommendations for human \ndecision makers to act on. The regulatory requirements for such applications should be different than those \nthat would be suitable for AI applications that generate and execute decisions with limited human \ninvolvement.       \n3.  Prescriptive, pre-market assessment of conformity should be avoided \n \n    2', 'Workday would caution against introducing prior conformity assessments; such an approach would \nrepresent a significant departure from the EU’s past treatment of standalone software. Workday urges the \nCommission not to pursue a regulatory framework that is based on prescriptive pre-market conformity \nassessments for high-risk AI, which could result in barriers to market entry and chill innovation. Particularly \nin the case of software, which is updated frequently (on a weekly basis in our case), prior conformity \nassessments would slow the pace of updated features. As an alternative, we think that impact assessments \nakin to the Data Protection Impact Assessments required under GDPR and which would be available to \nregulators to review would protect individuals while being less likely to create a lag to market or unduly \nburden smaller enterprises.  \n4.  Labelling systems would not add value \nThe option of setting up a mandatory European labelling system for AI applications that fall outside the \nhigh-risk determination does not seem to be a viable way forward. There are such a broad range of \napplications and use scenarios that it would be difficult to design a single labelling scheme which would \nprovide meaningful guidance to users about the trustworthiness of the many diverse applications. Such a \nlabelling system would inevitably have to be extremely complex as would the governance mechanisms \nnecessary to oversee it. The associated costs and complexity would most likely outweigh the benefits: Per \ndefinition, the applications to be covered would pose little or no risk to users’ fundamental rights. Rather, \nvoluntary labelling and certification schemes, tailored to the area of AI deployment, represent the best path \nforward.  This has worked well in the privacy space, where a variety of codes of conducts and certifications \nare available for different data uses. \n5.  The current liability regime should be maintained \nLiability rules should be technology neutral, and products should not be subject to additional or different \nliability rules simply because they integrate AI and machine learning. This would hamper innovation and \nslow deployment of AI, depriving society of its numerous benefits. Furthermore, given the vast array of AI \nuse cases, “one-size-fits-all” liability rules would be unworkable. Any changes to the EU’s liability regime \nmust be driven by a clear and demonstrated need. The current EU product liability regime as set out in the \nProduct Liability Directive sets out clear and time-tested rules that apply across a vast range of products, \nincluding those with embedded software, and give consumers the possibility to obtain compensation for \npossible harms caused by products or services that embed AI. Any change in liability rules without an \nassessment of the current regime and a demonstrated need risks chilling innovation, with little benefit for \nconsumers. It would be premature to amend the liability rules until a regulatory framework for AI (e.g. to \nrequire data quality, transparency, and robustness) has been established.   \n* * * \nWorkday appreciates the opportunity to provide input on the European Commission’s Inception Impact \nAssessment on artificial intelligence. If you have any questions or if we can provide further information, \nplease  do  not  hesitate  to  contact  Jens-Henrik  Jeppesen,  Director  of  Public  Policy,  EMEA,  at \njens.jeppesen@workday.com. \n    3']"
F550846,10 September 2020,EDiMA EDIMA,Wirtschaftsverband,EDiMA,sehr klein (1 bis 9 Beschäftigte),53905947933-43,Belgien,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"EDiMA welcomes the opportunity to provide feedback on this Roadmap. EDiMA and its members can offer insight into the interplay between the technical aspects of AI and its broader impact on the economy and society, and are committed to maximising the benefits of AI for Europe. While the current and potential benefits of AI are numerous, concerns about AI are important and legitimate, and should not be discounted. Developments in AI are advancing quickly and will have a transformative impact on our society, so we agree with the Commission on the importance of fostering an ecosystem of trust.

The need for a clear, workable definition 
A crucial prerequisite for targeted and effective regulation will be to clearly define AI. A delicate balance needs to be struck between avoiding an overly-broad definition, encompassing all contemporary software systems, and an excessively narrow definition, which could quickly become outdated due to the rapid pace of digital innovation.
A sensible definition of ‘high-risk AI’ is also vital, and the two-step approach proposed in the EC’s White Paper goes in the right direction. We strongly recommend removing vague and open-ended clauses from the risk definition - such as ""exceptional instances"" and ""immaterial damage"" - to improve legal certainty and avoid any potential overreach. The process to decide whether new sectors should be added to the list of ‘high-risk sectors’ must also be robust and transparent, also considering the views of relevant experts and consulting with all stakeholders.

A balanced approach to tackle future AI challenges
EDiMA members continue to agree with the White Paper’s viewpoint that regulatory oversight should  be limited to applications carrying the highest risks for users A legislative instrument covering all types of AI applications - regardless of risk of harm, as mentioned in the Roadmap - would be unnecessary and disproportionate, with detrimental implications for innovation and competitiveness in the European economy, as well as for consumers. A wide intervention also bears its own risks in limiting certain AI technologies which are themselves known to reduce harm.  Furthermore, we believe that most of the concerns raised by AI applications are addressed through existing legislation, and there is merit in ensuring that this is properly implemented before putting in place any new AI-specific rules. For example, the GDPR, while the P2B Regulation and new consumer rules have recently revised transparency requirements for ranking guidelines.
We would also caution against the expansion of the scope of future AI regulation to the open-ended category of “automated decision making” – to do so would risk creating disproportionate regulatory obligations that would deter the development of automated systems that do not pose any risk nor harms.

Industry-led intervention: the most flexible way to anticipate and overcome AI challenges
EDiMA strongly believes that the best way to ensure that AI is trustworthy, secure and respectful of EU values and rules is a combination of ex-ante risk self-assessment and ex-post enforcement for high-risk AI applications.
Concerns related to AI are mostly factored into the development stages of the technology. Accounting for potential risk and having strong processes in place can help to minimise risks, which is essential to creating high quality products and services. An example of such a process is checking the quality of the datasets within the model used to train it, and by performing rigorous testing. It is essential that these processes are flexible in order to adapt to widely different applications of AI. Such a flexibility would also be needed if a single voluntary labelling system was to be considered. AI labelling system that is not flexible enough could quickly become outdated and provide false assurances to consumers, undermining their trust in AI applications. 

We look forward to working with the Commission on AI in the future."
F550845,10 September 2020,Ondřej Ferdus,Wirtschaftsverband,Confederation of Industry of the Czech Republic,mittel (50 bis 249 Beschäftigte),785320514128-81,Tschechien,Künstliche Intelligenz – ethische und rechtliche Anforderungen,Please find attached
F550840,10 September 2020,Charles Low,Wirtschaftsverband,(FERMA) Federation of European Risk Management Associations,sehr klein (1 bis 9 Beschäftigte),018778010447-60,Belgien,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"The Federation of European Risk Management Associations (FERMA) welcomes the opportunity to comment on the Commission’s Inception Impact Assessment, specifically on the relevant policy options and policy instruments in the area of Artificial Intelligence. Our input here is complemented by our attached position paper. 

FERMA brings together 22 risk management associations in 21 countries. They represent nearly 5000 professional risk managers active in a wide range of business sectors. Risk managers utilise a number of tools and methodologies for measuring risk, particularly Enterprise Risk Management (ERM) and the ‘Three Lines of Defense’ model. ERM and the ‘Three Lines of Defense’ have already been applied effectively in the digital context, such as GDPR and cybersecurity, and have proven build organisations’ capacity to mitigate and assess risk. 

We are therefore encouraged to see that the Commission has incorporated a risk-based approach to AI. The promotion and utilisation of risk management methodologies such as ERM and the ‘Three Lines of Defence’ can help organisations support such a risk-based approach. On this note, FERMA is committed proactively contribute its risk management expertise to the work of the European Commission to ensure a robust ‘risk-based approach’ is enshrined in AI legislation moving forward.

FERMA believes it is important to balance any future legislation against the clear need for innovation and market development. At the current stage of market maturity, FERMA is in favour of applying broad, market-based principles. With specific reference to the policy options presented in the Inception Impact Assessment document, FERMA supports Options 2, and 3. B therefore believes that Option 4, which would see a combination of policy tools being used that take into account the different levels of risk of particular AI applications, is ultimately the most sensible approach.

In addition to the comments above, FERMA takes the view that AI raises a number of concerns related to liability. This is why FERMA believes that the existing European legislative framework in this area, the Product Liability Directive, needs to be re-examined before creating a new liability regime specific to AI. To that end, FERMA believes the primary goal of action in this area should be to ensure legal certainty since this will allow risk managers and the insurance industry to adjust their solutions to market needs. 

Even with legal certainty, AI will always pose risks. To mitigate AI-related risk, FERMA recommends the Commission to create a mix of ex-ante incentives to encourage regulatory compliance based on good-faith disclosures and reasonable transparency requirements, combined with a light-touch ex-post enforcement mechanism.

FERMA remains committed to working with the European Commission to build a holistic AI legislative framework that accounts for risk inherent to AI without stifling innovation.","['Position Paper \n \n \n \n \n \nFERMA’s Position Paper on Artificial Intelligence \n12 June 2020 \n \nThe Federation of European Risk Management Associations (FERMA) thanks the European Commission \nfor this opportunity to share its thoughts on the Commission’s White Paper “on Artificial Intelligence \nA European approach to excellence and trust”.1 Herein, FERMA shares it’s thoughts on AI, in part, in \ndirect response to the Commission’s questionnaire, and in part to offer its own thoughts based on its \nrecent reflections on the topic. \n \nAI presents organisations with the greatest opportunities in the digital transformation of the European \neconomy. However, the use of AI also comes with the potential for significant risk. President Von der \nLeyen acknowledges this potential risk and has called for specific legislation on AI to be drafted \nexpeditiously. \n \nFERMA notes from the outset that it is encouraging to see the Commission has incorporated a risk-\nbased approach to AI in the White Paper. FERMA also welcomes the efforts of the Commission in other \ndomains, including the AI Ethics Guidelines of the High-Level Expert Group.2 FERMA commits itself to \nwork proactively with the European Commission to ensure a robust ‘risk-based approach’ is enshrined \nin AI legislation moving forward.  \n \nFERMA  and  its  members  have  a  long-standing  experience  in  risk  management,  which  is  the \nidentification,  evaluation,  and  prioritisation  of  risks3,  followed  by  coordinated  and  economical \napplication of resources to minimize, monitor, and control risk exposure (probability or impact of \nevents), or to maximize the realisation of opportunities through minimizing losses. Risk managers \nutilise  a  number  of  tools  and  methodologies  for  measuring  risk,  particularly  Enterprise  Risk \nManagement (ERM) and the ‘Three Lines of Defense’ model.4 ERM and the ‘Three Lines of Defense’ \nhave already been applied in the digital context, such as GDPR and cybersecurity, and have proven to \nbe effective in building organisations’ capacity to mitigate and assess risk. \n \nThroughout this paper, FERMA suggests that European institutions consider promoting these already \nestablished and successful management structures to build a holistic AI framework that accounts for \nrisk inherent to AI without stifling innovation. \n   \n                                                      \n1 https://ec.europa.eu/info/sites/info/files/commission-white-paper-artificial-intelligence-feb2020_en.pdf   \n2 https://www.ferma.eu/building-trust-in-ai-first-european-ethics-guidelines/   \n3 Defined in ISO 31000 as the effect of uncertainty on objectives \n4 Guidance on the 8th EU Company Law Directive , FERMA, September 2011: \nhttps://www.ferma.eu/publication/guidance-on-the-8th-eu-company-law-directive/ \nhttps://www.ferma.eu/app/uploads/2019/02/ferma-perspectives-cyber-risk-governance-09-10-2018.pdf; ISO 31000 Risk \nManagement: https://www.iso.org/iso-31000-risk-management.html \n  1', 'Position Paper \n \n \n \n \nAn Ecosystem of Excellence \nTo build an ecosystem of excellence, the Commission proposes a large set of priorities to support the \ndevelopment of AI across the EU. Such a broad approach should not undermine the effectiveness and \nefficiency of executing on these priorities. To that end, FERMA encourages the Commission to \nprioritise funding so that priorities are executed to a high-degree. Funding should be directed \nprimarily towards (i) fostering research and innovation, (ii) developing skills to fill competency \nshortages, and (iii) ensuring that SMEs can access and use AI. \n \nStrengthening excellence in research and development can be combined with coordinated action to \ncreate synergies across excellence centres. FERMA highlights the fact that Europe already has pre-\nexisting AI excellence centres that should be networked together and improved upon through the \nEuropean Digital Innovation Hubs (EDIH). \n \nIn addition, the Commission may also consider creating a standing committee focussing on providing \noverarching strategic direction to research and development in the EU and helping to ensure that AI \nis developed in line with European values. In this the committee would monitor and ensure that \npublicly funded research and development is line with European fundamental rights enshrined in the \nCharter of Fundamental European Rights of The European Union and the European Convention on \nHuman Rights. The committee would ideally be staffed on a rotating basis with a combination of \nacademia, industry experts from various verticals, representatives of SMEs, and consumer and civil \nrights organisations. This will enable AI in the European Union to differentiate itself from the rest of \nthe world and ensure AI is developed in line with European values and helping to secure technology \nsovereignty. The Coordinated Plan, due for revision at the end of 2020, can also support the goal of \ndeveloping AI in conformity with European values.  \n \nAn Ecosystem of trust \nThe EU should set horizontal regulation applicable to AI across the Internal Market. There is \nsignificant potential for market fragmentation between Member States in the absence of European \nlegislation. A key advantage of this approach would be for companies, legal certainty regarding the \nfuture development of AI-based solutions, and for consumers it fosters trust in AI solutions, knowing \nthat they meet certain ethical standards.5 While for regulators it allows for a control mechanism that \ndoes not prevent innovation. Nonetheless, it is important to balance future legislation against the need \nfor innovation and the market development. At the current stage of market maturity, FERMA \nrecommends the EU take a similar direction to Directive 2000/31, applying broad, market-based \nprinciples. This will enable the market to develop and gives regulators tools to measure the real-world \neffects of AI. A combination of ex-ante compliance and ex-post enforcement mechanisms is the best \nway to ensure that AI is trustworthy, secure and respects European values and rules. \n \nThe risk-based approach suggested in the White Paper is the correct approach but calls for close \nmonitoring of the market due to the nature of AI and its ability to learn and adapt overtime with little \nto no human intervention. To mitigate this risk, FERMA recommends the Commission creates a mix of \nex-ante  incentives  to  encourage  regulatory  compliance6  based  on  good-faith  disclosures  and \n                                                      \n5 It may also be relevant for Member States to actively contribute to a definition roadmap so that specific needs are \nprioritised and addressed. \n6 Examples of such ex ante are listed in the White Paper on pages 20-21 and include: (i) Requirements ensuring that the AI \nsystems are robust and accurate, or at least correctly reflect their level of accuracy, during all life cycle phases; (ii) \nRequirements ensuring that outcomes are reproducible; (iii) Requirements ensuring that AI systems can adequately deal with \n  2', 'Position Paper \n \n \n \n \nreasonable transparency requirements, combined with a light-touch ex-post enforcement mechanism, \nbearing in mind always that increasing risks outpace the provision of regulation7 Another key aspect \nto address is how to assess this risk. One option would be for European standards organisations to \nundertake work on standardised risk assessments. \n \nMandatory requirements for high-risk AI applications are appropriate. Without prejudice to what \nthose specific mandatory requirements should be, FERMA encourages the Commission to align those \nrequirements to the risks create by AI in the fields of liability and fundamental rights. FERMA \nrecommends combining this with market monitoring by an existing institution or agency at European \nlevel.8 \n \nA voluntary labelling for low-risk AI application would be very useful. Standardisation organisations \nhave already begun some of this work, which the Commission should promote further especially as it \nis a market-driven solution that adheres to strong principles of consensus and transparency. FERMA \ncautions though that voluntary labelling schemes can be abused and, thus, the right safeguards will \nneed to be put in place to prevent this. It should also be made clear that a voluntary scheme would \nhave to go beyond any minimum threshold set by legislation.  \n \nWhether high-risk or low-risk, a risk assessment should take place as part of an ex ante procedure \nfor new products and subsequent re-assessment where applicable. To do otherwise would open the \nway for uncontrolled non-compliance within the AI legislative framework as the product evolves and \ndevelops.9 The assessment procedures may be sector specific or per AI type. In many instances self-\nassessment may be sufficient and risk management methodologies such as ERM already allow \norganisations to measure risks associated with AI and put in place the appropriate processes to \nmitigate them. Regardless of the risk profile, an appropriate risk-based model for AI will ensure that AI \ncapabilities will be neither over- nor under- estimated. To that end, control is key. \n \nSafety and liability implications of AI \nAI raises a number of concerns related to liability and therefore requires legal certainty. Principally, \nFERMA believes that if future AI legislation maintains the fault-based claim principle, consumers having \nsuffered harm will find it more difficult to obtain compensation because it may become difficult, if not \nimpossible, to allocate culpability.10 To that end, FERMA believes the primary goal should be legal \ncertainty since this will allow risk managers and the insurance industry to adjust their solutions to \nmarket needs. \n \n                                                      \nerrors or inconsistencies during all life cycle phases; (iv) Requirements ensuring that AI systems are resilient against both \novert attacks and more subtle attempts to manipulate data or algorithms themselves, and that mitigating measures are taken. \n7 Depending on the type of AI, a requirement for ex ante transparency would in effect prohibit the use of that particular \ntechnology, even where it may produce a superior result. See: Black, Julia and Murray, Andrew D. (2019) Regulating AI and \nmachine  learning:  setting  the  regulatory  agenda.  European  Journal  of  Law  and  Technology,  10  (3).  p.  16, \nhttp://eprints.lse.ac.uk/102953/ \n8 Examples of existing monitoring include, “The Observatory of European SMEs” established by the Commission in \nDecember 1992 in order to improve the monitoring of the economic performance of small and medium sized enterprises \n(SMEs) in Europe. Another example is the Centre for Media Pluralism and Media Freedom’s “Media Pluralism Monitor” co-\nfunded by the European Commission. \n \n \n10 Report from the Expert Group on Liability and New Technologies – New Technologies Formation: \nhttps://ec.europa.eu/transparency/regexpert/index.cfm?do=groupDetail.groupMeetingDoc&docid=36608   \n  3', 'Position Paper \n \n \n \n \nThe European legislative framework, such as the Product Liability Directive,11 and other sector specific \nlegislation,12 may need to be examined before creating a new liability regime specific to AI. Where it is \ndetermined that legislation is needed, a broad, principle-based AI legislative framework will create the \nlegal certainty sought if implemented correctly and supported by a holistic approach.  \n \nEven with legal certainty, AI will always have an inherent risk due to its very nature. To mitigate \nthese inherent risks, FERMA encourages the Commission to adopt a holistic approach to AI that \naccounts for the full range of risks, since it is often the case that AI is viewed exclusively through the \nlens of information technology without further consideration of effects further afield such as strategic \nand environmental risks, business risks, and operational risks.13 ERM and the ‘Three Lines of Defence’ \nare tried and tested risk assessment methodologies that support organisations assess and mitigate risk \nfostering a culture of robust corporate responsibility. \n \nA holistic approach towards AI will promote a culture shift that supports critical aspects of consumer \nprotection, so that organisations and consumers alike are more digitally literate and understand better \nthe  implications  of  AI,  making  for  better  informed  consumer  decisions.  To  improve  consumer \nprotection further, it should be incumbent on AI producers to clearly and accurately explain their AI \nsystem, especially how personal data is used in conjunction with the AI, and the possibility for users to \nremove their personal data from the system if needed.14 Consumer protection should also enshrine a \nhuman-in-the-loop principle and the ability to opt-out of certain AI decision making processes if \nrequested.15 \n \nThe  sensitive  nature  of  certain  types  of  data,  when  combined  with  AI,  such  as  biometric \nidentification systems understate the need for a careful balancing exercise between a number of \nrights guaranteed in the Charter of Fundamental Rights of the European Union (Article 7, respect for \nprivacy and family life, Article 8 protection of personal data) and the others public goods such as public \nsafety. As it is questionable whether private entities are best placed to undertake such a balancing \nexercise, the introduction of a judicial process is indispensable. Nonetheless, it is equally important to \nhighlight the valuable insights which can be drawn from biometric data for the purposes of a more \nthorough, decisive, and clear response to crises, as recently demonstrated in the Covid-19 crisis. \n \nIn that regard, the safety and liability implications of these specific areas, are, to an extent, already \naccounted for in the European Union’s data protection legislative framework suggesting that there is \nno need at present to add an additional layer of regulation. Nonetheless, FERMA believes special \nattention is needed in this area and would not rule out seeing a future need for specific targeted \nmeasures to remedy concerns. \n   \n                                                      \n11 Directive 85/374/EEC: https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:31985L0374 \n12 Such as the Medical Devices Regulation. Regulation 2017/745: https://eur-lex.europa.eu/legal-\ncontent/EN/TXT/PDF/?uri=CELEX:32017R0745    \n13 For a full breakdown of the risks that organisations should account for when implementing AI see: Artificial Intelligence \nApplied to Risk Management. Report published by FERMA. pp. 12-13 available at: \nhttps://www.ferma.eu/publication/artificial-intelligence-ai-applied-to-risk-management/   \n14 Article 7.3 of Regulation 679/2016 already provides the possibility for the data subject who has given their consent to the \nprocessing of data to withdraw their consent. \n15 Human-in-the-loop make sure that possible decisions which deal with certain categories of fundamental rights are first \nverified by a human, or reviewable by a human afterwards. The human-in-the-loop approach means that particularly those \nhigh-risk AI, humans maintain a substantive control over the decision-making processes. \n  4', 'Position Paper \n \n \n \n \nConclusion  \n \nFERMA thanks the European Commission for the opportunity to share its thoughts on its AI White \nPaper. FERMA especially welcomes the risk-based approach to AI adopted and further recommends \nthe promotion and utilization of risk management methodologies such as ERM and the ‘Three Lines of \nDefence’ to support this approach.  In addition, the Commission may also consider the following  \n \n\uf0b7  Building  the  existing  excellences  centres  into  a  network  supported  by  the  EDIHs  and \ncoordinated at European level towards ensuring that AI development in AI fits with European \nvalues \n\uf0b7  The differentiation between high and low risk AI applications is the correct approach, each \nentailing measures ranging from mandatory compliance and certification, through to self-\nassessment and voluntary labelling. \n\uf0b7  Ex-ante risk-assessments should take place for new products which would assess compliance \nproportionate to the context.  \n\uf0b7  Legal certainty can be achieved through a horizontal regulation that provides broad, market-\nbased principles \n\uf0b7  The fault-based claim principle may need further examination with existing European liability \nlegislation to help allocate culpability. \n\uf0b7  Supporting a shift in culture so that organisations and consumers are more digitally literate \n\uf0b7  Biometric identification systems and other sensitive areas call for a careful approach that \nbalances fundamental rights against the public good that can be achieved to solve social issues \nand catastrophes like COVID-19 \n \nFERMA remains committed to working with the European Commission to build a holistic AI legislative \nframework that accounts for risk inherent to AI without stifling innovation.  \n \nAbout FERMA: \nFERMA has been the single recognised voice of European risk managers for over forty years. With a \nmembership of 21 risk management associations spanning 20 European countries, FERMA is the leader of an \ninternational network that influences industry, the public sector, finance and other services. FERMA advocates \non behalf of nearly 5000 risk management professionals while promoting communication and education across \nthe Federation. 50% of member organisations are listed on the stock exchange, and over 80% have a turnover \nof more than €50 million, making them notable players in the European economy.  \nContact: Typhaine Beauperin, Chief Executive Officer: typhaine.beauperin@ferma.eu \n \n  5']"
F550832,10 September 2020,Committee Digital Economy,Wirtschaftsverband,Keidanren,mittel (50 bis 249 Beschäftigte),267303827175-27,Japan,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"The feedback by AI Utilization Strategy Task Force, Committee on Digital Economy, Keidanren is over 4000 characters. Please find attached file.
","['“Proposal for a legal act of the European Parliament and \nthe Council laying down requirements for Artificial Intelligence” \nComments on Inception Impact Assessment \n \nSeptember 10, 2020 \n \nAI Utilization Strategy Task Force \nCommittee on Digital Economy \nKeidanren \n \nPage 2 \nAt the same time, AI may generate new safety risks for users and third parties, which \nare not yet explicitly tackled clearly by the product safety legislation. For example, in \nprinciple stand-alone software is not explicitly covered by EU product safety \nlegislation, with the consequence that the risks generated by the probabilistic nature of \nAI are not yet clearly and specifically addressed by existing safety rules. Additionally, \nsuch legislation focuses on safety risks present at the time of placing the product on the \nmarket and presupposes “static” products, while AI systems can evolve. In addition to \ngenerating new safety risks for user and third parties, the lack of clear safety provisions \ntackling such risks may give rise to: \n  legal uncertainty for businesses that are marketing their products involving AI \nin the Union, as well as for those using such products in their own processes, and   \n  challenges for market surveillance and supervisory authorities which may find \nthemselves in a situation where they are uncertain whether they can intervene, because \nthey may not be empowered to act and/or may not have the appropriate tools and means \nto inspect AI-enabled systems. \nSpecific challenges on product safety are currently also being addressed by other \nongoing initiatives, such as the revisions of the Machinery Directive and of the General \nProduct Safety Directive. The Commission will ensure coherence and complementarity \nbetween those initiatives and this initiative. \n \n\uf06c  When considering the introduction of new regulations, full attention must be given \nto existing regulations and systems, and they must not be extended to AI \ntechnology embedded in hardware, including software and services. Changes in the \n1', 'targets of regulation will mean AI system developers being held accountable for \nissues they cannot be directly involved with, and this may undermine the \ndevelopment and utilization of AI systems. \n \n\uf06c  Since existing safety regulations have not envisioned application to AI technology, \na solution to the issue of lack of explicit rules to address new safety risks will take \ntime. Discussions on this issue need to take into account a balance between the \nbenefits for society as a whole—including developers, providers, deployers, and \nusers of AI systems—and the possible risks. \n \nPage 3 \nMore specifically, the aims are: \n(a) to ensure the effective enforcement of rules of existing EU law meant to protect \nsafety and fundamental rights and avoid illegal discrimination by ensuring the relevant \ndocumentation for the purposes of private and public enforcement of EU rules;   \n(b) to provide legal certainty for businesses that are marketing their AI-enabled products \nor using such solutions in the EU as regards the rules applicable to such products and \nservices;   \n(c) to prevent where possible or to minimise significant risks for fundamental rights and \nsafety; \n(d) to create a harmonised framework in order to reduce burdensome compliance costs \nderived from legal fragmentation, which could jeopardise the functioning of the Single \nMarket; \n \n\uf06c  For business operators, it is desirable that the legal position of their businesses is \nclear, that the credibility of AI-related products and services is enhanced by legal \nunderpinning, and that compliance cost is reduced through the creation of a \nharmonized framework. We, therefore, welcome the discussion in the inception \nimpact assessment. \n \n\uf06c  A discussion on accountability, which is inevitable in the process of introducing \nregulations, should give consideration to the areas of AI utilization, social impact, \nand other background factors. Imposing uniform accountability rules for businesses \noperating in diverse areas may hinder AI utilization in multiple areas and is, \ntherefore, undesirable. \n2', '\uf06c  There are cases where it may not be possible to sort out all AI-related rights and \nobligations with legal provisions and cases where comprehensive categorization \nwill be difficult. Even in such cases, the legal stability of businesses and the safety \nof users can be ensured through the clarification of rights and obligations by the \nparties in a contract. It is desirable to hold discussions on new rules while paying \ndue attention to security and safety guaranteed by voluntary and flexible free \ncontracts by the parties involved. \n \nPage 4 \nAlternative options to the baseline scenario   \n \n\uf06c  Striking a balance between innovation and regulation is important for the \nrealization of the interests of an advanced and highly credible data-driven society. \nLikewise, it is desirable to ensure the harmony of international rules to prevent \nunnecessary fractionalization. Lack of clarity on what is allowed and not allowed \nunder the regulations is one obstacle to the development and deployment of AI \nsolutions in Europe. Concerns about proof of full compliance with regulations have \nundermined many business negotiations with both government and the private \nsector. \n \n\uf06c  Basically, it is desirable not to complicate the legal systems developers must \ncomply with, such as by regulating areas that can be regulated by existing rules and \nsystems under these frameworks (e.g. regulating AI used in medical equipment \nthrough regulations on medical equipment). New regulations should be limited to \nthe minimum required, and when considering the introduction of such regulations, \ncompatibility with existing rules and systems should be ensured, and particular \nattention needs to be paid to preventing overlapping administration. \n \nPage 4 \n(1) Option 1: EU “soft law” (non-legislative) approach to facilitate and spur industry-\nled intervention (no EU legislative instrument) \n \n\uf06c  With regard to areas that existing rules and systems are unable to regulate, \n3', 'considering policies with the “soft law” approach is effective. Under the current \nsituation where it is still premature to define and regulate high-risk AI, imposing \nprior regulations on AI-enabled products and services without any explicit basis \nmay hinder innovation that will contribute to industrial development and help \nresolve social issues in Europe. \n \n\uf06c  Therefore, it is better to promote industry-led measures and enhance the credibility \nof AI in the market by developing a joint government-private sector scheme for the \nappropriate evaluation of the voluntary steps taken by businesses. \n \n\uf06c  Compatibility with international standards is necessary for the voluntary steps \ninitiated by the industrial sector to be widely recognized and adopted. Continuous \ninvestigation, discussion, and improvement for all stakeholders are necessary in the \nprocess of determining policies to address the risks relating to the development and \nutilization of AI applications. Corporate self-governance is particularly important \nfor the protection of fundamental rights. \n \nPage 5 \n(3) Option 3: EU legislative instrument establishing mandatory requirements for all or \ncertain types of AI applications (see sub-options below). \n \na. As a first sub-option, the EU legislative instrument could be limited to a specific \ncategory of AI applications only, notably remote biometric identification systems (e.g. \nfacial recognition). Without prejudice to applicable EU data protection law, the \nrequirements above could be combined with provisions on the specific circumstances \nand common safeguards around remote biometric identification only. \n \n\uf06c  A consensus has yet to be reached on the definition and correct understanding of \nbiometric data, such as face recognition and dactyloscopy (fingerprint) data, which \nhas been identified as “high-risk AI application,” and legal systems pertaining to its \nutilization have not been established. It is necessary to clarify the definition of the \nscope and uses of systems such as remote biometric identification and biometric \nauthentication, as well as the difference between the two, ensuring that this issue is \ndiscussed carefully, in order not to restrict utilization by the private sector \n4', 'unnecessarily. \n \n\uf06c  The creation of practical guidelines is necessary to clarify the conditions and \noperational requirements for using remote biometric identification systems. The \nguidelines must also define remote biometric identification systems and classify \ntheir utilization methods. \n \nPage 5 \nb. As a second sub-option, the EU legislative instrument could be limited to “high-risk” \nAI applications, which in turn could be identified on the basis of two criteria as set out \nin the White Paper (sector and specific use/impact on rights or safety) or could be \notherwise defined. \n \n\uf06c  Evaluation as “high-risk” must be based on the current discourse and definition of \n“risk” at institutions deliberating international standards. Uniform regulations must \nnot be imposed on diverse sector-specific definitions and methods for risk \nassessment and management, and measures based on existing regulations in each \nsector should be considered. \n \n\uf06c  While the criteria for defining high-risk AI set out in the European Commission’s \nwhite paper are indispensable for the realization of credible AI as an issue for the \nfuture, technical validation of these criteria is still very difficult at present. It must \nalso be noted that in cases where the risks of systems using high-risk AI can be \nfully eliminated or mitigated through physical safety measures and operations, the \ncriteria defining high-risk AI must not be applied uniformly. Therefore, it is \ndesirable to adopt a step-by-step approach to arrive at a realistic timetable and \ncriteria based on a road map to be drawn up after consultations with experts. The \nsubstance of the road map and the standards must be consistent with and conform to \nEuropean as well as world standards. \n \nPage 5 \nc. In a third sub-option, the EU legislative act could cover all AI applications. \n \n\uf06c  There are many AI applications, such as those relating to the optimization of \n5', 'production processes and energy use, that do not pose any risks to the fundamental \nrights and safety being considered in this assessment. The imposition of uniform \ncriteria on all applications will result only in demerits, such as increased cost, for \nsuch applications. Therefore, Sub-option 3 under Option 3 is inappropriate. \n \nPage 5 \n(4) Option 4: combination of any of the options above taking into account the different \nlevels of risk that could be generated by a particular AI application.   \n \n\uf06c  The standards for fairness, safety, and quality differ for each country, culture, \nsector, user, and so forth. Therefore, it is necessary to make appropriate choices of \nmeasures to be adopted for each AI technology and sector, such as the “soft law” \napproach and measures based on existing regulations, in order to be able to adapt \nflexibly to AI and other rapidly evolving new technologies. \n \n\uf06c  In the introduction of regulations, consistency and conformity with laws recognized \nas a vital framework for the privacy and security of European citizens’ personal \ndata, which are also regarded as models by many countries, particularly the General \nData Protection Regulation (GDPR), is very important. In order to promote \nconsistency and conformity, cooperation with the European Data Protection Board \n(EDPB) and national and international data protection bodies in the implementation \nof such regulations is advisable. \n \nPage 5 \nThe public intervention may however impose additional compliance costs, in so far as \nthe development of some AI systems may have to account for new requirements and \nprocesses. If compliance costs outweigh the benefits, it may even be the case that some \ndesirable AI systems may not be developed at all. \n \n\uf06c  The imposition of conditions on companies developing AI applications that \nconstitute a burden on them may have economic consequences, including weakened \ninternational competitiveness of EU companies due to the delay in the launch of \nproducts and services in the EU market compared to other economic zones, or even \nfailure to launch them. It must also be noted that the result may impede innovation. \n6', '\uf06c  It must be noted that even with uniform rules for EU members, lack of conformity \nwith international regulations, which are important for businesses engaged in global \noperations, will increase compliance cost. \n \nPage 6 \nThe assessment will also have to consider which measures a responsible economic \noperator would take even without explicit public intervention.   \nThe extent of the economic benefits depends, all other things being equal, on the \nincrease in trust. Other things being equal, users will have more trust when they can rely \non legal requirements, which they can enforce in courts if need be, than if they have to \nrely on voluntary commitments   \n \n\uf06c  Regardless of whether there is explicit public intervention, since AI undergoes \nmodel changes through learning after the start of service, even if measures that need \nto be taken by the responsible operators at the start of service are anticipated, these \noperators may still face unforeseen litigation risks as a result of subsequent model \nchanges, thus compromising the social benefits AI would have generated. For this \nreason, the terms that suppliers of products and services need to abide by should be \nclarified to ensure legal certainty. \n \nPage 6 \nDue to the high scalability of digital technologies, small and medium enterprises can \nhave an enormous reach, potentially impacting millions of citizens despite their small \nsize.  \n \n\uf06c  As stated elsewhere in this document, the extent of the impact of AI on society, the \nenvironment, basic human rights, and so forth cannot be measured by the capital or \nnumber of employees of the corporate developers but is largely determined by the \nnumber of users and the utilization methods of the products and services. Therefore, \nit is not necessarily appropriate to use the size of the corporate developer or \nwhether it is a small or medium-sized enterprise as a criterion for determining \nwhether it is subject to regulation. \n \nPage 6 \nLikely social impacts \n7', '\uf06c  We hope for greater credibility of AI applications in society and greater social \nacceptance of these applications. \n \n\uf06c  It must be noted that the imposition of conditions on companies developing AI \napplications that constitute a burden on them may prevent users in EU nations from \nenjoying the benefits of cutting-edge technology due to delay in the launch of \nproducts and services in the EU market compared to other economic zones, or even \nfailure to launch them, with the result of jeopardizing innovation. \n \nPage 7 \nImpact assessment \nThe completion of the impact assessment is scheduled for December 2020.   \n \n\uf06c  The start time for the impact assessment of this initiative should be specified. \nFurthermore, this assessment should be implemented with ample lead time, taking \ninto account the effect of the COVID-19 pandemic, and with sufficient \nopportunities to engage in dialogue with the industrial sector. \n8']"
F550805,09 September 2020,-,-,-,-,-,-,Künstliche Intelligenz – ethische und rechtliche Anforderungen,Se attachment,"[""Submission to the European AI IIA public consultation September 2020 \n \nReinvent Europe as a startup continent  \nWe also see great value in finding the right balance between ethical use of AI technology and \nthe opportunities it provides for Europe. But most of all, we believe in Europe’s startups and \nwant to see them thrive, scale and help Europe recover post Covid-19.  \n \nMany people believe in EU regulations as the key to putting Europe and European values  first. \n\u200b\nIn this light, many new regulations were adopted over the years; rules on copyright, e-privacy, \nP2B, GDPR - just to name a few. Rules aimed at the biggest players, but which ultimately hurt \nthe smallest, who don’t have the capital and skills to administer new processes, approvals, \nregistrations, and other - sometimes - burdensome bureaucracy.  \n \nTime to market is absolutely crucial for a startup. If it takes too long to get your product, service \nor solution to market, there is a risk that your investors will no longer back you up or a \ncompetitor comes first. Entrepreneurship is not built to wait, while larger companies can more \neasily manage to wait half a year for an approval or process to be set up before launching. \n \nAlready, we have lost almost half of our unicorns to Brexit. In many ways, AI has the potential to \nreinvent Europe as a startup continent, and with AI solutions, we can actually solve some of our \nsociety's greatest challenges. Our member and Danish startup success, Corti, is a brilliant \nexample of that; using AI technology to analyze millions of conversations with patients, \nidentifying patterns and helping healthcare professionals to more efficiently predict and detect \nlife-threatening situations. But in order for companies like Corti to thrive in Europe, we have to \nchallenge the logic that more EU regulations will strengthen competition. At least we can see \nthat previous regulations have made it harder and more difficult to scale as startups in Europe. \n \nComments to the European Commission’s proposed policy options \nPolicy option 0 (baseline). We do believe there is merit in ensuring that the existing EU \n\u200b\nregulation is properly implemented with regards to AI before putting in place any new \nprescriptive AI-specific rules. Currently AI does not operate in a vacuum and is subject to a \nnumber of existing rules, including GDPR and medical devices regulation. \n \nPolicy option 1 (industry-led intervention). No matter what policy options are pursued, \n\u200b\nlending support to the industry in establishing and implementing norms of responsible practices \nand sharing best practices is worthwhile.  \n \nPolicy option 2 (legislation on voluntary labelling). We are sceptical of the impact of a \n\u200b\nlabelling scheme on the uptake of trustworthy AI in Europe. An administrative burden on \nstartups and SMEs to comply with the onerous labelling obligations - if drafted on the basis of \nthe EU High-Level Expert Group on AI - could significantly outweigh the benefits of such a \nscheme."", 'Policy option 3 (legislation with mandatory requirements). It’s important to take a \n\u200b\nproportionate, risk-based approach balancing potential harms with the social/economic benefits. \nAny regulatory framework should be flexible to evolve with this dynamic technology and without \nunduly hindering AI-driven innovation. \n● We support a well-defined risk-based approach to the AI regulation that takes into \n\u200b\naccount both the severity and likelihood of harm. A sector and use base criteria at large \nseem to be a good starting point. Attempting to regulate things that may happen in the \nfuture holds back creativity. COVID-19 has shown it’s vital to have fast and effective \nsolutions. We must ensure that any future AI regulations don’t compromise AI’s ability to \nprovide the right responses at the right time in such a crisis. \n● We strongly caution against an EU legislative act for all AI applications that would \n\u200b\nmake no distinction between the AI applications that can pose significant risk/harm and \nthose with no or lower risk profile. Such a legislative instrument would be significantly \nunproportional to the challenges identified by the Commission, and create significant \nbarriers for AI adoption, additional costs, delay and administrative burdens. \n \nWe support the proposed ex-post enforcement model for when problems arise. \nRegulation should look at causes, effects and intended use. Ex-ante conformity assessments \n\u200b \u200b\nresult in longer time-to-market, which hamper startups’ potential success and result in higher \ncosts. Instead of nourishing innovation, an ex ante model will effectively end up ring fencing \nestablished players. And leading AI-startups will need to relocate to the US or UK if they cannot \nfocus money and resources on experimenting and developing their technologies. \n \nIf the Commission insists on an ex-ante enforcement model, we strongly caution against third \nparty ex-ante assessments and recommend instead self-assessment procedures based on \nclear due diligence guidance. A practical approach would be for regulators to provide detailed \ntemplates and guidance on how to carry out and document the risk assessment, but delegate \nresponsibility to those using and most familiar with the AI system.  \n \nAvoid legal uncertainty - apply clear definitions for AI regulation. We caution the European \n\u200b\nCommission to significantly expand the scope of future AI regulation to “automated decision \n\u200b\nmaking”. Such a broad scope would create unproportional, unjustified regulatory obligations and \n\u200b\nlikely lead to over-regulation by including all automated systems, even those that do not even \npose any risk nor harms. It would also go against the initial direction to focus on a risk-based, \ndouble-criterion model for sectorial and use-based AI technologies.  Further, we are concerned \nwith introducing an open-ended concept of “immaterial harm”. We propose as an alternative to \n\u200b \u200b\nrefer to “significantly restricting the exercise of fundamental rights” which would also align with \n\u200b \u200b\nexisting legislative frameworks.']"
F550804,09 September 2020,Marta Marazzi,Wirtschaftsverband,ACEA (European Automobile Manufacturers Associations),klein (10 bis 49 Beschäftigte),-,Belgien,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"ACEA  welcomes the opportunity to comment on this Inception Impact Assessment on a potential initiative on AI.

AI represents a key technological area for the entire automotive value-chain and EU industry at large. We therefore endorse Commission’s plans to mobilise EU resources and R&D efforts, as no Member State alone could support our global competitive standing in this field. However, for the EU to remain competitive against global players and preserve its innovation power, a light-touch and sectoral approach to regulation is advisable. A horizontal AI initiative would not appropriately address the automotive sector’s specificities: hence, ACEA recommends policymakers to integrate new potential AI requirements in the sectoral legislation.

The automotive sector is already subject to strict ex-ante conformity controls (Type-Approval) and ex-post (CoP, Market surveillance, in-service compliance). Before adopting any proposed option, Commission is encouraged to consider where existing vertical certification requirements and regulatory frameworks can be used in a way to promote technological innovation and completed where necessary with practical guidelines. Only afterwards should gaps be determined. If legal gaps exist based on demonstrable evidence, new potential AI requirements should be included in the existing sectoral frameworks. This is essential to prevent duplication or invalidation of certification testing and market surveillance for the AI Regulatory Framework and European Union Whole Vehicle Type Approval (potential AI requirements put forward in the AI White Paper are well-established in the processes governing the automotive sector and tackled by applicable legislation).

In the specific case of automated driving, regulation should take into account the level of automation and vehicle behavior (functional requirements and methods for validating the function) rather than the technology used for implementing transport automation – something which would allow for a technology neutral approach to regulation. The work carried out at UN level on automated driving, where DG GROW is actively involved, goes in this direction: the recently adopted UN regulation on Automated Lane Keeping Systems is an example of how safety relevant aspects can be covered; also the concept of a future Automated Driving Systems will incorporate all safety aspects including those related to the specificities of using AI algorithms for driving functions. Coordination and alignment with UNECE WP.29 and DG GROW are crucial.

It is in the interest of vehicle manufacturers to put on the market/on road vehicles that comply with the maximum safety and ethical standards. Safety validation is ensured by provisions dictated in the Vehicle Type Approval/General Safety Regulation. In parallel, in order to accelerate the adoption of practical tools nurturing the objectives of the HLEG Ethical principles for AI, vehicle manufacturers support, as suggested in this Inception Impact Assessment, the further development of industry-led initiatives (e.g. standards/codes of conduct) for implementing road transport automation and promoting the deployment of AI in a way that secures greater trust in and public acceptance of new technologies. 

ACEA calls for a risk-based approach for the assessment/classification of AI applications, carried out on a case-by-case basis and by applying a set of criteria. A narrow definition of high-risk is required, so to avoid moderate risk/narrow AI use cases being classified as high-risk and being overregulated. Criteria for the risk assessment should be spelled out and consist of factors, such as: safety-relevancy, autonomy of learning, autonomy of decision-making, type of algorithmic model, potential impact of AI systems on a larger group of people. It is finally recommended that the remits of existing type-approval authorities are extended to cover in-vehicles AI applications, so that the sectoral expertise is further enhanced."
F550748,09 September 2020,Josua Semme,Unternehmen/Unternehmensverband,EnBW Energie Baden - Württemberg AG,groß (250 oder mehr Beschäftigte),13324391892-74,Deutschland,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"As EnBW Energie Baden-Württemberg AG we welcome the approach of European Commission to conduct an Impact Assessment concerning ethical and legal requirements for AI. For several years already we have developed AI solutions in the energy sector. As operator and service provider in the field of critical infrastructures, we at EnBW operate multiple self-developed AI services already productively. These include AI-assisted overhead power line inspection by using drones, predictive maintenance of offshore wind turbines, securing public places by means of AI-assisted barrier systems, automatic detection of road damage and detection of damage to gas and water pipes. Lately we have been awarded “AI-Champion of Baden-Württemberg” - as EnBW we would like to share our expertise in an ecosystem of excellence in the EU.

Our view on the outlined alternative options to the baseline scenario:

Option 3c would most probably be an overall inhibitor as from the start.

3b puts up heavy hurdles for some selected sectors. As outlined in the white book on AI, the energy sector would suffer under this form of general suspicion of being a “high-risk” sector (white paper on AI, page 17). The first criterion already gives rise to fears of bureaucracy, while being too broad at the same time. In the coming years we need room for innovation instead of discrimination of individual sectors. For a risk-assessment of particular applications, a clear definition of “high-risk” would be very much needed from an industrial perspective. Rather soft measures (e.g. guidelines or other approaches which leave room for innovation) should be preferred before introducing bureaucratic certification processes.

Concerning the outlined alternative options to the baseline scenario, we would advocate option 2. A voluntary labelling scheme would complement the existing EU legislation, which already sets high standards for example concerning liability (e.g. the product safety directive for products) or personal data (GDPR). A good example to operationalise AI ethics using labels can be found under: https://www.ai-ethics-impact.org/en

Similarly, page 19 of the German report on the data ethics commission lines this out. (https://www.bmjv.de/SharedDocs/Downloads/DE/Themen/Fokusthemen/Gutachten_DEK_Kurzfassung.html;jsessionid=6202CA13F73BDB281E78F2F374F783E8.1_cid334?nn=11678504)

Because of the diversity of products, services, customers and processes, it would be best to have individual labels and industry-inspired processes to qualify. Common guidelines might be helpful. For customers and employees it is important to understand the path of getting to a label – transparency is important, explainability creates trust. 
"
F550741,09 September 2020,Ioana Smarandache,Wirtschaftsverband,EGMF,sehr klein (1 bis 9 Beschäftigte),82669082072-33,Belgien,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"EGMF, the European Garden Machinery Federation, would like to provide its feedback in the attachment, on the Commission inception impact assessment regarding a proposal for a legal act on AI requirements.","['EGMF comments on the EC roadmap on proposal for a legal act on AI \nrequirements \n \nBrussels, 9 September 2020 \n \nEGMF represents European manufacturers of garden, landscaping, forestry and turf maintenance \nequipment.  \nEGMF is pleased to provide its comments on the new Roadmap on proposal for a legal act on AI \nrequirements. Firstly, we greatly appreciate the initiative of the Commission to address specific ethical \nand legal issues raised by AI with the overall objective of stimulating the uptake of trustworthy and \nlawful AI across the Single Market. EGMF fully supports the focus on trust-based and human-centric \nAI.  \nEGMF’s view is that AI should not be addressed as a threat, but rather as an opportunity to enhance \nproduct performance, innovation and social welfare. However, when drawing up possible regulatory \nmeasures, it is important to ensure that these measures do not overregulate, are consistent with other \nregulations (e.g. Privacy, Cybersecurity, IoT, etc.) and leave enough room for innovation.  \nWe acknowledge the need to verify whether the existing legislation sufficiently covers risks resulting \nfrom AI, yet we call for a sound impact assessment in order to prevent over-regulation and/or legal \nuncertainty. For example, EGMF is convinced that current EU product safety regulation is largely fit \nfor purpose and sufficiently covers AI as a risk cause.  \nIt should also be kept in mind that AI may not only increase risks but can offer different technological \noptions to maintain or potentially reduce risks. For further political development of the issue, EGMF \nproposes to have a discussion amongst the relevant stakeholders on the definition of risk and also \nabout the risk levels.  \n \nFurthermore, the sectorial approach of the EU when defining high-risk seems to lead to sectorial \nlegislation in the future. EGMF calls for a horizontal approach as we fear that a vertical approach would \nlead to significant legal gaps and uncertainty. It would be very difficult to revise all relevant vertical \nlegislation in a coherent manner. As a result, it would be worth the effort to aim for a European \nhorizontal AI law which would allow the introduction of basic principles such as prohibition of \ndiscrimination, ethic rules, permissible purpose etc. \nAmong the policy options proposed, EGMF is in favour of either option 0 or option 1. Current \nlegislation with Risk Assessment requirements (e.g. MD) already cover AI indirectly, so such legislation \ndoes not need changing. Option 1 would offer the opportunity of soft legislation which can focus on \nindustry-led actions, such as the development of standards. This would allow manufacturers to \ncontinue referring to the Machinery Directive to provide machinery compliance and safety, while \ncommitting to soft legislation on AI via specific industry standards/codes of conduct in relation to \ntrustworthy uses of data \nWith regard to option 2, a voluntary labelling scheme could be set up for those AI applications that do \nnot qualify as ‘high-risk’. If complied with certain requirements on AI, companies could label their \nproducts and services as trustworthy, and participate in the scheme on voluntary basis. However, once \na company has decided to use the label, the requirements would be binding. EGMF believes that,', 'before any type of voluntary labelling scheme is introduced, clear transparent rules and metrics based \non international standards have to be agreed on, and national schemes should be avoided. \nIn general, whatever option is considered, the main demand of the garden machinery industry is to \nhave rapid development and citation of relevant harmonised standards within the New Legislative \nFramework.  \n \nConclusion \nAI-related issues covering more than one industry/product category, should be tackled via horizontal \nEU legislation, while keeping flexibility in specific, vertical EU legislation based on its application. For \nthis reason, options 0 and 1 are most preferred.  \n \n \n \n \n \n \n \n \n \n \n \n \n \nFor further information, please contact: EGMF Secretariat, secretariat@egmf.org  \n \nThe European Garden Machinery Industry Federation – EGMF – has been the voice of the \nentire garden machinery industry in Europe since 1977. With 30 European corporate \nmembers  and  7  National  Associations  representing  manufacturers  of  garden, \nlandscaping, forestry and turf maintenance equipment, we are the most powerful network \nin this sector in Europe.  \nwww.egmf.org']"
F550740,09 September 2020,Almudena Díaz,Unternehmen/Unternehmensverband,Enel SpA,groß (250 oder mehr Beschäftigte),6256831207-27,Italien,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"Dear Members of DG CNECT .A.2,
Enel SpA, a multinational company in the energy sector, highly appreciates the EC proposal for a legal act laying down requirements for Artificial Intelligence. 

At Enel, we use Artificial Intelligence and technology to make the energy and power systems more efficient, more predictable and more sustainable, making easier for our customers to interact with us and to play a more active role in the energy system, mainly in the liberalised energy market.

The European digital strategy released on February this year, stresses that citizens need to be able to trust the technology itself as well as the way in which it is used. The European Commission White Paper on AI, maps out various policy options and reveals that clear and mandatory requirements would “in principle” apply only to AI systems or applications which are considered ‘high-risk’, i.e. employed in sectors where significant risks can be expected to occur, or used in such a manner that significant risks are likely to arise.

In line with the non-binding Ethics Guidelines by the AI HLEG, the White Paper on AI suggested that the mandatory requirements for high-risk systems could cover the following aspects:
• Training data;
• Data and record-keeping;
• Information to be provided;
• Robustness and accuracy, with an ex-ante consideration of the potential risks;
• Human oversight;
• Specific requirements for certain AI applications, such as remote biometric identification.

Considering this and welcoming a risk-based approach, Enel advices the European Commission that the risks are in the infringement, not in the technology or in the sector and, recommends to assess and establish the characteristics of different types of risks and threats with a ‘sector-by-sector’ approach, requiring inter alia a ‘sectoral’ data protection assessment. 
Within this view, the different classes of use-cases must be integrated. It is worth noting that in the energy sector, which is considered “high-risk” according to section C of the White Paper, all its AI based applications cannot systematically be categorised such as “high-risk”. 

For instance, an application utilised to interact online with clients about customer care activities will normally not pose risks of such significance to justify, at this time, legislative intervention. Possible mandatory legal requirements to be imposed on high-risk applications should be carefully evaluated before establishing them as (ex-ante) obligations for businesses.

Enel believes also necessary to better define ‘high-risk’ and the methodology to assess it. Poor categories and definitions might deter private investments and become a competitive disadvantage to European companies. 

Given the actual economic crisis together with the financial risk nature of research and innovation activities, Enel urges the Commission to build a financial structure to back organisations involved in the mentioned ‘high-risk’ sectors. If the EU system is not supportive towards innovation, the opportunity for innovations to come from Europe will be reduced.

Therefore, the Enel Group promotes the development of long-term European action plans on digital policy, which should be evidence-based, promote security by design, guarantee high data quality standards, promote ethical evaluation for different use cases and enable new technologies and business models, without imposing unnecessary burdens and costs nor obstacles to innovation. 
"
F550729,09 September 2020,Alexandru Circiumaru,NRO (Nichtregierungsorganisation),The Good Lobby,sehr klein (1 bis 9 Beschäftigte),-,Belgien,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"The Good Lobby welcomes the opportunity of providing feedback on this legislative initiative. Without a doubt, Artificial Intelligence (AI) is already changing the world, its potential impacts having long been discussed and analysed. Other than the many benefits of this technology, presented by the Commission both in its White Paper and in its Communication on AI, it also poses numerous risks, some of which have been highlighted in the same documents and in the Ethics Guidelines prepared by the High-Level Expert Group on AI. Therefore, we believe it is encouraging and commendable that the European Commission pays close attention to the way in which AI can be regulated and that this is the right time for it to do so.

Looking at the options provided, The Good Lobby strongly supports a combination of options 2, 3a, and 3b. The risks that AI poses, which are very much present, with plenty of concrete examples of biased algorithms from the past 12 months alone, are too great to be addressed through a soft-law approach. We believe that in order to address these risks appropriately there is a need for a decisive and comprehensive legislative intervention. 

While a voluntary labeling scheme - option 2 - is not in and of itself sufficient, we recognise that there is some value to this idea. As suggested in the White Paper, such an approach could be appropriate for low-risk applications of AI. Nevertheless, we are not persuaded that it would properly address the challenges posed by high-risk AI.

While option 3c would have the advantage of minimising the risk of fragmentation and uncertainty, it would also greatly stifle innovation and have negative consequences over the development of AI in Europe, discouraging organisations, and in particular SMEs, from engaging with this technology.

We do not support regulating only certain categories of AI. Such an approach has the risk of being too narrow, too broad, or both too narrow and too broad depending on the circumstances. Should that happen, there is a danger that certain applications of AI that pose considerable risks would not be considered because they do not fall within a certain more loosely or narrowly defined category or that low-risk applications from one such category are overly regulated. Nevertheless, we see value in option 3a and that is why we are recommending it is pursued, together with options 3b and 2 as discussed above. The value we see is related to regulating certain particular uses of AI, for example, remote biometric identification systems. In such a case the risks are so clear and blatant that comprehensive regulation is, without a doubt, necessary.

From the options put forward, the most appropriate, in our view, is option 3b despite the difficulties with defining different categories of risks accurately and efficiently, which have been repeatedly emphasised and the risks of fragmentation and uncertainty already discussed. While the White Paper does provide two criteria on which risks could be assessed, this is just the beginning of a difficult process of setting out categories of risks which will have to be kept under constant review and flexible enough to allow for modifications where necessary.

From the two criteria provided, we believe that risks should be calculated taking into account the impact on rights and safety rather than the sector and specific use. As we talk about a human-centric approach, it seems only fitting that this should be the case.

On enforcement, The Good Lobby believes that for it to be effective, it needs to combine an ex-ante mechanism, which to allow for the scrutiny and questioning of the design of an AI system with an ex-post approach, to review how the system is actually working and the concrete decisions it produces to allow for correction of legal errors and the intervention of human oversight - or equity and mercy, as described by Lord Sales.","['The Good Lobby welcomes the opportunity of providing feedback on this legislative \ninitiative. Without a doubt, Artificial Intelligence (AI) is already changing the world, its \npotential impacts having long been discussed and analysed. Other than the many \nbenefits of this technology, presented by the Commission both in its White Paper \nand in its Communication on AI, it also poses numerous risks, some of which have \nbeen highlighted in the same documents and in the Ethics Guidelines prepared by \nthe High Level Expert Group on AI. Therefore, we believe it is encouraging and \ncommendable that the European Commission pays close attention to the way in \nwhich AI can be regulated and that this is the right time for it to do so. \n \nLooking at the options provided, The Good Lobby strongly supports a \ncombination of options 2, 3a, and 3b. The risks that AI poses, which are very \nmuch present, with plenty of concrete examples of biased algorithms from the past \n12 months alone, are too great to be addressed through a soft-law approach. We \nbelieve that in order to address these risks appropriately there is a need for a \ndecisive and comprehensive legislative intervention.  \n \nWhile a voluntary labeling scheme - option 2 - is not in and of itself sufficient, we \nrecognise that there is some value to this idea. As suggested in the White Paper, \nsuch an approach could be appropriate for low-risk applications of AI. Nevertheless, \nwe are not persuaded that it would properly address the challenges posed by high-\nrisk AI. \n \nIn expressing our strong support for a combination between options 3a and 3b, \nwe are taking into account the need to encourage innovation, the risk of \nfragmentation and uncertainty in the absence of a singular common set of rules, and \nthe difficulties inherent to defining different categories of risks. \n \nWhile option 3c would have the advantage of minimising the risk of fragmentation \nand uncertainty, it would also greatly stifle innovation and have negative \nconsequences over the development of AI in Europe, discouraging organisations, \nand in particular SMEs, from engaging with this technology. Should that happen, \nEurope would lag behind the rest of the world, missing out on the benefits that AI can \nbring. Without a strong position in terms of development, Europe is also likely to lose \ncredibility in terms of regulation. Furthermore, we are also aware that numerous \napplications of AI are innocuous such as GPS applications using AI to predict the \nquickest route to a destination. Given the differences between the nature of AI \napplications and their potential negative impacts, despite the existing risk of \nfragmentation and uncertainty which we hope the Commission will have in mind, \nwe believe different approaches for different applications are necessary.  \n \nWe do not support regulating only certain categories of AI. Such an approach has \nthe risk of being too narrow, too broad, or both too narrow and too broad depending \n1', 'on the circumstances. Should that happen, there is a danger that certain applications \nof AI that pose considerable risks would not be considered because they do not fall \nwithin a certain more loosely or narrowly defined category or that low-risk \napplications from one such category are overly regulated. Nevertheless, we see \nvalue in option 3a and that is why we are recommending it is pursued, together with \noptions 3b and 2 as discussed above. The value we see is related to regulating \ncertain particular uses of AI, for example, remote biometric identification systems.  \nIn such a case the risks are so clear and blatant that comprehensive regulation is, \nwithout a doubt, necessary. We believe a serious debate is necessary on the use of \nfacial recognition, together with very strong regulation, and we regret that the initial \nplan for a complete EU-ban on this technology seems to have been dropped.  \n \nFrom the options put forward, the most appropriate, in our view, is option 3b despite \nthe difficulties with defining different categories of risks accurately and efficiently, \nwhich have been repeatedly emphasised1 and the risks of fragmentation and \nuncertainty already discussed. While the White Paper does provide two criteria on \nwhich risks could be assessed, this is just the beginning of a difficult process of \nsetting out categories of risks which will have to be kept under constant review and \nflexible enough to allow for modifications where necessary.  \n \nFrom the two criteria provided, we believe that risks should be calculated taking into \naccount the impact on rights and safety rather than the sector and specific use. As \nwe talk about a human centric approach, it seems only fitting that this should be the \ncase.  \n \nOn enforcement, The Good Lobby believes that for it to be effective, it needs to \ncombine an ex-ante mechanism, which to allow for the scrutiny and questioning of \nthe design of an AI system with an ex post approach, to review how the system is \nactually working and the concrete decisions it produces to allow for correction of \nlegal errors and the intervention of human oversight - or equity and mercy, as \ndescribed by Lord Sales.2 \n \nFinally, as an overarching point, The Good Lobby wants to emphasise the need to \nensure that existing EU legislation can adequately protect individuals for the risks \nposed by AI. While it is true that the EU has a strong non-discrimination framework in \nplace, the Commission must ensure, through this legislative proposal or others, that \nsuch legislation is fit for purpose and fit for the current times.  \n \n                                                \n1 See J. Khan, “The Problem with the EU’s AI strategy”, Fortune, February 2020, available here \nhttps://fortune.com/2020/02/25/eu-a-i-whitepaper-eye-on-a-i/ \n2 Lord Sales, “Algorithms, Artificial Intelligence and the Law”, Judicial Review, 2020, Vol. 25, No.1, 46-\n66, pag. 53. \n \n2', 'The reality is that a technology such as AI which has so many applications and can \ngive rise to so many different risks needs to be approached with a certain degree of \nflexibility, particularly so as not to hamper innovation. We recognise that having \ndifferent rules could be difficult and confusing for business, civil society organisations \nand citizens alike, at least initially, but this, we believe, despite the shortcomings, is \nthe best way forward. At the same time, we hope that the Commission will take the \nnecessary steps to address any resulting uncertainty, by engaging in awareness-\nraising and by supporting other organisations who do so too.  \n \n3']"
F550724,09 September 2020,Martina Barbero,Sonstiges,Big Data Value Association,sehr klein (1 bis 9 Beschäftigte),042849916153-53,Belgien,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"BDVA welcomes the possibility to provide feedback on the Inception Impact Assessment concerning a Proposal for a legal act of the European Parliament and the Council laying down requirements for Artificial Intelligence. As already highlighted in BDVA response to the AI Whitepaper, BDVA strongly supports the development of a solid AI European approach based on European values. BDVA response to the AI Whitepaper already focused on one of the key themes of the present Inception Impact Assessment and notably the fact that PRCS (Policy, Regulation, Certification, and Standards) issues are pivotal for building an AI ecosystem based on trust and they are likely to become a primary area of activity for the new AI, Data and Robotics Partnership. Building on these considerations and on the response to the AI Whitepaper, BDVA wishes to underline a few important elements concerning both the challenges identified in the Inception Impact Assessment and the possible policy options.

Comments on the issues identified
•	European businesses see Industrial AI as more of an opportunity than a threat. However, the business, economic and societal context to which AI is applied needs to be considered as decisions are not made in a vacuum but within the socio-economic context of a society of humans which, in the European case, requires the AI application to be trustworthy. 
•	Businesses are aware that AI systems may be used in value chains, and see the possibility that liabilities emerge; for example, when an AI system bases its outputs on data that is created by another AI system from a value chain partner. 
•	Requirements on AI algorithms may have to be scoped carefully; usually, an algorithm is trained before it can be used operationally (it is called a model, then) and in such case, the training data is also part of the behaviour of the AI system. Thus, requirements for AI systems may have to be extended to training data as well. It may even be considered that the specific business process in which the AI system operates can be seen as part of the algorithm, or that the design criteria (including team composition and stated business goals) could be in scope. This will become complex, so careful scoping is needed.
Comments on the policy options
•	Many stakeholders see certification in relation to AI systems as a critical trust-building mechanism for adoption of AI solutions. A methodological approach to certification could include best practice from other sectors being mapped to AI in tandem with the Standardization Landscape approach. Standards provide the foundational documentation for certification, regulation, legislation, compliance and ultimately enforcement.
•	Awareness of potential issues needs to be addressed. Voluntary certification and labelling schemes can have several benefits, both for purchasers of the certified AI system as well as for its producer. Such certification increases the confidence of users in AI systems as it indicates the producer’s commitment towards higher safety and quality standards. At the same time, however, voluntary certification should be carefully addressed as it can result in a meaningless label and evens increase non-compliant behaviour when there are no proper verification mechanisms. Voluntary labelling may make end-users more aware, just like Nutriscore intends to make consumers more aware of the features of the food that they are buying. When a voluntary labelling scheme is adopted, producers of AI will also become aware that end users may assess their products or services in a specific way; which will mean opportunities for producers who want to be transparent about their products and services.
•	It is already acknowledged in the AI Whitepaper that regulatory intervention should be targeted and proportionate. Such an approach will reduce the risk of overregulation and hence slow down technological innovation. In the Whitepaper the European Commission seemed not to want to regulate all AI systems but ","['BDVA response to the public consultation on ethical and legal \nrequirements for AI  \n \nBDVA welcomes the possibility to provide feedback on the Inception Impact Assessment concerning \na Proposal for a legal act of the European Parliament and the Council laying down requirements for \nArtificial Intelligence. As already highlighted in BDVA response to the AI Whitepaper, BDVA strongly \nsupports the development of a solid AI European approach based on European values.  \n \nBDVA response to the AI Whitepaper already focused on one of the key themes of the present \nInception Impact Assessment and notably the fact that PRCS (Policy, Regulation, Certification, and \nStandards) issues are pivotal for building an AI ecosystem based on trust and they are likely to \nbecome a primary area of activity for the new AI, Data and Robotics Partnership. Building on these \nconsiderations and on the response to the AI Whitepaper, BDVA wishes to underline a few important \nelements concerning both the challenges identified in the Inception Impact Assessment and the \npossible policy options. \n \nComments on the issues identified \n•  European businesses see Industrial AI as more of an opportunity than a threat. However, the \nbusiness, economic and societal context to which AI is applied needs to be considered as \ndecisions are not made in a vacuum but within the socio-economic context of a society of \nhumans which, in the European case, requires the AI application to be trustworthy.  \n•  Businesses are aware that AI systems may be used in value chains, and see the possibility that \nliabilities emerge; for example, when an AI system bases its outputs on data that is created by \nanother AI system from a value chain partner.  \n•  Requirements on AI algorithms may have to be scoped carefully; usually, an algorithm is trained \nbefore it can be used operationally (it is called a model, then) and in such case, the training data \nis also part of the behaviour of the AI system. Thus, requirements for AI systems may have to be \nextended to training data as well. It may even be considered that the specific business process \nin which the AI system operates can be seen as part of the algorithm, or that the design criteria \n(including team composition and stated business goals) could be in scope. This will become \ncomplex, so careful scoping is needed. \nComments on the policy options \n•  Many  stakeholders  see  certification  in  relation  to  AI  systems  as  a  critical  trust-building \nmechanism for adoption of AI solutions. A methodological approach to certification could include \nbest practice from other sectors being mapped to AI in tandem with the Standardization \nLandscape  approach.  Standards  provide  the  foundational  documentation  for  certification, \nregulation, legislation, compliance and ultimately enforcement. \n•  Awareness of potential issues needs to be addressed. Voluntary certification and labelling \nschemes can have several benefits, both for purchasers of the certified AI system as well as for \nits producer. Such certification increases the confidence of users in AI systems as it indicates the \nproducer’s  commitment  towards  higher  safety  and  quality  standards.  At  the  same  time, \nhowever, voluntary certification should be carefully addressed as it can result in a meaningless \nlabel and evens increase non-compliant behaviour when there are no proper verification \nmechanisms. Voluntary labelling may make end-users more aware, just like Nutriscore intends \nto make consumers more aware of the features of the food that they are buying. When a \nvoluntary labelling scheme is adopted, producers of AI will also become aware that end users \n1', 'may assess their products or services in a specific way; which will mean opportunities for \nproducers who want to be transparent about their products and services. \n•  It is already acknowledged in the AI Whitepaper that regulatory intervention should be targeted \nand proportionate. Such an approach will reduce the risk of overregulation and hence slow down \ntechnological innovation. In the Whitepaper the European Commission seemed not to want to \nregulate all AI systems but only high-risk AI systems. Systems that are not considered high-risk \nshould  only  be  covered by  more  general legislation  mentioned  above.  Such  a  risk-based \napproach should be maintained.  \n•  Regulatory sandboxes may provide an excellent way to enable exploratory research while still \nbeing able to effectively reduce potential risks when AI is ‘released in the wild’. \n \n \nAbout BDVA \nThe Big Data Value Association (BDVA) is an industry-driven international not–for-profit organisation \nwith over 200 members all over Europe and a well-balanced composition of large, small, and medium-\nsized industries as well as research and user organisations.  \nBDVA is the private counterpart to the European Commission to implement the Big Data Value PPP \nprogram. BDVA and the Big Data Value PPP pursue a common shared vision of positioning Europe as \nthe world leader in the creation of Big Data Value. BDVA is also a private member of the EuroHPC JU \nand one of the main promoters and driving forces of the AI, Data and Robotics Partnership planned \nfor the MFF 2021-27. \nThe mission of the BDVA is “to develop the Innovation Ecosystem that will enable the data-driven \ndigital transformation in Europe delivering maximum economic and societal benefit, and, achieving \nand sustaining Europe’s leadership on Big Data Value creation and Artificial Intelligence”.  BDVA \nenables existing regional multi-partner cooperation, to collaborate at European level through the \nprovision of tools and know-how to support the co-creation, development and experimentation of \npan-European data-driven applications and services, and know-how exchange.  \nBDVA maintains and fulfils a Strategic Research and Innovation Agenda (SRIA) for Big Data Value \ndomain, contributes to the Horizon 2020 work programmes and calls for proposals and it monitors \nthe progress of the BDV PPP (BDVA is in charge of producing the Monitoring Report of the whole \nprogramme). BDVA manages over 25 working groups organised in Task Forces and subgroups, and \ntackling all the technical and non-technical challenges of Big Data Value. BDVA has developed, \ntogether with euRobotics, the two consultation versions of the SRIDA (Strategic Research, Innovation \nand Deployment Agenda) for AI, Big Data and Robotics Partnership. \nContact for further information: info@core.bdva.eu \n2']"
F550710,09 September 2020,Marius Raul Bostan,NRO (Nichtregierungsorganisation),Romanian Business Leaders - Repatriot,groß (250 oder mehr Beschäftigte),-,Rumänien,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"O definiție clară și pe larg înțeleasă a IA este esențială pentru eficacitatea viitorului cadru de reglementare. La fel cum Cartea albă a Comisiei privind IA a descris principalele elemente ale IA ca date și algoritmi, la fel și această evaluare a impactului inițial încearcă să sugereze un domeniu de aplicare prea larg pentru reglementarea viitoare a IA. De exemplu, în cazul în care AI ar fi definit ca „luarea de decizii automatizată”, ar pierde focalizarea dorită bazată pe risc și ar include sisteme automate care nu prezintă niciun risc. Orice viitoare reglementări AI ar trebui să evite obligațiile de reglementare disproporționate și nejustificate, deoarece altfel ar avea efecte negative asupra dezvoltării și implementării aplicațiilor bazate pe IA în Europa. 

Noile norme prescriptive ar trebui luate în considerare numai în domeniile în care reglementarea existentă este în mod clar insuficientă. 
Industria europeană a IA ar beneficia mai mult dacă Comisia Europeană ar oferi principii directoare pentru auto-reglementare și cooperare  Întreprinderile europene trebuie să dezvolte tehnologii avansate în mod responsabil și noi bariere de reglementare ar frâna dezvoltarea.

Un sistem de etichetare voluntară ar putea crea o povară administrativă grea pentru inovatorii AI care sunt adesea IMM-uri cu resurse limitate. În consecință, costurile unui astfel de sistem ar putea depăși rapid beneficiile încurajării adoptării AI în întreaga Europă. Am dori, în special, să avertizăm că o schemă de etichetare se bazează pe lista de evaluare pentru AI de încredere din Grupul de experți la nivel înalt al UE în domeniul IA, deoarece natura sa limitează inerent variația între setări pentru diferite cazuri de aplicare. Comisia să colaboreze îndeaproape cu industria AI pentru a elabora un meniu de scheme de etichetare pentru diferite setări ale aplicațiilor AI. 

Costul de oportunitate al neutilizării IA va fi foarte mare dacă intervenție de reglementare în aplicațiile de IA nu sunt făcute ""inteligent"". În deliberarea posibilelor opțiuni, este vital să reflectăm nu numai prejudiciile potențiale, ci și oportunitățile sociale. Beneficiile AI vor depăși adesea riscurile, mai ales dacă riscurile pot fi atenuate într-un mod atent, cu garanții puternice. Regulamentul nu trebuie să descurajeze inovarea, dezvoltarea și nici să limiteze utilizarea AI. Proporționalitatea și concentrarea clară a oricărei reglementări vor contribui la asigurarea certitudinii juridice pentru inovatorii AI și vor spori încrederea în AI, fără a împiedica în mod nejustificat inovația bazată pe AI.

Viitoarea reglementare AI ar fi bine să se refere doar la aplicații AI cu „risc ridicat”, bine definite. Aș sublinia sublinia necesitatea proporționalității atunci când definim aplicațiile „cu risc ridicat” ale IA. Procedând astfel, este important să reflectăm la probabilitatea de a face rău și nu doar posibila gravitate a prejudiciului. De asemenea, ar trebui să țină cont de contextul operațional mai larg atunci când se evaluează riscul, deoarece aceeași aplicație de IA utilizată în același scop va prezenta riscuri diferite în funcție de modul în care este integrată în operațiunile comerciale (de exemplu, amploarea supravegherii umane , măsuri de protecție suplimentare , cum ar fi monitorizarea). Poate o combinație de sector și utilizare / aplicație ca criterii pentru a stabili abordarea bazată pe risc.    

Vătămarea imaterială nu este un concept legal cunoscut și ar putea însemna orice, de la pierderi economice la inhibarea încrederii, și ar putea duce la incertitudine juridică, descurajând investițiile și inovația. Pentru a aduce claritate practică inovatorilor AI ar fi cel mai bine să se ia în considerare un concept alternativ - „restricționarea semnificativă a exercitării drepturilor fundamentale”, care credem că ar fi mai ușor de interpretat și aliniat cu cadrul legislativ existent.  "
F550700,09 September 2020,Matt Allison,Unternehmen/Unternehmensverband,Vodafone Group,groß (250 oder mehr Beschäftigte),-,Vereinigtes Königreich,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"Vodafone welcome the opportunity to respond to the European Commission Roadmap Inception Impact Assessment (IIA) on the proposal for a legal act of the European Parliament and council laying down the requirements for AI to operate within the single market. These comments, alongside our full response to the AI white paper consultation falls within the wider plan we have established for supporting Europe through the Coronavirus crisis and helping to rebuild our economies and societies thereafter. In addition to our five point plan to respond to the immediate health crisis, Vodafone has developed a long term package of measures to assist with the recovery from the protracted economic crisis we are entering into. 
Adoption of AI and data driven tools is a vital part of this strategy and Vodafone wholeheartedly supports the objective of the European Commission to encourage the development of an ecosystem of excellence and an ecosystem of trust for AI development and adoption in Europe. We consider that increased use of AI and data driven technology is a necessary precondition for establishing a resilient European digital society.
On the key areas of regulation proposed by the Commission to establish a clear and predictable legal framework for AI operating within the single market, Vodafone has the following holds the following high level views: 
•	Regulation of high-risk AI: We broadly support the approach outlined by the Commission to avoid duplication of existing regulatory obligations, and ensure that any new requirements are proportionate and strictly targeted at AI applications that pose a high risk. 
•	Voluntary labeling: We raise particular concerns with the lack of focus given to incentives to develop Trustworthy AI (voluntary labeling, certification, industry codes) and ask for more clarity on how such a labeling scheme would be established and maintained.
•	Product Liability: we note that the existing EU horizontal and sector-specific legislative framework governing liability has proven to be robust and reliable. Any new requirements should be targeted at providers of high-risk AI applications & must not impose excessive high burdens on industry deploying AI that presents a low risk to EU citizens. 
•	Access to data: we underline the importance of availability of high quality training data for AI, to ensure that European businesses can continue to innovate and succeed in this field, and call for joined up policy making in view of the EU Data Strategy and legislative actions outlined therein. 
Our views on the specific policy options contained in the roadmap can be found in the attached PDF. ","['Vodafone Group Response  \nEuropean Commission Roadmap Inception Impact Assessment Proposal for a legal act of the European \nParliament and the Council laying down requirements for Artificial Intelligence  \n \nVodafone welcome the opportunity to respond to the European Commission Roadmap Inception \nImpact Assessment (IIA) on the proposal for a legal act of the European Parliament and council \nlaying down the requirements for AI to operate within the single market. These comments, \nalongside our full response to the AI white paper consultation falls within the wider plan we have \nestablished for supporting Europe through the Coronavirus crisis and helping to rebuild our \neconomies and societies thereafter. In addition to our five point plan to respond to the immediate \nhealth crisis, Vodafone has developed a long term package of measures to assist with the recovery \nfrom the protracted economic crisis we are entering into.  \nAdoption of AI and data driven tools is a vital part of this strategy and Vodafone wholeheartedly \nsupports  the  objective  of  the  European  Commission  to  encourage  the  development  of  an \necosystem of excellence and an ecosystem of trust for AI development and adoption in Europe. \nWe consider that increased use of AI and data driven technology is a necessary precondition for \nestablishing a resilient European digital society. \nOn the key areas of regulation proposed by the Commission to establish a clear and predictable \nlegal framework for AI operating within the single market, Vodafone has the following holds the \nfollowing high level views:  \n\uf0b7  Regulation of high-risk AI: We broadly support the approach outlined by the Commission \nto  avoid  duplication  of  existing  regulatory  obligations,  and  ensure  that  any  new \nrequirements are proportionate and strictly targeted at AI applications that pose a high \nrisk.  \n\uf0b7  Voluntary labeling: We raise particular concerns with the lack of focus given to incentives \nto develop Trustworthy AI (voluntary labeling, certification, industry codes) and ask for \nmore clarity on how such a labeling scheme would be established and maintained. \n\uf0b7  Product Liability: we note that the existing EU horizontal and sector-specific legislative \nframework governing liability has proven to be robust and reliable. Any new requirements \nshould be targeted at providers of high-risk AI applications & must not impose excessive \nhigh burdens on industry deploying AI that presents a low risk to EU citizens.  \n\uf0b7  Access to data: we underline the importance of availability of high quality training data \nfor AI, to ensure that European businesses can continue to innovate and succeed in this \nfield, and call for joined up policy making in view of the EU Data Strategy and legislative \nactions outlined therein.  \nWith regards to the specific policy options outlined in the Roadmap IIA, we agree with the \npreliminary assessment of the European Commission that the ‘baseline scenario’ whereby no EU \npolicy change is introduced would have a harmful effect on the integrity of the single market and \nsafety of European citizens and consumers. By failing to intervene, the baseline scenario risks \nincreasing fragmentation and resulting in the development and propagation of untrustworthy and \npotentially unsafe AI applications that could lead to a significant public backlash and hinder the \nadoption of transformational AI technologies.   \nVodafone considers that are significant advantages to options 1 & 2 (soft-law/non-legislative \napproach, EU legislative instrument establishing a voluntary labelling scheme) as these would \n \nC2 General', 'Vodafone Group Response  \nEuropean Commission Roadmap Inception Impact Assessment Proposal for a legal act of the European \nParliament and the Council laying down requirements for Artificial Intelligence  \nhelp to incentivise industry-led intervention and best practice without overburdening firms or \npreventing  the  application  of  transformative  AI  technology  through  over-rigorous  ex  ante \nrequirements.  \nThese options should also allow for deployment of ex post enforcement mechanisms to ensure \nthat AI applications that fail to meet relevant standards for security, privacy and consumer safety \nare removed from the market. In particular we think that options 1 & 2 would be optimum in \nupdating existing EU product liability rules to address the challenges posed by AI applications \n(opacity and complexity of technology ecosystems, lack of sufficient redress possibilities for \nconsumers, uncertainty about attribution of liability between industrial actors).  \nWith regards to option 3 (EU legislative instrument establishing mandatory requirements for \ncertain types of AI applications) Vodafone is of the view that such requirements should only be \napplicable in situations of severe potential risk. For all AI applications falling below this designation, \nexisting regulation combined with voluntary schemes and EU accredited certification/labelling as \noutlined under options 1 & 2 would be sufficient.  \nFor a number of narrowly prescribed high risk AI applications, we consider that certain mandatory \nex ante requirements could be appropriate (the  approach described under option 3.b).  We \nelaborate in our response to the AI white paper the criteria which we believe should be used to \ndetermine whether an AI application is deemed to be ‘high-risk’ and therefore subject to additional \nregulatory requirements. At a high level, we believe such an assessment should rest on a \ncalculation of the likelihood of harm occurring, the degree of harm to an individual user and the \nscale of harm (i.e. number of those who could be affected). In addition, the Commission should \nalso consider a number of other normative requirements for additional obligations to bite, for \nexample the intended purpose of the AI application, the risk level present in a given sector, or the \nmarket position of the firm developing the AI application. \nIt follows that this approach should be backed up by EU-wide enforcement mechanisms to ensure \ncompliance with applicable requirements. However we note that a huge amount of regulation \nexists in this space already, particularly for firms such as ours that are subject to both horizontal \nand sectoral regulation. Any new enforcement mechanism would need to ensure consistency with \nand not duplicate existing enforcement bodies and processes, and where possible should take \naccount of and instrumentalise organisational, firm level best practice.  \nSuch an approach will help to embed “organisational accountability” as the key element of the EU \napproach to AI to foster trust and sustainable AI practices and provide incentives for organisations, \nin both the public and private sector in the EU, to develop accountable frameworks for AI.  \n \n \n \nC2 General']"
F550657,09 September 2020,Ann-Kathrin Schäfer,Sonstiges,Deutsche Gesetzliche Unfallversicherung e.V.,groß (250 oder mehr Beschäftigte),-,Deutschland,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"Aus Sicht der Deutschen Gesetzlichen Unfallversicherung e.V. (DGUV) ist ein Rechtsrahmen, mit dem Fälle des Einsatzes von Künstlicher Intelligenz (KI) angemessen behandelt werden können, grundsätzlich wünschenswert, aber es gilt vorrangig zu prüfen, ob der bestehende allgemeine, d.h. nicht spezifisch auf KI ausgerichtete, Rechtsrahmen nicht in den meisten Fällen ausreichend ist, um auch Fälle des Einsatzes von KI angemessen zu behandeln.
In der Stellungnahme der Deutsche Sozialversicherung Arbeitsgemeinschaft Europa e.V. (DSVEV) zum Weißbuch zur Künstlichen Intelligenz wurde bereits festgehalten, dass der von der Europäischen Kommission vorgeschlagene „risikobasierte“ Ansatz im Prinzip zu befürworten ist, wenn von Fragen nach der Haftung, die unabhängig vom Risiko für den Einzelnen oder die Allgemeinheit zu beurteilen sind, abgesehen wird. Soweit die Sozialversicherung und die durch sie finanzierten Leistungen betroffen sind, ist zunächst keine Notwendigkeit für eine generelle Regelung durch Anpassung des bestehenden EU-Rechtsrahmens unter Berücksichtigung von KI erkennbar. Durch KI aufgeworfene Fragen sind weder neu noch mithilfe von allgemeinen Grundsätzen zu beantworten, sondern bedürfen je nach konkretem Fall komplizierter Abwägungen und Kompromisse. Vor diesem Hintergrund bietet jedenfalls eine generelle Regelung keinen Mehrwert oder keine zusätzliche Rechtssicherheit. Mithilfe der Auslegung und Fortentwicklung des bestehenden allgemeinen Rechtsrahmens können bereits Lösungen gefunden werden. Es mag ergänzend ein ethischer Rahmen, der weder entwicklungshemmend noch technologiefeindlich ist, denkbar sein und einen verantwortungsvollen Umgang mit KI fördern. Ausgeschlossen ist nicht, einzelne Vorschriften zu überprüfen und auf die Vornahme von Änderungen hinzuwirken.
Kommt man in dem Zusammenhang zu dem Ergebnis, dass der bestehende allgemeine Rechtsrahmen nicht ausreichend ist, scheinen die Optionen 3 und 4 in Bezug auf Produktsicherheit und –haftung vorzugswürdig zu sein. 
"
F550620,09 September 2020,Philipp Goedecker,Wirtschaftsverband,Zentralverband Elektrotechnik- und Elektronikindustrie e.V. (ZVEI),mittel (50 bis 249 Beschäftigte),94770746469-09,Deutschland,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"The Zentralverband Elektrotechnik- und Elektronikindustrie e.V. (ZVEI) welcomes the opportunity to provide feedback to the Inception Impact Assessment of the European Commission for a proposal for a legal act of the European Parliament and the Council laying down requirements to stimulate the development and uptake of Artificial Intelligence (AI) and new technologies.
AI will be a core enabling technology for many sectors of European industry. Therefore, ZVEI supports a European approach. A patchwork of national rules and initiatives in 27 EU Member States must be avoided.
ZVEI welcomes a debate about the risks and opportunities but calls for a very cautious approach when regulating an essential transversal technology. The immense opportunities must not be ignored, innovation and regulation must facilitate the development and uptake of AI. ZVEI would like to bring forward four points:
1) There is currently no need for a new, horizontal regulation for AI technologies, because there is no evidence yet of any fundamental regulatory gaps. Particularly, in the case of AI embedded in products and machines, AI-characteristics (opacity, complexity and scalability) are limited by existing regulation and functional constraints. 
2) The EU-legislator should not regulate specific technologies but should instead target the effects of AI applications in a technology-neutral way. Otherwise, there is a risk of hampering innovation and having to constantly adapt laws to technological progress. After careful assessment of regulatory gaps, AI applications where personal privacy or where life and limbs are at stake should be critically evaluated in a case-by-case approach. Hereby the risk level should be objectively determined in a short range for the beginning by the criticality of the application itself. Transparent and comprehensible criteria are needed to classify AI-based products and services. Legal uncertainty would be a huge barrier to the use of AI, in particular for SMEs and MidCaps, and would hence hamper the use of AI in Europe.
3) We do not see a need for legislative action on product safety. In the General Product Safety Directive, the safety requirements are technology-neutral and expand without preventing technical innovation and thus cover AI applications.
4) ZVEI does not believe that a readjustment of the Product Liability Directive and national liability regimes is needed at this stage. The Product Liability Directive is formulated in a technology-neutral way and the courts have applied it over the years to a wide range of products, many of which did not exist when the Directive was adopted, like AI now.
ZVEI favours a combination of policy option 1 and 3b in the presented Inception Impact Assessment. Policy option 3c (covering all AI) would raise a prohibitive barrier in a important sector of industrial AI and will finally will bring Europe’s industry to fall behind international developments, especially in competition with US and Chinese companies.
-The basis should be a “Soft Law”-approach, which builds upon existing initiatives and guidelines on trustworthy AI across a wide range of experts within the industry. This will not only leave room for cutting-edge AI-solutions, but also allow a precise assessment of the real risks of AI and potential regulatory gaps. The use of “regulatory sandboxes” could help to find this balance. 
-In a second step, if necessary, specific requirements could be established for clearly defined “high-risk”-applications - if they are not covered by existing regulation. It is crucial to focus on ""high risk applications"" and avoid hampering harmless AI-applications. In the industrial context (B2B), AI offer enormous economic potential and is in general uncritical -non-personal machine data is often used to optimise production processes or increase efficiency along the value chain-. This is already covered by a technology open legislation where AI applications do not require further legislation.
"
F550619,09 September 2020,Mauritz Kop,Sonstiges,NL AIC (Dutch AI Coalition),groß (250 oder mehr Beschäftigte),-,Niederlande,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"NL AIC welcomes the opportunity to provide feedback to the European Commission’s Inception Impact Assessment on the “Artificial intelligence – ethical and legal requirements” legislative proposal. 
1. We support the Commission’s mission to foster the development and uptake of safe and lawful AI that offers legal certainty, a favourable investment climate and an innovation optimum across the Digital Single Market, while respecting fundamental rights, ensuring inclusive societal outcomes, protecting citizen’s wellbeing and safeguarding our common Humanist moral values.
2. We believe that the EU should step up and take the lead to set global norms and standards that will shape the international Law of AI & Data system. 
3. The EU should use interoperability in combination with data portability as a policy lever. AI & data driven products and services created within the EU or elsewhere in the world should abide by EU benchmarks, together with associated IEC, ISO and NEN standards, before they can obtain a CE-marking and enter the European markets. 
4. AI’s dynamic and elusive nature asks for agile, flexible governance solutions. Designing a system that can quickly adapt to changing circumstances should be a key starting point. 
5. In our view Option 4, ‘a combination of any of the options above taking into account the different levels of risk...’, would best serve the EC’s objectives. Since both innovation incentive & reward mechanisms, as well as safety/security risks vary per industry and per technology, policy makers should differentiate more explicitly between economic sectors when they design their digital governance solutions. We suggest a differentiated risk-based approach that contains industry specific boundary setting requirements and sector-specific AI regimes.
6. We prefer a broad definition of AI (subject matter) that includes synergies with other disruptive tech such as DLT and quantum computing. A broader scope (Option 3c) means more impact (though perhaps more initial costs/investments) and increased long term benefits.
7. To make AI and machine learning thrive, we have to re-examine the applicability and scope of (intellectual) property rights to data, and construct territorially applicable antitrust laws. Forum shopping should be avoided. There should be a right to process (e.g. access, share, analyse, re-use) data for machine learning purposes. We advise against introducing new layers of innovation stifling exclusive rights. A robust public domain, that includes open, democratized data should be promoted in general. 
8. The EU must also provide incentives to build and augment datasets, algorithms and inference systems, by layering traditional and alternative innovation incentive & allocation options such as prizes, subsidies, fines, benchmarks and competitions. 
9. In our view, guidance is an important part of the implementation and enforcement phase of the Law of AI, as explaining its requirements encourages trust, legal certainty and freedom to operate in the data-driven economy.
10. Adjacent to regulation we can see an important role for harmonized AI Impact Assessments such as the Dutch AIIA & Code of Conduct that combines technical, legal and ethical standards, HLEG’s ALTAI and CoE’s Recommendations. Self-regulation alone should never be enough: industries simply do not have the same incentives to promote public good as governments do.
11. Synchronous to a coordinated, differentiated industry-specific approach regarding incentives and risks, the EU should actively shape technology for good and embed norms, standards, principles and values into the architecture of our technology, by means of Trustworthy AI by Design. 
12. Lastly, we believe it is crucial for the EU to work together with countries that share our European digital DNA, based on common interests and mutual values. It is essential to incentivise systematic transatlantic cooperation. Sovereignty will ensure strong partnerships amongs equals.","['NL AIC welcomes the opportunity to provide feedback to the European Commission’s \nInception Impact Assessment on the “Artificial intelligence – ethical and legal requirements” \nlegislative proposal.  \n1. We support the Commission’s mission to foster the development and uptake of safe and \nlawful AI that offers legal certainty, a favourable investment climate and an innovation \noptimum across the Digital Single Market, while respecting fundamental rights, ensuring \ninclusive societal outcomes, protecting citizen’s wellbeing and safeguarding our common \nHumanist moral values. \n2. We believe that the EU should step up and take the lead to set global norms and \nstandards that will shape the international Law of AI & Data system.  \n3. The EU should use interoperability in combination with data portability as a policy lever. \nAI & data driven products and services created within the EU or elsewhere in the world \nshould abide by EU benchmarks, together with associated IEC, ISO and NEN standards, \nbefore they can obtain a CE-marking and enter the European markets.  \n4. AI’s dynamic and elusive nature asks for agile, flexible governance solutions. Designing a \nsystem that can quickly adapt to changing circumstances should be a key starting point.  \n5. In our view Option 4, ‘a combination of any of the options above taking into account the \ndifferent levels of risk...’, would best serve the EC’s objectives. Since both innovation \nincentive & reward mechanisms, as well as safety/security risks vary per industry and per \ntechnology, policy makers should differentiate more explicitly between economic sectors \nwhen they design their digital governance solutions. We suggest a differentiated risk-based \napproach that contains industry specific boundary setting requirements and sector-specific \nAI regimes. \n6. We prefer a broad definition of AI (subject matter) that includes synergies with other \ndisruptive tech such as DLT and quantum computing. A broader scope (Option 3c) means \nmore impact (though perhaps more initial costs/investments) and increased long term \nbenefits. \n7. To make AI and machine learning thrive, we have to re-examine the applicability and \nscope of (intellectual) property rights to data, and construct territorially applicable antitrust \nlaws. Forum shopping should be avoided. There should be a right to process (e.g. access, \nshare, analyse, re-use) data for machine learning purposes. We advise against introducing \nnew layers of innovation stifling exclusive rights. A robust public domain, that includes open, \ndemocratized data should be promoted in general.  \n8. We think that the EU must also provide incentives to build and augment datasets, \nalgorithms and inference systems, by layering traditional and alternative innovation \nincentive & allocation options such as prizes, subsidies, fines, benchmarks and competitions.  \n1', '9. In our view, guidance is an important part of the implementation and enforcement phase \nof the Law of AI, as explaining its requirements encourages trust, legal certainty and \nfreedom to operate in the data-driven economy. \n10. Adjacent to regulation we can see an important role for harmonized AI Impact \nAssessments such as the Dutch AIIA & Code of Conduct that combines technical, legal and \nethical standards, HLEG’s ALTAI and CoE’s Recommendations. Self-regulation alone should \nnever be enough: industries simply do not have the same incentives to promote public good \nas governments do. \n11. Synchronous to a coordinated, differentiated industry-specific approach regarding \nincentives and risks, the EU should actively shape technology for good and embed norms, \nstandards, principles and values into the architecture of our technology, by means of \nTrustworthy AI by Design.  \n12. Lastly, we believe it is crucial for the EU to work together with countries that share our \nEuropean digital DNA, based on common interests and mutual values. It is essential to \nincentivise systematic transatlantic cooperation. Sovereignty will ensure strong partnerships \namongst equals. \n \nOn behalf of NL AIC, \nMauritz Kop \nSeptember 8, 2020 \n \n2']"
F550618,09 September 2020,László Balázs,EU-Bürger/-in,-,-,-,Ungarn,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"Tisztelt Európai Uniós Tisztségviselők!

A mesterséges intelligenciával (a továbbiakban: MI) összefüggésben az alábbi álláspontot képviselem erkölcsi, morális és jogi formulák tekintetében.
Először kénytelen, kelletlen vagyok párhuzamot vonni a biológiai determinizmus és az MI kettősében. Napjainkban pusztító, pandémiás időszak vonulatában a biológiai esszenciális reziduumok jelenléte egyértelműen detektálható. Az anyatermészet a túlszaporulat és a népesség kritikus tömeggé növekedése okán fel kellett, hogy állítson egy olyan védvonalat, amivel kísérletet tehet a ""természetes"" közegek, folyamatok visszaállítására, helyrehozatalára. Mindenképp kár ez, vagy érthető reakció arra a felelőtlen kondíció halmazra, amit az emberiség ""követett el"" egy egész homeosztázis, mikrobiom ellen? Utóbbit adekvátabbnak érzem.

Gondolván a kultúránk, társas kultúrák és a vele együtt járó, olykor azokat túllépő tudományos eredményekre úgy vélem, hogy a moralitás, mint az emberiség fejlődésével együtt fejlődő, alakuló archaikus maradvány, nem szabhat gátat annak a metódusnak, amit a tudomány területén dolgozó, kiváló szakemberek elképzeltek. Igazán megközelítve, a morális értékek, jogi szabályozások, közjogi szervezetszabályozó eszközök nem mások, mint ugyanannyira mesterséges - ellenben közösségi igény alapján szerveződött - klisék, mint a tudomány által szerveződött, irányítva alakult, fejlődött ""termékek"", gondolati elemek fizikai síkon való megvalósulása.

Fentieket összegezve kijelenthető, hogy a morális, jogi normák a tudomány eredményeinek, termékeinek tökéletes reprezentánsai. Csupán az előbbiek az emberiség filozofikus kogníciója, míg utóbbiak a tanulás, a tapasztalat és empirizmus mentén szerveződött kognitív kivetülések, tárgyiasulások.

A MI tekintetében fentiek dacára, kijelenthető, hogy senki és Semmi (Heidegger után szabadon) nem kérdőjelezheti meg annak létjogosultságát, főleg annak ismeretében, hogy már maga az emberi lét alapvető gyakorlati elemei, mentális kivetülései és egyéb kognitív alaki reprezentánsai ugyancsak a mesterséges vonalon keletkeződtek. Filozófiai értelemben lehet ennek viszonylatában disputákat folytatni, ellenben, mint minden disputa, ez is csak azt az eredményt hordozná magában, hogy az időt nyújtsa a cselekvés vesztére, kárára.

Az egészségügynek, az atomiparnak, a környezettudatosságnak és számos tudományos, de az emberiség jövőjét garantáló tudásnak, gyakorlatnak, jelentős szüksége van a MI-ra. 

Le kell csupaszítani a MI-vel szemben kialakult sztereotípiákat, aberrált gondolati elemeket, előre vetített negatív érzelmeket; meg kell teremteni a marketing magasabb fokú jelenét, mely végre nem az ""olcsó sörök"" kategóriáját foglalja el, hanem az emberiség elé tár egy sokkal mélyebb, tartalmasabb, és nehezebb területet, a tudományt és MI-t.

Tisztelettel és köszönettel:
Balázs László"
F550611,09 September 2020,Mauritz Kop,Sonstiges,NL AIC,groß (250 oder mehr Beschäftigte),-,Niederlande,Künstliche Intelligenz – ethische und rechtliche Anforderungen,-
F550610,09 September 2020,Mauritz Kop,Sonstiges,NL AIC (The Netherlands),groß (250 oder mehr Beschäftigte),-,Niederlande,Künstliche Intelligenz – ethische und rechtliche Anforderungen,-
F550609,09 September 2020,Ernani Cerasaro,Verbraucherverband,BEUC - The European Consumers Voice,klein (10 bis 49 Beschäftigte),9505781573-45,Belgien,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"BEUC welcomes the opportunity to comment on the Commission’s Inception Impact Assessment for the upcoming proposal for a legal act laying down requirements for Artificial Intelligence. BEUC particularly welcomes that the Commission – contrary to its White Paper published in February 2020 – envisages a new policy option which foresees an EU legislative instrument establishing mandatory requirements for all AI applications and not only for those considered as ‘high-risk’. 
Therefore, in terms of the options proposed in the Inception Impact Assessment, and starting with the consideration that option 0 (no regulation) should be discarded, we consider that the most suitable option is option 3 (c) (proposing a legislative act covering all AI applications), and urge the Commission to move in this direction. 
AI and ADM systems impact the daily lives of consumers but operate within a worrying regulatory vacuum. For this reason, BEUC has underlined many times the importance and urgency of intervening to fill this gap and welcomes that the Commission is envisaging a legislative proposal (please see our publications on AI at http://www.beuc.eu/search?keys=artificial%20intelligence). 
As also outlined in our response to the public consultation on the Commission’s White Paper on AI (in attachment and to which we refer for any further information) we think that such a legislative proposal should be as comprehensive as possible. For this reason we would not limit it only to certain sectors or applications. We think such an option is the most respectful of consumers’ demands as well as of the need for businesses to operate within a clear regulatory framework. 
In particular, we believe that the new regulatory framework for AI and ADM must properly address existing consumer concerns and ensure that this technology is developed and deployed in a manner that embeds strong and tangible safeguards during its whole lifecycle. 
The results of a recent survey on consumer perceptions on AI (September 2020 – http://www.beuc.eu/publications/beuc-x-2020-078_artificial_intelligence_what_consumers_say_report.pdf ), where our member organisations collected opinions of consumers from nine different EU countries about AI, indicate that a majority of respondents do not think that current regulation is adequate to effectively regulate AI-based activities and have low trust in authorities to exert an effective control over AI. This lack of trust is even more significant when it comes to the transparency of AI systems. For example, a large part of consumers agreed that companies use AI to manipulate their decisions or that is not clear who is accountable if AI is not secure or causes harm. 
The survey also shows that although consumers believe that AI can bring benefits, they have serious concerns related to personal data protection, unintended consequences and malicious uses. These are transversal and general concerns, even impacting on fundamental rights such as privacy, which do not necessarily depend on the single sector where AI is deployed nor on the specific system being used. 
This confirms that – when drafting any legislative proposals – the Commission should envisage a broad scope of application which should cover all AI and ADM based technologies so that there will be no vacuums anymore. This would certainly help to build trust in consumers and facilitate business developments ensuring legal certainty. 
For any further analysis, we refer to the attached or linked documents  and remain at the disposal of the Commission for answering any queries.","['The Consumer Voice in Europe \nBEUC’S RESPONSE TO THE EUROPEAN \nCOMMISSION’S WHITE PAPER ON ARTIFICIAL \nINTELLIGENCE \n  \n \n \n \n \nContact: Ernani Cerasaro – digital@beuc.eu \n \nBUREAU EUROPÉEN DES UNIONS DE CONSOMMATEURS AISBL | DER EUROPÄISCHE \nVERBRAUCHERVERBAND \nRue d’Arlon 80, B-1040 Brussels • Tel. +32 (0)2 743 15 90 • www.twitter.com/beuc • www.beuc.eu \nEC register for interest representatives: identification number 9505781573-45 \n \n  Co-funded by the European Union \n \nRef: BEUC-X-2020-049 - 12/06/2020 \n0', 'Why it matters to consumers \nArtificial Intelligence (AI) and Algorithmic-based Decision Making (ADM) applications are \nalready shaping consumers’ lives. For example, online video platforms use algorithms to \npersonalise  users’  content  and  recommendations;  banks  use  them  to  track  suspicious \nactivities and prevent fraud; public authorities make use of AI and ADM to process and answer \ncitizen  requests;  smart  phones  integrate  virtual  personal  assistants  and  social  media \napplications organise the feeds that consumers see in their timeline on the basis of automated \nanalysis of their past behaviour, online activities and interactions. We are still just at the \nbeginning of the digital transformation of our society. While AI may offer many innovative \nopportunities for consumers, its widespread use brings profound social, legal and economic \nchallenges affecting consumers and the entire society. A strong regulatory framework is \nnecessary to ensure that the use of AI is adequately regulated and controlled. It should \nfacilitate innovation and guarantee that consumers can fully reap the benefits of the digital \ntransformation of our societies but are protected against the risks posed by AI. \nTable of Contents \n1. PROBLEM DEFINITION: WHAT IS AI? ................................................................. 6 \n2. A REGULATORY FRAMEWORK FOR AI AND ADM ................................................. 7 \n2.1. The scope of the EC proposal and its risk-based approach ................................................. 7 \n2.2. Specific requirements for high-risk applications ............................................................... 10 \n3. THE WAY FORWARD ......................................................................................... 10 \n3.1. A precautionary approach and more gradual establishment of risks and corresponding \nlegal requiremements................................................................................................................ 11 \n3.2. Approach to data management and control must favour consumers and public interest 12 \n3.3. AI must not further entrench digital commercial surveillance .......................................... 12 \n3.4. Consumers must have a strong set of rights ..................................................................... 13 \n3.5. A strong and streamlined approach to sustainability and environmental protection is \nneeded ....................................................................................................................................... 14 \n3.6. Liability rules must be updated to ensure compensation in case of harm arising out from \nAI-powered products ................................................................................................................. 15 \n3.7. Existing legislation must be updated to ensure consumers are adequately protected .... 16 \n3.8. A coherent oversight, enforcement and redress system is necessary .............................. 16 \n3.8.1. Control and oversight ................................................................................ 17 \n3.8.2. Accountability and transparency .................................................................. 17 \n3.8.3. Enforcement ............................................................................................. 18 \n3.8.4. Remedies ................................................................................................. 18 \n4. VOLUNTARY LABELLING SYSTEM AND LOW-RISK APPLICATIONS .................... 18 \n5. BIOMETRIC TECHNOLOGIES ............................................................................. 18 \n1', 'Summary of recommendations \nIn response to the European Commission’s White Paper on Artificial Intelligence, BEUC make \nthe following recommendations to design a regulatory framework for AI and ADM which \nresponds to consumers’ needs and expectations:  \n1.  The definition of ‘AI’ provided in the White paper should be refined and aligned \nwith the one agreed by the AI High Level Expert Group (HLEG)1. In addition, we \nrecommend the use of terms such as Algorithmic-based Decision Making (ADM), \nrobotics or algorithmic systems, depending on the context and on the technology.  \n2.  The proposed risk-based approach for the development of the new legal framework \non AI and ADM should be revised and broadened: \n•  New rules should not only cover applications considered to be “high-risk’’. A \nbroader,  more  inclusive,  approach  should  be  envisaged.  Legal  obligations \nshould gradually increase alongside the identified level of risk, starting from \nthe principle that some basic obligations (e.g. regarding transparency) should \nbe applicable to all AI applications. From there, the greater the potential of \nalgorithmic systems to cause harm, the more stringent the legal requirements. \n•  The new rules should apply to algorithmic systems, including AI, machine \nlearning, deep learning, ADM and robotics regardless of the level of risk. The \nnew framework should be applicable where consumers are users of or subject \nto an algorithmic system, irrespective of the place of establishment of the \nentities developing and/or deploying the system. \n•  The new rules should also encompass provisions on the admissibility and design \nof algorithmic systems; organisational and technical safeguards; and establish \nan institutional structure for effective supervision and enforcement.  \n•  The process which determines the level of risk of an application (in form of an \nimpact assessment) must be trustworthy, verifiable and objectionable. Such \nimpact  assessment  should  take  into  account  the  possible  risks  arising \nthroughout the whole life cycle of the system for both individuals and society \nat large. Enforcement authorities should be tasked to propose and update valid \nmethodologies for assessing the level of risks and potential harms. \n \n3.  When  proposing  legislation  on  AI  and  ADM,  the  Commission  should  adopt  a \nprecautionary  approach.  We  consider  this  to  be  essential  to  ensure  that \ntechnologies that pose significant harms for individuals and society are not deployed \nuntil they are tested and certified. As an ultima ratio measure, it should be possible to \nban the use of certain AI or ADM systems. Self-assessment of compliance with the \n \n1 BEUC is a member of the European Commission’s High Level Expert Group on AI. To download the definition: \nhttps://ec.europa.eu/newsroom/dae/document.cfm?doc_id=60651  \n2', 'new rules by operators should be in principle avoided, at least for the application \npresenting a high level of risk.  \n4.  Consumers should have control of their data when it is used by AI and ADM products \nand services. In particular, consumers must know how their data is processed through \nenhanced transparency provisions and should be able to manage the processing \nthrough user-friendly interfaces.  \n5.  The Commission should enshrine a set of AI rights for consumers in any future \nregulation. This set of rights should at least include: right to transparency, explanation, \nand objection; right to accountability and control; right to fairness; right to non-\ndiscrimination; right to safety and security; right to access to justice; right to reliability \nand robustness.  \n6.  While highlighting the need to protect consumers, the white paper lacks specific \ninitiatives  for  mitigating  the  negative  consequences  of  the  widespread  use  of \nalgorithmic systems on consumers’ fundamental rights and wellbeing. In particular, \nwe urge the Commission to specifically address in any future regulation the negative \neffects  of  businesses’  large-scale  commercial  surveillance  of  consumers  and  its \npotential influence on their online and offline choices and behaviours.  \n7.  AI has the potential to help achieve the green transition but also comes with a big \nenvironmental footprint. We urge the European Commission to explore the \nopportunities offered by AI but also to consider the environmental harms – such as \ncarbon-dioxide emissions and electronic waste – resulting from the data-driven \ninfrastructures needed to power the large-scale deployment of AI and ADM powered \nproducts and services. We recommend incentivising the use of greener \ninfrastructures for the development and deployment of these technologies so that \nthey support the achievement of sustainable development, climate neutrality and \ncircular economy goals.  \n8.  Any future regulation should envisage a coherent and efficient compliance and \nenforcement system which: \n•  Obliges businesses to ensure built-in control mechanisms for the development \nand use of ADM systems. \n•  Ensures  a  high  level  of  protection  for  consumers  via  a  combination  of \nindependent  ex-ante  verification  mechanisms  and  continued  ex-post \ncompliance checks by authorities in presence of high-risks applications.  \n•  Ensures a coherent structure and harmonised procedures for authorities to deal \nwith pan-European/cross-border infringements.  \n•  Guarantees the active cooperation among the relevant enforcement authorities, \nas well as between public and private enforcement bodies, including consumer \norganisations. Ensures that enforcement authorities are equipped with the \nnecessary financial, technical, and human resources, as well as the necessary \nlegal powers, to do their job efficiently. \n•  Provides the enforcement authorities with the necessary powers (e.g. right to \nobtain information, the right to inspect and access) so that they can scrutinise \nand  evaluate  these  ADM  systems  and  impose  penalties  in  case  of  law \ninfringements. \n3', '•  Ensures that companies are transparent about their use and expected results \nof ADM systems and processes and build in specific interfaces in order to allow \nauthorities to exercise meaningful oversight and ultimately enforce the rules \n(compliance by design).  \n•  Ensures  the  availability  of  effective  remedies  for  consumers  and  the \naccessibility of procedures to claim the violations of their rights through the use \nof ADM systems and AI technologies.  \n9.  An updated liability framework for digital goods and services is urgently \nneeded to ensure effective access to justice for consumers when things go wrong with \ntheir products. In particular, we call for a sound revision of the Product Liability \nDirective2. \n10. An  updated  legal  framework  on  consumer  protection,  including  safety \nlegislation, is equally needed to ensure that consumers are fully protected against \nthe risks created by AI products and services. In particular, we urge the Commission \nto modernise the General Product Safety Directive (GPSD). \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n2 For more info on our position on product liability, please refer to our recent position paper “Product Liability 2.0: \nEU rules fit for consumers in the digital age”, published in May 2020. \n4', 'Introduction \nOn 19 February 2020, the European Commission published a  ‘White Paper on Artificial \nIntelligence: A European approach to excellence and trust’. The aim of the Commission is to \nlaunch a European strategy promoting the uptake of AI and addressing the risks associated \nwith certain AI applications. Europe’s ambition is to become a ‘global leader in innovation in \nthe data economy and its applications’ by fostering a development of an AI ecosystem which \nprofits citizens, business and public sector. The White Paper identifies two main elements \nallowing for such an ecosystem to arise: excellence and trust.   \nFor the creation of an ‘ecosystem of excellence’, the Commission focuses on concrete actions \nto support research, development and uptake of AI across the EU economy and public \nadministration. For the creation of an ‘ecosystem of trust’, the Commission builds on the AI \nHigh Level Expert Group ""Ethics Guidelines for Trustworthy Artificial Intelligence"", published \nin April 2019. These guidelines identify seven key requirements for the development of \ntrustworthy AI applications: human agency and oversight; technical robustness and safety; \nprivacy  and  data  governance;  transparency;  diversity,  non-discrimination  and  fairness; \nsocietal and environmental wellbeing; accountability. \nThe Guidelines, however, are not legally binding. In light of this, and in line with the \nCommission  President’s  political  guidelines,  the  White  Paper  recognises  the  need  for  a \nEuropean regulatory framework which would build trust among consumers and businesses, \nand therefore speed up the uptake of the concerned technologies. \nThe Commission’s White Paper sets out a first outline of a possible new regulatory framework \nwhich  is  based  on  a  risk-based  approach.  BEUC  agrees  that  a  risk-based  approach  is \nappropriate. However, we are concerned that an approach which, as envisaged by the \nCommission, focuses solely on high risk applications would significantly reduce the scope of \nthe new rules and ultimately inadequately protect consumers.  \nWhile AI applications are already subject to European legislation inter alia on data protection, \nprivacy, non-discrimination, consumer protection, product safety and liability, the existing \nregulatory framework is not fit for purpose to address the risks posed by AI. Therefore \nadditional measures are needed3. Consumers expect effective protection and respect of their \nrights whether or not a product or service relies on AI.  \nRecent developments following the COVID19 pandemic have brought AI to the spotlight once \nagain, highlighting its potential to improve health treatments for example. While we recognise \nthat AI has a lot of positive potential, we would like to highlight that the current situation \ndoes not change the fact that AI comes with many challenges and risks which require the use \nof this technology to be properly regulated.  \n \n3 We already expressed some of our concerns in other position papers: AI rights for consumers; Automated decision \nmaking and Artificial Intelligence; AI must be smart about our health; Access to consumers\' data in the digital \neconomy; When innovation means progress - BEUC’s view on innovation in the EU.  \n5', '1. PROBLEM DEFINITION: WHAT IS AI? \nBefore addressing the practical and regulatory implications of AI, we must agree on its \ndefinition.  \nAs a starting point, it is worth highlighting that there is no common nor legal definition of AI, \nand that the definition provided by the Commission in its White Paper should be considered \noverly simplistic. For the sake of straightforwardness, we often refer to ""AI systems"", “AI \napplications”, “uses of AI”, and similar. The concept of AI is blurry and can embrace different \nperspectives. The reason is quite simple: there are thousands of different techniques currently \nused to develop very complex and (partially) autonomous technologies that can fall into the \nartificial intelligence basket. At a regulatory level there are different definitions of AI being \nused4. In the European regulatory framework, a first definition was provided in the ""Ethics \nguidelines for trustworthy AI"", published by the European Commission AI High-Level Expert \nGroup5 (AI HLEG) in April 20196. Based on this definition, AI can be either a system (software \nor possibly hardware) or a scientific discipline. In the first case, such a system should have: \n•  a human mind that designs the technology;  \n•  a given dataset (structured or unstructured);  \n•  a complex goal to be achieved autonomously; \n•  a reasoning on the knowledge acquired from the dataset;  \n•  a scientific technique to be applied; \n•  the capability to behave;  \n•  the capability to adapt to external reaction on its previous actions.  \nHowever, in an attempt – albeit understandable – to provide a simple and straightforward \ndefinition, in its White Paper, the Commission states that: ’Simply put, AI is a collection of \ntechnologies that combine data, algorithms and computing power’. As is evident from the \nabovementioned definition adopted by the AI HLEG, this wording cannot be considered \nsufficient to define AI and the Commission should take utmost account of this.  \nThe White Paper’s definition, in fact, could almost be applied to any software ever written. \nThis definition lacks crucial components of what AI is. By only referring to data and algorithms \nin combination with computing power it does not explain the behavioural characteristics of AI \nand it overlooks the social and human context where AI technology is created7. It does not \nexplain its purpose: Is AI modelling human behaviours? Is it modelling human thoughts? Is \nit a model that can behave intelligently? Is it a mix of all of this?  \nHaving a solid definition is crucial and has major regulatory consequences. If we \nwere to follow the definition provided in the White Paper at regulatory level, all the concepts \nthat descend from it could be questioned by simply arguing that a specific application cannot \nbe considered as AI.  Such a definition would allow organisations to easily  bypass and \ncircumvent future regulations and would ultimately lead to lack of accountability. \n \n4 See, for example, US FUTURE of Artificial Intelligence Act of 2017. \n5 To download the definition: https://ec.europa.eu/newsroom/dae/document.cfm?doc_id=60651  \n6 A first draft of the Guidelines – then subject to public consultation - was published in December 2018: \nhttps://ec.europa.eu/digital-single-market/en/news/ethics-guidelines-trustworthy-ai \n7 Yoshua Bengio, one of the godfathers of AI, defined it as: “[AI is] about making computers that can help us that \ncan do the things that humans can do but our current computers can’t.”\u200a Although it is not adaptable to a regulatory \ninstrument, this definition allows us to understand why and how AI was born and developed. \n6', 'Then, if we want to use AI as a term, it is first of all appropriate to debate on a valid and \ncomprehensive definition which could encompass present and future applications creating \nrisks for individuals (persons or legal entities), for society at large and for specific social \ngroups. For the time being, our proposal is to use more specific terms depending on the \ncontext, not reducing everything to AI. Thus, it seems that the use of algorithmic based \ndecision making (ADM hereinafter) is more aligned with the regulatory objectives HLEG of the \nwhite paper and more suitable to define its action field.  \nADM is a technology neutral term, that includes the technologies that the AI HLEG and the \npublic generally referred to as artificial intelligence. In the same line, the German data ethics \ncommission chose to focus on ‘algorithmic systems’, rather than ‘artificial intelligence’. An \nADM system comprises much more than just program code or an algorithm. It refers to the \nentire process from data acquisition and data analysis to the interpretation of the results and \nthe  derivation  of  a  decision  or  recommendation  from  the  results8.  ADM  systems  are \ncharacterised by the fact that they contain an algorithmic component (control system) which \nproduces an output (decision) on the basis of an input and outputs it in the form of a \n(numerical) value. As such ADM-Systems also include ‘learning’ systems that derive decision \nrules from data by means of machine learning and can adapt them over time. The systems \ndiscussed under the keyword artificial intelligence (AI) usually fall under this definition. The \nkey element of the term ‘ADM-System’ is its relevance from a policy point of view, as it \nstresses the element that the system produces an output that is used to prepare or make a \ndecision that has an impact on people or legal entities. \n2. A REGULATORY FRAMEWORK FOR AI AND ADM \n2.1. The scope of the EC proposal and its risk-based approach \nConsumers’ concerns in relation to AI and ADM systems range from the lack of transparency, \nto concerns about safety, unintended consequences and malicious uses. For example, as \nshown  in  a  survey  commissioned  by  our  German  member  Verbraucherzentrale \nBundesverband (vzbv) automated decisions are regarded as a risk for 75% of consumers if \nthe underlying data and principles applied are unclear9. The new regulatory framework for AI \nand ADM must properly address the whole set of consumer concerns and ensure that this \ntechnology  is  developed  and  deployed  in  a  manner  that  embeds  strong  and  tangible \nsafeguards during its whole lifecycle. Such safeguards should be ensured to anyone who is \naffected by an ADM system.  \nTo ensure a trustworthy development of AI technologies, the White Paper refers to the non-\nlegally binding requirements stipulated by the Ethical Guidelines of the AI HLEG: human \nagency  and  oversight;  technical  robustness  and  safety;  privacy  and  data  governance; \ntransparency;  diversity,  non-discrimination  and  fairness;  societal  and  environmental \nwellbeing; accountability. \nAlthough these points certainly contribute to shape more trustworthy technologies, these \nguidelines are not legally binding. To date, there is no specific legal framework at EU level \naimed at regulating AI. That being said, AI applications are in certain instances already subject \nto a range of existing laws (e.g. data protection and consumer protection legislation), as it \nhappens with any other products or services falling into the scope of such laws. For example, \n \n8  Vieth,  Kilian;  Wagner,  Ben:  Teilhabe,  ausgerechnet  (2017),  URL:  https://www.bertelsmann-\nstiftung.de/de/publikationen/publikation/did/teilhabe-ausgerechnet [16.04.2019].  \n9 https://www.vzbv.de/sites/default/files/2019_vzbv_factsheet_artificial_intelligence.pdf  \n7', 'if the use of a chatbot for customer support in the EU is processing personal data for delivering \nsolutions to the consumer (which means that it needs process the customer’s information, \ncommunications,  etc.),  such  a  technology  should  respect  the  General  Data  Protection \nRegulation (GDPR). \nIn its White Paper, the European Commission acknowledges that existing legislation may not \nbe effective or might otherwise be difficult to apply in the case of AI technologies. This \nprincipally because of the inner opaqueness of such technologies (so-called ""black box-\neffect""), their complexity, volatility and autonomy.  \nThe White Paper therefore sets out the possibility to adapt existing legislation and add new \nrules. BEUC welcomes that the European Commission wishes to examine how to adapt \nexisting legislation – such as EU legislation on product safety and product liability – to ensure \nan effective consumer protection and re-think the allocation of responsibilities between those \nactors involved in the development and deployment of technologies (developers, business, \netc). As BEUC has repeatedly highlighted the need to update the EU’s current rules and bring \nthem up to speed with technological developments. The Commission also identifies a potential \nneed for additional regulation to address the risks inherent to the use of AI. For the purpose \nof designing these new rules and obligations, the Commission puts forward a risk-based \napproach.  \nBEUC considers that new legislation is necessary to address the risks posed by AI \nand ADM and also that such legislation should adopt a risk-based approach. In this \nsense, we welcome the direction envisaged by the Commission. We are however \nconcerned about the risk assessment methodology, the risk management and the \nnarrow scope of the new legal regime envisaged by the Commission, as explained \nfurther below. In particular, we underline that the mere fact that certain applications pose a \nhigher risk than others, doesn’t mean that only such riskier applications should be further \nregulated.  \nThe main risks identified by the Commission are related to fundamental rights (in particular, \ndata protection and non-discriminations) and to safety and the effective functioning of the EU \nliability regime (e.g. safety risks related to autonomous vehicles and allocating liability if such \ncar causes an accident). On the basis of these main risks, the White Paper draws a clear-cut \nline between high-risk AI applications and all other AI applications. The new legal obligations \nenvisaged by the Commission would only apply to high-risk applications. Such applications \nwould  face  stricter  legal  requirements,  including  for  example  technological  conformity \nassessments  and,  in  some  cases,  mandatory  regulatory  pre-approval  before  market \ndeployment. AI applications not considered high risk would be exempted from these new legal \nrequirements,  the  only  additional  measure  envisaged  for  such  applications  would  be  a \nvoluntary labelling scheme awarding those which meet certain EU-wide, yet undefined, \nstandards. According to the White Paper, for an AI application to be classified as high-risk two \ncumulative elements should be present: \n \n1) High-risk sector: the technology is developed in a sector where “significant risks \ncan be expected”. Such high-risk sectors should be “specifically and exhaustively” \nindividuated by the new legislation and might initially include “healthcare; transport; \nenergy and parts of the public sector.” Such a list should be “periodically reviewed \nand  amended  where  necessary.”  In  addition  to  these  sector-based  high-risk \napplications, the Commission expects “exceptional instances [where] … the use of AI \napplications for certain purposes is to be considered as high-risk as such[.]” as “the \nuse of AI applications for recruitment processes as well as in situations impacting \n8', 'workers’  rights,  …  specific  applications  affecting  consumer  rights”  and  facial \nrecognition technology. \n \n2) High-risk use: high-risk sector technologies are “used in such a manner that \nsignificant risks are likely to arise”. Such uses include “uses of AI applications that \nproduce legal or similarly significant effects for the rights of an individual or a \ncompany;  that  pose  risk  of  injury,  death  or  significant  material  or  immaterial \ndamage; that produce effects that cannot reasonably be avoided by individuals or \nlegal entities”. \n \nAccording to the White Paper a sum of the two abovementioned conditions would ensure a \nnarrow scope of application while, at the same time, providing the maximum level of legal \ncertainty.  \n \nAlthough BEUC share’s the view that new additional regulation is necessary, we think that the \nrisk-based approach envisaged by the Commission is too narrow in scope and lacks nuance:  \n•  First, the definition of high-risk provided in the paper is tautological. To define whether \nthere is a high-risk, the elements to be taken into account are still high-risks which \nremain  undefined.  We  encourage  the  Commission  to  redefine  such  a  concept, \nspecifying and elaborating on the concrete and precise factors that would cause risks \nfor individuals and society. In this sense, we would recommend to follow the example \nof the opinion of the German Data Ethics Commission, according to which a risk-based \napproach addresses AI applications “which are associated with regular or significant \npotential for harm”. \n•  Secondly,  regulating  by  sectors  is  confusing  and  unsuitable.  While  we \nacknowledge that some AI applications present higher risks than others, if the binary \napproach put forward in the White Paper is accepted, we believe that the scope of the \nregulation is too narrow render any future measures ineffective. It would, in fact, \ncontradict the obligation to provide for a high level of protection for consumers across \nall sectors. In our opinion, first there is a need for a horizontal intervention that \ncovers all sectors and which possibly takes into account the single specificities of a \ngiven sector. When introduced as a cumulative requirement, the ex-ante identification \nof  high-risk  sectors  could  cause  a  dangerous  illusion  for  consumers  subject  to \ntechnologies  classified  as  ‘not  risky’.  For  example,  AI  tools  present  in  everyday \nconsumers’  lives  such  as  smart  cars,  home  assistants,  drones  for  delivering, \nalgorithmic selection of social media feeds, music and media streaming would fall \noutside the scope but also financial services. Although it is true that all these services \nmay present a different level of risk, it is equally true that it is not possible to exclude \nthat they will fit into a low or high risk category only because they are part of a given \nsector. Furthermore, technologies often do not differ from each other as per sector. \nWe wonder why a certain decision in sector X, adopted on the same parameters and \nwith the same techniques as  a decision adopted in sector Y, must be assessed \ndifferently if the damage is caused by a biased or inaccurate algorithm. \n•  Lastly, a bifurcation of AI into high and low/no risk as envisaged by the Commission \nfails to capture the essence of the risks in AI technologies. The starting point should \nbe that AI deployments can be risky as such but, as they have different intensities, \ncan be mitigated.  \nA  careful  analysis  of  the  actual  digital  world  demonstrates  that  the  deployment  of  AI \ntechnology should be approached cautiously, keeping in mind the real and present dangers \n9', 'brought by this technology10. Generally speaking, over-stating the potential benefits of \nAI or having a too narrow approach towards its risks can have very negative \nconsequences. The Commission itself states that AI can generate harm and that such harm \ncan be material (safety and health of individuals, including loss of life, damage to property) \nand immaterial (loss of privacy, limitations to the right of freedom of expression, human \ndignity, discrimination for instance in access to employment). We should not lose sight of all \nthese risks and ensure regulation brings a high level of protection against them.  \n2.2. Specific requirements for high-risk applications \nThe  distinction  between  high-risk  and  non-high-risk  applications  envisaged  by  the \nCommission has consequences in terms of legal requirements which would be applicable, \ndepending on which category the application in question falls into. Only high-risk AI solutions \nwould be subject to specific requirements in relation to training data, keeping records of data, \ntransparency  requirements,  robustness  &  accuracy,  human  oversight.  In  addition,  the \nCommission suggests a prior conformity assessment, possibly including procedures for testing \nand inspection of certification of algorithms and data sets. \nBEUC welcomes the provision of such requirements which are certainly capable of contributing \nto a more transparent and responsible development of new technologies.  \n \nWhile waiting for these requirements to be better specified by the Commission in a clearer \nregulatory framework, we underline that, we need a gradual approach to risk assessment and \ncorresponding mandatory requirements for each risk-level. These requirements, should thus \nnot be limited to high risk applications. Even more so if the risk is identified through \ninaccurate or unclear factors. \n3. THE WAY FORWARD \nThe future EU law on AI is a crucial test for Europe’s digital policy. Europe must design a \nregulatory framework that ensures that innovation can flourish in a way that is respectful of \nindividuals’ fundamental and consumer rights and of our societal values. The Commission \nmust ensure it  captures and properly considers all the various aspects  relating to the \nsustainable development and deployment of powerful and intelligent technologies in our \nmarkets and society. \n \nThe narrative throughout the White Paper indicates that the Commission is aware of the \nchallenges we are facing and takes them seriously. The two building blocks of the White Paper, \nan ‘ecosystem of excellence’ and an ‘ecosystem of trust,’ are well-grounded. However, there \nare several elements where greater nuance and a more concrete approach would be required.  \n \nOne of the shortcomings of the White Paper is that while consumers are mentioned in different \nparts, there is no focus on the specific risks arising for them when using AI and ADM \ntechnologies. Algorithmic advanced systems might cause adverse impacts for consumers \nwhich are not properly addressed in the White Paper, such as discrimination or social and \neconomic exclusion. For example, in the case of big data analytics algorithms are used to \ncreate credit scores and inform loan screening also via the incorporation of non-financial data \n \n10 For example, the U.S. National Institute of Standards and Technology recognises biases in AI facial recognition \ntools. Recently, a Dutch court stated that  bias in systems to detect welfare fraud determined a violate human rights. \nRacial bias in a health care delivery algorithm was discovered by researchers after the algorithm  being used for \nyears.  Alike, unfortunately, many other examples could be quoted.   \n10', ""sets (such as where people live, internet browsing habits and purchasing decisions). The \ndecisions taken on the basis of the algorithmic reasoning of these systems are largely \nunregulated while they are often discriminatory. However, the White Paper does not seem to \nconsider them in its regulatory proposal. Similarly, while the intention behind the human-\ncentric approach of the Commission seems well-defined, there is a lack of critical socio-\npolitical  analysis  surrounding  diversity,  equality  and  environment  which  constitute \nindispensable aspects for such humanistic development to take place.  \n \nFinally, we underline the importance of building a regulatory spectrum that enables a fairer \naccess to technology. Such belief is the building block for ensuring that the interests of \nmarginalised and vulnerable consumers are adequately taken into account. In this sense, we \nwould encourage the Commission to introduce a duty of care for developers in order to deploy \nconsumer-oriented systems. In particular, by incentivising a consumer-centric approach we \ncould  safeguard  access  to  all  intended  users  avoiding  exclusions  especially  for  those \nconsumers who are currently left behind.  \n \nIn the following sections we describe some of the main elements where we expect the \nCommission to develop a more detailed and ambitious approach.  \n3.1. A  precautionary  approach  and  more  gradual  establishment  of  risks  and \ncorresponding legal requiremements  \nOne of the major weakness of the Commission’s White Paper is the absence of a precautionary \napproach to the development and use of AI and ADM systems. It would seem that the \nintention of the Commission is to allow the development and use of the ADM services/products \nregardless of their riskiness. For example, the word 'ban' is never mentioned in the White \nPaper except when referring to the work done by the German Data Ethics Commission.  \n \nWe consider it is essential that the assessments with regards to the risks for individuals and \nsociety are developed in the form of a preventive impact assessment, as to allow the \nblocking of highly dangerous technologies.  \n \n•  The causes of risk must be assessed from the conceptual phase of the system and \nmust, to the fullest extent possible and according to the state of the art knowledge, \nforesee possible harms during its whole lifecycle. Furthermore, the risks should be \nconsidered  as  a  non-exhaustive list  and  should  take  into  account  numerous \nfactors, including: the type and nature of the data used (e.g. personal/non personal \ndata); the type of algorithmic model; the types of logical reasoning carried out by the \nsystem; the security measures put in place; the methods used to test and maintain \nthe system; sectors; harms for the environment; the desirable dissemination of the \nproduct/service; the business model. \n•  The process which determines the level of risk of an application (in a form of a \npreventive impact assessment) must be trustworthy, verifiable and objectionable. Such \npreventive impact assessment should take into account the possible harms arising \nthroughout the whole life cycle of the system for both individuals and society.  \n•  Authorities should be tasked to develop a methodology and to set the criteria \nneeded to define the level of risk of an application. The authorities could refine this \nset of criteria and the methodology depending on different sectors.  \n•  The intensity of the risk can go from a low level up to irreversible harm for the \nindividual or the society, in which case the technology should be banned. For \nexample, some companies are investing more and more resources into very intrusive \nand potentially very harmful technologies such as those meant to figure out how to \nobjectively  “read”  emotions  in  people  by  detecting  facial  expressions.  However, \n11"", 'researchers demonstrated that “the science of emotion is ill-equipped to support any \nof these initiatives.11” For this reason we think that businesses intending to use such \nform of emotion recognition should not have access to markets (as already happens) \nuntil it is demonstrated that such practices are not harmful and can fulfil the respective \nrisk mitigating requirements. \n•  Legal obligations should gradually increase alongside the identified level of potential \nharm, as described in the German data ethics commission opinion.  \n \n3.2. Approach to data management and control must favour consumers and public \ninterest  \nData  is  a  crucial  resource  of  the  digital  economy  and  it  can  be  indispensable  for  the \ndevelopment  of  services  and  products  powered  by  algorithmic  technologies.  A  future \nregulatory framework for algorithmic systems should take outmost account of various aspects \nrelating to data and ensure a consumer centric approach to data access and control.  \n \n•  Consideration  should  be  given  to  power  asymmetries  between  institutions, \nbusinesses and individuals arising from the growth of digital devices and systems and \nthe rapid expansion of digital data that they generate. Where appropriate, for instance \nin situations of individual or collective harm, consideration should be given to whether \nadditional regulatory measures to those stipulated by the GDPR may be needed to \naddress this (please, also refer to point 3.4 below). \n•  Second, it is important to ensure that consumers are fully aware of what happens with \ntheir personal data, as also required by the GDPR. Consumers should always be \ninformed in a timely clear and intelligible manner about the existence, process and \nrationale of algorithmic systems.  \n•  Third, consumers should be fully in control of their personal data. While it is important \nto ensure competition among businesses also by allowing them to access essential \ndata, personal data sharing always has to be done under full control by consumers and \nin compliance with the GDPR’s right, obligations and principles.  \n \n3.3. AI must not further entrench digital commercial surveillance \nThe development of algorithmic technologies is directly linked to the widespread growth of \n‘intelligent’ services and products on the market. Such technologies are rapidly changing the \nway that consumers search and shop for products. For example, a current trend is the use of \ndevices such as smart assistants allowing consumers to search and order products using voice \ncommands (e.g., Amazon’s Alexa).  \n \nWhile these new AI powered products and services may have many attractive features and \ncould provide benefits like customised services, they also provide the possibility to monitor \nand analyse consumers’ behaviour in detail and therefore to greatly influence their choices. \nAI and ADM technology elevates the levels of pervasiveness and power of the so-called \ncommercial surveillance ecosystem that has come to dominate the online world, further \nendangering consumer’s autonomy and freedom of choice. \n \nDigital services rely on algorithm powered technologies for processing consumer data and for \ntargeting  consumers  with  ads  and  other  messages.  Algorithms  can  be  used  to  exploit \n \n11 Pag. 48: Barrett, L. F., Adolphs, R., Marsella, S., Martinez, A. M., & Pollak, S. D. (2019). Emotional Expressions \nReconsidered: Challenges to Inferring Emotion From Human Facial Movements. Psychological Science in the Public \nInterest, 20(1), 1–68. https://doi.org/10.1177/1529100619832930.  \n12', ""consumer’s weaknesses and biases in order to convince them to purchase products limiting \ntheir choices. Via massive data collection, companies are able to oversee consumers’ online \nactivity, record it and use it to discover possible correlations that may be useful in influencing \nconsumers through the most effective ads12.  \n \nThe  wide  diffusion  of  smart  products  and  services  is  also  able  to  incentivise  price \ndiscriminations among consumers. It has been demonstrated that it is efficient for a brand \noperating in a competitive environment to price discriminate less tech-friendly consumers \nacross distribution channels13. Consumers with clearer preferences find discounts and reduced \nprices  that  are  often  unavailable  to  those  who  are  less  customary  to  using  online \nmarketplaces. In this sense, we highlight that there should be no price differentiation by \nmeans  of  personalised  and  non-personalised  automated  assessments.  Moreover,  it  is \nimportant to bear in mind that services are often provided 'for free' in order to maximise the \nnumber of users and therefore the amounts of profit-generating consumer data collected for \nAI and ADM systems. The scope, invasiveness and potential consequences of this commercial \nsurveillance is difficult, if not impossible, for consumers to comprehend. This happens, for \nexample, in the context of digital advertising or social networks. \n \nIn our opinion, in presence of such phenomena, the potential damage suffered by the \nindividual consumer or by a group of consumers can be very high. However, the Commission \nWhite Paper seems not to embrace “everyday” activities such as shopping activities within its \ndefinition of high risks.  \n \nWe therefore call on the Commission to restrict the use of systems building on \nconsumers’ commercial surveillance and to encourage the deployment of consumer-\ncentric systems based fair and non-discriminatory practices.  \n3.4. Consumers must have a strong set of rights  \nIn our  2019 position paper14 “AI rights for consumers”, we outlined a non-exhaustive list of \nAI rights ensuring a fair, safe, and just society and set to guarantee a high level of consumer \nprotection. We urge the Commission to concretise these rights in its future legislative proposal \nby translating them into enforceable rules so that ADM powered technologies serve consumers \nand does not harm them. In particular, at least the following rights should be guaranteed:  \n \nRight to transparency, explanation, and objection: consumers should have a right to \nget a clear picture of how decisions that affect them are made and be able to oppose wrong \nor unfair decisions and request human intervention. In particular consumers should be able \nto object automated decisions independently of the restrictions individuated by article 22 \nGDPR, i.e. that such decisions are taken ‘solely’ on automated data processing, that the data \nprocessed is qualified as ‘personal data’, and that such decisions should have ‘legal effects’ or \n‘similarly significantly affect’ them. We have concerns that if such right to object is aligned to \nthe GDPR article 22 as far as its scope is concerned, commonly used automated decision-\nmaking processes that have an impact on consumers would not be able to be objected. \n \nRight to accountability and control: consumers should have a right that appropriate \ntechnical and organisational systems as well as measures are put in place that ensure legal \ncompliance and regulatory oversight.  \n \n12 Sartor, G., New aspects and challenges in consumer protection, Study for the committee on the Internal Market \nand Consumer Protection, Policy Department for Economic, Scientific and Quality of Life Policies, European \nParliament, Luxembourg, 2020. \n13 Liu, Yi & Yildirim, Pinar & Zhang, Z. (2019). Artificial Intelligence and Price Discrimination. \n14 https://www.beuc.eu/publications/beuc-x-2019-063_ai_rights_for_consumers.pdf.  \n13"", 'Right to fairness: consumers should have a right that algorithmic decision making is done \nin a fair and responsible way.  \n \nRight to non-discrimination: consumers should have a right to be protected from illegal \ndiscrimination and unfair differentiation.  \n \nRight to safety and security: consumers should have a right that ADM-powered products \nare safe and secure throughout their lifecycle.  \n \nRight to access to justice: consumers should have a right to redress and public enforcement \nif risks associated with ADM materialise.  \n \nRight to reliability and robustness: consumers should have a right that ADM-powered \nproducts are technically reliable and robust by design.  \n \n3.5. A  strong  and  streamlined  approach  to  sustainability  and  environmental \nprotection is needed \nDigitalisation and AI can help the urgently needed green transformation and the move towards \nmore global sustainability. But it can also act as a ‘fire accelerant’ if not managed properly. \nTo this end, the connection between the carbon footprint and computer processing is another \nof the essential considerations to be made when regulating ADM and AI. Empirical findings \nhave shown that digital technologies contribute to 4% of overall greenhouse gas emissions, \na number expected to double by 202515. Other studies show that training a single AI model \nemits carbon dioxide in amounts comparable to that of five cars over their lifetimes16. This \nproblem must not be underestimated, particularly in the context of the European Green Deal. \nIn this sense, a general rethink of political strategies is needed to ensure coherence between \nsustainability and digital policy objectives. For example, it is contradictory to push for a \nmassive use of IT systems that require infrastructures that are potentially very energy/carbon \nintensive without adequate safeguards. The Commission must provide more clarity how it \nintends  to  ensure  that  generalised  development  and  extended  access  to  algorithmic \ntechnologies is carried out in compliance with environmental requirements.  \n \nIn particular it is key to ensure that the development of innovative technological solutions will \naddress sustainability challenges, by for example incentivising companies to reduce the \ncarbon footprint of data centres and IT devices (including smartphones).  While certain \nmeasures have already been taken such as addressing the energy efficiency of servers \nthrough Ecodesign measures and by proposing a new strategy on ICT products as part of the \nsecond Circular Economy Action Plan, much more needs to be done in relation to the scale of \nthe problem. Besides handling the infrastructure which underpins the internet and AI in a \nsustainable manner it will be crucial to shape digitalisation in a way that it can serve a \nfundamental transition towards sustainability.  \n \n15 Maxime Efoui-Hess, Climate Crisis: The Unsustainable Use of Online Video, Shift Project (2019)  \n16 Karen Hao, Training a Single AI Model Can Emit as Much Carbon as Five Cars in Their Lifetimes, MIT Tech. Rev., \nhttps://www.technologyreview.com/s/613630/training-a-single-ai-model-can-emit-as-much-carbon-as-five-cars-\nin-their-lifetimes/ ; Emma Strubell, Ananya Ganesh & Andrew McCallum, Energy and Policy Considerations for Deep \nLearning in NLP, Ann. Meeting Ass’n Computational Linguistics (2019).  \n14', ""3.6. Liability rules must be updated to ensure compensation in case of harm \narising out from AI-powered products \nAs the Report on the safety and liability implications of Artificial Intelligence, the Internet of \nThings and robotics17 (hereafter “the Report”) accompanying the AI White Paper highlights, \nAI and ADM technologies nowadays importantly disrupt liability rules18. In parallel, EU \nand national liability rules have been designed with traditional business models and traditional \nproducts in mind. For example, products that the drafters of the Product Liability Directive \nhad in mind in the 1980s are a far cry of those surrounding consumers nowadays. The existing \nframework established by the Product Liability Directive in 1985 is no longer adapted to the \nmultiple  challenges  brought  by  new  technologies  in  2020.  This  situation  creates  legal \nuncertainty for both businesses and consumers, multiplies risks of unequal treatment of \nconsumers in the Single Market, prevents redress and ultimately hinders trust in digital goods \nin general. An updated liability framework for digital goods is urgently needed to \nensure effective access to justice for consumers when things go wrong with their products.19 \nWe  therefore  call  on  the  Commission  to  take  outmost  account  of  the  following \nrecommendations:  \n•  Liability rules for digital goods should  ensure a higher level of protection for \nconsumers and should be fair and cost-effective. \n•  In order to mitigate the existing informational asymmetries, we notably call for a \nreversal of the burden of proof: it should be up to the party that has access to the \nrelevant information to investigate the cause of the problem when problems arise. \n•  The liability framework should be clear and enforceable. The fact that multiple actors \nmay  potentially  intervene  in  the  product  supply  chain  (e.g.  manufacturer,  app \ndeveloper, programmer, designer, etc.) should not prevent consumers from obtaining \ncompensation. All professionals involved in the supply of digital goods should be held \njointly liable in case of harm. \n•  Liability rules should provide the right set of incentives to all actors involved in the \nsupply chain. Actors should be required to fully internalise the risks of their products \nand to take the precautionary measures that would prevent harmful situations from \noccurring in the first place.  \n•  Among others, the notion of “product”, “defect”, “producer” and “damage” should be \nrevised and adapted to the digital context. \n•  Targeted changes in national liability based on a risk-approach rules should be \ncarefully assessed. In any event, these targeted changes should by no means \nreplace a sound revision of the Product Liability Directive. \n•  Finally, if special liability rules were to be introduced for some categories of digital \nproducts, it will be essential to clarify the interplay between the upgraded EU product \nliability framework and the special liability rules applying for certain AI products. Again, \n \n17 European Commission, COM(2020)64 final. \n18 This situation precludes consumers from obtaining compensation when things go wrong. Digital goods relying on \nalgorithms are overly complex, opaque, data-driven, may evolve in directions that were not initially expected and \nvulnerable to cyberattacks. For example, in February 2020, a research conducted by the Dutch consumer organisation \nConsumentenbond revealed that many ‘smart’ products are vulnerable to hacks. Consumentenbond tested 10 \nproducts and found security issues with two sex toys, two children's GPS watches and two baby cameras. In total, \nthe investigations and hack tests revealed 27 vulnerabilities: www.consumentenbond.nl/nieuws/2020/fabrikanten-\nlaks-met-veiligheid-slimme-apparaten \n19 In May 2019, the European Parliament already regretted “that no legislative proposal was put forward during [the \nlast] legislature, thereby delaying the update of the liability rules at EU level and threatening the legal certainty \nacross the EU in this area for both traders and consumers” (European Parliament, P8_TA(2019)0081, 12 February \n2019, pt 132). \n15"", 'such a consistency and coherence are necessary for ensuring a clear and enforceable \nliability framework for all stakeholders.    \nWe have further detailed the necessary changes in our recent position paper “Product Liability \n2.0: EU rules fit for consumers in the digital age” published in May 2020.  \n3.7. Existing  legislation  must  be  updated  to  ensure  consumers  are  adequately \nprotected  \nIn addition to liability rules, also safety and consumers’ rights legislations need to be updated. \n \nIn particular, in our forthcoming position paper “Views for a modern regulatory framework on \nproducts safety” we call for a modernisation of the General Product Safety Directive \n(GPSD) adopted in 2001. Although  the Directive  constitutes  a key piece  of  consumer \nprotection policy especially by creating a general obligation for producers to place only safe \nproducts on the market, it is now outdated and unable to grasp the challenges arising from \ntechnological developments such as AI and ADM technologies. For example, the current legal \ndefinition of “product” does not explicitly include software that may be incorporated in a \nconnected product or downloaded after its placing on the market. In this sense, we are \nconcerned that if a safety issue arises due to a software update or inefficiency consumers \nwould not be able to be sufficiently protected. Similarly, the current definition does not offer \nclarity about who is responsible for the safety of self-learning AI products. We have therefore \nissued some recommendations for an update of the rules. Among others, we underline the \nneed to establish a principle of “security by design and by default” which constitutes a priority \nfor connected products. This would for instance require manufacturers of such products to \nrespect  minimum  cybersecurity  requirements  (e.g.  strong  authentication  features; \nencryption) from an early stage of and throughout their design process, before putting their \nproducts on the market.  \n \nRegarding the EU consumer law aquis, it is necessary to assess whether horizontal legislation \nregarding unfair commercial practices, unfair contract terms, and consumer rights when \nbuying on-line (just to name a few central pieces of consumer protection legislation)  are still \nfit for purpose. For example, as outlined in a previous position paper the law on unfair \ncommercial practices has its roots in the idea that consumers must be given essential \ninformation so that they can make an informed decision. Is “essential information” still a valid \nconcept when nobody can retrace why and how a specific decision has been taken? \n \nA targeted REFIT exercise should be undertaken to evaluate whether these directives can still \neffectively meet their legislative objectives in an ADM environment.   \n \n3.8. A coherent oversight, enforcement and redress system is necessary  \nIn  order  to  ensure  the  trustworthy  deployment  of  algorithmic  based  technologies,  the \nCommission recognises the need for the applicable legal requirements to be complied in \npractice and be effectively enforced both by competent national and European authorities and \naffected  parties  at  national  and  European  level.  The  Commission  also  underlines  that \ncompetent authorities should be in a position to investigate individual cases, but also to assess \nthe impact on society.  \n \nThe proper functioning of any future regulation will depend on the effectiveness of its \nprovisions and therefore on strong, clear and sound public enforcement. But it will also depend \non providing the necessary means to civil society, and in particular consumer organisations, \n16', 'to fulfil their role as market watchdogs, either via testing of products and services, private \nenforcement, such as collective redress actions, or via the collaboration with public authorities \nby providing them with alerts about illegal practices or market failures.  \n \nFor this to happen, it is firstly crucial to  disclose information about each automated decision \nsystem, including details about its purpose, design features, potential use and implementation \ntimeline  in  order  to  ensure  that  ADM  systems  are  comprehensible  for  consumers  and \nsupervisory authorities. This way, consumers’ trust will increase and authorities will be able \nto scrutinise systems and consequently  minimise the harms by imposing modifications, \nrestricting or prohibiting the use of the system.  \nStrong oversight by supervisory authorities should be ensured regardless of the \nlevel of risks. We also highlight the need for closer cooperation among the authorities, and \nfor  an  enforcement  system  that  can  deliver  EU  wide  results  for  EU  wide \ninfringements/challenges, ensuring a harmonised and effective approach.  \n \nOur recommendations can be summarised as follows: \n \n3.8.1. Control and oversight \n•  ADM technologies should as a matter of principle be subject to independent control and \noversight. Whether an algorithm-based decision is accurate, fair, or discriminative can \nonly be assessed if an appropriate control system is in place. As a general principle, \ncompanies and operators should be able to demonstrate that they comply with the law, \nsuch as rules on consumer or data protection, as well as non-discrimination rules.  \n•  In the pre-marketing phase, at least for high risk applications, it should be mandatory \nfor a producer/service provider to involve independent third-parties which assess legal \ncompliance and can for example request design changes before a product goes into mass \nproduction or a service can be brought to the market.  \n•  For applications that present the highest levels of risk, ex-ante scrutiny procedures by \nauthorities  (e.g.  regulatory  pre-approval  before  market  deployment,  publication  of \nimpact assessments) should be put in place.  \n•  For the post-marketing phase, it will be important to ensure that the compliance of a \ncertain product or service with the legal requirements of any future regulation will be \nassessed during its whole lifecycle, establishing a principle of ‘continued conformity’. \nThis  concept  is  particularly  important  in  an  advanced  technological  environment  as \nrecurring software updates and self-learning algorithms may change the properties of \nproducts and services over the time and consequently have an impact on how the system \nrespects the legal requirements. Therefore, continuous internal and external control and \noversight will be crucial to keep consumers protected.   \n \n3.8.2. Accountability and transparency \n•  Depending on the level of risk, accountability measures should comprise ADM impact \nassessments, documentation, internal audits or transparency measures for the users. \n•  Operators must be transparent about their business model and use interfaces \nwhich will allow authorities to exercise meaningful oversight and ultimately enforce the \nrules.  \n•  To enable both ex-ante and ex-post assessments, independent third-party testing and \nenforcement measures by Member States, companies need to be accountable and must \nput in place measures to allow for external control of their ADM systems.  \n•  Rules for an effective auditing system should be put in place so that authorities are able \nto check the compliance of relevant ADM processes. This would also reveal which — \n17', 'potentially unintended — consequences the processes have for consumers’ everyday \nlives20. \n•  The Commission should also evaluate the possibility of imposing the use of mandatory \nstandards for technical design, logging, documentation and description of ADM systems \n(transparency by design)21. \n3.8.3. Enforcement \n•  Enforcement  authorities  should  have  the  necessary  powers  (e.g.  right  to  obtain \ninformation, the right to inspect and access) so that they can scrutinise and evaluate \nAI and ADM systems and, in case of law infringements, stop illegal practices, \nimpose  remedies  and  award  damages  where  relevant  as  well  as  impose \npenalties.  \n•  Authorities must also be capable of conducting ex post checks on relevant ADM systems \nat any time. It must be possible, in particular for the competent supervisory authority, to \nreview and verify the tests performed by the operators.  \n•  As an ultima ratio measure, authorities should be able to ban the use of certain AI or \nADM systems upfront. \n3.8.4. Remedies \n•  We also recall the importance of ensuring that effective remedies for consumers are \neasily available. Consumers should have the concrete possibility to interact with a human \nable to handle and explain the processing activities of the system and its decisions. It \nshould be guaranteed a minimum level of human oversights so that the system’s decisions \ncan be checked, timely contested and corrected. \n \n4. VOLUNTARY LABELLING SYSTEM AND LOW-RISK APPLICATIONS  \nThe Commission envisages the possibility of introducing a voluntary labelling scheme for those \napplications that would not fall under the high-risk category and therefore into the scope of \nthe new regulation. In our view, such a scheme is not suitable to provide meaningful \nprotection to consumers, even if it is just envisaged for low-risk applications. \n \nLabels are fruitful only in relation to the requirements and enforcement systems they are \nbased on. Once clear legal rules and enforcement mechanisms will be in place, the role of a \ntrustworthy label could be considered. It is important to also bear in mind that the inherent \ninformation asymmetry in complex and evolving algorithmic learning systems, makes the role \nof a label very complex and different from other sectors such as environmental, fair trade or \nfood labels for products and services which do not change properties constantly. \n \n5. BIOMETRIC TECHNOLOGIES \nCompanies are increasingly using consumers’ biometric data for different purposes. All over \nthe world facial recognition is used for ‘tagging’ people on social media platforms, to unlock \n \n20 See BEUC’s German member (vzbv) factsheets ‘ARTIFICIAL INTELLIGENCE: TRUST IS GOOD, CONTROL IS \nBETTER’. \n21 REGULATION OF ALGORITHMIC DECISION MAKING FOR THE BENEFIT OF CONSUMERS – by BEUC’s German \nmember (vzbv).  \n18', 'smart phones or to authenticate/identify customers in the context of financial services. \nRetailers can leverage facial recognition to identify a premium customer. Biometrics are \nparticularly  sensitive  data,  and  their  illegitimate  processing  can  have  very  serious \nconsequences. For example, one aspect which is very worrying for consumers is the use of \nbiometrics for emotion recognition (e.g. real time facial recognition that analyses feelings and \nadapts what consumers see/or are offered accordingly). This can lead to serious infringements \nof consumers’ privacy as well as to their manipulation. Due to their inner intrusiveness, the \nWhite Paper assigns to the use of biometrics a specific role in the regulatory landscape, \nconsidering the need to provide with ‘specific requirements’ for their use. \n \nThe White Paper rightly distinguishes biometric data processed for the purpose of customer \nauthentication, from those used to identify them. This second category22 certainly raises more \nconcerns than the first (which is however not harmless) and pose greater risks of harm.  \n \nBiometric  identification  systems  are  already  covered  by  the  General  Data  Protection \nRegulation (GDPR). The processing of biometrics data for uniquely identifying purposes, such \nas facial recognition, is forbidden pursuant to Article 9(1) of GDPR, unless it falls under the \nscope of one of the exemptions listed in such article. Thus, an effective implementation of \nGDPR may ensure that facial recognition is used in a duly justified manner and does not \nexcessively interfere with the right to privacy. However, when these systems are used – \nespecially in public spaces – ethical and social questions arise. For example: Is it socially \nacceptable that facial recognition is used in public places? For what purposes? Who and how \ncan guarantee a use that respects the principles of necessity and proportionality? Which is \nthe role of Governments, society and the other stakeholders? These questions are not \nanswered by the GDPR. Well aware of these dilemmas, the Commission determines that it \n“will launch a broad European debate on the specific circumstances, if any, which might justify \nsuch use, and on common safeguards’.  \n \nBEUC welcomes the fact that the Commission wishes to have a broad and inclusive debate on \nthe use of these systems. However, in our view, this debate must not to be limited only to \nthe specificities identified by the Commission (remote identification in public spaces). There \nare actually many other uses of biometric data that should be subject to a public debate as \ntheir effect can be irreversible for society and future generations. For example, more attention \nshould be paid to the risks arising from the use of biometric data of vulnerable subjects, such \nas children. A recent study took the processing of children’s biometric data as paradigmatic \nexample underlying that ‘that closer attention must be paid to the actual social contexts in \nwhich data relating to children comes to affect their lives through AI practices’23. In this sense, \nwe emphasise that all biometric systems and all collections of biometric data, regardless of \nbeing for identification or for authentication purposes, can lead to problematic outcomes. We \nwould therefore recommend to apply heightened safeguards also to seemingly less intrusive \nbiometric techniques. \n \n \n \n \n \n22 In fact, there are simple biometric identification systems which compare individual physical or behavioural \ncharacteristics with the information stored in the system in order to find a match for the purpose of identifying the \nperson. Such systems – which seem to be out of the scope of the White Paper - differ from those which embed \nmachine learning techniques which – in addition to data collection and combination - allow the system to learn, react \nand adapt its endeavours on the basis of its findings. These latter technologies are defined in the White Paper as \n‘remote biometric identification’.  \n23 Velislava Hillman, Nick Couldry, Elettra Bietti, Gretchen Greene, Response to the European Commission’s \ncommunication to the European Parliament, the Council, the European Economic and Social Committee and the \nCommittee of the Regions on Artificial Intelligence (White Paper COM 2020-65).  \n19', 'In order to tackle these problems, we recommend the Commission to: \n•  Ensure full compliance with the GDPR and strong application of key principles such \nas, transparency, data minimisation and purpose limitation. \n•  Ensure that individuals can remain in control and exercise their rights when they are \nnot directly interacting with the technology (e.g. when a consumer is walking down \nthe street and is inadvertently captured by a facial recognition system installed in a \nshop or an interactive billboard). \n•  Develop a risk assessment system and enable possibilities for independent testing \nfor accuracy and unfair bias. \n•  Biometric technology should never be deployed without a prior impact assessment \n(not only limited to data protection impact) and consultation with the competent \nsupervisory authority.\u202f\u202f \n•  Clear red lines must be established to limit or, where appropriate, prohibit the use of \nbiometric technology for specific purposes or in specific situations (e.g. to monitor \nchildren in schools) where the risk for people’s rights and freedoms would be too high \nand the impact of this technology would be detrimental to the individual or to society \nas a whole.\u202f \n•  When it comes to consumer identification via biometric data (e.g. to enter a venue for \na concert, to access an online banking account, or to unlock a mobile device) the \nconsumer should, as a general rule and taking into account security risks, be provided \nwith the possibility to choose another identification system instead that does not \nrequire the processing of biometric data. \n•  The use of biometric identification technology, such as facial recognition, should not \nbe ‘normalised’ and widely deployed given the serious data protection and privacy \nimplications and risks of such technology.\u202f\u202f  \nEND \n \n20', 'This publication is part of an activity which has received funding under an operating \ngrant from the European Union’s Consumer Programme (2014-2020). \n \nThe content of this publication represents the views of the author only and it is his/her sole \nresponsibility; it cannot be considered to reflect the views of the European Commission and/or \nthe Consumers, Health, Agriculture and Food Executive Agency or any other body of the \nEuropean Union. The European Commission and the Agency do not accept any responsibility  \nfor use that may be made of the information it contains. \n3']"
F550608,09 September 2020,Claudia Russo,Wirtschaftsverband,"International Association of Scientific, Technical and Medical Publishers (STM)",sehr klein (1 bis 9 Beschäftigte),98356852465-08,Niederlande,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"STM welcomes the ambitions of the European Commission to lead globally in promoting the uptake of Artificial Intelligence whilst ensuring that the highest levels of excellence and trust are respected and delivered to European consumers and businesses.

As already noted in our response to the open consultation on the European Commission’s White Paper, given the extensive legislative framework already provided for by the European acquis, the first approach should be to consider the applicability of existing rules, including but not limited to the areas of IP, privacy (GDPR) and competition law. It may still be too early to tell what gaps there are and therefore if a new type of intervention is needed. 

Moreover, the evolution and application of AI systems are still in their early stages. Innovation is changing the landscape daily so it is difficult to assess what risks will arise or be mitigated. It would be prudent to take a flexible approach, providing effective and transparent criteria that can be applied by innovators and industry to determine risk, under similar principles as Article 22 of GDPR.

For these reasons, a new legislative instrument is likely premature. Instead, the Commission should focus on non-legislative activities. Efforts should concentrate on building on existing initiatives and protocols and coordinating with stakeholders to ensure alignment, minimise burdens and streamline activity. In particular, standards and initiatives should be developed over time by practitioners and communities of use. The Commission should work to channel appropriate funding and support towards those.

To enable the development and deployment of excellent AI systems, it is necessary that those are trained and fed with excellent, high-quality proprietary content, datasets, metadata, curated items or databases. Appropriate incentives and funding to curate pre-existing or purpose-built IP for this purpose should be provided, working with the owners and creators of such content to make sure that systems and tools are built with integrity. The owners and creators of high-quality content should be viewed as key partners with policymakers and system developers to promote the reliability and accuracy of AI systems.

Compliance with and enforcement of existing Union rules on fundamental rights and product safety will also help gain users’ trust, which will bring to a corresponding increase in the demand and use of AI systems. Efforts should also be dedicated to promoting positive, understandable examples of AI applications to build public understanding and support and underpin sustainable long-term government investment in AI.

STM has already contributed to the open consultation on the Commission’s White Paper on AI, and will be looking forward to further engaging with the European Commission and other European institutions."
F550556,09 September 2020,Sylwia GIEPMANS-STEPIEN,Unternehmen/Unternehmensverband,Google,groß (250 oder mehr Beschäftigte),03181945560-59,Belgien,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"Google welcomes the opportunity to provide further input to the European Commission’s deliberations around AI governance. The promise of AI to deliver societal benefits cannot be realised without well-founded public trust in AI’s use. If done well, establishing oversight mechanisms will reassure the public that there are controls in place, while also providing useful clarity and directional guidance for industry — both of which are vital for a thriving and innovative AI ecosystem. The challenge is to ensure that any interventions in the development and use of AI — especially those that are mandatory and impose upfront costs  — are suitably tailored and appropriately balanced so as to have the desired effect with minimal unwanted repercussions.  

The attached document is intended as a companion to Google’s detailed submission to the earlier White Paper consultation (https://www.blog.google/documents/77/Googles_submission_to_EC_AI_consultation_1.pdf), responding to the specific options laid out in the Commission’s AI inception impact assessment published in July. 
","['Consultation on the AI Inception Impact Assessment\xa0\nGoogle’s submission - 9 September, 2020\xa0\n\xa0\xa0\nIntroduction\xa0\n\xa0\nGoogle welcomes the opportunity to provide further input to the European Commission’s\xa0\ndeliberations around AI governance. The promise of AI to deliver societal benefits cannot be\xa0\nrealised without well-founded public trust in AI’s use. If done well, establishing oversight\xa0\nmechanisms will reassure the public that there are controls in place, while also providing useful\xa0\nclarity and directional guidance for industry — both of which are vital for a thriving and\xa0\ninnovative AI ecosystem. The challenge is to ensure that any interventions in the development\xa0\nand use of AI — especially those that are mandatory and impose upfront costs  — are suitably\xa0\ntailored and appropriately balanced so as to have the desired effect with minimal unwanted\xa0\nrepercussions.\xa0\xa0\n\xa0\nThis document is intended as a companion to G\u200b oogle’s detailed submission\u200b to the earlier White\xa0\nPaper consultation, responding to the specific options laid out in the Commission’s A\u200b I inception\xa0\nimpact assessment \u200bpublished in July.\xa0\xa0\n\xa0\nDefinitions\xa0\n\xa0\nThere are several terms in the Inception Impact Assessment for which a question of definition\xa0\narises on which we offer specific  comments:\xa0\n\xa0\n● Definition of AI\u200b — In the discussion of policy option 4, it is noted that a “core question\xa0\nrelates to the scope of the initiative, notably how AI should be defined (narrowly or\xa0\nbroadly) (e.g. machine learning, deep neural networks, symbolic reasoning, expert\xa0\nsystems, automated decision-making)”. There is no answer to this that everyone will\xa0\nagree on, but there are some definitions that will be more appropriate than others.\xa0\nGoogle has already put forward a suggestion in our White Paper submission based on the\xa0\nHigh Level Expert Group definition; an alternative but similar approach that may be more\xa0\nresonant globally is that put forward by the \u200bOECD’s expert group\u200b.\xa0\xa0\n\xa0\nThe definition of AI is crucial not just in terms of determining what is in and out of scope,\xa0\nbut as importantly because of the impact it will have on technology development and\xa0\nadoption by European businesses. For instance, if AI were defined as broadly as\xa0\n‘automated decision making’ and permitted only after undergoing onerous procedural\xa0\nreview, it will slow the uptake of automated decision making of every stripe. In some\xa0\ninstances this may help to eliminate overly risky and potentially harmful uses; but in most\xa0\ncases it risks deterring and delaying European businesses from employing safer, more', 'reliable, and faster AI-based approaches to decisions. This would go against the EU’s\xa0\nstated desire to create an ecosystem of excellence for AI in Europe, as it would put\xa0\nEuropean companies at a competitive disadvantage. Furthermore, how “automated\xa0\ndecision making” is defined (e.g., what counts as a “decision”?; what role does an\xa0\nalgorithm need to play operationally to count as “making” a decision?) and implemented\xa0\n(e.g., for all systems or only newly developed systems?) could also inadvertently bring\xa0\nmany innocuous automation tools and applications into regulatory scope.\xa0\xa0\n\xa0\nIn short, it is critical to strike the right balance in defining the AI applications that are in\xa0\nscope, focusing on those that are more likely to present real risks now and into the future,\xa0\nbut at the same time being careful not to inhibit EU innovation and the economic benefits\xa0\nof various automated systems in an ever more competitive global environment.\xa0\xa0\n\xa0\n● Definition of immaterial harm\u200b — If such a term is to be used in legislation, it will be\xa0\nimportant to provide an exhaustive and specific catalogue of covered harms. The\xa0\ninception impact assessment takes a step towards this by suggesting that “immaterial\xa0\nharm can relate to, for instance, loss of privacy or limitations to the right of freedom of\xa0\nexpression … or unlawful discrimination …  or discrimination in the access to products and\xa0\nservices such as lack of accessibility for persons with disabilities, to name but a few.”  This\xa0\ndefinition, however, is still too open-ended. An alternative approach if a broader framing\xa0\nis desired could be to avoid use of the term “immaterial harm” and instead refer to\xa0\n“significantly restricting the exercise of fundamental rights”. This would have the\xa0\nadvantage of clarity by anchoring the definition in a body of existing law, and would\xa0\nappear (based on the description here) to match directionally with the Commission’s\xa0\nambition.\xa0\xa0\n\xa0\nPolicy options\xa0\n\xa0\nThe Inception Impact Assessment lays out five options for policy development. Our comments\xa0\non each are as follows.\xa0\xa0\n\xa0\nBaseline (no EU policy change, Option ""0""):\u200b \u200bin the absence of EU action on establishing specific\xa0\nrequirements for AI, the risks linked to the latter would remain unaddressed. While EU legislation on the protection of\xa0\nfundamental rights and consumer protection as well as on product safety and liability remains relevant and\xa0\napplicable to a large number of emerging AI applications, problems with enforcement of existing EU law and national\xa0\nliability rules may emerge. There could also be a lack of clarity regarding possible obligations to address the new\xa0\nrisks raised by AI. Given the significant commercial opportunities offered by AI solutions and the pressure to\xa0\nconquer market shares, whether by European or foreign developers interested in the EU market, “untrustworthy” AI\xa0\nsolutions could ensue, with a likely backlash against AI technology as a whole by citizens and businesses. Moreover,\xa0\nas indicated, no EU policy change could lead to increased fragmentation due to interventions at Member States\xa0\nlevel.\xa0\n\xa0\nAs Google’s CEO said in January, we agree that AI is too important not to be regulated — the\xa0\nonly question is how to approach it. There are already many regulations and legal codes that are\xa0\ntechnology neutral in nature, and thus broad enough to apply to AI. Clarifying expectations as to', 'how these existing rules should be interpreted for AI applications would be extremely helpful,\xa0\nboth in providing guidance for industry as well as pinpointing any areas where change is needed.\xa0\xa0\n\xa0\nWhile we do not dispute that current legislation may have some gaps, we disagree with the\xa0\npremise that risks linked to AI can only be addressed by putting in place new bespoke legislation.\xa0\nIn some instances the problem could be simply due to a lack of awareness and poor\xa0\nenforcement, or lack of clarity in applicability of existing rules. If existing rules were highlighted\xa0\nand resources invested to support swifter and appropriately calibrated enforcement, even a\xa0\nbaseline of no policy change could result in substantial ecosystem improvements.\xa0\xa0\n\xa0\nFor example, consider the issue of discrimination and AI. Existing EU legislation1\u200b \u200balready includes\xa0\nthe concept of “indirect discrimination” in a manner that is broad enough to encompass\xa0\ninadvertent discrimination by algorithms, including AI systems. However it would be useful to\xa0\nprovide formal commentary regarding how it should be translated, in particular in terms of what\xa0\nare plausible, reasonable grounds for lawful discrimination in sample contexts, and what counts\xa0\nas satisfactory evidence of non-discrimination in instances of AI systems that are not fully\xa0\nexplanatory.\xa0\xa0\n\xa0\nSimilarly, guidance issued in 2019\u200b2 \u200bby the Medical Device Coordination Group (MDCG) on the\xa0\nqualification and classification of software as a medical device or accessory is broad enough to\xa0\napply to AI systems, although it could be clarified if AI changes anything relating to the\xa0\nrespective classification as described in the guidance.\xa0\n\xa0\nOption 1: EU “soft law” (non-legislative) approach to facilitate and spur industry-led\xa0\nintervention (no EU legislative instrument):\u200b \u200bUnder this option, EU “soft law” would promote industry\xa0\ninitiatives for AI. A large number of AI principles and ethical codes have already been developed by industry actors\xa0\nand other organisations. In the Union, the HLEG developed a set of Ethics guidelines for trustworthy AI with an\xa0\nassessment list aimed at providing practical guidance on how to implement each of the key requirements for AI. The\xa0\n“soft law” approach could build upon existing initiatives and consist of monitoring and reporting on the voluntary\xa0\ncompliance with such initiatives based on self-reporting; encouraging industry-led coordination on a single set of AI\xa0\nprinciples; awareness raising among developers and deployers of AI systems around the existence and utility of\xa0\nexisting initiatives; monitoring and encouraging the development of standards.\xa0\n\xa0\nActive engagement by industry in tackling AI issues is crucial, because those who are at the\xa0\nfrontlines of developing and applying AI are in a prime position to help spot problems and\xa0\npropose workable remedies.  Regulation will never be perfect in covering every eventuality, so it\xa0\nis important that regulatory compliance is not seen as an acceptable excuse for neglecting to\xa0\nstay alert to wider complications. No matter what policy options are pursued, lending support to\xa0\nindustry in establishing norms of responsible behaviour and sharing best practice learnings is\xa0\nworthwhile.\xa0\xa0\n1 Specifically, the Racial Equality and the Goods and Services Directives say that discriminatory decisions that are\xa0\n\u200b\nmediated by an ‘apparently neutral provision, criterion or practice’ but still disproportionately affect certain protected\xa0\ngroups are qualified as indirect discriminations. In an AI context, quite obviously, the algorithm itself constitutes an\xa0\n‘apparently neutral criterion’. Thus, to the extent such criterion is indeed not neutral but perpetuates bias,\xa0\n‘algorithmically mediated discrimination’ will generally fall within the scope of indirect discrimination. For a fuller\xa0\nanalysis of this see Philipp Hacker, Teaching Fairness to Artificial Intelligence: Existing and Novel Strategies against\xa0\nAlgorithmic Discrimination under EU Law (Common Market Law Review 2018, 1143-1185)\xa0\n2 https://ec.europa.eu/docsroom/documents/37581', 'The focus however should be on supporting initiatives that are likely to make a genuine\xa0\ndifference to what happens in practice operationally. In this respect, there is little to be gained by\xa0\nhaving a single set of AI Principles when there is already strong alignment across those that exist.\xa0\nSimilarly, there is little need to incentivise the development of AI standards when there is already\xa0\nso much momentum and progress being made — not only within the confines of established\xa0\nstandard setting bodies like ISO, but also through more informal industry benchmarking\xa0\ncollaborations such as M\u200b LPerf\u200b.\xa0\xa0\n\xa0\nBased on Google’s experience, there are gaps in the understanding of AI issues between\xa0\nindividual expert practitioners whose extensive experience has fine-tuned their intuition for\xa0\nspotting likely problems more so than those newer to the field whose senses are not yet honed.\xa0\nFor this reason Google has made it a priority to share the materials used to train our employees\xa0\nwhen they too were first learning to grapple with challenges posed by designing AI systems that\xa0\nlearn from data (e.g., the 15 hour \u200bMachine Learning Crash Course,\u200b 4 hour \u200b\u200bTesting and Debugging\xa0\nin Machine Learning\u200b \u200bcourse, F\u200b airness in Machine Learning module\u200b and more, available for free at\xa0\nthe L\u200b earn with Google AI site.\u200b The Commission could help in packaging up,  highlighting and\xa0\nbuilding upon such skill-building resources available from Google and others as part of\xa0\npromoting a responsible AI ecosystem.\xa0\xa0\n\xa0\nAnother challenge is the often subtle gap in practical understanding between those with\xa0\nacademic or media-based knowledge of potential issues, and those who have tackled AI issues\xa0\nand tradeoffs firsthand in product development. The Commission can help to bridge these\xa0\ndifferences by level-setting the expectations for ‘due diligence’, and by providing incentives and\xa0\nopportunities for respectful collaboration and cross-pollination of viewpoints. The goal should be\xa0\nto establish clear and concise expectations that reflect the views of a broad set of stakeholders\xa0\nin terms that can be easily understood and efficiently integrated into product development\xa0\nworkflows by AI experts.\xa0\xa0\n\xa0\nOption 2: EU legislative instrument setting up a voluntary labelling scheme\u200b: U\u200b nder this option,\xa0\na EU legislative instrument would establish a voluntary labelling scheme to enable customers to identify AI\xa0\napplications that comply with certain requirements for trustworthy AI. While participation in the labelling scheme\xa0\nwould be voluntary, the economic operators who choose to participate would have to comply with certain EU-wide\xa0\nrequirements (in addition to existing EU legislation) in order to be able to display a quality AI label. The label would\xa0\nfunction as an indication to the market that the labelled AI application is trustworthy. The voluntary labelling scheme\xa0\ncould follow a model similar to, or be inspired by, the assessment list of the Ethical guidelines piloted by the HLEG. A\xa0\nlabel could also be used for issues that go beyond regulated aspects and the respect of fundamental rights.\xa0\n\xa0\nGoogle remains skeptical of the value of a labelling scheme — voluntary or otherwise — as a\xa0\nvehicle for influencing consumer behaviour in their choice of AI application. While there may be\xa0\nsome contexts where it helps (e.g., public sector use of AI, where it could be a requirement for\xa0\nprocurement), in other settings it is likely to be only a minor influence, with most consumers\xa0\nfavouring functionality over labels or following the advice of people they know, be it an expert\xa0\nsuch as their doctor or a friend or colleague.', 'If the Commission decides to proceed with a labelling scheme, we urge caution in the scoping\xa0\nand operational framework imposed, to reduce the burden on SMEs, and ensure there is\xa0\nflexibility to evolve in line with emerging standards.\xa0\xa0\n\xa0\nIn this respect, while the \u200bAssessment List for Trustworthy AI (ALTAI) \u200bdeveloped by the High Level\xa0\nExpert Group (HLEG) is a useful reference and exhaustively covers a number of critical issues, a\xa0\nless strident approach might be more compelling in fostering uptake in key application areas. As\xa0\nit stands, the final version of the ALTAI is over 20 pages long and the pointed yes/no nature of\xa0\nthe questions about actions taken implies that all efforts are appropriate for every context. In\xa0\nreality, there will be much variation across settings as to what is relevant and most effective.\xa0\nRather than basing a labelling scheme on a long and generic checklist, a better approach would\xa0\nbe to work with industry to craft application- and context-specific versions that prioritise the key\xa0\nconsiderations and actions that will be most impactful in each setting.\xa0\xa0\n\xa0\nFor example, rather than a binary set of yes/no questions as in the ALTAI section on “Avoidance\xa0\nof Unfair Bias”, a more helpful prompt for application developers  would be open-ended\xa0\nquestions accompanied by performance benchmarks.  See Table 1 for a (non-exhaustive)\xa0\nillustration of how such questions could be framed to foster a thoughtful and responsible\xa0\napproach. Such an approach would not only provide better directional guidance to those\xa0\nimplementing AI systems, it would also enable richer insight for regulators and consumers of the\xa0\nfactors considered and trade offs associated with a given product or service.\xa0\xa0\n\xa0\nTable 1: Comparison of alternative approaches\xa0\nExtract from ALTAI: questions from section on\xa0 ALTERNATIVE: illustrative (non-exhaustive)\xa0\nAvoidance of Unfair Bias\xa0 open-ended questions and benchmarks for avoiding\xa0\nunfair bias\xa0\xa0\nDid you establish a strategy or a set of procedures\xa0 How have people in different groups been historically\xa0\nto avoid creating or reinforcing unfair bias in the AI\xa0 affected by this kind of product or use case? \u200b\u200bHow will\xa0\nsystem, both regarding the use of input data as well as\xa0 they be affected by this product? C\u200b onsider the product\xa0\nfor the algorithm design?\xa0 at issue, not just how groups have been affected by such\xa0\n\xa0 technologies generally.\xa0\nDid you consider diversity and representativeness\xa0 \xa0\nof end-users and/or subjects in the data?\xa0 Recommended:\u200b No groups have been historically\xa0\n● Did you test for specific target groups or\xa0 negatively affected by this specific kind of product; OR\xa0\nproblematic use cases?\xa0 one or more groups have been negatively affected by this\xa0\n● Did you research and use publicly available\xa0 specific product or use case in the past, but this particular\xa0\ntechnical tools, that are state-of-the-art, to\xa0 product offers clear benefits to these groups and there\xa0\nimprove your understanding of the data,\xa0 has been significant user testing to show this product will\xa0\nmodel and performance?\xa0 be beneficial, and the product team has consulted with\xa0\n● Did you assess and put in place processes to\xa0 relevant internal/external experts.\xa0\xa0\ntest and monitor for potential biases during\xa0 \xa0\nthe entire lifecycle of the AI system (e.g.\xa0 Requires additional work:\u200b One or more groups has been\xa0\nbiases due to possible limitations stemming\xa0 historically negatively affected by this specific kind of\xa0\nfrom the composition of the used data sets\xa0 product, but it is possible that this particular product\xa0\n(lack of diversity, non-representativeness)?\xa0 offers some benefits to these groups. An evaluation is\xa0\n● Where relevant, did you consider diversity\xa0 needed to assess the impact of this product on these\xa0\nand representativeness of end-users and or\xa0 groups, and pre-launch user testing should be conducted.\xa0\xa0\nsubjects in the data?\xa0 \xa0\n\xa0 How does the product perform across different user\xa0\nDid you put in place educational and awareness\xa0 types (\u200be.g., gender, age, skin tone, face/body feature\xa0\ninitiatives\u200b to help AI designers and AI developers be\xa0 shapes, effect of lighting, effect of makeup/clothing,\xa0\nmore aware of the possible bias they can inject in\xa0 language, disabilities)? In determining what performance\xa0\ndesigning and developing the AI system?\xa0 distribution is acceptable, consider factors including:\xa0\n\xa0 what groups may be most impacted by false positives and', 'Did you ensure a mechanism that allows for the\xa0 negatives, performance benchmarks of existing relevant\xa0\nflagging of issues \u200brelated to bias, discrimination or\xa0 products or previous-generation technologies, existing\xa0\npoor performance of the AI system?\xa0 human levels of accuracy and using noise in human\xa0\n● Did you establish clear steps and ways of\xa0 ground truth as an upper bound, expectations from user\xa0\ncommunicating on how and to whom such\xa0 studies, published research on accepted standards and\xa0\nissues can be raised?\xa0 failure rates within given sectors and communities.\xa0\xa0\n● Did you identify the subjects that could\xa0 \xa0\npotentially be (in)directly affected by the AI\xa0 Recommended:\u200b There is a clearly defined distribution that\xa0\nsystem, in addition to the (end-)users and/or\xa0 would be appropriate for variant performance across\xa0\nsubjects?\xa0 groups, based on user testing, noise in human rater\xa0\n\xa0 ground truth, published research, legal requirements, and\xa0\nIs your definition of fairness commonly used and\xa0 other relevant inputs. This product has been tested across\xa0\nimplemented\u200b in any phase of the process of setting\xa0 a diverse set of user groups and meets or is narrower than\xa0\nup the AI system?\xa0 the target performance distribution among groups.\xa0\n● Did you consider other definitions of fairness\xa0 Where appropriate, the performance distribution and/or\xa0\nbefore choosing this one?\xa0 determination process is shared to provide users with\xa0\n● Did you consult with the impacted\xa0 more information and the opportunity to compare with\xa0\ncommunities about the correct definition of\xa0 alternative products.\xa0\xa0\nfairness, i.e. representatives of elderly\xa0 \xa0\npersons or persons with disabilities?\xa0 Requires additional work:\u200b The product performs at a\xa0\n● Did you ensure a quantitative analysis or\xa0 significantly wider distribution than the defined target for\xa0\nmetrics to measure and test the applied\xa0 one or more groups; OR the product has not been\xa0\ndefinition of fairness?\xa0 sufficiently tested for fairness; OR various categories of\xa0\n● Did you establish mechanisms to ensure\xa0 target users have not been considered or identified.\xa0\xa0\nfairness in your AI system?\xa0\n\xa0\n\xa0\nOption 3: EU legislative instrument establishing mandatory requirements for all or certain\xa0\ntypes of AI applications\u200b: \u200bUnder this option, the EU legislative instrument would establish certain mandatory\xa0\nrequirements on issues such as training data, record-keeping about datasets and algorithms, information to be\xa0\nprovided, robustness and accuracy and human oversight.\xa0\n● As a first sub-option, the EU legislative instrument could be limited to a specific category of AI applications\xa0\nonly, notably remote biometric identification systems (e.g. facial recognition). Without prejudice to\xa0\napplicable EU data protection law, the requirements above could be combined with provisions on the\xa0\nspecific circumstances and common safeguards around remote biometric identification only.\xa0\n● As a second sub-option, the EU legislative instrument could be limited to “high-risk” AI applications, which in\xa0\nturn could be identified on the basis of two criteria as set out in the White Paper ( sector and specific\xa0\nuse/impact on rights or safety) or could be otherwise defined.\xa0\n● In a third sub-option, the EU legislative act could cover all AI applications.\xa0\n\xa0\nIn general, while Google is supportive of principled constraints and expectations of due diligence\xa0\nto ensure responsible use of AI, it is important to avoid overly prescriptive requirements. The\xa0\nmore stringent and tightly defined the requirements are, the less flexibility there will be to adapt\xa0\nthem as the technology evolves. The benefits of regulation in reducing real and perceived risks\xa0\nmust also be balanced against the opportunity cost of lost societal benefits from innovation and\xa0\neconomic growth that can result from over-regulation relative to other markets. The goal should\xa0\nbe a properly balanced regulatory approach that builds trust in AI, provides greater clarity for\xa0\ntechnology development and business planning, and maximises the social and economic good\xa0\nfor Europe’s citizens.\xa0\xa0\n\xa0\nGoogle’s thoughts on the specific mandatory requirements proposed are given at length in o\u200b ur\xa0\nsubmission \u200bto the White Paper consultation, with key concerns summarised in Table 2. Therefore\xa0\nwe have limited the rest of our commentary in this section to just the question of scope.', 'Table 2: Key concerns with proposed mandatory requirements\xa0\nKeeping of datasets should not be mandated: \u200bKeeping datasets is likely to conflict with GDPR provisions requiring\xa0\ndeletion of personal data, as well as presenting challenges for copyrighted datasets authorised for only short-term\xa0\naccess. It would destroy the privacy benefits of on-device processing because it would effectively force data to be\xa0\ncollected and stored centrally. It would prevent the use of off-the-shelf, open-source models, since developers will\xa0\ngenerally have no access to the data used to train them.\xa0\nToo much emphasis on training data quality; not enough on testing output: \u200bWith enough expertise and care, it’s\xa0\npossible to create a high-performing model even using biased, low-quality training data. Thus, rather than putting\xa0\nrequirements on training data, it would be better to have requirements based on testing model performance using\xa0\nbenchmark datasets, to make sure that the outputs are within an acceptable range, since it is the model output that\xa0\nultimately determines the real world impact of an AI system.  In addition, the proposed obligations for developers to\xa0\n“ensure datasets are sufficiently representative” conflict with GDPR under which developers are not meant to have\xa0\naccess to sensitive attributes like ethnicity. It is also unclear how to determine what is “sufficient”, especially for\xa0\nproviders of multipurpose AI systems.\xa0\xa0\nAvoid taking a literal approach to “reproducibility”: \u200bThe whitepaper proposes “requirements ensuring that outcomes\xa0\nare reproducible”. A too literal interpretation of reproducibility would be impossible to satisfy, as many AI systems\xa0\nhave randomness built in, which makes it impossible to guarantee you get the identical output every time even if\xa0\nthe input is the same. To be workable, there will need to be scope for broad notions of “predictability at scale” that\xa0\ndo not require exact matching.\xa0\n\xa0\nImposing mandatory requirements on only a bounded and clearly defined set of applications\xa0\nprovides legal certainty, and enables precise targeting of measures where they are most needed.\xa0\nIt provides a framework to which additional applications can be added over time, if evidence of\xa0\nconcrete harms emerge. It also reduces the scale of resourcing needed for enforcement, and a\xa0\nnarrower focus will make it easier to equip teams with the necessary expertise to carry out\xa0\nassessments.\xa0\xa0\n\xa0\n● Regarding sub option 1: G\u200b iven that there is already a robust discussion in civil society\xa0\nregarding the use of facial recognition systems, and it is clearly an area of higher risk,\xa0\nremote biometric identification systems seem a good example of an application to which\xa0\nmandatory requirements and safeguards could be usefully applied.\xa0\n\xa0\n● Regarding sub option 2: C\u200b onceptually, Google supports a risk-based approach to a new\xa0\nregulatory framework, but believes it is important to ensure that any potential regulation\xa0\nis targeted at the right use cases, taking into account the likelihood of harm and not just\xa0\nthe severity of the harm, as well as a nuanced consideration of the opportunity cost (that\xa0\nis, the forgone benefits) of not using AI. We outlined a number of factors to take into\xa0\naccount in our earlier submission. However, in terms of process it may help to frame risk\xa0\nassessment as having several distinct steps in questioning:\xa0\n\xa0\nQ1. What is the inherent risk of applying this technology to this specific problem?\xa0\xa0\nWhile there may be some cases where a certain technology is in itself inherently\xa0\nrisky, more often the primary driver of risk will be derived from the use context.\xa0\n○ E.g., even in a sensitive field such as law enforcement, the risks of facial recognition\xa0\ntechnology will vary by application. For instance, using facial recognition to authenticate', 'officers’ identities carries different risks than using facial recognition to conduct\xa0\nsurveillance and identify criminal suspects.\xa0\xa0\n\xa0\nQ2. How do the attributes of this particular AI system impact overall risk?\xa0\xa0\nSpecific design features and operational constraints — both technological and in\xa0\nterms of business processes — may justify reducing or increasing the overall\xa0\nassessment of risk from its inherent level.\xa0\xa0\n○ E.g., in some use contexts, a  system that has been designed to perform reliably with a\xa0\nsimilar degree of accuracy across demographic groups may be deemed less risky than a\xa0\nsystem that is often more accurate but has less consistent performance across groups.\xa0\xa0\n○ E.g., in some use contexts, a system deployed to operate autonomously may be deemed\xa0\nmore risky than systems with established monitoring procedures and other operational\xa0\nchecks in place.\xa0\xa0\n\xa0\nQ3. What additional mitigations can be put in place to better manage overall risk?\xa0\xa0\nA wide variety of mitigations should be considered, including technical mitigations\xa0\n(e.g., utilising additional data points to improve accuracy), operational mitigations\xa0\n(e.g., minimum confidence thresholds), and environmental mitigations (e.g.,\xa0\nphysical barriers between autonomous systems and human beings).\xa0\xa0\n\xa0\nQ4. What is the overall risk of this application, and is this tolerable when compared to\xa0\nexisting alternatives?\xa0\xa0\nAI systems will never be perfect — but nor is human decision making. Taking into\xa0\naccount the inherent risks (Q1) of the technology and use case, specific attributes\xa0\nof the particular system (Q2), and any additional mitigations that can be put in\xa0\nplace to manage those risks (Q3), how risky is the AI system overall?  Is this\xa0\nremaining risk greater or less than the risks of not using AI, and is it tolerable?\xa0\xa0\n○ E.g., if the overall risk of an AI system is deemed less than the risk inherent in established\xa0\nmethods of carrying out a task, arguably it should be tolerable.\xa0\n○ E.g., in some use contexts, it may be tolerable to accept an AI system that has higher risk\xa0\nand performs more poorly than a human expert carrying out the same task, if there are not\xa0\nenough people willing and able to do the job.\xa0\xa0\n\xa0\n● Regarding sub-option 3: \u200bMaking no distinction between higher and lower risk AI\xa0\napplications makes it far harder to create regulation that is proportionate and effective,\xa0\ngoing against the EU’s better regulation principles. If mandatory requirements are set to a\xa0\nlevel appropriate for higher risk applications, they will be unnecessarily burdensome for\xa0\nthose that are lower risk (likely the vast preponderance of applications) creating\xa0\nadditional costs, delay and friction for European developers of all sizes. This would\xa0\nsignificantly hamper Europe’s ambition to increase the uptake of AI, and needlessly\xa0\ndisincentivise innovation that is lower risk, thus slowing or even preventing development\xa0\nof many socially beneficial applications. At the other extreme, if mandatory requirements\xa0\nare set at a level to avoid imposing overly burdensome requirements on low risk\xa0\napplications, they may not be sufficient for those that are of higher risk.\xa0\xa0\n\xa0\nIn addition, the cost of enforcing all-encompassing regulation would be far higher for\xa0\nregulators, in terms of building the necessary operational infrastructure and equipping an\xa0\narmy of people with skills spanning every possible application context required to assess\xa0\nand regulate a\u200b ll\u200b AI applications. The time it would take, and scale of investment, training', 'and coordination needed to support blanket regulation across all AI applications should\xa0\nnot be underestimated.\xa0\xa0\n\xa0\nOption 4: combination of any of the options above taking into account the different\xa0\nlevels of risk that could be generated by a particular AI application: A\u200b ny of the sub options\xa0\nabove can be combined with industry-led intervention or not. In one scenario (industry-led intervention or\xa0\nco-regulation), the legislative instrument would primarily consist in high-level principles and obligations to be\xa0\ncomplemented by industry-led norms such as in the form of standards or codes of conduct. In another scenario\xa0\n(detailed regulatory framework), the legislative instrument would establish a regulatory framework with a higher\xa0\ndegree of detail and specificity, possibly to be achieved also with implementing powers of the Commission.\xa0\xa0\n\xa0\nAllowing a combination of approaches (ranging from the baseline of option 0 through to option\xa0\n3) seems a commonsense way to ensure proportionality, offering the greatest flexibility to tailor\xa0\nhow potential harms are addressed without making overly expansive and burdensome demands\xa0\non applications that pose little risk. This approach would also provide a sensible foundation for\xa0\nevolution in light of practical learnings that arise and technology developments, and provide due\xa0\nconsideration for real-world resourcing limitations.\xa0\xa0\n\xa0\nEnforcement mechanisms \n\xa0\nThe Inception Impact Assessment notes that for any of the policy options:\xa0\xa0\n \u200b“...the EU legislative instrument should include enforcement mechanisms to ensure effective compliance with any\xa0\napplicable requirements, in particular existing and future requirements under the EU acquis. Such mechanisms could\xa0\nbe ex-ante or/and ex-post. Ex-ante mechanisms could consist of conformity/safety assessment procedures that are\xa0\naligned with the procedures that already exist in the product safety legislation. For AI applications where no\xa0\nmechanisms exist, new ex-ante conformity/safety assessment procedures may need to be established. Moreover,\xa0\nexisting ex-ante and ex-post enforcement structures would need to be competent and fully equipped to fulfil their\xa0\nmandate where AI tools are used. Ensuring this competence may include requirements for adequate funding,\xa0\ncapacities, competences and mechanisms to work together”\xa0\n\xa0\nGoogle welcomes the acknowledgment of the importance of aligning any ex-ante assessment\xa0\nwith existing procedures, and ensuring that enforcement structures are suitably resourced and\xa0\ncompetent to carry out their role. The scoping of any new rules (or heightened enforcement of\xa0\nexisting ones) will determine what is necessary, and so it is vital that a pragmatic approach is\xa0\ntaken. Any new rules should take into account the implementation challenges for both regulators\xa0\nand those regulated alike.\xa0\xa0\n\xa0\nJust as there is a need for proportionality in the nature of requirements, so too is there a need\xa0\nfor similar balancing in enforcement mechanisms, including leveraging self-assessment\xa0\nprocedures where appropriate. Except in fields where upfront third party reviews are already\xa0\nestablished practice, Google recommends an approach of ex post enforcement if problems\xa0\narise, coupled with clear guidance as to “due diligence” processes and expected performance\xa0\nstandards that providers could self-assess against upfront.\xa0\xa0\n\xa0\nFor example, if there were differing requirements depending on the risk classification for an\xa0\napplication, there needs to be an upfront decision made as to what is the risk classification for', 'any given application. A practical approach would be for regulators to provide detailed\xa0\ntemplates and guidance on how to carry out and document the risk assessment, but delegate\xa0\nresponsibility to those using the AI system to conduct it. Whether carried out by internal expert\xa0\nteams with a variety of backgrounds (technical, ethical, legal etc) or by enlisting trusted third\xa0\nparty consultants, the resultant documentation would provide evidence of its satisfactory\xa0\ncompletion, and could be available to view on demand by regulators, or even be filed\xa0\nconfidentially with a certification body if more routine transparency is desired.\xa0\xa0\n\xa0\nWhile an application-specific assessment will make sense for individual high risk applications, if\xa0\nregulation is scoped more broadly, a more scalable approach may be to focus instead on\xa0\ncertifying a company’s internal governance processes rather than every single use of AI in their\xa0\noperations. For example, companies could conduct periodic audits of their governance\xa0\nprocesses using independent expert auditors who are professionally qualified and entrusted to\xa0\nonly certify organisations who meet the appropriate standards.\xa0 \xa0\n\xa0\n*******\xa0\nGoogle welcomes the Commission’s continued outreach and momentum in establishing an AI\xa0\ngovernance framework in Europe. Without a foundation of trust, the opportunities that AI offers\xa0\nwill not be fully realised. We remain committed to engaging constructively and sharing learnings\xa0\nfrom our internal governance process, which is central to ensuring our responsible use of AI and\xa0\ninstilling trust in our services. We look forward to providing further comments as the\xa0\nCommission’s thinking develops.']"
F550551,09 September 2020,EDRi Policy,NRO (Nichtregierungsorganisation),European Digital Rights (EDRi),klein (10 bis 49 Beschäftigte),16311905144-06,Belgien,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"Aims:
    1. EDRi argues that promoting AI uptake should not be an end in itself, rather a detailed framework to ensure all AI is lawful and fundamental rights compliant. 

Problem definition:
    2. The definition of “remote biometric identification” is overly narrow. All mass surveillance uses of biometrics invoke severe fundamental rights implications, and therefore should be in scope.
    3. Greater attention is needed to the potential impact of AI applications on human rights.
    4. Dataset bias is only one source of AI bias, not overlooking bias in programming of the systems, and the discrimination resulting from deployment of AI systems in already discriminatory contexts, e.g. facial recognition and predictive policing in the context of proven systemic racial discrimination. 
    5. Discrimination is not “immaterial harm”; it can have highly material consequences: threats to liberty and safety when AI is deployed for policing, migration control; safety and financial implications when sensitive identity traits are inferred in the delivery of essential public services; or in employment.
    6. Dual uses of AI pose numerous threats to data protection and fundamental rights and must be addressed. 

Impact assessment
    7. The impact on fundamental rights must be given primary attention over assumed economic impact. All presumed benefits of AI, particularly when invoked in public sector must be demonstrated with scientific criteria. 
    8. Societal level harms such as impact on democracy must be included, with a specific reference to uses of AI for content moderation and the impact on democracy.
    9. AI has an environmental impact in terms of encouraging wasteful development and consumption. When AI systems are proposed as solutions to climate change, evidence must be presented.
    10. Fundamental rights impact must be further detailed in an HRIA, in particular with respect to rights to privacy, freedom of assembly and non-discrimination. The HRIA must articulate how fundamental rights will be de facto protected within the  proposed regulatory framework.

Policy Options:
    11. EDRi recommends that the policy options are amended to include legislation which defines the scope of lawful AI and introduces prohibitions for uses which violate fundamental rights, including untargeted biometric surveillance, predictive policing, migration control and other uses detailed in EDRi’s AI recommendations and case bank of impermissible uses. The prohibition of certain uses of AI is necessary to achieve the stated  aim of preventing harm.
    12. For legal uses of AI, EDRi favours the application of safeguards to all uses of AI (3C) . EDRi recommends a rejection of the risk-based categorisation and the sectoral delineation of ‘high risk’, which will create a gap in fundamental rights protections. Mandatory, external HRIAs, at all stages should evaluate the impact of all AI systems on a case-by-case basis to determine safeguards, with a range of options (including prior conformity assessment, notification to oversight body,  prohibition).
    13. For public sector uses of AI, the EU must introduce mechanisms of democratic oversight. This includes consultation with marginalised communities and people most affected by particular deployments. Democratic oversight must extend to  decision-making power for those affected.
    14. The Commission must take specific steps to ensure enforcement of fundamental rights in upcoming legislation, and individual and collective redress.
    15. EDRi rejects options 0-2. Individuals impacted are not ‘customers’ and often not consumers of AI, but subjects to AI systems. Soft law, industry-led or voluntary schemes will be insufficient to protect their rights.

Find furhter information in EDRi’s recommendations on artificial intelligence regulation and ban on biometric mass surveillance position. EDRi attaches use cases of AI deployments in Europe which require prohibition in law. "
F550523,09 September 2020,Sarah Collen,NRO (Nichtregierungsorganisation),European Association of Urology,mittel (50 bis 249 Beschäftigte),703095536854-37,Belgien,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"The European Association of Urology, a membership organisation of more than 18 000 urologists from all over Europe, sees value in the use of Artificial Intelligence (AI) both in terms of the activities of our professional association (e.g and decision support and development of evidence based clinical guidelines), and in our clinical roles as medical professionals (e.g. in the early detection of cancer, treatment of tumours and other urological conditions, robotic surgery and for use in clinical trials). 

In urology, we already have experience of this important interaction between the uptake / use of novel technology and medical practice in the field of robotic surgery.
 
In the use of AI technologies, medical professionals will often be users and/or deployers of these tools and will often be responsible for the ‘human oversight’ when they are used in care settings. It is therefore essential that these new technologies have checks and balances in place that ensure that they are safe and effective for their intended use, just as they would need to be in place for new medical devices or pharmaceuticals.

Many of the AI tools we use in a clinical setting are likely to fall under ‘high risk’ devices mentioned in option 3b. Indeed, many are likely to have important legal and physical impacts on our patients (supporting decisions on treatments, for example) and the general public (when it comes to decisions regarding risk based screening, for example). These technologies will have an important impact on the medical profession, requiring the need for new skills and training, and a clear legal framework outlining responsibility and liability when things go wrong. Just like any new technology or pharmaceutical, it must be clear what functions any new AI tool has been proven to be effective and safe in delivering. In the case of AI, the limitations of the tool will directly relate to the limitations of the data driving the tool in the first place. The limitations of the data used must be explained in a clear and transparent way.

A mandatory, stand alone, legal standard for high risk AI defined in 3b using a conformity assessment procedure  (mentioned in the White Paper on AI) would give legal clarity on a number of these issues. For medical devices, this may also be achieved by updating of the relevant standards (ieg IEC 82304)  linked to the Medical Devices Regulation. 

In terms of application of such legislation in the health sector,  it must make provision for feedback from experts from healthcare professionals, patients and the general public. The medical world is rightly highly regulated already. It is a sector well used to asking ethical questions. The risks and benefits will need to be defined by all actors, including healthcare professionals and patients, payers and industry.  There will  be interaction with data protection, pharmaceutical and medical device legislation and the boundaries and complementarity must be carefully defined.  It will be essential to understand the human oversight and control/ responsibility over which aspects of the AI device and this information must be clearly described to the user by the manufacturer. 

For this reason, there must be provision for opinions from healthcare professionals and experts imbedded into the AI conformity assessment  process in the health sector. The Medical Devices Regulation, for example,  has a included the creation of Medical Expert Panels through the JRC who will play a role in the assessment process of new medical devices. This model could be considered as a model to be replicated in any proposed AI legislation. 

The new regulatory approach must go hand in hand with a broader package of education, training and skills for medical professionals so that these tools can be safely deployed in the most effective manner, as outlined in the AI White Paper. European medical societies such as the EAU can support these initiatives. ","['The European Association of Urology, a membership organisation of more than 18 000 urologists from \nall over Europe, sees value in the use of Artificial Intelligence (AI) both in terms of the activities of our \nprofessional association (e.g and decision support and development of evidence based clinical \nguidelines), and in our clinical roles as medical professionals (e.g. in the early detection of cancer, \ntreatment of tumours and other urological conditions, robotic surgery and for use in clinical trials).  \n \nIn urology, we already have experience of this important interaction between the uptake / use of novel \ntechnology and medical practice in the field of robotic surgery. \n  \nIn the use of AI technologies, medical professionals will often be users and/or deployers of these tools \nand will often be responsible for the ‘human oversight’ when they are used in care settings. It is \ntherefore essential that these new technologies have checks and balances in place that ensure that they \nare safe and effective for their intended use, just as they would need to be in place for new medical \ndevices or pharmaceuticals. \n \nMany of the AI tools we use in a clinical setting are likely to fall under ‘high risk’ devices mentioned in \noption 3b. Indeed, many are likely to have important legal and physical impacts on our patients \n(supporting decisions on treatments, for example) and the general public (when it comes to decisions \nregarding risk based screening, for example). These technologies will have an important impact on the \nmedical profession, requiring the need for new skills and training, and a clear legal framework outlining \nresponsibility and liability when things go wrong. Just like any new technology or pharmaceutical, it \nmust be clear what functions any new AI tool has been proven to be effective and safe in delivering. In \nthe case of AI, the limitations of the tool will directly relate to the limitations of the data driving the tool \nin the first place. The limitations of the data used must be explained in a clear and transparent way. \n \nA mandatory, stand alone, legal standard for high risk AI defined in 3b using a conformity assessment \nprocedure  (mentioned in the White Paper on AI) would give legal clarity on a number of these issues. \nFor medical devices, this may also be achieved by updating of the relevant standards (ieg IEC 82304)  \nlinked to the Medical Devices Regulation.  \n \nIn terms of application of such legislation in the health sector,  it must make provision for feedback from \nexperts from healthcare professionals, patients and the general public. The medical world is (rightly) \nhighly regulated already. It is a sector well used to asking ethical questions. The risks and benefits will \nneed to be defined by all actors, including healthcare professionals and patients, payers and industry.  \nThere will  be interaction with data protection, pharmaceutical and medical device legislation and the \nboundaries and complementarity must be carefully defined.  It will be essential to understand the \nhuman oversight and control/ responsibility over which aspects of the AI device and this information \nmust be clearly described to the user by the manufacturer.  \n \nFor this reason, there must be provision for opinions from healthcare professionals and experts \nimbedded into the AI conformity assessment  process in the health sector. The Medical Devices', 'Regulation, for example,  has a included the creation of Medical Expert Panels through the JRC who will \nplay a role in the assessment process of new medical devices. This model could be considered as a \nmodel to be replicated in any proposed AI legislation.  \n \nThe new regulatory approach must go hand in hand with a broader package of education, training and \nskills for medical professionals so that these tools can be safely deployed in the most effective manner, \nas outlined in the AI White Paper. European medical societies such as the EAU have a long tradition of \ntraining, disseminating knowledge/skills and developing multi-disciplinary guidelines and look forward \nto engaging with the Commission on next steps to bring benefits for patients.']"
F550505,09 September 2020,Fabian FECHNER,Wirtschaftsverband,Handelsverband Deutschland (HDE),klein (10 bis 49 Beschäftigte),31200871765-41,Deutschland,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"Der HDE begrüßt das ausgeglichene Inception Impact Assessment der Kommission, das die Interessen der betroffenen Wirtschaftsakteure – insbesondere von KMU – ausdrücklich mitberücksichtigt und deren potenzielle Belastung durch etwaige Regulierung dem Nutzen gegenüberstellt. Auch der explizite Fokus auf legislative Kohärenz und Konsistenz in den Bereichen Haftung, Produktsicherheit und Grundrechtsschutz ist positiv zu bewerten. Dieser eingeschlagene Weg muss mit der Folgenabschätzung und dem Gesetzgebungsvorschlag konsequent weiter beschritten werden. Denn: Wir bewegen uns beim Thema KI in einem wettbewerblichen Spannungsfeld und müssen dem Innovationsraum Europa einen Freiraum für technologische Entwicklungen und wirtschaftliches Wachstum bieten, anstatt Fortschritt unbegründet zu erschweren. Das Verständnis von KI sollte unserer Einschätzung deshalb unter einer gestalterischen Prämisse stehen: Wir erschaffen Künstliche Intelligenz; wir gestalten, trainieren und entwickeln die Systeme weiter.

KI ist keine weitere technische Entwicklung, die einer Sonderregulierung bedarf. Es ist vielmehr eine Basis-Innovation, die zahlreiche Geschäftsmodelle verändern und neue ermögli-chen wird. Deshalb ist der HDE überzeugt, dass es keiner gesonderten KI-Gesetzgebung bedarf, sondern der bestehende Rechtsrahmen, wie die DSGVO, das Gesetz gegen unlauteren Wettbewerb (UWG) oder Vorschriften des Allgemeinen Gleichbehandlungsgesetzes (AGG), Verbraucher*innen hinreichend schützt. 

Was in der analogen Wirtschaft mit menschlichen Entscheidungen gilt, sollte auch in der digitalen Wirtschaft mit datenbasierten Entscheidungen mitgedacht werden. Existierende Vorschriften sollten daher überprüft und nur bei nachgewiesenem Bedarf gezielt an die von KI-Systemen ausgelöste Entwicklung angepasst werden. Somit haben wir als HDE eine klare Präferenz für die Regulie-rungsoptionen 1 und 2 aus dem Inception Impact Assessment - bzw. eine Kombination davon. 

Ein freiwilliges Kennzeichnungssystem könnte für die notwendige Transparenz und Orientierung sor-gen, ohne die betroffenen Unternehmen übermäßig zu belasten.  Dabei muss gewährleistet sein, dass vor allem KMU in der Breite extern generierte „KI as a service“ auch praktisch nutzen können, da sie vielfach nicht in der Lage sein werden, KI-Systeme selbst zu entwickeln.

Sollte aus politischen Gründen entgegen der Position des HDE dennoch eine Regelung für erforder-lich gehalten werden, so muss diese anwendungsbezogen und risikobasiert sein. Vor dem Hinter-grund der aufgeführten wirtschaftlichen Auswirkungen sollte sich der Gesetzgeber daher maximal für die Optionen 3a oder 3b entscheiden. Sollte man sich für eine risikoorientierte Regulierung entschei-den, ist es absolut notwendig, dass „KI-Anwendungen mit hohem Risiko“ klar, zukunftsfest und rechtssicher definiert und abgegrenzt werden. Grundsätzlich halten wir den Ansatz, sich bei der Risi-kobewertung vor allem auf den Sektor zu konzentrieren für richtig, allerdings wirft die vorgeschlage-ne Risikobewertung Fragen und Probleme bei der Abgrenzung auf: Ist die Liste der betroffenen Sek-toren abschließend? Ist die Liste der kritischen Verwendungszwecke abschließend? Was genau sind „Auswirkungen auf die Rechte einer natürlichen Person“? Dieser Begriff scheint extrem offen. Wer-den die beiden Faktoren „Sektor“ und „Auswirkung“ immer kumulativ angewendet? Welche weiteren Ausnahmen von der Zwei-Faktor-Bewertung – neben KI bei der Einstellung von Beschäftigten – gibt es noch?

In Bezug auf Optionen 3a (Beschränkung des Rechtsinstruments auf bestimmte KI-Anwendungen wie biometrische Identifizierungssysteme) möchten wir zudem zu bedenken geben, dass bestimmte biometrische Identifikationssysteme zu maßgeblichen Innovationen im Handel beitragen, wie z.B. das Bezahlen per Fingerabdruck. Diese sollten im Sinne der Innovationsförderung sowie der Erleich-terung von Prozessen für den Kunden und Anbieter unter bestimmten Bedingungen weiterhin mög-lich sein. 
","['brief statement\non the EU legislative initiative ""Artificial Intelligence -\nethical and legal requirements”\n \n \n \n \nAs of September 9, 2020\n[EU Transparency Register number: 31200871765-41]', '1. Background\nThe German Retail Association (HDE) would like to thank you for the opportunity to comment on the EU\nInitiative on the subject of ""Artificial Intelligence - ethical and legal requirements"" as part of the\nroadmap published by the European Commission on 23 July 2020.\nWith the planned initiative, the EU Commission wants to guarantee that artificial intelligence\n(KI) ""is safe, lawful and in line with fundamental rights in the EU"". while AI\nis a rapidly developing and strategically important technology that offers enormous opportunities\nHowever, if some possible uses were “specific, significant risks for the application\nvarious EU regulations to protect fundamental rights and ensure security\nand solving liability issues"". According to the Commission, the overarching goal is\npromote the use of trustworthy AI in the EU.\n2nd position of the HDE\nArtificial intelligence is no longer a phenomenon of science fiction: the commercial\nset of AI is clearly emerging and will continue to advance rapidly. A promising one\nThe field of application for AI technologies is retail - both online trade and retail\nstationary trade. Because due to their interface function, traders are in the complex\nadorned the network of relationships between customers, manufacturers, logisticians and platforms. around in\nIn order to survive the competition, it is important to identify customer needs in the best possible way and as efficiently as possible\nand to fulfill them precisely – i.e. the right decisions for the ideal integration of the actors\nhold true. AI systems can perform highly complex tasks associated with large amounts of data.\nprocesses in real time and generate an optimal solution that meets the requirements.\nThe customer in the digitized world demands tailor-made offers, positions himself confidently in\nthe center and wants to experience its individual retail world. This is how 65% of Germans are above all\nLoyal to providers who tailor their offer to the needs and preferences of the customer.\nTo meet this demand, 45% of retailers are planning artificial\nuse common intelligence. Accordingly, AI is a success factor for digitization and for future ones\ntrade business models.\nThe areas of application of AI in retail are diverse: Intelligent systems can\naccompany the tenth business process from the head office via logistics to the branch and customer experience.\nStandard examples here are inventory management with the help of intelligent systems in the head office\n(regardless of whether dealers choose online or stationary sales channels), smart tour planning\nin logistics, the digital fitting room at the point of sale to the visual product search in the customer\nthe contact.\nThe German Retail Association is convinced: Intelligent applications are an opportunity for the\nsustainable trade. In addition, we expressly support the Commission\'s objective of\n   \n \n \nGerman Trade Association (HDE) e. v. page 2\nFabian Fechner fechner.europa@hde.de | +32 (0)2 737 03 76 | Brussels\nDara Kossok Spit | kossok-spiess@hde.de | +49 (0) 30 72 62 50-33 | Berlin www.einzelhandel.de', 'Promote the use of trustworthy AI in the EU economy. Here lies the key to success\nof this technology on our continent.\na) Think artificial intelligence instead of regulating individual cases\nArtificial intelligence is not another technical development that requires special regulation.\nRather, it is a basic innovation that changes numerous business models and enables new\nwill. The HDE is therefore convinced that there is no need for separate AI legislation\nbut the existing legal framework, such as the GDPR, the law against unfair competition\n(UWG) or provisions of the General Equal Treatment Act (AGG), consumers\nsufficiently protects.\nWhat applies to human decisions in the analogue economy should also apply in the digital one\neconomy with data-based decisions. Existing regulations should\ntherefore checked and only targeted to the development triggered by AI systems if there is a proven need\nwinding can be adjusted. As a result, we as HDE have a clear preference for the regulatory op-\ntion 1 and 2 from the Inception Impact Assessment - or a combination thereof.\nA voluntary labeling system could provide the necessary transparency and orientation\nwithout placing an excessive burden on the companies concerned. It must be ensured that\nSMEs in particular can also practically use externally generated ""AI as a service"" on a large scale, since they\noften will not be able to develop AI systems themselves.\nIf, for political reasons, contrary to the position of the HDE, a regulation for required\nbe kept, it must be application-related and risk-based. in front of the\nDue to the economic effects listed, the legislature should therefore maximally for\nchoose options 3a or 3b. Should one decide in favor of risk-oriented regulation\nden, it is absolutely necessary that ""high-risk AI applications"" are clear, future-proof and legally\nbe clearly defined and delimited. Basically, we stick to the approach of dealing with risk\nassessment to focus primarily on the sector is correct, however, the proposed\nrisk assessment questions and problems with the demarcation: Is the list of affected sectors\nfinally? Is the list of critical uses exhaustive? What exactly are\neffects on the rights of a natural person”? This term seems extremely open. Will the\nboth factors ""sector"" and ""impact"" always applied cumulatively? What other exceptions\nThere are still two-factor assessments – in addition to AI when hiring employees –?\nRegarding option 3a (limiting the legal instrument to certain AI applications such as\nbiometric identification systems) we would also like to point out that certain bio-\nmetric identification systems contribute to significant innovations in retail, such as the\nPay by fingerprint. These should be in the spirit of promoting innovation and facilitating\nof processes for the customer and provider may still be possible under certain conditions.\n \n   \n \n \nGerman Trade Association (HDE) e. V. page 3\nFabian Fechner fechner.europa@hde.de | +32 (0)2 737 03 76 | Brussels\nDara Kossok Spit | kossok-spiess@hde.de | +49 (0) 30 72 62 50-33 | Berlin www.einzelhandel.de', 'b) AI is not just AI\nIn order to enable an objective debate, however, we need not only the risk-based approach\nalso a distinction of intelligent systems in weak and strong systems. weak AI system\nteme support people in decision-making in a certain area, e.g. in\nSpeech or image recognition systems, while strong AI systems and a so-called super-intelligent\nability to imitate human intelligence – including the ability to combine knowledge in a variety of\nto be applied across departments. So far, no such strong AI systems exist.\nNevertheless, much of the discussion about ""digital ethics"" and ""ethical AI"" is\nunderground to the dystopian effects of a superintelligence.\nJust as it is grossly negligent, the successes of algorithms in the commercial area just like that\nTransferring it to other problem areas such as human actions inhibits innovation\nand out of place from regulation for sensitive social processes to simple recommendations\nclose algorithms and pattern recognition.\nPersonalized product recommendations, special offers and discounts take into account individual\nThe wishes and needs of the customers and can thus play out relevant offers. This is a\nAdded value for consumers that we see in the complexity and amount of information in the\nInternets, should cherish and protect. Intelligent product presentation and website design\nrelevant content for the customer in a clear manner, e.g. by searching for product reviews\nTopics are filtered\nc) Without (high-quality) data, no AI\nSmart applications are an opportunity for trading both online and stationary with tech\nto convince at the point of sale. Artificial intelligence generates insights through pattern recognition\nbased on large amounts of data (big data) and self-taught algorithms. With increasing\nThe amount of training data increases the accuracy of the conclusions and forecasts. important here-\nIn addition to the amount of data required for this, sufficient data quality with which the AI \u200b\u200bsystem\nto be trained.\nWe therefore share the Commission\'s assessment that damage caused by AI is mainly caused by the\nuse of poor quality or distorted data can be caused and see\nthere is also the danger that incorrectly programmed AI or AI trained on the basis of distorting data\nsystems can have a discriminatory and stigmatizing effect. About artificial intelligence\nto use and in particular to further develop data economy in the European area\nto be lived. Data protection is a valuable asset, which is important for consumers and retailers in\nof the EU must be preserved. However, this European data protection must be as coherent as possible\nand allow a legitimate interest of the dealer.\nGreat market power is already based on its data power, because data is the raw material of digital\nBusiness models and training devices for AI. In global competition, data protection must therefore not\n   \n \n \nGerman Trade Association (HDE) e. V. page 4\nFabian Fechner fechner.europa@hde.de | +32 (0)2 737 03 76 | Brussels\nDara Kossok Spit | kossok-spiess@hde.de | +49 (0) 30 72 62 50-33 | Berlin www.einzelhandel.de', ""become a competitive disadvantage for European companies. For example, a deletion\nmandatory for training data generated by means of text and data mining, as is currently the case in the implementation\nof the European Copyright Directive into German law is being discussed, an obstacle for the\n(Further) development and application of AI systems.\nWhile we are building an EU data space as a prerequisite for the success of AI policy\ntherefore expressly welcome, we would like to note: A fundamental obligation to data\nshare, also includes newly generated data from our own databases. This know-how should\nprotected and not disclosed in order to foster innovation in the EU. We therefore see primarily\na need to exchange data provided by public bodies\nwill.\nd) Protection of trade secrets\nSustainable retail operates on all channels: Whether online or offline, algorithms design mo-\nother trading companies with: They enable the product range to be adapted to the individual\nvidual needs and wishes of the customers, allow an assessment of the risk of non-payment\nsikos and optimize sales forecasts and delivery routes. This means that algo-\nrithms has become an important differentiator.\nRegulatory inconsistencies can lead to competitive disadvantages for European retailers\nmutate when different legal standards are applied to offline, online and smart, i.e. AI-operated\nben, to be laid. On the one hand, the existing legal framework offers consumers an appropriate\nhis protection. On the other hand, in the digital world we cannot prevent the disclosure of business secrets\nsen that would be protected in the offline world.\nBecause there is a risk of restrictions and distortions of competition if the core content of Algorithm-\nmen would have to be disclosed. Anyone who has to disclose this loses the incentive to continue and\nNew developments and thus the connection to the global competition. In addition, the efficiency\nbe doubted by such a review, since algorithms are often complex and change frequently\nand random moves included. We therefore favor a principle-based approach, the ethical one\nPrinciples of a fair use of algorithms as defined by the independent expert group\nfor artificial intelligence of the EU Commission in the ethical guidelines for AI. we\nwould like to call on the Commission to take these considerations into account when designing any enforcement\nmechanisms to consider.\n3. Conclusion: value-based innovation\nThe HDE welcomes the Commission's balanced Inception Impact Assessment, which\ns of the affected economic actors - in particular SMEs - are expressly taken into account and\nwhose potential burden from any regulation is compared with the benefit. Also the explicit\ncite focus on legislative coherence and consistency in the areas of liability, product safety and\n   \n \n \nGerman Trade Association (HDE) e. V. page 5\nFabian Fechner fechner.europa@hde.de | +32 (0)2 737 03 76 | Brussels\nDara Kossok Spit | kossok-spiess@hde.de | +49 (0) 30 72 62 50-33 | Berlin www.einzelhandel.de"", ""Protection of fundamental rights is to be rated positively. This chosen path must be combined with the impact assessment\ntion and the legislative proposal are to be consistently pursued further.\nBecause: When it comes to AI, we are in a competitive field of tension and we have to\nInnovation area Europe a free space for technological developments and economic\nProvide growth instead of making progress unjustifiably difficult. The understanding of AI should\nIn our opinion, therefore, we are subject to one design premise: we create artificial ones\nIntelligence; we design, train and further develop the systems.\nAlgorithms are just tools. They don't have prejudices, but humans do. And they are\nprecisely these people who interpret the results provided by algorithms and above\ndecide how to deal with the errors of algorithms. For these human decisions\nfar-reaching legal regulations have already been made in the analogue world, which also\napply in the digital world. We need to think along with the digital world instead of special digital regulations\nments to promote innovation in the future as well as to absorb unwanted by-products\nticipate.\n  \n   \n \n \nGerman Trade Association (HDE) e. V. page 6\nFabian Fechner fechner.europa@hde.de | +32 (0)2 737 03 76 | Brussels\nDara Kossok Spit | kossok-spiess@hde.de | +49 (0) 30 72 62 50-33 | Berlin www.einzelhandel.de""]"
F550504,09 September 2020,Caio Lobo,Wirtschaftsverband,CECE - Committee for European Construction Equipment,sehr klein (1 bis 9 Beschäftigte),60534525900-25,Belgien,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"CECE welcomes the opportunity to comment on the European Commission’s Inception Impact Assessment on a proposal for a legal act laying down requirements for Artificial Intelligence. Regarding the different policy options proposed, we would like to share the following comments:

- POLICY OPTION 1
For the construction machinery sector, we do not see any benefits of additional requirements for AI-enhanced subsystems in our products, since this is already covered by the Machinery Directive 2006/42/EC; we therefore prefer option 1.
 
For the products of our industry – mobile machinery of all sizes – the Machinery Directive 2006/42/EC, having technology neutrality as one of its core principles, already covers safety for those machines using AI-enhancements for operation.
 
According to the Machinery Directive 2006/42/EC, the manufacturer has to take the necessary measures to eliminate any risk throughout the foreseeable lifetime of the machinery, which includes the phases of transport, assembly, dismantling, disabling and scrapping. This includes any machine learning capabilities.
 
- POLICY OPTION 2
For the products under the scope of the Machinery Directive 2006/42/EC, the CE marking already indicates that the risks associated with state-of-the-art AI-enhancements have been sufficiently mitigated, therefore additional labelling is not necessary.
 
- POLICY OPTION 3A + 3B
We reserve our position on option 3 depending on the criteria to be defined for the category of ‘high-risk’ applications. We expect stakeholders (especially the industry) to be  thoroughly consulted when those criteria are defined.
 
- POLICY OPTION 3C
A legislation covering all types of AI use poses the threat of hampering innovation and putting unjustified burden on many narrow AI-enhanced products; where the AI-enhanced function may have tight operational limits, operates in a low-risk application, or is subject to close human oversight during its entire learning phase.
 
- POLICY OPTION 4
No specific statement for policy option 4."
F550483,09 September 2020,Aleksandra Appelfeld,Unternehmen/Unternehmensverband,Philips,groß (250 oder mehr Beschäftigte),035366013790-68,Niederlande,Künstliche Intelligenz – ethische und rechtliche Anforderungen,Philips welcomes the opportunity to provide feedback on the roadmap for AI ethical and legal requirements. Please see our feedback in the attachment.,"['Response to EC roadmap:  Artificial intelligence – ethical and legal requirements \nPhilips  welcomes  the  opportunity  to  provide  feedback  on  the  roadmap  for  AI  ethical  and  legal \nrequirements.  \nAs rightly pointed in the roadmap and the AI White Paper, AI is developing fast and has the potential to \nimprove and reshape healthcare. Applying AI in healthcare can improve people’s lives across the health \ncontinuum from personalized lifestyle support through targeted prevention of disease, earlier precision \ndiagnosis and personalized therapy to monitoring patients proactively from the hospital to the home. At \nPhilips, we embrace AI as a combination of data science technology with knowledge of the clinical, \noperational and personal context of a patient. As a result, AI adapts to patient-specific contexts and can \nbe embedded into workflows or people’s daily environment. It augments healthcare professionals, rather \nthan replacing them. \n- Baseline (Option 0): no policy change \nAs indicated in the roadmap, ‘EU legislation on the protection of fundamental rights and consumer \nprotection as well as on product safety and liability remains relevant and applicable to a large number of \nemerging AI applications’. For instance, the EU Medical Devices Regulation (EU MDR) in combination with \nthe General Data Protection Regulation (GDPR) already contain requirements for AI in healthcare to be \nsafe and performant. These requirements, both ex-ante and ex-post, ensure that medical devices based \non AI are safe and performant throughout their entire lifecycle, including the management of changes in \nsoftware. We therefore see no need for new regulatory frameworks for AI-based devices in healthcare. In \norder to facilitate the practical implementation of the existing legal requirements, we support the \nadoption of practical guidance, preferably accompanied by the development of international standards.   \nFor a detailed analysis of AI in the EU Medical Device legislation, we refer to a recent paper developed by \nCOCIR1. \nProposed alternative policy options \n-  Option 1: EU soft law \nPhilips supports an ‘ethics by design’ approach to the development and use of AI systems. It is important \nfor EU citizens to be able to trust AI, and that recommendations or decision-making support by AI is \ntransparent to those relying on it for healthcare decisions. Technology should be inclusive and respectful \nof everyone. Ethics and principles should not come up after a crisis or breach of trust: they should be part \nof an organization’s DNA.  \nWe therefore support the EU Ethical Guidelines and sectoral recommendations developed by the AI High \nLevel Expert Group (AI HLEG) and their voluntary adoption per industry sector. We also support the \ndevelopment of self-regulating codes of conduct for responsible application of AI. An EU ethical approach \nto AI is key to enabling responsible competitiveness, as it will generate societal and user trust, and facilitate \nbroader uptake of AI.  \n \n                                                           \n1 https://www.cocir.org/media-centre/publications/article/cocir-analysis-on-ai-in-medical-device-legislation-\nseptember-2020.html  \n1', 'Response to EC roadmap:  Artificial intelligence – ethical and legal requirements \n-  Option 2: EU legislative instrument setting up a voluntary labelling scheme \nAn EU legislative instrument establishing a voluntary labelling scheme could enable consumers and \npatients to identify AI applications that comply with certain requirements for trustworthy AI and serve as \nan indication to the market that the labelled AI application is trustworthy. However, when creating labeling \nschemes, care shall be taken that the rules and requirements for such schemes do not overlap with or \nduplicate already existing mandatory requirements, and such labels are clearly used as an additional sign \nindicating compliance with additional requirements. It should be clear whom would such schemes apply \nto. Furthermore, such legislative instrument shall also arrange for appropriate, fair and efficient standard \nsetting process and enforcement infrastructure. However, in case the enforcement infrastructure is \nmissing or insufficient, there is a risk of unfair use of labelling schemes.  \n-  Option 3: EU legislative instrument establishing mandatory requirements for all or certain types \nof AI applications \nAI applications will need to comply with various regulations as explained above in the Baseline scenario. \nThis is particularly true for AI applications that qualify as a medical device, and which are regulated by and \nneed to comply with the [strict] Medical Devices Regulation and CE marking framework. If AI applications \nprocess personal data, they also need to comply with the GDPR.  \nIn the context of this proposed option, we would like to reiterate the points made above, as well as, in our \nfeedback to the Commission’s public consultation on the White Paper on Artificial Intelligence: \n\uf0b7  Healthcare is a well-regulated industry. AI solutions may be separate products (e.g. stand-alone \nsoftware) or be built in medical devices. When they constitute a medical device, Medical Device \nRegulation will apply.   \n \n\uf0b7  Regulatory approach in healthcare is already risk-based, particularly when it comes to the medical \ndevices framework. Risk classification and ex-ante conformity assessment (also during the entire \nlifecycle of the device) are the fundamental principles and are even reinforced under the upcoming \nMedical Devices Regulation. These rules will apply to software (with embedded AI) as medical \ndevice. GDPR may also apply to AI (i.e. any processing of data through algorithms) and requires to \nensure compliance with its principles. \n \n\uf0b7  MDR is fully adequate for ‘locked AI’ applications that do not learn in the field and those AI \napplications which change within pre-defined boundaries, and for which a conformity assessment \nwas carried out. However, the MDR does not allow manufacturers to place devices on the market \ncomprising AI that changes outside of pre-defined boundaries, i.e. AI-based devices intended to \nchange outside of the change envelope or to suggest claims, intended uses or use conditions to \nthe device for which no conformity assessment was carried out2. \n \n                                                           \n2 For a detailed analysis and examples please consult the paper developed by COCIR : \nhttps://www.cocir.org/media-centre/publications/article/cocir-analysis-on-ai-in-medical-device-legislation-\nseptember-2020.html  \n2', 'Response to EC roadmap:  Artificial intelligence – ethical and legal requirements \n\uf0b7  With regard to sub-option 2 in option 3: when considering the use of categories like ’high-risk AI’, \nthere shall be clarity on the correlation (or lack of it) between the MDR risk classes and the \ndefinition of ‘high-risk AI’. Under the MDR, the approach to software as medical device is risk-\nbased according to the intended use of the device i.e. it already requires a risk assessment and \nidentification of the device’s risk level. When determining the risk level, it should therefore be \ntaken into account if an assessment has already taken place as part of a conformity assessment \nunder the MDR. For instance, a medical device, classified as an AI application as proposed in the \nCommission’s AI White Paper, currently may possibly fall into the MDR class 1, 2a, 2b or 3. None \nof these classes shall render the medical device to be considered as a ‘high-risk AI application’, but \nthe manufacturer should have the possibility to clarify as to whether such system is to be regarded \nas high-risk AI system or not. It is important to acknowledge that not every use of AI in healthcare \nnecessarily involves significant risks. \n \n\uf0b7  Principles of the Product Liability Directive have been proven in last decades and fit for use in \ndiverse situations. Product liability is reinforced under the MDR (Article 10.16). There is therefore \nno need for specific AI-based product liability regime. \n \n\uf0b7  Under the GDPR, the principle of accountability requires to ensure compliance and ability to \ndemonstrate it. Already now, under current legislation, all potential risks that the use or creation \nof algorithms can pose to the rights and freedoms of persons must be considered and properly \naddressed. \nIn light of the above, the existing regulations generally cover AI in healthcare in an appropriate manner. In \nany event, in case of still introducing AI specific legislation, it should be done very carefully not to create \nconflicts or duplications between the various regulations and new barriers for the development of AI-\nsupported medical devices. Therefore, any potential gaps should be clearly identified, assessed and, if \nneeded, addressed, always taking into account the existing regulations to ensure legal consistency and \ncertainty. Instead of a one-size-fits all, generic AI legislation with high level principles, sector-specific \nlegislation would be preferred, ensuring that knowledgeable experts identify real gaps in their own field \nand address them in a way that corresponds to the specific industry’s needs, way of working and base of \nstandards. In this regard, it is welcome to combine this option with the conformity assessment process as \nopposed to a completely new set of standards and enforcement bodies. However, in such approach care \nshall be taken that all relevant sectors are addressed (e.g. software/applications, which are not medical \ndevices, and not even subject to CE marking) and not unfairly only those being in the focus of attention. \n-  Option 4: combination of any of the options above \nThe preferred combination would include the existing legislative framework, i.e. EU MDR and GDPR \naccompanied with practical guidance, international standards and soft law, including self-regulating \nsectoral codes. \nDefinition of AI \nWe embrace the definition of AI as proposed by the AI HLEG3:  \n                                                           \n3 file:///C:/Users/320076840/Downloads/AIDefinitionpdf%20(4).pdf  \n3', ""Response to EC roadmap:  Artificial intelligence – ethical and legal requirements \n‘Artificial intelligence (AI) systems are software (and possibly also hardware) systems designed by humans  \nthat, given a complex goal, act in the physical or digital dimension by perceiving their environment through \ndata acquisition, interpreting the collected structured or unstructured data, reasoning on the knowledge, \nor processing the information, derived from this data and deciding the best action(s) to take to achieve \nthe given goal. AI systems can either use symbolic rules or learn a numeric model, and they can also adapt \ntheir behaviour by analysing how the environment is affected by their previous actions. As a scientific \ndiscipline, AI includes several approaches and techniques, such as machine learning (of which deep \nlearning and reinforcement learning are specific examples), machine reasoning (which includes planning, \nscheduling, knowledge representation and reasoning, search, and optimization), and robotics (which \nincludes control, perception, sensors and actuators, as well as the integration of all other techniques into \ncyber-physical systems).’ \nPhilips offers its support to the European Commission in further refining of the AI policy options and \nachieving the EU’s ambition to become a globally significant player in AI.  \n*** \nRoyal Philips (NYSE: PHG, AEX: PHIA) is a leading health technology company focused on improving people's \nhealth and enabling better outcomes across the health continuum from healthy living and prevention, to \ndiagnosis,  treatment  and  home  care. Philips  leverages  advanced  technology  and  deep clinical and \nconsumer insights to deliver integrated solutions. Headquartered in the Netherlands, the company is a \nleader in diagnostic imaging, image-guided therapy, patient monitoring and health informatics, as well as \nin consumer health and home care. Philips generated 2019 sales of EUR 19.5 billion and employs \napproximately 80,000 employees with sales and services in more than 100 countries. \n4""]"
F550477,09 September 2020,-,-,-,-,-,-,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"AI is a fast growing technology that can and will be very useful to predict and optimise resources. The City of Stockholm welcomes the EUs ambition to ensure an appropriate ethical and legal framework based on the Unions values. It is however necessary to ensure that the AI we use is safe, lawful and in line with EUs existing and fundamental rights. AI must be trustworthy eg regarding protection of personal data, non-discrimination, product safety and liability. 

A changing world brings with it new challenges; globalization, digitalisation, the need for sustainable development, skills supply and a strong demographic development with demands for a rapid expansion of the city's infrastructure, housing and welfare sector and an aging population. The expectations on public authorities are high; citizens expect the same level of service that the private sector can offer. Consequently local authorities need to constantly improve and streamline services and working methods. AI and automation offers new tools to tackle the challenges we face;eg it has the opportunity to acquire and use new information at a pace that humans cannot thus helping us in keeping up with the development of knowledge. 

Development and usage of AI brings about new challenges linked to new digital technology. A number of ethical and legal issues related to AI need to be addressed; in the absence of coherent EU guidance, so-called ""untrustworthy"" AI could arise because there is strong commercial pressure on the issue. The City of Stockholm supports the Commissions review or risks connected to AI and that these are also communicated. Some concerns/potential risks regarding AI are for example:

•	Non-transparent decision-making
•	Different types of discrimination (gender, race)
•	Invasion of citizens’ privacy 
•	Usage of AI for criminal purposes

There is also a need to be observant of the social aspects; there may be an increased risk of loneliness and isolation if AI-based, automated welfare services are used to replace the service that currently are performed with the help of personal contact.

There is a strong global competition within the field of digitalisation and AI. The EU must act as one, based on European ethical values, to promote AI development in Europe. In this context it is important to emphasize the need to find viable ways for introducing and using secure AI that contribute to local, regional, national and European development, in an ethically and legally sustainable manner.

In the development of regulations for AI there is a need to clearly specify which types of legal requirements that are mandatory for the different authorities, organisations and companies(etc) concerned, especially with regard to high-risk applications and quality assurance of AI systems.

New technology such as AI is dependent on the good management of data; the amount, the quality and the availability of data is key to the successful implementation of AI. Open data needs to be correct, consequent and coherent. AI systems and algorithms can hide bias which might lead to high risks for people’s safety. Safety and reliability tests of AI systems are burdensome for local public authorities. There is need for the EU to enforce obligatory measure for companies giving them obligations to report which algorithms are used in systems which they deliver. 

The Commission states that different options for AI risk management will be evaluated. ""Soft legislation"" would mean building on ethical principles that have already been developed by the EU through the HLEG (High-Level Expert Group on Artificial Intelligence) and by various industries and organizations. “Hard legislation” would mean significantly greater requirements for documentation and follow-up by both supplier and customer. The City of Stockholm would therefore like to urge the Commission to make an assessment of how costs incurred for various actors can be set against the security aspect/effect of a stricter legislation."
F550387,08 September 2020,Rudolf REIBEL,NRO (Nichtregierungsorganisation),Bundesärztekammer / German Medical Association,mittel (50 bis 249 Beschäftigte),89648243865-50,Deutschland,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"Comments by the German Medical Association (Bundesärztekammer):

Further to our response to the public consultation on the White Paper on Artificial Intelligence (AI), the German Medical Association would like to comment on the Inception Impact Assessment:

Regulatory options

The Inception Impact Assessment considers several options for regulating AI applications at EU level.

Despite the undisputed opportunities AI can provide, e.g. in diagnosis, treatment or patient monitoring, the use of AI in healthcare can also cause serious and irreversible harm to the life and health of patients, for instance in the case of recommendations of the wrong medicine, erroneous assessments in cancer screening, injuries during surgery, or errors in prioritisation of patients.

To address such risks, the Commission should propose binding regulation (option 3b in the Inception Impact Assessment), designating any applications used in healthcare as high-risk. We believe that such regulation should be sector-specific and must be detailed and include enforcement mechanisms in order to ensure equally effective patient protection in the EU.

Additional introduction of a voluntary labelling scheme could be envisaged for other areas, such as lifestyle applications.

The White Paper and Inception Impact Assessment already include a number of important aspects to be addressed by an EU regulatory framework on AI, such as:

-	the quality of training data sets,
-	the keeping of records and data, including the possibility of retracing AI-driven decisions,
-	information duties,
-	robustness and accuracy of AI systems, and
-	human oversight over final decisions.

We also agree with the approach outlined in the White Paper to adapt the EU Product Safety Directive to the characteristics of AI applications, as the existing legislation does not sufficiently address new risks linked to the complexity, opacity, autonomy and openness of AI applications.

In addition to these aspects, an EU regulatory framework on AI should address the following aspects, which are particularly relevant for physicians and their patients:

Liability of practitioners using AI applications

Patients should be effectively protected from harm incurred from the use of AI. At the same time, a liability regime must take account of the fact that physicians using AI applications cannot be expected to have expert IT skills. Therefore, a physician who demonstrates due diligence in selecting, installing and maintaining an AI application, or who diligently selects and supervises a provider to fulfil this task, should not be held liable for harm or damage caused by this AI system. The same should apply if the damage is caused by a third party, e.g. a hacker who interferes with the AI system in a criminal or illicit way, if there is no negligence on the part of the physician.

Moreover, liability rules should not interfere with the physician’s therapeutic autonomy. Unless clinical guidelines state otherwise, physicians should not be held liable for their decision to use or not to use an AI application, or for their decision not to follow a recommendation provided by AI.

Conformity assessment

Applications that can create a risk for patient safety should be subject to a strict and independent conformity assessment before being placed on the market. Ex-post safeguards like liability for damages are insufficient, as the harm caused to patients may be irreversible.

The assessment procedure should be designed to include the consultation of representatives of medical practitioners and patients, to enhance their user-friendliness and promote their acceptance, and to ensure their conformity with practitioners’ professional duties and clinical guidelines, where applicable.

Ownership of data generated using AI applications

Practitioners using AI applications should be permitted to continue to use data generated by them when changing AI application providers.

"
F550383,08 September 2020,Dominique Grelet,Unternehmen/Unternehmensverband,Atos SE,groß (250 oder mehr Beschäftigte),249876817241-03,Frankreich,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"Artificial intelligence – ethical and legal requirements
Atos Feedback – 8 Sep 2020

Atos welcomes the opportunity to respond to the Inception Impact Assessment proposed by the European Commission on the proposed legal act on Artificial Intelligence. 
Atos supports implementing policies aiming at mitigating the risks associated with AI applications. We believe that a non-legislative approach would not meet this objective, and conversely that mandatory requirements, especially if applied indiscriminately to all applications, are not economically and technically effective.

Please find below our comments on each presented option.



EU “soft law” (non-legislative) approach (option 1)

We believe that a totally non-binding approach would not be sufficient to ensure sufficient security.
In the framework of current existing legislation, we recommend imposing a standard for high-risk applications to ensure that they comply with the 7 principles for an ethical AI defined by the Commission. This could be done by first defining a common risk repository and then classifying applications evaluated within this repository,


Setting up a voluntary labeling scheme (option 2)

Atos supports the creation of voluntary labels. 
The creation of labels (for example one label for each of the seven principles of the European Commission for an ethical artificial intelligence), based on an evaluation template defined by the Commission, can be envisaged, making it possible to highlight the characteristics of an application on the AI axes of trust proposed by the Commission. 
The label would be awarded after analysis by a competent European structure - to be created, if necessary - and which could also define and manage the standards for applications identified as critical. 

In particular, Atos recommends this label be used as a selection criterion in the context of public or private procurement processes, in order to be able to rule out solutions that do not respect the commitments linked to the label, and therefore do not offer all the desired guarantees for the user.

This approach would make it possible to reveal the potential threats posed by existing solutions and potentially become a growth lever for complying solutions, becoming a competitive asset for their developers. Conversely, in the absence of global standards, not adopting this labelling approach could prevent the efforts made by both European and non-European AI actors on ethical dimensions, from being valued.


Regulations and requirements relating to high-risk AI, specific applications or all AI (options 3 and 4)

Atos advises against imposing specific requirements for all AI applications as presented in option 3c. Atos recommends taking into account the notions of the impact (or severity) of a damage and the probability that this damage will occur.

It should be noted that regulation is necessary but not sufficient to enhance the value of ethically compliant assets beyond the Union. Standards and labels could make it possible to promote the specificities of these solutions in other geographies or contexts not subject to European regulations.
","['Artificial intelligence – ethical and legal requirements - Atos feedback \nArtificial intelligence – ethical and legal requirements \nAtos Feedback – 8 Sep 2020 \n \nAtos welcomes the opportunity to respond to the Inception Impact Assessment proposed by the \nEuropean Commission on the proposed legal act on Artificial Intelligence.  \nAtos  supports  implementing  policies  aiming  at  mitigating  the  risks  associated  with  AI \napplications. We believe that a non-legislative approach would not meet this objective, and \nconversely  that  mandatory  requirements,  especially  if  applied  indiscriminately  to  all \napplications, are not economically and technically effective. \n \nPlease find below our comments on each presented option. \n \n \nEU “soft law” (non-legislative) approach (option 1) \nWe believe that a totally non-binding approach would not be sufficient to ensure sufficient \nsecurity. \nIn the framework of current existing legislation, we recommend imposing a standard for high-\nrisk applications to ensure that they comply with the 7 principles for an ethical AI defined by the \nCommission. This could be done by first defining a common risk repository and then classifying \napplications evaluated within this repository, \n \n \nSetting up a voluntary labelling scheme (option 2) \nAtos supports the creation of voluntary labels.  \nThe creation of labels (for example one label for each of the seven principles of the European \nCommission for an ethical artificial intelligence), based on an evaluation template defined by \nthe Commission, can be envisaged, making it possible to highlight the characteristics of an \napplication on the AI axes of trust proposed by the Commission.  \nThe label would be awarded after analysis by a competent European structure - to be created, \nif necessary - and which could also define and manage the standards for applications identified \nas critical.  \n \nIn particular, Atos recommends this label be used as a selection criterion in the context of public \nor private procurement processes, in order to be able to rule out solutions that do not respect \nthe commitments linked to the label, and therefore do not offer all the desired guarantees for \nthe user. \n \nThis approach would make it possible to reveal the potential threats posed by existing solutions \nand potentially become a growth lever for complying solutions, becoming a competitive asset \nfor their developers. Conversely, in the absence of global standards, not adopting this labelling \napproach could prevent the efforts made by both European and non-European AI actors on \nethical dimensions, from being valued. \n \n \n  Page 1 of 2', 'Artificial intelligence – ethical and legal requirements - Atos feedback \nRegulations and requirements relating to high-risk AI, specific \napplications or all AI (options 3 and 4) \n \nAtos advises against imposing specific requirements for all AI applications as presented in option \n3c. Atos recommends taking into account the notions of the impact (or severity) of a damage \nand the probability that this damage will occur. \n \nIt should be noted that regulation is necessary but not sufficient to enhance the value of ethically \ncompliant assets beyond the Union. Standards and labels could make it possible to promote the \nspecificities of these solutions in other geographies or contexts not subject to European \nregulations. \n \n  Page 2 of 2']"
F550327,08 September 2020,Armand NACHEF,Universität/Forschungseinrichtung,CEA,groß (250 oder mehr Beschäftigte),52774696782-43,Frankreich,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"CEA welcomes the opportunity to provide feedback on the Commission’s Inception Impact Assessment for a European legal act aimed at addressing the ethical and legal issues raised by AI. We welcome the initiative objectives, in particular the intention to create a harmonised framework in order to reduce burdensome compliance costs derived from legal fragmentation.

It is worth noting, a trustworthy AI as explained in HLEG on AI is at the same time a lawful, an ethical, and a robust AI. It takes into account all the concerns reflected in the trustworthy AI assessment list such as technical robustness, safety, security, privacy, traceability, explainability, auditability, in addition to diversity, non-discrimination, fairness, and respectability of fundamental rights. Actually, there is no need for an entirely new legislation, as current European legislation and standards already address many of these concerns. However, they may have some gaps that need to be filled.

Below are our comments on the outlined legislative options.

-	Option 1 - non-legislative approach to facilitate and spur industry-led intervention:
option 1 is probably convenient for low-risk AI solutions. However, there is a need of a minimum legislation that classifies the solutions according to their level of risk. Self-reporting of compliance with the HLEG ethical guidelines must remain verifiable and auditable by law. The Commission's support and encouragement of industry-led intervention towards trustworthy AI is very important, however in no case can this replace the enforcement of legislations that define the responsibilities and obligations of AI solution stakeholders. 

-	Option 2 - legislative instrument setting up a voluntary labelling scheme:
the diversity and multiplicity of uses of AI applications may make it very hard to have a simple labelling for applications with variable risks. On the other hand, a multitude of labelling depending on the use cases could be difficult to understand by the consumer. Particular attention should be paid to the concept of labelling that may lead to dangerous issues: labelling must always be carried out against a charter verifiable by a third party. Self-labelling involves too much risk with regard to its verification. CEA is not in favour of this option.

-	Option 3a: legislation for a specific category of AI applications only, notably remote biometric identification systems:
biometric identification systems must be subject to strong legislation. However, CEA considers it is more efficient to put in place a legislative instrument that applies to all AI applications and that modulates according to the AI application risk aspects.

-	Option 3b: legislation establishing mandatory requirements for “high-risk” AI applications.
“High-risk” AI applications should not be binary; there must be a gradation of risks as in all sectors with a safety or security aspect. Different rules adapted to the application risk levels (like those in the IEC 61508 standard) are needed.

-	Option 3c: the EU legislative act could cover all AI applications.
CEA supports option 3C. Laws and standards cover all human activity, but this does not prevent creativity, innovation and freedom of expression. On the contrary, the absence of laws and therefore an increase of incidents combined with a legal vacuum may impede the AI development and deployment.

In summary, CEA supports option 3C; however, legislation should consider an AI application like any software application. We cannot blame the AI: following the use case, the application providers, issuers, operators, or users are responsible for a malfunctioning of the applications.

Finally, changes in the system may regularly occur, thus the requirements of trustworthiness will be too light without a continuous risk assessment. There is a real need for more R&D in the risk area in order to conceive and create continuous risk assessment processes.
","[""Commissariat à l’énergie atomique et aux énergies alternatives \nAtomic Energy and Alternative Energies Commission (CEA) \n \n \n \nArtificial intelligence – ethical and legal requirements \nCEA Feedback on the roadmap / inception impact assessment \nFeedback period: 23 July 2020 - 10 September 2020 \nDate of issue: September 8th 2020 \nCEA welcomes the opportunity to provide feedback on the Commission’s Inception Impact \nAssessment for a European legal act aimed at addressing the ethical and legal issues raised by AI. We \nwelcome the initiative objectives, in particular the intention to create a harmonised framework in \norder to reduce burdensome compliance costs derived from legal fragmentation. \nIt is worth noting, a trustworthy AI as explained in HLEG on AI is at the same time a lawful, an ethical, \nand a robust AI. It takes into account all the concerns reflected in the trustworthy AI assessment list \nsuch as technical robustness, safety, security, privacy, traceability, explainability, auditability, in \naddition to diversity, non-discrimination, fairness, and respectability of fundamental rights. Actually, \nthere is no need for an entirely new legislation, as current European legislation and standards already \naddress many of these concerns. However, they may have some gaps that need to be filled. \nBelow are our comments on the outlined legislative options. \n-  Option 1 - non-legislative approach to facilitate and spur industry-led intervention: \noption 1 is probably convenient for low-risk AI solutions. However, there is a need of a minimum \nlegislation that classifies the solutions according to their level of risk. Self-reporting of \ncompliance with the HLEG ethical guidelines must remain verifiable and auditable by law. The \nCommission's support and encouragement of industry-led intervention towards trustworthy AI is \nvery important, however in no case can this replace the enforcement of legislations that define \nthe responsibilities and obligations of AI solution stakeholders.  \n \n-  Option 2 - legislative instrument setting up a voluntary labelling scheme: \nthe diversity and multiplicity of uses of AI applications may make it very hard to have a simple \nlabelling for applications with variable risks. On the other hand, a multitude of labelling \ndepending on the use cases could be difficult to understand by the consumer. Particular \nattention should be paid to the concept of labelling that may lead to dangerous issues: labelling \nmust always be carried out against a charter verifiable by a third party. Self-labelling involves too \nmuch risk with regard to its verification. CEA is not in favour of this option. \n \n-  Option 3a: legislation for a specific category of AI applications only, notably remote biometric \nidentification systems: \nbiometric identification systems must be subject to strong legislation. However, CEA considers it \nis more efficient to put in place a legislative instrument that applies to all AI applications and that \nmodulates according to the AI application risk aspects. \n \n-  Option 3b: legislation establishing mandatory requirements for “high-risk” AI applications. \n“High-risk” AI applications should not be binary; there must be a gradation of risks as in all \nsectors with a safety or security aspect. Different rules adapted to the application risk levels (like \nthose in the IEC 61508 standard) are needed. \n \n1"", 'Commissariat à l’énergie atomique et aux énergies alternatives \nAtomic Energy and Alternative Energies Commission (CEA) \n \n \n \n-  Option 3c: the EU legislative act could cover all AI applications. \nCEA supports option 3C. Laws and standards cover all human activity, but this does not prevent \ncreativity, innovation and freedom of expression. On the contrary, the absence of laws and \ntherefore an increase of incidents combined with a legal vacuum may impede the AI \ndevelopment and deployment. \n \nIn summary, CEA supports option 3C; however, legislation should consider an AI application like any \nsoftware application. We cannot blame the AI: following the use case, the application providers, \nissuers, operators, or users are responsible for a malfunctioning of the applications. \nFinally, changes in the system may regularly occur, thus the requirements of trustworthiness will be \ntoo light without a continuous risk assessment. There is a real need for more R&D in the risk area in \norder to conceive and create continuous risk assessment processes. \n2']"
F550322,08 September 2020,William Moreau,Wirtschaftsverband,CLEPA (European Association of Automotive Suppliers),klein (10 bis 49 Beschäftigte),91408765797-03,Belgien,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"Automotive suppliers play a central role in the development of connected and automated vehicles. AI applications are becoming more and more common in cars: automated driving is the most well-known example, but a broad range of other applications are also concerned, such as many vehicle safety functions, comfort functions, advanced driver-assistance systems warnings, connectivity systems, infotainment systems, etc.
CLEPA welcomes the AI White Paper, and believes that an appropriate legislative framework can boost the development and uptake of AI by providing market participants more legal certainty and by ensuring consumer trust. We support the risk-based approach outlined in the Paper as it is more likely to ensure proportionality.
We stress the importance of not hindering innovation unnecessarily. Requirements should always remain proportionate to the risks and leave room for testing/experimenting. A balance must be achieved to ensure that the goals of this new initiative do not jeopardise the development of safer vehicles.
CLEPA supports setting up horizontal fundamental principles on AI to increase trust and ensure the necessary level playing field between market players. Nevertheless, automotive suppliers would prefer a sector-based approach for the upcoming regulatory framework and its compliance mechanism. Our sector is very specific and should not be covered by a one-size-fits-all cross-sectoral legislation which would not be adapted to the way automotive products are developed, produced, tested, and put on the market. Indeed, our sector is already subject to strict ex-ante conformity controls, such as the type approval process. AI-related requirements for our sector should be included into that existing framework rather than as a new set of requirements controlled separately. Certification, testing, and market surveillance should not be duplicated. Aside from the additional costs and administrative burden, it would create a risk of inconsistencies arising between the requirements under type approval and those under the AI framework, as both would cover safety and AI applications in automotive are deeply integrated in the vehicles’ systems.
Workstreams should be coordinated to avoid duplication and/or conflicting requirements. Discussions on automated driving are ongoing at the UNECE. The EU’s recently revised General Safety Regulation also introduced new safety measures, some of which may rely on AI. The delegated and implementing acts that will set the technical requirements for these measures are currently being drafted. The EU legislative framework on AI and the UNECE requirements for Automated Driving Systems should be aligned, with future UNECE requirements to be considered valid AI-related requirements, instead of adding another regulatory layer.
Regarding the policy options outlined in the Impact Assessment, we support leaving room for industry-led initiatives (option 1). We are also open to the creation of voluntary labelling scheme(s) (option 2), however, it is difficult to fully support such an initiative without knowing how it would be implemented practically. Before any type of labelling scheme can be introduced, transparent rules and metrics based on international standards should be agreed upon. The HLEG’s assessment list provides an interesting basis, but is not sufficiently adapted to the automotive sector’s specificities. National labelling schemes should be avoided.
We believe that option 3.2 (EU legislative instrument limited to high-risk applications) would be the most compatible with the existing regulatory framework for automotive products. We support mandatory requirements for safety-critical AI applications related to automated driving (SAE level 3 and above), so long as they are integrated into the type approval system, as mentioned above. On the other hand, non-safety-related applications (e.g. comfort functions, driver assistance systems, infotainment…) should clearly be considered low-risk.","['CLEPA comments on the European Commission’s \nWhite Paper on Artificial Intelligence –  \nA European approach to excellence and trust \n \n \n \n \nCLEPA represents the European automotive supplier industry. An average passenger car is made of around \n30,000 parts, making up approximately 75% of the vehicle’s total value. Suppliers provide all type of \nvehicle  parts  and  components,  including  powertrain,  chassis  and  frame,  brakes,  lighting,  interior, \nelectronics, and software. \n \nAutomotive suppliers play a central role in the development of connected and automated vehicles. \nArtificial Intelligence (AI) applications are becoming more and more common in vehicles: automated \ndriving is the most well-known example, but a broad range of other applications are also concerned, such \nas a number of vehicle safety functions, comfort functions, advanced driver-assistance systems (ADAS) \nwarnings, connectivity systems, infotainment systems, etc. \n \n \n1)  General comments \n \n•  CLEPA welcomes the White Paper on AI, and believes that an appropriate legislative framework can \nboost the development and uptake of AI in the EU, by providing market participants more legal \ncertainty, and by ensuring consumer trust in AI products. \n \n•  We support the high-risk vs non high-risk approach proposed, with mandatory ex-ante requirements \nfor high-risk applications. \n \n•  We agree that certain transport-related applications should be considered high-risk, however we urge \nthe Commission to make clear that some transport-related applications may not be high-risk (e.g. \ninfotainment, comfort functions, ADAS warnings…). This should be made clear in the upcoming \nlegislative framework, and these applications exempted from the ex-ante requirements. \n \n•  CLEPA supports setting up horizontal fundamental principles on AI to increase trust in the technology \nand guarantee the necessary level playing field between market players. Nevertheless, automotive \nsuppliers would prefer a sector-based or product-based approach for the upcoming regulatory \nframework and its compliance mechanism. The automotive sector is very specific (details further \ndown) and should not be covered by a one-size-fits-all cross-sectoral legislation which would not be \nadapted to the way automotive products are developed, produced, tested, and put on the market. \n \n1', '•  We recommend that the Commission be more specific in its definition of “artificial intelligence.” How \nAI is defined will have important consequences on the scope of the upcoming legislative framework. \nTherefore, the Commission should ensure proper consultation of both experts and economic actors. \nSimilarly,  the  Commission  should  consider  differentiating  between  different  types  of  artificial \nintelligence,  for  example:  machine  learning  and  deep  learning.  This  is  especially  relevant  for \ntechnologies used in ADAS and automated driving. Each type of technology presents different kinds \nof risks, and this should be taken into account when designing the legislative framework. \n \n•  We stress the importance of not hindering innovation unnecessarily. Requirements should always \nremain proportionate to the possible risks and leave room for testing/experimenting. In addition, a \nbalance must be achieved to ensure that the goals of this new initiative do not jeopardise the global \nR&D’s contribution to safer and cleaner vehicles. We support technically justified requirements, which \ndo not discriminate AI developed in non-EU countries. \n \n \n2)  Automotive-specific \n \n•  The automotive sector is already subject to strict ex-ante conformity controls (type approval). We \nbelieve that AI-related requirements for our sector should be included into that existing framework, \nrather than as a new set of requirements controlled separately. It is of paramount importance that \ncertification, testing, and market surveillance are not duplicated. Aside from the implications in terms \nof additional costs and administrative burden, it would create a risk of inconsistencies arising between \nthe requirements under type approval and those under the new AI framework, as both would cover \nsafety and AI applications in automotive are deeply integrated in the vehicles’ systems. \n \n•  In addition, the new requirements that will be introduced should take into account the development \ncycle length of automotive products. Vehicles with automated functions that will be on the roads in \nthe next few years are being trained and it is important manufacturers have a suitable timeframe to \ncomply with the requirements. \n \n•  For those vehicle types that fall out of scope of the EU type approval framework (e.g. last mile delivery, \noff-road  vehicles,  etc.),  the  AI  framework  should  apply.  This  ensures  sufficient  level  or \nperformance/redundancy and transparency in the development of non-road vehicles whilst excluding \nthem from unsuitable passive safety requirements. \n \n•  Workstreams should be coordinated to avoid duplication and/or conflicting requirements. Discussions \non automated driving are already ongoing at the UNECE, the UN body which develops many of the \nvehicle technical standards that apply in the EU. In addition, the recently revised General Safety \nRegulation (GSR) also made mandatory a number of safety measures, which may rely on AI: the \ndelegated and implementing acts that will set the technical requirements for these measures are \ncurrently being drafted. The EU legislative framework on AI and the UNECE requirements for \nAutomated  Driving  Systems  (ADS)  should  be  aligned,  with  future  UNECE  requirements  to  be \nconsidered valid AI-related requirements, rather than adding another regulatory layer. \n \n \n2', '3)  Liability \n \n•  From the perspective of the automotive sector, the current EU legislation on security, liability, and \nresponsibility is effective and does not need to be fundamentally changed for artificial intelligence. \nThe Product Liability Directive (PLD), in particular, already provides a sound legal basis to address \nconsumer protection and may therefore serve as a foundation for discussions and evaluations with \nrespect to effective consumer protection and compensation for AI products. \n \n•  Therefore, we believe that any revision of the current EU legislation should be assessed carefully. The \nreview should focus on whether and to what extent AI applications and their specificities are \naddressed by the current liability framework. \n \n•  We support clarification of the term “product” as used in the PLD so as to allow for product liability \nclaims if any relevant automotive product or service has not complied with or neglected safety \nstandards and other state-of-the-art requirements and, in doing so, did not comply with justified \nsafety expectations of the public/end users, and as a result damage has been caused. \n \n•  CLEPA believes that every market participant whose product is making use of AI technology has to \nensure that the technology is reliable, comprehensible, secure, and safe – to the extent that it can be \nreasonably expected from the market participant’s product (justified safety expectations, see above). \n \n•  We would welcome more legal certainty from the Commission on how liability in case of damage is \nto be determined in the context of AI applications, which may act as a “black box” and whose decisions \nmade by machine learning algorithms cannot necessarily be explained. A common understanding \nwould avoid different interpretations in each Member State and, thus, a national fragmentation of \nthe internal market. \n \n \n4)  Mandatory requirements \n \n•  Before the Commission proposes a legislative framework, CLEPA recommends a case study on its \napplication to automotive (e.g. for automated driving), to ensure that any requirements proposed are \ntechnically feasible. \n \na.  Data sets \n \n•  Any requirements on data sets that will be imposed by the upcoming legislative framework should \ntake into account the development cycle length of automotive products, which includes time for \ntesting and certification. Vehicles with automated functions that will be on the roads in the next few \nyears are already being trained now. \n \n•  Among state-of-the-art technologies is the use of pre-trained models, where it is not always possible \nto refer to all the data the system has been trained with. The criteria suggested by the White Paper \nmight make the use of pre-trained models impossible. \n \n3', '•  With regards to the requirement of keeping records and data, CLEPA stresses that this would require \nsignificant effort to catalogue, store, and maintain (e.g. fully historicise all data and models). In \napplication areas that operate on low margins, AI applications might become economically not \nfeasible. \n \n•  Coverage of data sets, and their quality, can be critical for the safety of high-risk applications and \nshould be assessed by demonstrating compliance with safety requirements under vehicle type \napproval or other established automotive standards. \n \n•  While coverage, and more generally quality of data, is important, it should not be a mandatory \nrequirement. With certain techniques, such as semi-supervised learning, it is possible to train good \nsystems even on datasets that are not of the highest quality, which is especially useful when the \nhighest-quality datasets might not be available, or be prohibitive or unsustainable in terms of time, \ncosts, and safety. Furthermore, there is currently no widely agreed upon tool that exists to define and \nassess the quality of a dataset. \n \n•  It is typical that deep learning algorithms are developed using three datasets for: training, validation, \nand testing. Models are fitted using the training dataset, while the validation dataset is used during \nthe training process to verify the quality of the current fitting. The testing dataset is used to verify the \nperformance of trained models after training has finished. All three datasets are carefully constructed \nto suit their purpose. There should only be specific obligations on manufacturers to ensure AI systems \nare tested on data sets that are sufficiently broad, the data that is used in the training and validation \nphases should be dependent upon the manufacturer. \n \n•  We would like further clarification on the requirements outlined in the White Paper, regarding in \nparticular the non-discrimination and privacy requirement, and how these should be taken into \naccount in the context of automated driving applications. \n \n•  Regarding cybersecurity provisions for data sets, CLEPA believes there is no necessity at the moment \nfor cybersecurity certification schemes for automotive AI products over and above the applicable type \napproval regulatory requirements. CLEPA is nevertheless ready and available to contribute via the \nrelevant channels at both the European Commission and ENISA, the Agency for Network and \nInformation Security (e.g. in the Stakeholder Cybersecurity Certification Group), in carefully assessing \nif any additional cybersecurity schemes may address further risks associated with the intended use of \nAI products in the automotive sector. In fact, it is of utmost importance to cater for the specificities \nof the automotive sector, which cannot be covered adequately by generic or IT product legislation. \nMoreover, it is essential to ensure alignment of the cybersecurity principles and methodologies in EU \nlegislative acts, UN regulations and international standards such as ISO. \n \nb.  Information provision \n \n•  With regards to the obligation to inform consumers/users that they are interacting with an AI, \nautomotive suppliers should have the duty to inform their direct customers (vehicle manufacturers, \nor other suppliers for tier 2-3 suppliers), but the responsibility for informing the end consumer should \nrest on vehicle manufacturers. \n4', 'c.  Robustness and accuracy \n \n•  The requirements proposed by the Commission in the White Paper are relevant for products that are \nnot already subject to strict performance assessments. Automotive products already undergo type \napproval, and the requirements should be checked under this existing framework (as per our remarks \nabove). \n \n•  Robustness would need a clearer definition, with strict limits, so as not to impose technically \nunfeasible requirements (e.g. against adversarial attacks). \n \nd.  Human oversight \n \n•  We agree that AI systems must remain under the principle of human oversight, but the specific \ncontext of automated driving should be taken into account. It is not possible to oversee every single \ndecision taken by an automated car, due to most decisions being taken in real time. The human \noversight should therefore be conceived as an ex-ante verification of the logic of the decision-making \nfor automated and fully automated vehicles. \n \n•  One possibility of human oversight mentioned in the white paper is imposing operational constraints \non the system, for example by imposing rules on the behaviour of a fully automated vehicle in the \ndesign phase. The guidelines on the exemption procedure for the EU approval of automated vehicles, \ndeveloped by the Commission and Member States in 2019, give five main rules to the behaviour of an \nautomated vehicle: “the vehicle shall be able to keep a safe distance with other vehicles in front, \nexhibit caution in occluded areas, leave time and space for others in lateral manoeuvres, be cautious \nwith right-of-ways, and if an accident can be safely avoided without causing another it shall be \navoided.” CLEPA suggests addressing the issue of human oversight by defining formal rules in order \nto assess the behaviour of automated vehicles during the conformity assessment phase. This should \nbe done by establishing a transparent, technology-neutral, and performance-based evaluation of the \ndecision-making of automated vehicles, following the key principles already defined in the 2019 \nguidelines. \n \n•  Human oversight requirements should also not unduly restrict machine learning applications. \n \ne.  Biometric identification systems \n \n•  We would like some clarification on what would be considered biometric identification (e.g. facial \nrecognition). Some safety-related automotive applications, such as driver awareness/drowsiness \nmonitoring or external sensors, may scan human faces but should not be considered facial recognition \nas they do not pose risks for fundamental rights as described in the White Paper. \n \n•  Clarification would also be useful as to the meaning of “public spaces.” Personal cars are usually not \nconsidered to be a public space, but the definition becomes blurrier for company cars, carpooling \nvehicles, taxis, or robo-taxis. \n \n5', '5)  Voluntary labelling scheme for low-risk AI \n \n•  While a voluntary labelling scheme could, in principle, be a useful addition for low-risk applications, \nCLEPA considers it difficult to support such a scheme without knowing how it would be implemented \npractically. AI applications are often incorporated into other products. Will the label apply for AI \nalgorithms? for products? for companies? How will KPIs be defined? We urge the Commission to \nprovide more clarity on how such a scheme would work. \n \n•  Before any type of voluntary labelling scheme can be introduced, transparent rules and metrics based \non international standards would have to be agreed upon. National schemes should be avoided in any \ncase. \n   \n6', 'About CLEPA \n \nCLEPA, the European Association of Automotive Suppliers, represents over 3,000 companies supplying \nstate-of-the-art components and innovative technologies for safe, smart, and sustainable mobility. CLEPA \nbrings together over 120 global suppliers of car parts, systems, and modules and more than 20 national \ntrade associations and European sector associations. CLEPA is the voice of the EU automotive supplier \nindustry linking the sector to policy makers. \n \no  The automotive sector accounts for 30% of R&D in the EU, making it the number one investor. \n \no  European automotive suppliers invest over 25 billion euros yearly in research and development. \n \no  Automotive suppliers register over 9,000 new patents each year. \n \no  Automotive suppliers in Europe generate five million direct and indirect jobs. \n7']"
F550315,08 September 2020,Antonio CIMORRA,Wirtschaftsverband,"AMETIC AMETIC (ASOCIACIÓN MULTISECTORIAL DE EMPRESAS DE LA ELECTRÓNICA, LAS TECNOLOGÍAS DE LA INFORMACIÓN Y LA COMUNICACIÓN, DE LAS TELECOMUNICACIONES Y DE LOS CONTENIDOS DIGITALES)",klein (10 bis 49 Beschäftigte),013076816891-46,Spanien,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"Scope 
○	We caution the Commission on considering to significantly expand the scope of the future AI regulation to the open ended category of “automated decision making” This would go against the initial, thoughtful direction proposed in the AI whitepaper that proposes to focus on the risk-based, double-criterion for sectorial and application/use-based AI technologies. If AI were defined as “automated decision making” for the purpose of the future AI regulation, it would create unproportional, unjustified regulatory obligations that would not only deter development and deployment of AI-based applications in Europe, but also automated systems that do not pose any risk nor harms. 
○	Definition of immaterial harm: We reiterate the concerns around introducing the open-ended concept of “immaterial harm” into the future AI legislation. We propose as an alternative to refer to “significantly restricting the exercise of fundamental rights”.

Policy options
○	Option 0 (baseline): We do believe that there is merit in ensuring that the existing EU regulation is properly implemented with regards to AI before putting in place any new prescriptive AI-specific rules. 
○	Option 1 (industry-led intervention): No matter what policy options are pursued, lending support to the industry in establishing and implementing norms of responsible practices and sharing best practices is worthwhile. 
○	Option 2 (legislation on voluntary labelling): We remain skeptical on the impact of a labelling scheme on the uptake of trustworthy AI in Europe. An administrative burden on SMEs to comply with the onerous labelling obligations -- if drafted on the basis of the update Assessment List for Trustworthy AI from the EU AI HLEG-- could significantly outweigh the benefits of such a scheme.
○	Option 3 (legislation with mandatory requirements): Overall, the opportunity cost of not using AI should be part of the assessment when considering any future legislation aiming to reduce  risk and harms from the use of AI applications. Legislation should ensure legal certainty, be proportionate and increase trust in AI without unduly hindering AI-driven innovation.
-	Remote biometric identification systems may be a good example of an application to which mandatory requirements could be applied in a risk-based framework.
-	We support a well-defined risk-based approach to the AI regulation that takes into account both the severity and likelihood of harm. A sector and use/application base criteria -- as proposed by the EC’s whitepaper -- at large seem to be a good starting point.
-	We strongly caution against an EU legislative act for all AI applications, that would make no distinction between the AI applications that can pose significant risk/harm and those with no or lower risk profile. Such a legislative instrument would be significantly unproportional to the problems so far identified by the European Commission, create significant barriers for AI adoption (additional costs, delay, administrative burden) in Europe, result in opportunity costs in some applications due to lack of AI implementation, and risk lowering the bar for those AI applications that are very likely to raise significant risks. 
Enforcement
○	We support the ex post enforcement for when problems arise as the most appropriate and proportionate mechanism, except in fields where ex-ante assessments are already established practice. In those situations, we recommend aligning any ex-ante assessment with existing procedures.
○	If the Commission insists on the ex-ante enforcement, we strongly caution against a third party ex-ante assessment and recommend instead self-assessment procedures based on clear “due diligence” guidance from the regulators. A practical approach would be for regulators to provide detailed templates and guidance on how to carry out and document the risk assessment, but delegate responsibility to those using and most familiar with the AI system to conduct an accurate assessment. 
"
F550259,08 September 2020,Thomas MIESSEN,Gewerkschaft,ACV-CSC Belgium,groß (250 oder mehr Beschäftigte),80866452991-55,Belgien,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"The ACV-CSC-Belgium Congress on the Future of Work - #Arbeidmorgen, #Queltravaildemain, #Arbeitmorgen - gathered in October 2019 more than 850 delegates for the adaption of its final conclusions. Thousands of members of ACV-CSC-Belgium participated before in a process of preparation and elaboration of the political guidelines and lines of action for this congress. Issues as the digital and the socioecological transitions were at the top of the agenda of this congress on the Future of Work. The document in attachment resumes its political guidelines. The chapters #2 to #6 focus very much on AI and technology, and their impacts for labor. The chapters #19 and #20 concern the working conditions in the global supply chains and the giants of the Web. A fundamental baseline of the demands of this congress document is that strong regulation is needed if we really want to shape the future world of work and our democracy positively: Without rights, rules and responsibilities, there is no way forward. That's evidently also the position we defend together with the Trade Unions from all Europe united in the European Trade Union Confederation (ETUC), more specifically and most recently on AI in the ""Resolution on the European strategies on artificial intelligence and data"" approved by its Executive Committee on the 2nd of July 2020:  https://www.etuc.org/sites/default/files/document/file/2020-07/Adopted%20-%20ETUC%20resolution%20on%20%20the%20European%20strategies%20on%20artificial%20intelligence%20and%20data%20-%20EN.pdf"
F550245,08 September 2020,Miika Blinn,Verbraucherverband,Verbraucherzentrale Bundesverband (Federation of German Consumer Organisations - vzbv ),mittel (50 bis 249 Beschäftigte),2893800753-48,Deutschland,Künstliche Intelligenz – ethische und rechtliche Anforderungen,Please see the attached statement,"['A EUROPEAN LEGAL ACT LAYING\nDOWN REQUIREMENTS FOR\nARTIFICIAL INTELLIGENCE\nResponse of Federation of German Consumer Organizations\n(vzbv) to the public consultation on the inception impact as-\nsession for the “Proposal for a legal act of the European\nParliament and the Council laying down requirements for Ar-\ntechnical intelligence”\nSeptember 8, 2020\n   \nimprint\nConsumer Center\nFederal Association e.V.\nteam\ndigital and media\nRudi-Dutschke-Strasse 17\n10969 Berlin\ndigitales@vzbv.de\nFederal Association of Consumer Centers and Consumer Associations\n \n  Consumer Center Federal Association e.V.', 'Verbraucherzentrale Bundesverband e.V. \n2 l 8  A European Legal Act laying down Requirements for Artificial Intelligence \n \n \n \nTABLE OF CONTENTS \nI. SUMMARY  3 \nII. INTRODUCTION  4 \nIII. LOW CONSUMER TRUST IN AI AND CURRENT REGULATION  5 \nIV. POLICY OPTIONS  6 \n1. Horizonal Approach covering all AI Applications ......................................................... 6 \n1.1 Gradual Risk-based obligations covering all AI Applications .................................... 7 \n1.2 Precautionary Policy Approach protecting Consumes .............................................. 7 \n1.3 Transparency for Consumers .................................................................................... 7 \n1.4 Transparency for the Public ...................................................................................... 7 \n1.5 Effective Enforcement and Governance Structure .................................................... 7', 'Verbraucherzentrale Bundesverband e.V. \nA European Legal Act laying down Requirements for Artificial Intelligence  3 l 8 \nI. SUMMARY \nThe Federation of German Consumer Organisations (Verbraucherzentrale \nBundesverband - vzbv) welcomes the European Commission’s initiative for a legal act \nlaying down requirements for Artificial Intelligence (AI). \nA recent survey1 shows that European consumers do not trust AI. Consumers think \nthat current legislation cannot effectively regulate AI activities. Most people think \ncompanies use AI to manipulate consumer decisions. German consumers do not \nthink that soft law, like voluntary codes of conduct, is fit to address the risks posed by \nAI-based discrimination and the majority prefers stronger regulation to address \nthese problems. \nOf the options proposed in the Inception Impact Assessment, vzbv recommends to \nfollow option 3 (c), an EU legislative act covering all AI applications, and urges the \nEuropean Commission to move in this direction: \n•  In order to inspire consumers’ trust and encourage uptake and dissemination \nof AI, the European regulatory framework on AI must regulate AI in a compre-\nhensive manner through strong and effective horizontal legislation to ensure \nthat AI adheres to EU laws.  \n•  AI-systems making/preparing relevant decisions on consumers must adhere to \nhigh quality standards. They must be highly transparent for consumers and \nenable consumers to exercise their legal rights (including a right to explanation \nin case of sensitive/critical systems and the publication of a risk assessment). \n•  Risk and regulatory requirements should be assigned gradually according to \nthe risk level of an application. Some basic obligations (e.g. regarding trans-\nparency) should be applicable to all AI systems. \n•  A precautionary approach: Consumers must be able to trust that technologies \nthat can pose significant harms for individuals and society are independently \naudited/tested before they are deployed in the market.  \n•  Competent authorities must be endowed with financial, technical, and human \nresources and legal powers, to effectively audit and scrutinise AI systems at \nall times. \n___________________________________________________________________________________________ \n1 BEUC, 2020, Artificial Intelligence: what consumers say: Findings and policy recommendations of a multi-country sur-\nvey on AI, https://www.beuc.eu/publications/beuc-x-2020-078_artificial_intelligence_what_consumers_say_report.pdf \n[download 08.09.2020]', ""Verbraucherzentrale Bundesverband e.V. \n4 l 8  A European Legal Act laying down Requirements for Artificial Intelligence \nII. INTRODUCTION \nThis statement provides the Federation of German Consumer Organisations’ \n(Verbraucherzentrale Bundesverband - vzbv) feedback to the European Commission’s \nInception Impact Assessment for the upcoming proposal for a legal act laying down \nrequirements for Artificial Intelligence (AI).  \nvzbv welcomes the opportunity to comment on the European Commission’s Inception \nImpact Assessment as algorithm-based decision making (ADM) and AI increasingly de-\ntermine the way in which consumer markets and our societies function. \nAI and ADM systems increasingly make or predetermine human decisions on consum-\ners. Although they have a growing impact on the daily lives of consumers they often op-\nerate within a worrying regulatory vacuum. Consequently, it is not surprising that \nmany consumers – rightly – distrust AI, largely due to the obscure nature through which \ndecisions are made or prepared. This level of consumer distrust hampers the adop-\ntion and demand for AI in the European Union (EU). \nIf the European Commission wants to inspire consumers trust in AI and encourage up-\ntake and dissemination of the technology, it must enact a European regulatory frame-\nwork on AI that ensures that AI adheres to European laws and values. Therefore \nvzbv stresses the need to regulate AI in a comprehensive manner through strong and \neffective horizontal legislation.  \nThe legislative framework on AI must ensure that relevant systems that affect people’s \nlives can be independently controlled, that AI-systems making/preparing decisions on \nconsumers adhere to high quality standards, are highly transparent for consumers \nand that it enables consumers to exercise their rights. This must include mandatory re-\nquirements for providers and deployers of AI systems including transparency require-\nments vis-à-vis consumers that allow consumers to understand how AI systems work \nand why a specific result in an individual case came about. \nFor this reason, vzbv repeatedly stressed the importance and urgency of a legal act \nlaying down comprehensive obligatory requirements to fill this regulatory gap, and wel-\ncomes that the European Commission is envisaging a legislative proposal. vzbv made \ndetailed proposals for a regulation in its response to the public consultation on the Eu-\nropean Commission 's White Paper on AI (see attachment to this inception impact as-\nsessment)."", 'Verbraucherzentrale Bundesverband e.V. \nA European Legal Act laying down Requirements for Artificial Intelligence  5 l 8 \nIII. LOW CONSUMER TRUST IN AI AND CUR-\nRENT REGULATION  \nThe line of argument for a strong, effective regulation is confirmed and reflected in vari-\nous empirical studies, like the BEUC2 consumer survey on perceptions on AI in 8 EU \nMember States (September 2020): The results show that, although consumers are gen-\nerally in favour of AI development, they have serious concerns in relation to AI and \nADM systems:  \n•  While consumers see benefits of AI, they have low trust in AI and its added \nvalue. This is displayed in concerns ranging from the lack of transparency, unin-\ntended consequences or the abuse of personal data. A majority of consumers \nstrongly agree that companies use AI to manipulate consumer decisions \n(e.g. 64 % in Belgium, Italy, Portugal and Spain). \n•  Most consumers think that current rules are not adequate to effectively regu-\nlate AI-based activities (50% in Sweden and 55% in Portugal). Around 56% of \nall EU consumers have low trust in authorities to exert effective control over AI. \n___________________________________________________________________________________________ \n2 BEUC, 2020, ‘Artificial Intelligence: what consumers say: Findings and policy recommendations of a multi-country sur-\nvey on AI,’ https://www.beuc.eu/publications/beuc-x-2020-078_artificial_intelligence_what_consumers_say_report.pdf \n[download 08.09.2020]', 'Verbraucherzentrale Bundesverband e.V. \n6 l 8  A European Legal Act laying down Requirements for Artificial Intelligence \nIV. POLICY OPTIONS \nWith regards to the policy options proposed in the Inception Impact Assessment, it can \nbe safely said that Option 0 (no regulation) should be discarded right away.  \nGiven the complex and obscure nature of AI-based systems Option 1 and Option 2 \nare bound to fail the policy objectives of this initiative: Policy Option 1, the non-legis-\nlative “soft law” approach (for example based on ethical codes, like those developed \nby industry actors) and Option 2, a voluntary labelling scheme will not create con-\nsumers’ trust. Also, they are unlikely to guarantee that all AI-systems that signifi-\ncantly impact consumers’ lives will adhere to European laws and values. Such volun-\ntary commitments have all too often disappointed in the past and there is no reason to \nassume that they will be effective in the realm of obscure AI-systems. \nThis is illustrated by the BEUC study mentioned above and another recent survey on \ndiscrimination and AI3: It shows that German consumers consider voluntary code of \nconducts as much less effective to tackle discrimination caused by AI as compared to \nstronger regulation and mandatory certification schemes. When asked to prioritize the \ninstruments that could be employed to tackle AI-based discrimination, 55% prefer \nstronger regulation, 47 % prefer mandatory certification schemes while merely 7 % \nprefer voluntary code of conducts. \n1. HORIZONAL APPROACH COVERING ALL AI APPLICATIONS \nvzbv considers option 3 (c), an EU legislative act covering all AI applications, to be \nviable and urges the European Commission to move in this direction. The reasons for \nthis preference can be summarised as follows: \nOption (3a) is not suited, as it limits an EU legislative instrument establishing manda-\ntory requirements to specific categories of AI applications only (like remote biometric \nidentification systems). This instrument is too narrow in scope and many other applica-\ntions that can cause serious harm will fall out of scope. Applications in any sector \nmust be subject to legal requirements if they pose a high risk/can cause significant \ndamage (e.g. insurance, discrimination in e-commerce or smart digital assistants). \nOption (3b) is not suited for similar reasons. First the White Paper’s4 binary (high \nrisk/low risk) and cumulative (two criteria to be defined a high risk) approach to deter-\nmine high-risk applications should be discarded. It imposes obligatory requirements \nonly for high-risk applications in predefined high-risk sectors. Second, the EU legislative \nact should cover all AI applications, as potentially any AI system could pose a harm for \nindividual consumers, groups of consumers or society at large. Therefore, all AI appli-\ncations should be subject to at least “light” obligations such as transparency re-\nquirements. \nThe EU legislative instrument should follow the following principles: \n___________________________________________________________________________________________ \n3 Meinungsmonitor Künstliche Intelligenz, 2020 ‚Künstliche Intelligenz und Diskriminierung‘. Factsheet Nr. 2 des Mei-\nnungsmonitor Künstliche Intelligenz. https://www.cais.nrw/wp-94fa4-content/uploads/2020/08/Factsheet-2-KI-und-Dis-\nkriminierung.pdf [download 07.09.2020] \n4 European Commission, 2020, ‘White Paper on Artificial Intelligence - A European approach to excellence and trust’, \nCOM(2020) 65 final, https://ec.europa.eu/info/sites/info/files/commission-white-paper-artificial-intelligence-\nfeb2020_en.pdf [download 07.06.2020]', 'Verbraucherzentrale Bundesverband e.V. \nA European Legal Act laying down Requirements for Artificial Intelligence  7 l 8 \n1.1 Gradual Risk-based obligations covering all AI applications \nRisk and regulatory requirements should be assigned gradually according the risk \nlevels into which AI applications fall5. Some basic obligations (e.g. regarding trans-\nparency) should be applicable to all AI systems. Applications posing a higher risk \nshould be subject to stricter, more demanding mandatory requirements. The granular \napproach ensures that all risky applications across all sectors are subject to adequate, \napplication-specific mandatory requirements, thereby reducing the regulatory burden \nand increasing legal certainty for all stakeholders. \nThe assessment of the risk level of AI applications must not only consider potential \ndamages to individual entities (physical persons or legal entities) but include the im-\npact on members of specific social groups and society at large. \n1.2 Precautionary policy approach protecting Consumes  \nThe European Commission should adopt a precautionary approach to the regulation \nof AI and ADM. Consumers must be able to trust that technologies that can pose signif-\nicant harm for individuals and society are independently audited/tested before they \nare deployed in the market. Also, as an ultima ratio measure, it should be possible for \nthe authorities to ban the use of certain AI/ADM systems if the risk they pose is not tol-\nerable. For example, the operation of remote biometric identification systems should \nbe prohibited in public places until the associated risks and consequences for individ-\nuals and society have been adequately researched. \n1.3 Transparency for Consumers \nThe mandatory rules for transparency vis-a-vis consumers must go beyond merely \nlabelling high-risk AI so that users know that they interact with an AI system. Consum-\ners need to know about the risks, data base etc. Developers and operators of AI must \nensure traceability (and accountability) of their systems. Data subjects must be pro-\nvided with all the information necessary to exercise their rights when necessary, e.g. \nproviders of high-risk AI applications must be mandated to explain consumers the re-\nsult of the individual case in a comprehensible, relevant and concrete manner. \n1.4 Transparency for the Public \nThe EU legislative instrument should include the obligation for deployers of AI to pub-\nlish a risk assessment. This must not contain business secrets but information that the \npublic needs to conduct an informed debate.  \n1.5 Effective enforcement and governance structure \nThe legal act should establish a coherent and efficient compliance and enforcement \nsystem. It must ensure that businesses employ effective internal control mechanism en-\nsuring that the development and use of AI and ADM systems complies withEU rules. \nThe governance structure must guarantee the active cooperation among the relevant \nenforcement authorities, as well as between public and private enforcement bodies, in-\ncluding consumer organisations. Competent authorities must be endowed with the fi-\nnancial, technical, and human resources as well as legal powers, to effectively audit \nand scrutinise AI systems at all times. \n___________________________________________________________________________________________ \n5 Such an approach has been proposed by the German Data Ethics Commission: Data Ethics Commission, ‘Opinion of \nthe Data Ethics Commission’, 2019 https://www.bmjv.de/DE/Themen/FokusThemen/Datenethikkommission/Dateneth-\nikkommission_EN_node.html [06.06.2020].', 'Verbraucherzentrale Bundesverband e.V. \n8 l 8  A European Legal Act laying down Requirements for Artificial Intelligence \nThe European Commission should work towards a governance structure or agency that \nsupports the sector-specific national and EU competent authorities directly in supervis-\ning AI systems with methodological and technical expertise. This could be particularly \nhelpful for competent authorities in smaller Member States who might find it difficult to \nbuild up such technical and methodological competence themselves.']"
F550212,08 September 2020,Daniel Leufer,NRO (Nichtregierungsorganisation),Access Now Europe,sehr klein (1 bis 9 Beschäftigte),241832823598-19,Belgien,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"Access Now opposes the uptake of AI as an objective for a potential regulatory intervention. Our opinion on a legislative proposal will be based on the assessment of whether it ensures adequate safeguards for the protection and promotion of fundamental rights including societal impacts. All points outlined here are dealt with more substantially in our White Paper response, uploaded here.

Access Now recommends regulatory option 4 to combine options 3.1 and 3.3. The EU legislative instrument should have one part to cover all AI applications (3.3) in order to enforce a basic level of transparency and due diligence, and a further component to focus on specific categories of AI applications (3.1) for which there should be an outright ban. We expand on these distinctions below.

On the provision for all AI applications (3.1), we note that all private companies have existing responsibilities to carry out human rights due diligence. Large companies have resources to do this, and through the establishment of national centres of expertise, SMEs can be helped to carry out due diligence procedures for all AI applications. 

For all public sector usage of AI applications, human rights impact assessments (HRIAs) should be carried out, and the results and technical details made available in public registers of AI systems. Such registers should contain publicly accessible information about all AI systems being used or proposed for use in the public sector. Further, a mechanism must be established for contesting the results of impact assessments. In all cases, the possibility of scrapping or not using the system in question must be on the table, if the result of a HRIA shows that the system undermines the fundamental rights of those affected by it.

The legislative instrument must also contain provisions for a ban on certain applications of AI. As Access Now has repeatedly stated, a ban is necessary where mitigating any potential risk or violation is not enough and no remedy or other safeguarding mechanism could fix the problem. We provide details in our uploaded document.

Additional points:
The document overstates the case when it says that AI can “contribute to a wide array of economic and societal benefits across the entire spectrum of industries and social activities.” We have no evidence that AI can make meaningful contributions “across the entire spectrum” and overselling the positive impact of this technology is ultimately harmful and undermines trustworthiness as those expectations cannot be met. 
On the definition of ‘AI’, we believe that the important thing is to regulate the impacts of the technology, rather than the specific technical processes underlying machine learning or neural networks, for example. Where the specific functioning of a given system requires some special consideration, it should be given, but otherwise we must look at AI as a form of automation and apply generally applicable measures.
We support the acknowledgement that SMEs must also fall under the scope of legislation due to the scalability of AI applications. We further note that some of the most harmful developments in AI have come from SMEs and ‘innovative startups,’ such as Clearview AI and PimEyes, both of whom develop surveillance systems which are incompatible with fundamental rights.
The document mentions that “not a lot of currently valid evidence is available at this stage” for the effects of AI systems. This is not the case. Access Now has already provided the Commission with such evidence, and a new report from Algorithm Watch adds more in relation to Covid-19:  'Automated Decision-Making Systems in the COVID-19 Pandemic.'
The document mentions targeted consultations organised with “technical experts, conformity assessment bodies, standardisation bodies and experts on biometric data.” We request that consultations also be arranged with civil society organisations and with representatives of groups likely to be adversely affected by these systems."
F550209,08 September 2020,John BLAKEMORE,Unternehmen/Unternehmensverband,Hutchison Europe,groß (250 oder mehr Beschäftigte),3779891566-57,Belgien,Künstliche Intelligenz – ethische und rechtliche Anforderungen,Please see attached document.,"['CK Hutchison’s comments on the roadmap for regulation of Artificial \nIntelligence \nThe Commission has published its roadmap on Artificial Intelligence (AI), listing the regulatory options \navailable, from doing nothing, issuing guidelines and voluntary labelling, to mandatory regulation of \nsome or all AI applications.   \nWe agree with the need for a regulatory framework for AI.  Having “trustworthy AI” will help with \nconsumers’ acceptance of AI.  At the same time, a regulatory framework should also give developers \nand businesses the confidence to deploy AI in a stable legal and regulatory environment.   \nA new, comprehensive regulatory framework is required, similar to the Electronic Communications \nCode for telecoms or the GDPR for privacy, empowering national regulatory authorities with its \ndetailed application, while ensuring a harmonised approach at the EU level to promote consistent \ndevelopment of AI. \nAny framework for regulating AI should cover all applications and all aspects (including the regulation \nof risk and liability), rather than a subset, and should establish clear definitions of the different actors \nin the AI value chain. \nLimiting regulation to a subset of AI aspects and applications, such as those that are high risk, would \nrequire an ex ante determination of risk and this would likely need constant revision to keep up with \nnew applications.  It is unclear what such an ex ante determination would entail: the identification of \nsituations where employing AI is high risk requires exceptional competence and foresight on behalf of \nthe designers, owners and operators of AI.  The notion that humans need AI to provide better \nsituational choices, and yet that humans must (from a regulatory perspective) be able to anticipate \nand guard against all and any prejudicial outcomes is difficult to envisage.  It could result in many uses \nof AI being regulated in categories that are not appropriate to their risk.  It must also be acknowledged \nthat limiting regulation to a subset of AI applications would fail to provide a regulatory framework for \nother  (non-high  risk)  AI  applications,  leaving  them  with  the  uncertainty  of  an  unregulated \nenvironment.  \nWe are concerned that the European Commission would have to frequently update AI regulation as \nthe field of human expertise (for managing AI) develops, and market experiments mature.  However, \nregulatory updates and amendments may become problematic, and humanly unscalable, when \ndevelopments occur at the speed of machine learning.  It is not clear that the very slow speed of \nhuman oversight, nor iterative regulatory structures, will always be capable of constraining AI \nappropriately due to the limitations of humanly foreseeable consequences.  \nWe are also concerned by the Commission’s list of obligations that would fall on high-risk AI \napplications.  The proposed regulatory obligations, for example the rules on training data, record-\nkeeping about datasets and algorithms, should be framed in a way that does not create unnecessary \nburdens that limit the options for deployment and use.  Imposing too many obligations would impact \nthe ability and incentives to deploy AI, undermining the business cases, with the result that the EU \nbecomes an unattractive market to provide AI solutions.  \nIn summary, we are not convinced that the Commission’s approach of incremental AI regulation is \npracticable. \nInstead, to provide certainty, we propose that any EU-wide regulatory framework should, as simply as \npossible, set out the rules and obligations for all aspects of AI.  This includes having clear definitions \nof the various roles in the supply chain (e.g. developer, user) and identifying where the responsibilities', 'and any liabilities fall.  A clear liability framework should therefore be part of the regulation.  For \ntelecoms businesses it is especially important that network operators are not held liable for AI \napplications they carry but in which they are otherwise not involved.  This requires a provision similar \nto the exemption from liability for content carried over a telecoms network (the “mere conduit” \nprovision).  \nThe Commission should also recognise that AI will be a tool employed for criminal and state security \nventures, and in such cases is beyond the scope of regulation.  Examples include the criminal use of AI \ntools to disrupt network automation, to poison data, to construct deep-fake facial images to purchase \na SIM and many other situations. \nJohn Blakemore \nDirector of European Regulatory Affairs \nHutchison Europe \nSeptember 2020']"
F549988,07 September 2020,David D'Haese,EU-Bürger/-in,-,-,-,Belgien,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"Dear,

I am a teacher in AI at the AP University College in Antwerp particularly concerned with ethical issues. My idea is that Europe should postpone any legislation concerning ethics in AI, as enforcement of such a future law is expected to be not practical and possibly even counterproductive. Instead,  I would imagine that devising and evangelising a clear set of moral rules comparable to Asimov's three 'laws' of robotics would have a real impact and pave to way for future legislation. Europe is currently considered one of the few remaining moral leading institutions in the world and with the propagation of such rule-set, it would reïnforce its position."
F548980,03 September 2020,Ignacio DORESTE,Gewerkschaft,European Trade Union Confederation,groß (250 oder mehr Beschäftigte),06698681039-26,Belgien,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"IIA provides additional but still general information on options the EC intends to consider for legal act on AI. IIA still lacks to address the specificity of the workplace. Imbalance of power between employers and workers should lead EC to consider robust AI frame to create quality jobs, invest in workers AI literacy,  promote and increase safeguarding of workers rights, of workers protection, consider risks/benefits related to OSH and ensure trade unions and workers reps participate actively shaping AI at work. The framework should address these labour approaches in ambitious and proactive manner because workers are particularly concerned by AI

AI is key issue for work and society. It brings opportunities and challenges. In the field of work AI supports often or replaces humans in dangerous, strenuous, and repetitive tasks. Yet there are risks of intrusive surveillance, breach of privacy and data protection and fundamental rights. ETUC thus supports the objective of the potential legal act of fostering development of safe and lawful AI that respects and enforces rights

Liability regime of AI applications at work deserves proper attention where burden of proof should lie on employers to balance limited access to information to workers. Liability should rest on AI designers and business not on AI

ETUC calls for EU legislative instrument with mandatory requirements for all AI, option 3c. Regulating AI cannot be given to private actors. This would lead to wide differentiation on protection, with bias for putting on the market AI based on productivity and competitiveness. EC should fill this gap and enact new EU law implemented through national legislation and/or where relevant collective agreements. Respect of human rights instruments and CFREU should lead any legislative proposal. Ethics Guidelines for Trustworthy Artificial Intelligence should be included in regulation

AI applications introduced at the workplace and impacting workers rights and working conditions should be classified as high-risk

Regarding preliminary assessment of expected impacts, legislation in general and any legally binding act on AI in particular should be addressed in positive manner, as a medium/long term investment in society and its people, where public interest and interests of society outweigh interests of few groups. IA impact assessment should address economic, social, and environmental dimensions on same footing and with same details and accuracy. ETUC believes an EC framework would serve businesses and the workforce, providing legal predictability and certainty and sound level playing field for all. ETUC opposes to exceptions to scope of legal act on the size of the company. Particularly for AI systems applying to employment, level playing field in matters of the guaranteeing fundamental rights and labour rights should be pursued

Concerning expected social impacts, and because of imbalance of power existing in an employment relationship, legal framework should address needs for workers to guarantee through appropriate legislation and collective agreements respect and enforcement of their rights when AI is introduced/implemented. This is crucial to prevent discrimination, abuse of privacy and guarantee data protection, security and OSH

AI will have impact on labour market in terms of the replacement of some jobs, while creating new ones. Challenge is to shape an inclusive transition to fair digital future by minimising the risks and creating opportunities for workers. Precautionary principle should be applied to avoid digitalisation further splits society and exacerbates more unequal distribution of wealth

Regarding the likely impacts on fundamental rights, ETUC strongly believes that only binding requirements will strengthen respect of existing fundamental rights for all AI. The current assessment of the impacts of the EU different options on fundamental rights is vague and superficial. Should be addressed in equal manner as economic.","['Resolution on  the European strategies on artificial intelligence and data  \nAdopted at the ETUC Executive Committee of 2 July 2020 \n \n \n \nKey messages: \n \nRobust European Artificial Intelligence (AI) and Data Strategies should:  \n \n• provide a legal and empowering European framework based on human rights, and therefore \nincluding labour and trade union rights and ethical rules.  \n• maintain and reinforce workers’ protection, prevent disproportionate and undue surveillance at \nwork, prohibit discriminatory treatments on the basis of biased algorithms, and prevent abuse of data \nprotection and privacy, ensuring compliance and going beyond GDPR and maintaining their privacy \nwhen not at work.  \n• classify AI applications affecting workers’ rights and working conditions as high-risk in principle and \nsubject to appropriate regulation.  \n• address the specificity of the workplace, including the bargaining inequality between workers and \nemployers. The principle of ‘human remains in control’ should apply to workers and managers.  \n• provide for AI and digital literacy schemes. Education and transparency of AI systems and of new \ntechnologies is important for workers to be able to understand, and be part of, the fair implementation  \n• provide for data governance at national, sectoral and company level, and strengthening workers’ \nparticipation in the design, deployment, use and monitoring of AI technology and data strategy.  \n• provide for rules on business and developers’ liability, including the reversal of the burden of proof \nin favour of workers, to balance the limited access to information to workers  \n• deliver a fit for purpose innovation, if AI technologies comply with the Treaty based precautionary \nprinciple \n• provide for an EU data strategy with ambitious initiatives on cybersecurity transparency, portability, \ninteroperability, fair taxation, regulating GAFAMs and other major platforms, operationalising the \nright to access, governance for access and sharing data, social benefits of data use/sharing. \n \nIntroduction and Context \nWorkers and labour relations have been facing disruptive transformation since the first industrial \nrevolution. Technology and digital innovation have drastically transformed work and employment in \nunprecedented ways. Digitalisation in society dates back to the introduction of computers, so does \nartificial AI in attempting to reproduce or simulate the functioning of the human brain and physical \nprocesses. AI technologies are not new. However, they have gained new momentum in society, as \nthey have rapidly evolved and covered a large part of the economy. Robotics and automation support \noften replace humans in dangerous, strenuous monotonous and repetitive tasks, as well as in tasks \nrequiring great precision, even on a microscopic scale. AI is now also gradually supporting and \npossibly replacing analytical work. \n \nAI refers to self-learning systems which can take over many human tasks (machine learning). AI is \ndata driven: it is about data, linking data, putting information into a context, knowledge to be applied \nand reflected upon. Combined with robotics and access to big data, AI technologies form the \nbackbone of the digital economy. They impact the daily life of citizens and have reached the \nworkplaces. As such, the introduction and use of AI are key issues for work and society. They bring \nopportunities  and  challenges:  on  one  side  the  undeniable  major  breakthrough  technological \nEuropean Trade Union Confederation  |  Luca Visentini, General Secretary |  Bld du Roi Albert II, 5, B - 1210 Brussels  |  +32 (0)2 224 04 11  |  etuc@etuc.org  |  www.etuc.org', ""advances brought to medicine, space, mobility, communication to name but a few. On the other side \nrisks of intrusive surveillance, breach of privacy and data protection and fundamental rights in \ngeneral are exponentially growing. \n \nThe COVID-19 crisis has changed the societal context dramatically, impacting present and future \ngenerations in an unprecedented rapid way in the long term. Data and artificial intelligence are put \nin the forefront as solutions to tackle the pandemic, in medicine in the search for vaccines, drugs or \ntreatments, as well as in the development of public tracing applications to identify potentially infected \npersons. The opportunities and challenges of such new technologies however should be carefully \nassessed. The Covid-19 crisis has also exacerbated the digital divide in all its aspects. \n \nETUC is convinced that labour protection and technological innovation can be compatible. Ensuring \na just transition towards fair digitalisation and work are ETUC’s guiding principle. While the EU must \nbe at the forefront of technical innovation that benefits people, it should equally base its strategy on \nthe European social model and its fundamental rights and values. The EU must be a driver in \nregulating innovation and in guaranteeing its compliance with workers’ dignity, working conditions \nand well-being. \n \nETUC is of the opinion that an EU framework on AI should address the workplace dimension in an \nambitious and proactive manner because workers are particularly concerned by AI technologies. \nThe imbalance of power between employers and workers should lead the EC to consider a robust \nAI framework to create quality jobs, invest in worker’s AI literacy,  promote and increase the \nsafeguarding of workers’ rights, workers’ protection and ensure that trade unions and workers’ \nrepresentatives participate actively in shaping AI at work. Such an AI framework should cover all \nworkers and employers in the private and public sectors, for all business models including online \nplatforms. \n \nThe EC has recently published two strategies which, steered in the right direction, can contribute to \ninnovate and enhance EU technological leadership as well as to respond to new challenges, like \nthe pandemic. These strategies are key determinants through which Europe is building its digital \nfuture1:- A White Paper on ‘Artificial Intelligence: a European approach to excellence and trust’2 \nand European Data Strategy3. It should be pointed out that the global AI market is currently led by \ncountries that often do not comply with human rights in the development of this technology. \nTherefore, in its attempts to enhance the EU technological leadership, the EC should not \ncompromise on issues such as human and workers’ rights nor on the necessary involvement of \nworkers’ representatives and trade unions in shaping sustainable and ethical AI tools. \n \nArtificial intelligence is undisputed for the sustainability of the economy, but Europe is lagging far \nbehind the USA and Asian countries. Only a small percentage of companies use AI or plan to do so \nin the coming years. In this respect, the Commission's objective to mobilise l 20 billion annually in AI \ninvestments over the next ten years is an important step. \n \nThe EC rightly echoes the call raised by ETUC and the trade unions and different civil society \norganisations to reinforce the governance of data use in society and in the economy, in particular as \ndata is the source of AI. This ETUC resolution addresses the main areas that the governance of data \nand AI should cover. \n \nThe EC has published a range of communications related to digitalisation simultaneously amongst \nthe main ones, the communication on Shaping Europe’s digital future4, and a new industrial strategy \nfor Europe dealing with digital transition5. In doing so, the EC clearly shows the strategic importance \nof digital technologies and of data. Most of the actions taken and foreseen in the digital fields intend \nto further deepen the internal market and in particular the Digital Single Market and will impact EU \n \n1 https://ec.europa.eu/commission/presscorner/detail/en/ip_20_273 \n2 https://ec.europa.eu/info/files/white-paper-artificial-intelligence-european-approach-excellence-and-trust_en \n3 https://ec.europa.eu/info/files/communication-european-strategy-data_en \n4 https://ec.europa.eu/info/sites/info/files/communication-shaping-europes-digital-future-feb2020_en_4.pdf \n5 https://ec.europa.eu/info/sites/info/files/communication-eu-industrial-strategy-march-2020_en.pdf \nETUC/EC241/EN/12  2"", 'industry, in particular the Digital Services Act package. ETUC welcomes the holistic approach to \ndigitalisation and sees the willingness to provide coherence and consistency between the various \ninitiatives to digitalise Europe, the economy and provide a better access to digital tools and \ninfrastructure to business, workers and citizens. ETUC will however deal with digitalisation in a \nseparate ETUC position on the new industrial strategy for Europe, due to its large scope.  \n \nWhy  is  it  important  for  ETUC  to  take  a  position  on  European  AI  and  EU  data \nstrategies? \n \nETUC needs to remain active in the societal debate on AI to make it compatible with the objectives \nof social Europe, decent work and social progress. While the prospect of creating new employment \nopportunities, higher productivity and better earnings due to the rapid developments in AI have \nemerged, there are also fears that AI could also cause job losses and increase inequality. Investment \nin educating and up-skilling/re-skilling is therefore of utmost importance. Education policies aimed to \nbetter equip workers with the skills and competences needed to design and operate AI systems are \ncrucial, however they will not be sufficient. Market dominance and market concentration of a handful \nof digital firms developing AI technologies and investing in AI ventures is a concern. Furthermore, \ntax policies should provide for a more balanced level playing field among companies, so as to allow \nAI technologies and their benefits to be shared more equally. \n \nETUC  strongly  believes  that  existing  legislative  tools  like  the  GDPR  do  not  adequately  and \nsufficiently protect workers from the downsides of AI technologies, such as work-related stress and \nexcessive pressure from intensive work schedules defined by AI. As developed further in this \nresolution, ETUC advocates for the strengthening of GDPR in the context of employment. ETUC \nfurther calls on national governments to ensure that institutions responsible for ensuring compliance \nwith GDPR have adequate resources to do so. \n \nLikewise,  the  ETUC  is  of  the  opinion  that  social  dialogue  structures,  collective  bargaining, \ninformation, consultation and participation of workers’ representatives and trade unions are key to \nproviding the necessary support for workers to better shape the introduction and monitoring of AI. \nTherefore ensuring that AI is respectful of workers’ rights. \n \nEmployers’ responsibility and accountability as industrial users of AI technologies, as well as ethical \nrules  for  developers  and  coders  of  algorithms,  should  create  the  appropriate  legal  certainty, \npredictability, and sound level playing field. Algorithms must comply with the rule of law and \nfundamental  rights  and  be  democratically  designed  and  be  understandable.  They  should  be \nembedded  in  ethical  rules.  Free  access  to  the  source  code  must  be  ensured  before  the \nimplementation of the AI system in the workplace. Risk assessments should demonstrate AI \napplications’ effectiveness and compliance with verifiable transparency obligations for AI providers \nof an AI tool to be used at the workplace, as a prerequisite for placing it on the market. \n \nETUC recalls that an extensive and unregulated trust in technologies, can lead to costly failures. In \nextreme cases to fatalities. Self-driving vehicles have created accidents; workers have been made \nredundant on the basis of algorithmic decisions and platform workers have been fired for undertaking \nindustrial  action.  The  algorithm  places  at  the  heart  of  the  problem  the  arbitrariness  and \ndecontextualisation of the decisions taken. Businesses have been hacked and workers’ data have \nbeen seriously compromised. The protection of workers is therefore at stake. Proactive action should \nbe undertaken to guarantee occupational safety, health, data protection, privacy, and human rights. \nAbove all, it is of paramount importance that humans remain in command of the use and implications \nof any AI technology. Only a transparent regulatory framework can lead to predictability and secure \nthe respect of fundamental rights of workers including privacy, and in which security and oversight \nare guaranteed. \n \nTherefore, it is important for ETUC to raise workers’ concerns and demands in the context of the EC \nwhite paper on AI, to ensure that AI and data driven technologies applied at the workplace are done \nin a safe, predictable and reliable legal framework. Such a framework must be without prejudice to \nnational labour market models and the autonomy of national social partners and must encourage \ncollective bargaining. ETUC stresses the importance of adopting a normative framework enabling \nETUC/EC241/EN/12  3', 'both workers and businesses to benefit from the opportunities of digitalisation, as well as establishing \nmeasures to prevent risks, work related abuses and violations of workers’ rights. The introduction of \nAI technologies at the workplace should involve workers’ representatives before introducing those \ntechnologies so as to organise upskilling and reskilling schemes. It should not lead to redundancies. \nETUC stresses the importance of adapting the current legislative framework to enable both workers \nand businesses to benefit from the opportunities of digitalisation, as well as establishing measures \nto prevent risks, work related abuses and violations of workers’ rights. \n \nETUC wants to make sure that real opportunities of deployment of AI and data are available to \nworkers, and their representatives, and that opportunities are available for social partners in the \ncontext of social dialogue at European, national, sectorial and company level. AI can be useful to \nhelp in arduous tasks, improve security, improve performance and redistribution of gains among the \nworkers. \n \nWhat are the current international and national legal protection? \n \nInternational and European legal instruments reaffirm the need to protect and involve workers when \nit comes to introducing new technologies at work. They build the fundamental basis on which any \npolicy on AI and data should be elaborated. \n \nInternational Standards \n \nThe first fundamental principle in the ‘Declaration of Philadelphia’ of the International Labour \nOrganization (ILO), is that “labour is not a commodity”. Workers’ rights and working conditions stem \nfrom this principle. When it comes to AI and data strategies, workers’ data and workers’ privacy are \nnot a commodity either, such data should not be extracted and processed for business profit or for \nany other commercial purposes. \n \nThe  ILO    Centenary  Declaration  for  the  future  of  work  adopted  by  the  International  Labour \nConference at its 108th session, in Geneva, on 21 June 2019, refers in its section III  that ‘The \nConference calls upon all Members, taking into account national circumstances, to work individually \nand collectively, on the basis of tripartism and social dialogue, and with the support of the ILO, to \nfurther develop its human-centred approach to the future of work by (…) promoting sustained, \ninclusive and sustainable economic growth, full and productive employment and decent work for all \nthrough policies and measures that ensure appropriate privacy and personal data protection, and \nrespond to challenges and opportunities in the world of work relating to the digital transformation of \nwork, including platform work’6. \n \nAlso, at the international level, the OECD has published ‘Principles on AI’, it specifically calls for \nworkers to be supported for a fair transition7. \n \nCouncil of Europe \n \nSurveillance, monitoring, and tracking practices are not only intrusive but raise specific concerns \nabout data protection. Related risks including the misuse of these technologies are extremely high, \ngiven the economic and financial importance of, and weight given to, data nowadays: data is the \nnew gold.  ETUC recalls the safeguards for the right to privacy and the right to data protection, laid \ndown by the Council of Europe in the ‘Convention for the Protection of Individuals with regard to \nAutomatic Processing of Personal Data’ the so-called ‘Convention 108+’8. In April 2020, the Council \nof Europe’s Committee of Ministers adopted Recommendation CM/Rec(2020) on the human rights \nimpacts  of  algorithmic  systems,  this  provides  a  set  of  guidelines  calling  on  Member  State \ngovernments to ensure that they do not breach human rights through their own use, development or \n \n6 https://www.ilo.org/wcmsp5/groups/public/---ed_norm/---relconf/documents/meetingdocument/wcms_711674.pdf \n7 https://www.oecd.org/going-digital/ai/principles/ \n8 For the moment the ‘old Convention 108’ is still in force. The Convention 108+ will only enter into force when ratified by all Parties to \nTreaty ETS 108, or on 11 October 2023 if there are 38 Parties to the Protocol at this date. Today (June 2020) the latter is ratified by \nBulgaria, Croatia, Lithuania, Poland and Serbia. \nETUC/EC241/EN/12  4', 'procurement of algorithmic systems as well as to ensure that governments, as regulators, should \nestablish effective and predictable legislative, regulatory and supervisory frameworks that prevent, \ndetect, prohibit and remedy human rights violations, whether stemming from public or private actors. \nFurthermore, reference can also be made to the ’Declaration of the Council of Europe on ‘Digital \ntracking and other Surveillance technologies’9. As well as to the Recommendation on Artificial \nIntelligence and Human Rights ""Unboxing artificial intelligence: 10 steps to protect human rights"" of \nthe Council of Europe Commissioner for Human Rights of May 2019. \n \nOn 11 September 2019, the Committee of Ministers of the Council of Europe set up an Ad hoc \nCommittee on Artificial Intelligence  (CAHAI), the main objective of which is to examine the feasibility \nand potential elements, on the basis of broad multi-stakeholder consultations, of a legal framework \nfor the development, design and application of artificial intelligence, based on the Council of Europe’s \nstandards on human rights, democracy and the rule of law10.  Finally, it was decided to set up within \nthe Steering Committee of Human Rights (CDDH), the overarching monitoring body to all Council of \nEurope conventions and instruments, a dedicated subgroup, a ‘Drafting Group’, on human rights and \nartificial intelligence (CDDH-INTEL).  The main task of this Drafting Group outlined as follows: ‘on \nthe basis of developments in the member States, within the Council of Europe and in other fora, \nprepare, if appropriate, a Handbook on Human Rights and Artificial Intelligence and contribute to \npossible standard-setting work which would be undertaken within the Organisation’. \n \nThe Council of Europe also reflected 11 on the impact of AI during the Covid-19 pandemic, providing \na detailed list of states implementing Covid-19 apps for tracing. These reflections show that AI is on \nthe one hand being used as a tool echoing the high hopes that data science and AI can be used to \nconfront the virus. On the other hand, AI seems to lead to a new era of personal control, revealing \nnew capacities for state surveillance of individuals, in particular when it becomes binding and \ncompulsory by making it part of ordinary law. More substantially, it may also create new cases of \ndiscrimination and stigmatisation, via contact tracing or proximity tracing. The trustworthiness and \nusefulness of these AI algorithms are therefore key and should be addressed to prevent inequalities. \nThe extensive use of AI technologies may also lead to the acceptance of such applications, even \nwhen the pandemic crisis is over, to maintain public order or control employees. Therefore, the \nquestion of the limitation and control of AI technologies impacting freedom and rights should be \naddressed. \n \nIn the same vein, the European Convention on Human Rights foresees that the ‘right to respect for \nprivate and family life’, whereby, ‘Everyone has the right to respect for his private and family life, his \nhome and his correspondence’12. It should also be recalled that Article 11 of the European Social \nCharter (ESC) enshrines the right to protection of health. In a ‘statement of interpretation on the right \nto protection of health in times of pandemic’ of April 2020, the European Committee of Social Rights \n(ECSR), the main supervisory body of the ESC, acknowledged that Member States must take all \nnecessary emergency measures in a pandemic and that this might include, amongst others, testing \nand tracing. However, all such measures must be designed and implemented having regard to the \ncurrent state of scientific knowledge and in accordance with relevant human rights standards. \n \nEuropean Union \n \nThe Treaty of the European Union foresees that ‘everyone has the right to the protection of their \npersonal data. The European Parliament and the Council (…), shall lay down the rules relating to \nthe protection of individuals with regard to the processing of personal data by Union institutions, \nbodies, offices and agencies, and by the Member States when carrying out activities that fall within \n \n9 https://rm.coe.int/09000016805916c9 \n10 For a more dedicated information on the work of the Council of Europe and AI, see also: https://www.coe.int/en/web/artificial-\nintelligence/home \n11 https://www.coe.int/en/web/artificial-intelligence/ai-covid19 \n12 Art. 8 ECHR https://www.echr.coe.int/Documents/Convention_ENG.pdf. It is further specified that There shall be no interference by a \npublic authority with the exercise of this right except such as is in accordance with the law and is necessary in a democratic society in \nthe interests of national security, public safety or the economic well-being of the country, for the prevention of disorder or crime, for the \nprotection of health or morals, or for the protection of the rights and freedoms of others. \nETUC/EC241/EN/12  5', 'the scope of Union law, and the rules relating to the free movement of such data. Compliance with \nthese rules shall be subject to the control of independent authorities.13. \n \nThe Charter of fundamental rights of the EU foresees that ‘Everyone has the right to the protection \nof personal data concerning him or her. Such data must be processed fairly for specified purposes \nand on the basis of the consent of the person concerned or some other legitimate basis laid down \nby law. Everyone has the right of access to data which has been collected concerning him or her, \nand the right to have it rectified. Compliance with these rules shall be subject to control by an \nindependent authority’14. \n \nThe Fundamental Rights Agency FRA dedicated a large range of its activities to data protection15. \nHowever, little to no attention is given to workers’ data protection and AI applications at work. \n \nEU directives and regulations framing minimum labour standards fall short on providing worker \nprotection when it comes to the impact of AI technologies at work. The European Framework \nDirective on Safety and Health16 has not been adapted and revised taking into account emerging \nand new risks pertaining to AI. It is however worth mentioning the general principle provided by the \nFramework Directive of setting the responsibility on risk prevention on the employer. The General \nData Protection Regulation17 has only one provision referring to processing worker’s data at the \nworkplace. ETUC and trade unions have repeatedly requested that these instruments should be \nadapted according to the reality of the workplaces of today. \n \nIn 2019, the EC issued guidelines on ethics for trustworthy AI18, this was elaborated by the High-\nLevel Expert Group on AI, according to which AI should be lawful  (i.e. respecting all applicable laws \nand regulations), ethical (i.e. in line with ethical principles and values, and robust (both from a \ntechnical perspective while taking into account its social environment). Furthermore, AI technologies \nshould meet 7 requirements: 1. human dimension and oversight, 2. technical robustness and safety, \n3. privacy and data governance, 4. transparency, 5. diversity non-discrimination and fairness, 6. \nenvironmental and societal well-being and 7. accountability. The 2020 European Social Partners \nFramework Agreement on digitalisation includes a section on Artificial Intelligence recalling the need \nfor the human-in-control principle, in which there is a commitment to respect and comply with safety \nand security controls. \n \nWithin the European Commission strategy ""Shaping Europe\'s digital future"", ETUC provided input to \nthe public consultation on the White Paper on Artificial Intelligence and that on the European Strategy \nfor data, stressing the human rights dimension which applies to the digital revolution and with a focus \non labour and trade union rights.  \n \nThis human rights approach has been the ETUC’s stance when contributing to recent initiatives of \nthe  European  Parliament  on  Artificial  Intelligence,  namely  the  ""Digital  Services  Act:  adapting \ncommercial  and  civil  law  rules  for  commercial  entities  operating  online""  [JURI/9/02298 \n2020/2019(INL)], the ""Framework of ethical aspects of artificial intelligence, robotics and related \ntechnologies"" [JURI/9/02261 2020/2012(INL)], and the ""Civil liability regime for artificial intelligence"" \n[JURI/9/02298 2020/2019(INL)]. \n \nETUC will follow up any further action on AI and data to be undertaken by the European governing \nbodies, such as the Digital Services Act aiming to establish clear rules for all businesses to access \nthe Single Market, to strengthen the responsibility of online platforms and to protect fundamental \nrights, which the Commission will present later this year. It will also propose a review of the eIDAS \n \n13 Art. 16 TEU https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX:12012E/TXT  \n14 Art. 8 CFREU https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX:12012P/TXT&from=EN \n15 https://fra.europa.eu/en/themes/data-protection  \n16 https://osha.europa.eu/en/safety-and-health-legislation/european-directives  \n17 https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX:32016R0679&from=EN  \n18 https://ec.europa.eu/digital-single-market/en/news/ethics-guidelines-trustworthy-ai  \nETUC/EC241/EN/12  6', 'regulation19, allowing for a secure electronic identity that puts people in control of the data they share \nonline. This resolution will provide the basis for ETUC’s contribution to the development of these \ninitiatives. The White Paper on Artificial Intelligence, setting out options for a legislative framework \nfor trustworthy AI, was adopted on February 2020, and a follow-up on safety, liability, fundamental \nrights and data is scheduled for the fourth trimester of 2020. The European Data Strategy aimed at \nmaking Europe a global leader in the data-agile economy was adopted in February 2020 and a \nlegislative framework for data governance is announced for the fourth trimester of 2020. A possible \nData Act may be put forward in 2021. New and revised rules to deepen the Internal Market for Digital \nServices, by increasing and harmonising the responsibilities of online platforms and information \nservice providers and reinforcing the oversight over platforms’ content policies in the EU, may be \nscheduled for the end of 2020, as part of the Digital Services Act package. \n \nAt the beginning of 2019, the CEN (European Committee on Standardization) and CENELEC \n(European Committee for Electrotechnical Standardization) Technical Boards created the Focus \nGroup on AI aimed at mirroring the activities at international level.  In this manner they could identify \npotential specific European requirements, and act as an interface with the European Commission. \nThis Focus Group is not meant to develop standards on AI but acts more as a forum to discuss \nEuropean policy and how it can translate in terms of standardisation. It also explores the potential \napplication of existing standardisation tools for AI. In the framework of ETUC’s work on standards, a \ntrade union task force has been created that follows the work of this Focus Group. \n \nThe Conclusions of the Council of the European Union on “Enhancing Well-being at Work”, invites \nMember States to “enforce the existing Union Framework, keeping in mind the Member States\' \npossibility  to  go  beyond  the  minimum  requirements  laid  down  therein,  proceed  with  the \nimplementation of the Work-Life Balance Directive and follow the Council Recommendation on \naccess to social protection for workers and the self-employed"".  \n \nETUC demands \n \nETUC’s demands on an EU framework for AI and an EU data related strategy address the need: for \ninclusive governance and clear rules; adequate regulation and policy infrastructure securing good \nprotection of workers and necessary investment; to secure that the precautionary principle should \napply to AI technologies; to guarantee AI technologies, by which humans remain in control, and \nwhich is compliant with labour rights and a sound use of data; to strengthen the application of GDPR \nto the reality of the workplace, and to involve social partners and strengthen industrial democracy. \n \n1. Ensure governance of data, AI and markets \n \nThe EU Commission intends to promote cross-sectoral measures for data access and use for a data-\nagile  economy,  involving  both  private  and  public  players  and  points  to  the  prioritisation  of \nstandardisation activities over legislation to foster data interoperability. \n \nETUC  is  convinced  of  the  need  for  clear  legal  provisions  on  the  responsible  use  of  data, \ninteroperability of the systems and conditions for data access. Such rules would not only provide \nmore  predictability  and  legal  certainty  for  business,  they  would  make  the  market  fairer  for \nbusinesses, workers, consumers and citizens. In managing better big data, interoperability rules \nwould enable other European service providers to enter the market and prevent concentration and \nquasi-monopoly or an excessive share in the market. The COVID-19 crisis has shown how video-\nconference services have been dominated by non-European players which have led to scandals on \ndata protection policies as GDPR does not apply to non-European players. Increased diversification \nwould ensure a more transparent and democratic environment. \n \nSecurity of data storage and access is key, therefore the EU and the Member States need to remain \nin control. It should be technically feasible and secure to ask an organisation to transfer the \nindividual’s personal data to another organisation, or to receive the data in an interoperable format. \n \n19 EC Regulation No 910/2014 of 23 July 2014 on electronic identification and trust services for electronic transactions in the internal \nmarket, eIDAS. \nETUC/EC241/EN/12  7', ""By doing this, individuals should get control over their data and organisations should demonstrate \ntheir compliance with EU data protection and privacy rules. Appropriate and effective sanctions \nshould deter from any violations of data protection rules. Currently, tracing applications barely \nprovide for the necessary security, in particular in terms of anonymisation and pseudonimisation of \nthe data collected and stored and access to it. The consent of workers is not properly informed, as \ninformation is often hidden or difficult to access, or actualisation of apps set tracing application as \ndefault option. Workers must receive clear and transparent information of the purpose and the use \nof the collection of their data. Collection of workers’ data must be done in consultation with and \nparticipation of the trade union representatives. The Data Protection Officers should be informed. \nWorkers must be able to make a free and voluntary decision. \n \nThe additional challenge is to ensure that public services have the capacity – particularly in terms of \nresources, staff and training – to be able to control the introduction of AI and make the necessary \ninvestment in independent AI technology. Public service policies on AI should be driven by the public \ninterest and users’ real needs. It is also crucial that public authorities continue to explore the potential \nto develop independent and autonomous digital public services and infrastructure. \n \nAdditionally, ETUC calls for an inclusive framework that protects and promotes open-source, no-\nprofit, social economy initiatives as relevant actors. A framework that gives these sectors the \npossibility to exist as AI and data driven technologies, which are mostly dominated by the corporate \nsector. An AI and data governance framework will help to achieve a common approach to operate \nin the European Digital Single Market, and a fair ground for the labour market where worker’s rights \nare guaranteed. \n \nData access, storage and processing compliance with GDPR is key. The EC’s strategy calls for the \nestablishment of mechanisms to make it easier for individuals to allow the use of the data they \ngenerate for the public good, a concept that the strategy refers to as “data altruism”. ETUC questions \nthe altruism objectives that may be pursued for commercial purposes and it expresses concern about \nviolation of privacy and surveillance, mostly in vulnerable situations like the employment relationship.   \n \nThe EC’s standpoint, when claiming that individuals allow the use of the data they generate on the \nbasis of explicit and unambiguous consent, is controversial. Situations such that data collection (not \nleast in the light of the coronavirus spread) where consent is based on opting out of a system, or \npractically inexistent or “forced”, have proven the contrary. In addition, risks brought by biased and \nmalicious management of data cover potential issues of discrimination, unfair practices and “lock-in \neffects” and cybersecurity threats. It should be reminded that non-discrimination is a fundamental \nright to be complied with. \n \n2. Develop regulation for all platforms in the EU market \n \nThe situation of platform workers is not covered in this resolution, as it will be addressed by the \nETUC in its upcoming position for the protection of non-standard and platform workers' rights.  \n \nCybersecurity is a key element of data protection, even at an individual level and requires substantial \ninvestment and specific training plans: education and trainings allows the creation of the necessary \nculture to recognise and limit cyber risks from the very beginning. \n \nTo help Europe achieve the Sustainable Development Goals set out by the United Nations, a strong \ndata governance framework should be in place. Such a framework should include the regulation of \nthe GAFAMs20 and similar platforms in the market, which are increasingly integrating different \nbusiness activities, distorting fair competition in the market. Competition rules on anti-trust should be \nadapted to more effectively address the specificities of digital markets characterised by platforms \nwith significant network effects, including risks of monopsony effects. Such regulation should be \nclear,  dissuasive  and  prevent  structural  competition  problems  resulting  in  unfair  competition, \nincontestable market concentrations and exacerbation of existing (social) inequalities. The rules \n \n20 https://ec.europa.eu/digital-single-market/en/news/ethics-guidelines-trustworthy-ai \nETUC/EC241/EN/12  8"", 'must be designed to defend public interests, beyond mere competition or economic considerations, \nalso taking into account effects on the labour market, working conditions and workers’ rights. \n \nDigital companies should contribute in a fair and progressive manner in terms of the tax on revenues \ngenerated within the EU. According to estimates, the EU has lost €5 billion in tax from Google and \nFacebook over the last three years21. Public authorities must be able to put in place fair taxation on \nadded value and capital, and on companies benefitting the most from the transition, like in the digital \nsector, and to benefit from a fair share of this massive wealth, to pay for the vital services and \ninfrastructure these companies depend on. This would echo demands and actions of member states \nlike France to adopt digital taxation, in the expectation of the finalisation of long awaited global digital \ntax negotiations. \n \nEqually, GAFAMs and other platforms should be subject to, and must comply with, the GDPR and \nprivacy rules in Europe. Workers’ rights should be fully respected, and workers’ representation \nshould be fully guaranteed. \n \n3. Europe needs adequate regulation and policy infrastructure  \n \nThe EC strategy on AI and data calls for the implementation of the different pieces of applicable \nlegislation  to  the  actions  of  the  strategy  through  “self-  and  co-regulatory  mechanisms  and \ntechnological means to increase trust”. \n \nThere is however little applicable AI legislation, in general and even less when it comes to the \nworkplace. In addition, delegating the responsibility to regulate AI to private actors with vested \ninterests is questionable. This will inevitably lead to wide differentiation in terms of protection, with a \nclear bias for putting on the market AI technologies on the basis of productivity and competitiveness.  \nETUC calls on the Commission to fill this regulatory gap and to enact new EU law to be implemented \nthrough national legislation or, where relevant, through collective agreements. As workers interact \nwith digital and fast changing working environment, there needs to be a framework that protects \nthem. ETUC is of the opinion that a European AI regulatory framework should be adopted and \nimplemented. ‘Ethics Guidelines for Trustworthy Artificial Intelligence’ should be included in such \nregulatory  framework.  It  would  equally  serve  businesses  and  the  workforce,  providing  legal \npredictability and certainty and a sound level playing field for all, whereas ethical guidelines deliver \nlittle to no practical added value and lead to a fragmented approach. The same goes for issues \nrelated to liability. \n \nIn  the  same  vein,  the  Commission  seems  to  favour  self-regulation,  codes  of  conduct,  and \nstandardisation processes regarding the creation of a cloud data services marketplace that should \ncomply with European requirements on data protection and security. Again, it is questionable \nwhether private norms can and should provide the necessary safeguards in term of security and \nreliability of AI technologies. They can only be complementary to legally binding rules that guarantee \na common playing field, which is much needed when it comes to regulating data servers operating \nin the EU. Cloud services in particular should be secure and reliable. \n \nSince cloud data services process a vast amount of data of individuals, ETUC calls on the EU \nCommission to assess the risks related to cloud services and to provide clear rules to regulate them. \nIn times of technological and societal changes, the EC should foster a governance framework to \nenhance data protection while keeping control of it. Such governance should include trade unions \nand workers representations, who are aware of the situations and needs of workers and businesses \nalike.  Furthermore, it will serve to guarantee the risk assessment of the development and use of AI \nsystems at the workplace. \n \nSuch a framework should not prejudice the national labour market models and the autonomy of \nnational social partners and must encourage collective bargaining. \n \n21 GUARASCIO, Francesco. (2017, September 13) EU lost up to 5.4 billion euros in tax revenues from Google, Facebook: report. \nReuters. Retrieved from: https://www.reuters.com/article/us-eu-tax-digital/eu-lost-up-to-5-4-billion-euros-in-tax-revenues-from-google-\nfacebook-report-idUSKCN1BO226 \nETUC/EC241/EN/12  9', '4. The precautionary principle should apply to AI technologies \n \nETUC is of the opinion that innovation and precaution go hand in hand and there is no evidence that \nprecaution hinders innovation. The often-invoked concept of “innovation principle”, with no legal \nvalue, is used to detract the precautionary principle, which is enshrined in the Treaty of the \nFunctioning  of  the  European  Union22.  The  precautionary  principle  can  sustain  technological \ndevelopments and give direction to innovation. More specifically, ETUC believes that it is key in the \ndecision-making process, where uncertainty prevails about the unintended consequences and the \npotential adverse effects that a technology might bring. \n \nThe Committee of Ministers of the Council of Europe has recently issued recommendations23 related \nto the impact of algorithmic systems on human rights. It called on its 47 Member States to take a \nprecautionary approach to the development and use of algorithmic systems and adopt legislation, \npolicies and practices that fully respect human rights. This rationale becomes even more relevant at \nthe workplace where workers are in an imbalanced power-relation derived from their employment \ncontract,  which  is  even  more  disadvantageous  for  atypical  and  precarious  workers.  The \nrecommendations of the Council of Europe are aimed at employers. Where automated decision \nmaking at the workplace can negatively impact workers, the precautionary principle should apply. \nThe ETUC is of the opinion that such technologies are inappropriate to resolve issues relating to \nboth individual and collective labour relations. \n \n5. Prioritise AI applications embedding labour rights and a sound use of data. \n \nThe deployment of AI technologies at the workplace can have many applications, where benefits \ncan be real. For example, these technologies can be deployed to improve safety and security, \nimprove working conditions through assistance systems, or to help master challenging tasks. These \napplications, and how they are used, need to be transparent to the workforce, so that workers can \nunderstand the purposes and outcomes of such applications as well as the use of the collected data. \n \nHowever, the risks associated with AI technologies are equally important and may vary depending \non the sector, the context in which the technology is applied, and the national regulatory framework. \nRisks should be properly identified and addressed to be avoided and mitigated. Specific attention \nshould be paid to the prevention of occupational health and safety risk, including psychosocial risks. \nETUC demands that risk assessment be carried out to counteract the possible adverse outcomes. \nThis also requires the involvement of trade unions. A prerequisite for risk assessment in the \nworkplace would set out the duty of AI developers and designers towards transparency and \ninformation about the functionality, opportunities and possible adverse effects that the AI technology \nand, in particular, the algorithms could bring. This is key to clarifying the responsibilities of the various \nactors involved. ETUC challenges current business models that abusively collect and use workers’ \ndata. \n \nAlgorithms are well-known to be a possible source of different kinds of discrimination, as they are \noften built on stereotypes and limited diversity in data sets and design team; ETUC identified high \nrisks of gender-based discrimination24. Equal emphasis should be put on risks related to the use of \nbiased algorithms when it comes to violation of workers’ rights. A significant example is Deliveroo’s \npractice to disconnect workers from the platform because of their industrial action. The same applies, \nwhen algorithms are circumventing hiring and firing procedures, where decisions are left to AI \napplications, for example Amazon algorithms dismiss workers without the intervention of a human \n \n22 See Art.191 TFEU \n23 Recommendation CM/Rec(2020)1 of the Committee of Ministers to member States on the human rights impacts of algorithmic \nsystems (Adopted by the Committee of Ministers on 8 April 2020 at the 1373rd meeting of the Ministers’ Deputies). Available at: \nhttps://search.coe.int/cm/pages/result_details.aspx?objectid=09000016809e1154  \n24 ETUC (2020) Artificial Intelligence will it make bias against women worse? https://www.etuc.org/en/publication/artificial-intelligence-\nwill-it-make-bias-against-women-worse \nETUC/EC241/EN/12  10', 'decision  on  the  basis  of  programmed  benchmarks25.  Such  practices  should  be  normatively \naddressed, discouraged and sanctioned. ETUC opposes business models unduly collecting and \nusing workers’ data. ETUC asks the governments’ institutions to support research to develop \nmeasurement  studies  and  bias  prevention  methodologies  to  prevent  the  spread  of  biased \ndiscriminatory algorithms. Algorithms should be audited in relation to their purpose. The use of \ntraceability helps to guarantee the social and environmental conditions of the manufacturing of AI \nproducts and services. \n \nThe EC White Paper proposes a risk-based approach to help ensure that the regulatory intervention \nis  proportionate.  Basically,  the  Commission  suggests  normatively  regulating  only  those  AI \napplications that, on the basis of criteria yet to be defined, are considered ‘high risk’26. All other AI \napplications would thus be only subject to voluntary self-regulation. \n \nThe ETUC welcomes the fact that ‘in light of its significance for individuals and of the EU acquis’ the \nEC has identified ‘the use of AI applications for recruitment processes as well as in situations \nimpacting workers’ rights be always considered as “high-risk”’ in principle, and has proposed \ncorresponding regulations to guarantee a higher protection for workers. ETUC explicitly demands \nthat AI applications introduced at the workplace and impacting workers’ rights and working conditions \nshould be classified as high-risk in principle and subject to appropriate regulation.  ETUC also \ndemands that the high-risk classification for applications affecting workers be extended to all \nemployed persons and include the working conditions and career prospects of employees. For \nconsistency and coherence, such rules should be binding and enforceable, via legislation and/or \ncollective agreement, as voluntary guidelines have reduced and fragmented impacts, and their \neffectiveness is very limited.  ETUC believes that the liability regime of AI applications at work \ndeserve proper attention, whereby the burden of proof should lie on employers, to balance the limited \naccess to information to workers. Liability should rest on developers, coders, AI  designers and \nbusiness, and not with AI systems. \n \nETUC calls for algorithmic transparency and accountability. All algorithm-based decisions should be \nexplainable, interpretable, understandable, accessible, concise, consistent, transparent, in line with \nGDPR principles and provisions. To comply with this requirement, all algorithm-based decisions that \nhave an impact on workers must be audited by an independent body. The approach to data \nprocessing needs to be dynamic because its use is part of a process, from  the extracting process \nto the correlations and causations between them. \n \nETUC calls for greater diversity in the designing of algorithms which takes into consideration gender \nperspective and the heterogeneity of society. Consequently, diversity standards must be established \nin the teams that program these computer solutions, favouring the presence of women and minority \ngroups in them, in order to transpose the existing gender balance in our society. For all these \nreasons, we propose to articulate measures to promote gender equality and diversity among those \nresponsible for programming and auditing algorithms. \n \n6. Strengthen GDPR rules at work \n \nThe EU white paper on AI and the EU strategy for data barely take into account workers’ interests. \nIt is unfortunate that no reference is made to Art.88 GDPR, on processing data in the context of \nemployment which could be used as leverage for enhanced data protection for workers.  Such data \ncould specifically relate to recruitment, performance, management, planning and organisation of \nwork, equality and diversity in the workplace, health and safety at work, and dismissals. Furthermore, \ntrade union representatives should be involved in monitoring the compliance with the GDPR of a \ngiven AI system at the workplace. The aim is to lay down measures to safeguard the human dignity, \n \n25 How Amazon automatically tracks and fires warehouse workers for ‘productivity’. \nhttps://www.theverge.com/2019/4/25/18516004/amazon-warehouse-fulfillment-centers-productivity-firing-terminations \n26 As example the White Paper suggests that ‘In light of its significance for individuals and of the EU acquis addressing employment \nequality, the use of AI applications for recruitment processes and in situations impacting workers’ rights would always be considered \n“high-risk”. (…)The use of AI applications for the purposes of remote biometric identification and other intrusive surveillance \ntechnologies, would always be considered “high-risk” and therefore the below requirements would at all times apply.’ \nETUC/EC241/EN/12  11', 'legitimate interests and fundamental rights, with particular regard to the transparency of processing \ndata, the transfer of personal data within a group of undertakings, or a group of enterprises engaged \nin a joint economic activity, and monitoring systems at the workplace. \n \nETUC calls on the Commission to enhance the Union framework on privacy, data protection and \nsecurity, to better regulate the internet-connected objects and networks that connect and process \ndata.  Therefore,  ETUC  is  of the  opinion  that employers should  inform  workers  and  workers’ \nrepresentatives about which data is collected, where this data is stored, who or which institution or \norganisation is processing this data, the purpose of this data processing and to whom or to which \nother institutions or organisations this data is transmitted. ETUC calls on the Commission to enhance \nthe EU framework on privacy, data protection and security, to better regulate the internet-connected \nobjects and networks that connect and process data. \n \nETUC is of the opinion that any EU framework on AI should therefore address specifically workplace \nrelated situations and tackle possible abuses. It should promote the recourse to social dialogue and \ncollective agreements to shape a sustainable design, introduction and monitoring of AI technology \nat work. Additionally, trade unions should be able to contact national data protection authorities and \nprovide them with information about specific situations of workers. \n \nAs a prerequisite, trade union representatives should be equipped with the necessary skills and \nknowledge to cope with this request and be involved in the process. Such support rests on \ninformation  and  consultation  rights  enshrined  in  EU  and  national  legislation  and  collective \nagreements. They should therefore be observed as mandatory rules for the governance of data \nappertaining to labour. However, the proposed framework fails to address the specific needs for \nworkers in the imbalance of bargaining power existing in an employment relationship to secure their \nrights. \n \nWhenever workers’ data is collected and processed at the workplace or in the frame of the enterprise \noperations, workers’ freely given, specific, informed and unambiguous consent is key as well as prior \ninformation and consultation of their representatives. However, and as ruled by the GDPR, for the \nmajority of data processed at work, the legal basis cannot and should not be the consent of the \nemployees due to the unbalanced nature of the employer and employee relationship. Processing \nmay be necessary on other grounds, e.g. for the performance of a contract or it may be imposed by \nlaw. When consent is used as a legal basis for data processing, consent should therefore in principle \nonly be valid if supported by a collective agreement and it is renewed throughout the process of data \nprocessing.  The  same  logic  needs  to  apply  when  the  employer  processes  workers’  data  for \nfurther/other uses that were not foreseen before. Likewise, sectorial and cross-sectorial collective \nagreements are key instruments, when it comes to reaching agreements on workers’ data going \nbeyond the company level. \n \nETUC calls for a democratisation of AI and data governance. Trade unions must play a significant \nrole in the new governance of data and AI and the EC must ensure that GDPR rights are fully \nguaranteed in the employment context. This is particularly true when it comes to the exercise of \nunambiguous and informed consent but also the right to access information on AI applications, the \nright to rectification, the right to erasure (thus extending ""the right to be forgotten"" to the work \nenvironment), the right to restrict processing, the right to data portability (as transferring the personal \ndata of a worker can be of particular interest in certain contexts, as ratings), the right to object \n(request ceasing of processing of the personal data of a worker)  and the right not to be subject to a \ndecision based solely on automated processing. \n \n7. Guarantee workers’ protection via social dialogue and collective bargaining \n \nA robust AI and data governance should enable workers to: (a) not be negatively affected by \nalgorithmic decision making and effectively get knowledge of the logic involved in any automatic \nprocessing of data concerning him/her; (b) not surrender their privacy rights; (c) adequately exercise \ntheir data protection rights at work, including the assistance of trade unions to achieve so and (d) \nenable workers’ representatives to play a key role in deciding how workers’ data is used, stored or \nshared within the context of  employment. \nETUC/EC241/EN/12  12', 'Public rules leading to even more intrusive surveillance based on AI technology are adopted in \nrelation to extraordinary events like a terrorist attack or a pandemic. Such rules should be subject to \nprior democratic debate, limited to a specific duration of time, and followed by democratic decisions. \nHowever, in many occurrences, due to the emergency, rules are passed, restricting or infringing \nfundamental rights, which are increasingly fighting back against these systems27.  Tracking and \nsurveillance technologies are increasingly present in society, which puts workplace privacy and data \nprotection at risk. Workplace surveillance can involve direct, indirect and remote technologies and \nanalytics. They often unduly impact workers and threaten their rights, such as the freedom of \nassociation, of expression, non-discrimination and digital freedoms.  They also amplify existing \ninequalities. Surveillance is not, by default, legitimate, necessary or proportionate at the workplace. \nAI and digital strategies must ensure that labour inspectorates and trade unions are effectively \nempowered  to  control  the  extent  and  the  lawfulness  of  AI  related  surveillance  technologies. \nWorkplace surveillance can involve direct, indirect and remote technologies and analytics. They \noften unduly impact workers and threaten their rights, such as the freedom of association, of \nexpression,  non-discrimination  and  digital  freedoms.  An  example  of  this  would  be  Facebook \nattempting to blacklist the word ""unionize"" from its application ""Facebook Workplace"". \n \nIn accordance to Art 9 GDPR on the processing of special categories of personal data, ETUC calls \non the EC to respond to this threat and ban intrusive surveillance technologies, including biometric \nand facial recognition, data driven analytics incorporating algorithmic predictions and chip implants.  \nThe related risks and potential for abuse largely outweigh the benefits. At work, such AI technology \nmay lead to significant violations of workers’ privacy. In exceptional situations, biometric processing \ntechnologies, such as facial recognition, should remain extremely exceptional and be limited to \nspecific and clear circumstances established by law or, where relevant, a collective agreement. This \ncould lead to a system designed on data protection (a privacy-by-design), allowing the use of \n""intrusive"" technologies only with a robust protection of users and their data. \n \nThe DNA of the European Social Model lies in the tripartite organisation of social policies. In the \nsame vein, European trade union organisations and employers\' associations should be associated \nwith the co-creation of the European AI and Data strategy to engage in AI technologies that serve \nquality employment, enhanced qualifications and upskilling, outsourcing of tasks, promoting working \nconditions and an environment that is respectful of workers, health and safety, workers’ privacy and \ndata protection. and effective access to social protection. \n \nETUC welcomes the call for investment in skills and general data literacy with a dedicated funded \nprogram under “Digital Europe”, included in the EU strategy for data. ETUC also welcomes the call \nfrom the Council of the European Union stressing that “adequate education, training, life-long \nlearning and skills are a prerequisite for an innovative and competitive workforce”. ETUC calls for \nconcrete support in early identification of skill needs and planning of employment and of future skills \nneeds.  Thus building ‘Data literacy’ through on-the-job training schemes to make AI more accessible \nto workers and workers’ representatives. Equally the EC should develop and support ‘AI literacy’, so \nthat workers are able to understand the role of data and AI in their workplace, its impact on the \norganisation of their work, be critically aware about the role and impact of working with AI systems, \nand level up their AI literacy for any job in their working life and achieve a stronger role in the socio-\neconomic transition. This would definitely raise the ability to engage with AI and data management \nby trade unions in the working environment. \n \nLikewise, if AI systems use personal data, that data should be used in a way that is legal and \ncomprehensible to anyone concerned. Workers and their representatives on all levels must be \nappropriately informed about AI methods, and AI systems should be designed and documented in a \nway that allows the decisions to be traced and understood. The demands concerning documentation, \ntransparency, and traceability may differ depending on each system’s function, application scope \nand risks"". \n \n \n27 EDRi (2020) Surveillance: Document Pool. https://edri.org/facial-recognition-document-pool/  \nETUC/EC241/EN/12  13', 'ETUC is of the opinion that an independent public authority could be dedicated to the monitoring of \nAI technologies and should address the specificities of the workplace. Such authority could be \nequipped with regulatory powers and trade unions should be significantly represented in its decision-\nmaking bodies. The idea of an EU Agency on AI would be innovative. However, ETUC would prefer \nto opt for the existing EU bodies and complement their mandate with AI related dimension, in \ncoordination with dedicated national authorities. ETUC could consider calling on the European \ngoverning bodies to give a mandate to the Fundamental Rights Agency (FRA) to deal with AI, with \na focus on labour related aspects. \n \nETUC/EC241/EN/12  14']"
F547524,01 September 2020,Fabian SCHMIDT,Unternehmen/Unternehmensverband,Software AG,groß (250 oder mehr Beschäftigte),253418726919-22,Deutschland,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"Software AG highly welcomes the Commission's objective to increase trust in AI thus fostering not only its use but also its development in Europe. We share the Commission's view that while AI can do much good, some of its uses and applications may cause both material and immaterial harm. Like the Commission we believe that trust is a precondition for the success of AI and that this objective can be best reached at Union level. As stated by the Commission, a public intervention can be one way to increase trust in AI both for citizens as well as for businesses. However, we share the Commission's view that there is a natural conflict between the benefits of a regulation on the one hand and its compliance costs on the other: the larger the scope, the higher the costs. As the Commission itself points out, this natural conflict may lead to the situation where some desirable AI systems may not be developed at all. To prevent the public intervention from such an unintended outcome, a balanced approach is needed.

Against this context, Software AG endorses option 3a. From our point of view, option 3a fulfills best the requirement for a balanced approach increasing trust in AI without being prohibitive. For the same reason, Software AG rejects option 1 as well as option 3c. While the former is inappropriate to significantly increase trust in AI, the latter would massively hamper the development and uptake of AI in Europe. Regarding the specific category of AI applications covered by Option 3a, we believe that all those AI applications should be considered that are of outstanding importance to citizens and therefore essential for their trust in AI. In our opinion, this is in particular true for AI applications that entail risks for fundamental rights (e.g. remote biometric identification systems). We share the Commission´s view, that whilst no new rights are needed, some characteristics of AI may hamper the effective enforcement of existing EU law meant to protect fundamental rights. In the implementation of option 3a, we prefer the scenario of co-regulation, where the legislative instrument would consist in high-level principles to be complemented by industry-led norms (e.g. stand-ards or codes of conduct). In our opinion, a mayor advantage of co-regulation is its friendliness for SMEs. As the Commission rightly points out, given the high scalability of digital technologies, SMEs must also be covered by the public intervention.

Apart from the protection of fundamental rights, we see neither the need nor the point for a dedicated regulation for AI. We believe that the vast majority of AI applications do not bear any risks at all and that for the very few high-risk applications adjustments to the existing legal and regulatory framework are absolutely sufficient - if they are not covered by Option 3a anyway. The revisions of the Machinery Directive and of the General Product Safety Liability Directive being prepared offer the opportunity to do so. As emphasized by the Commission, any fragmentation of the Single Digital Market must be prevented. This would not only hamper the uptake of AI in Europe, but above all it would put European AI providers at a serious competitive disadvantage. Software AG therefore welcomes the announced set up of a European governance structure on AI to improve cooperation between (national and European) regulatory authorities thus ensuring an effective and consistent enforcement of rules for AI across Europe."
F547341,31 August 2020,Chiara GIOVANNINI,Verbraucherverband,ANEC,sehr klein (1 bis 9 Beschäftigte),507800799-30,Belgien,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"As the European Commission’s assessment of product safety and liability legislation shows, there are gaps in present legislation and new AI related aspects such as explicability require new legal provisions, especially for enforcement purposes. We therefore support Option 3.c: EU legislative instrument establishing mandatory requirements for all applications.
The new rules have to cover risks posed by AI systems in a proportionate manner, with more stringent rules for high-risk applications. The EU regulatory approach on safety should be based and explicitly refer to the precautionary principle. 
We think that new rules should be adopted to make the appropriate risk assessment of all AI systems, taking into account of the nature of the hazard and the likelihood of its occurrence. Based on the assessment results, different rules can be applied in a proportionate manner. 
In order to assess whether the AI system is posing a high or low risk, criteria such as likelihood of the harm occurring, immediacy of the harm, the foreseeable use of the AI system (and not only the intended use which is not covering the potential effects of machine learning) have to be taken into account too. In addition, provisions have to deal with how uncertainties and assumptions impact the risk assessment. Once the risk is identified, mitigating measures have to be adopted (by industry, public authorities). Standards can be used to support the risk assessment. Any assessment, audit, certification, market surveillance activities have to cover the evolving nature of the AI system. For this, access to the AI system algorithms, codes and data sets must be ensured to understand and assess the risks.
About the level of risk, in our view, it is not a question of the kind of AI application as such, but of how to assess the risks posed to consumer protection by AI systems. In addition to cyber risks, personal security risks, risks related to the loss of connectivity and mental health risks, the risks to the environment should not be forgotten. 
We believe that product liability rules should be updated to ensure consumers are protected when they face problems with their digital goods.
We also believe that new consumers rights should be enshrined, for all AI systems, and not only high-risk applications, as follows:
-Right to Transparency, Explanation, and Objection
-Right to Accountability and Control
-Right to Fairness
-Right to Safety and Security
-Right to Access to Justice
-Right to Reliability and Robustness
We think that consumers interacting with AI systems must be able to keep full and effective self-determination/autonomy over themselves. This means securing human oversight over processes in AI systems. AI systems must not create asymmetries of power or information, such as between businesses and consumers. AI systems must not endanger the environment. 
We also suggest adopting the definition of AI provided by the EC HLEG on AI in the ethical guidelines. And to add a definition of ADM-Systems (Automated Decision Making). 
Legislation is needed to determine how and by whom biometrics technology can be used and the guarantees for citizens and consumers. Considering the high risk of abuse, discrimination and violation of fundamental rights to privacy and data protection, the European Union must develop a strong, privacy-protective approach for biometrics systems before they are largely used in public spaces.
Consumer information is useful in order to help transparency. However, labels are only as good as the requirements and enforcement systems on which they are based. Once clear legal rules and enforcement mechanisms are in place, the role of a label could be considered. 
Another important element to address is the inherent information asymmetry associated with AI or an evolving/machine learning system, making the function of a label different from a label linked to traditional, non-AI products (e.g. Ecolabel)."
F543855,19 August 2020,Barry O'BRIEN,Unternehmen/Unternehmensverband,IBM,groß (250 oder mehr Beschäftigte),7721359944-96,Irland,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"IBM welcomes the opportunity to respond to the Commission’s Inception Impact Assessment on the proposed legislative initiative on AI. We welcome the policy Objective and Aims of the initiative, in particular the intent to ensure coherence and complementarity with other possible initiatives, e.g. affecting the Machinery Directive, the General Product Safety Directive or the product liability regime. 

We support targeted policies that address companies’ accountability for developing and operating trustworthy AI. Below are our comments on the legislative options outlined.

Option 1: “soft law” approach.
We support this Option for low-risk AI applications.

Option 2: legislation setting up a voluntary labelling scheme.
Voluntary labelling schemes can be helpful to consumers or end-users in some markets but we do not believe a single, one-size-fits-all labelling scheme would be effective across such a broad field as AI, given the hugely diverse range of products and services that will be deployed across all sectors.

Option 3a: legislation establishing mandatory requirements for a specific category of AI applications only, notably remote biometric identification systems.
We support the need for a public dialogue on the use of facial recognition technologies, which could lead to targeted legislation.

Option 3b: legislation establishing mandatory requirements for “high-risk” AI applications.
Legislation should focus on high-risk applications, particularly applications where human autonomy or judgment are substantially ceded to an AI system. There should be a single risk assessment framework to identify high-risk AI, regardless of sector and without lists of exceptions. 
Any mandatory requirements for high-risk AI systems should be addressed to the actors best placed to address the risks. Similarly, liability is best allocated to the actor closest to the risk, as liability is highly context-specific. In a B2B context, contractual liability works well and should be maintained, allowing parties negotiate an efficient allocation of risk that takes account of the specific use case.

Option 3c: legislation covering all AI applications.
We do not support this option as it would significantly hamper the uptake and development of AI in the EU, against the stated Objective and Aims of the initiative.

Option 4: A combination of the options above taking into account the different levels of risk that could be generated by a particular AI application.
We believe the Objective and Aims can best be met with a combination of Options 1, 3a and 3b, implemented through a co-regulatory approach and supported by globally recognized standards and industry-led codes of conduct.

In relation to a European governance structure on AI, we believe that in sectors where established structures already exist (medical devices, aviation etc.) these existing bodies are best placed to cover high-risk AI in their sectors, having the necessary sectoral expertise, operational relationships and track-record with relevant stakeholders. There may be value in a new European mechanism that provides best practice sharing and guidance across sectors, but its scope must be limited and its relationship to existing regulatory bodies clearly defined, so as to avoid fragmentation, inconsistency and the risk of stifling innovation.

In summary, we agree with the need for a consistent EU-wide regulatory framework for trustworthy AI. This will be essential to give stakeholders the confidence to develop and adopt AI-based solutions and realize the enormous benefits they offer.  Building trust requires acknowledging valid concerns that exist regarding accountability, transparency, fairness, privacy and security, and putting in place appropriate regulatory mechanisms to manage those risks, while continuing to promote ongoing innovation and experimentation – getting that balance right requires a precision regulation approach that is clear and targeted.
"
F543779,19 August 2020,-,-,-,-,-,-,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"•	Certain rules or meaningful principles are needed to complement the existing legislative & regulatory framework (particularly those targeted at the avoidance of bias imposing a minimum of transparency & fairness obligations which will likely result in increased “explainability” efforts). In light of the existing fundamental rights and data protection regime, it is certainly desirable to only legislate with regard to actual concrete gaps (based on a thorough gap analysis) instead of crafting an additional comprehensive regime. New legislation should be limited to a necessary minimum to close existing gaps and not create a competitive disadvantage for ai-driven innovation in the EU. Moreover, any legislation governing AI and data usage should be principled based, providing enough leeway for companies to determine the most appropriate measures and ways to comply with those standards. 
•	Use cases should be differentiated and prioritized with regard to their potential to cause harm. A differentiated approach is key, considering that many of the cases do not impact a person (physically and morally) directly and personal data is either not used or, at a minimum, anonymized. We therefore suggest structuring any legislative proposal along a decision tree, making it possible to follow simple decision paths, based on pre-defined categorization of use cases. If all use cases had to be assessed against an exhaustive list, unnecessary bureaucracy and convoluted assessments would result; the purpose of creating “trustworthy AI” would be ill-served.
•	If not, companies subject to more restrictive regulations, such as in the EU, will be put at a disadvantage. We encourage the EU to aim for a reasonable balance between trustworthy aspirations and competitive realities.
"
F543638,17 August 2020,Kai PETERS,Wirtschaftsverband,VDMA,groß (250 oder mehr Beschäftigte),976536291-45,Deutschland,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"VDMA (German Mechanical Engineering Industry Association) welcomes the opportunity to provide feedback to the Inception Impact Assessment of the European Commission for a proposal for a legal act of the European Parliament and the Council laying down requirements for Artificial Intelligence.

AI will be a core enabling technology for many  sectors of European industry. Therefore, VDMA supports an European approach. A patchwork of national rules and initiatives in 27 EU Member States must be avoided.

VDMA welcomes a debate about the risks and opportunities, but calls for a very cautious approach when regulating an essential transversal technology. The immense opportunities must not be ignored and regulation must facilitate the development and uptake of AI. VDMA would like to bring forward four important points:

1) There is currently no need for a new, horizontal regulation for AI technologies, because there is no evidence yet of any fundamental regulatory gaps. In particular, in the case of AI embedded in products and machines, AI-characteristics (opacity, complexity and scalability) are limited by regulation and functional constraints. A precise analysis of the actual autonomy and risks is a prerequisite for assessing the need for legislation.

2) The EU-legislator should not regulate technologies but should instead target the effects of AI application in a technology-neutral way. Otherwise, there is a risk of hampering innovation and having to constantly adapt laws to technological progress. If future legislation is considered to be needed, the right approach would be to focus on high risk applications of AI. It is of crucial importance to develop a methodology to identify these high risk applications, to define the scope and to develop a legally sound definition of AI. Legal uncertainty would be a huge barrier to the use of AI, in particular for SMEs, and would hamper the use of AI in Europe.

3) We do not see a need for legislative action on machine safety. In the EU Machinery Directive ,the safety requirements are formulated in a technology-neutral manner and also apply to machines with AI elements. Factories with AI are therefore just as safe as without AI, as all safety requirements must be fulfilled in the same way.

4) VDMA does not believe that a readjustment of the Product Liability Directive and national liability regimes is needed at this stage. The Product Liability Directive is formulated in a technology-neutral way and the courts have applied it over the years to a wide range of products, many of which did not exist when the Directive was adopted. This legal framework for liability claims applies both to damage caused by a defective conventional product and to damage caused by a robot or other automated system. Therefore, they provide a legal framework within which AI problems can be solved. We also believe that further analysis, observation and examination are necessary before a proven liability regime is changed.

Conclusion: VDMA favours a combination of policy options (1 and 3b): The basis should be a “Soft Law”-approach. This will not only leave room for cutting-edge AI-solutions , but also allow a precise assessment of the real risks of AI and potential regulatory gaps. The use of “regulatory sandboxes” could help to find this balance. In a second step, if necessary, specific requirements could be established for clearly defined “high-risk”-applications - if they are not covered by existing regulation. It is crucial to focus on ""high risk applications"" and avoid hampering harmless AI-applications. In particular the industrial use of AI is in general uncritical and does not require more legislation.

Policy option 3c (covering all AI) would raise a prohibitive barrier to a wide range of AI-applications, including those which are harmless, yet beneficial. This would mean that Europe will fall behind international developments.","['European Office \n \n \n \n \n \n \n   \n \n \n \n \nArtificial Intelligence in industry - the \nroad to a European success \n \nStatement of the VDMA on the White Paper of the EU \nCommission (COM (2020) 65 final) \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nJune 2020', 'VDMA core messages: \n•  Artificial intelligence can become a European success story. This requires an \nobjective approach which respects key values, but which is inspired by opportunities. \nFrom the VDMA\'s point of view, the EU Commission\'s White Paper is a very good \nstart. However, the VDMA sees the necessity to discuss the risks and opportunities of \nthe industrial use of artificial intelligence in a more differentiated manner.  \n•  For the European mechanical engineering industry and its customers, AI is an \nopportunity to maintain global product leadership and master challenges such as \nresource scarcity and climate change. However, the enormous potential of industrial \nAI can only be tapped if AI is used on a broad industrial scale. This requires a holistic \nview of algorithms, data and application context. \n•  The VDMA fully supports the fact that the EU Commission wants to shape AI and the \nexchange of data at European level. A patchwork of national rules must be avoided at \nall costs. However, the international level must also be kept in mind to avoid \ncompetitive disadvantages of European companies on world markets. \n•  In the view of the VDMA, a new, horizontal regulation for AI technologies is currently \nnot justified. The White Paper does not provide evidence of any fundamental \nregulatory gaps. Instead, it should be observed and examined whether the existing \nregulation has gaps and where it needs to be improved. The legislator should not \nregulate technologies, but should instead target the effects of AI application. \nOtherwise, there is a risk of hampering innovation and having to constantly create \nnew laws for adapting to technological progress. \n•  Should horizontal legislation be considered necessary in the future, the right \napproach would be a limitation to high risk applications. AI must not be placed under \ngeneral suspicion. Instead, there must be room for the many harmless and promising \nAI applications. It is therefore essential to develop a clear methodology to identify \n""high risk applications"" and to define the scope of eventual legislation. \n•  At present, products and machines are hardly affected by the specific characteristics \nof AI (opacity, unpredictability and autonomous behaviour) as listed in the White \nPaper. A precise analysis of the actual autonomy and learning capacity is a \nprerequisite for assessing the risk and the need for legislation. \n•  In the view of the VDMA, the safety concerns expressed in the White Paper do not \napply to mechanical engineering and industrial production. The safety requirements \nare formulated in a technology-neutral manner and also apply to machines with AI \nelements. Factories with AI are just as safe as without AI, since all requirements must \nbe met in the same way as now.  \n•  Currently, the VDMA does not consider a readjustment of the product liability \nregulations to be necessary. Further analysis, observation and examination are \nnecessary before a proven liability regime is changed. A distinction should be made \nbetween products with embedded AI and pure AI software as they differ \nfundamentally in the area of safety legislation. \n•  The VDMA calls for a focus on informed users and markets instead of regulation. In \nprinciple, ex-ante requirements should be avoided wherever possible. Instead, \nanalysis, monitoring and information should be the basis for further decisions. \n \n \n1', '1.  Introduction: Artificial intelligence is an opportunity for Europe’s companies \n \nMechanical engineering companies are the central multiplier for the application of \nartificial intelligence in industrial production. Their machines, plants and engineering \nservices use AI technologies and thus bring new solutions to a multitude of customers, \nfactories and industries. Mechanical engineering builds on its experience with efficient \ntechnology integration and responsible design of human-machine cooperation - for \nexample in robotics, automation technology or sensor technology. \nFor the mechanical engineering industry, AI is an opportunity to maintain its global \nproduct leadership. AI helps to increase efficiency, develop new business models and \nimprove the quality and safety of products. Production processes can be optimized and \nmachines are enhanced with intelligent functions by AI elements. If these opportunities \nare not successfully exploited, the leading role of European mechanical engineering will \nbe lost to competitors from other technology regions, such as the United States and \nChina. However, the integration of AI technologies is not only a competitive advantage. It \nalso enables us to use materials and energy more efficiently, make better decisions and \nthus overcome challenges such as resource scarcity and climate change. The integration \nof AI in mechanical engineering is therefore an absolute must for companies, research \nand politics. \nLike any technology, artificial intelligence also involves risks. From the point of view of \nthe VDMA, however, artificial intelligence is not a new, independent challenge, but is \ncovered by ""classic"" fields of action such as product and machine safety, the design of \nthe working world and standardization. For example, even machines in which AI is used \nare already subject to the legal requirements for product safety and EU harmonisation \nlegislation.  \nThe VDMA is convinced that AI can become a European success story - for consumers, \ncompanies and the environment. However, this requires an objective approach that \nrespects key values, but is primarily inspired by opportunities. From the perspective of \nthe VDMA, the EU Commission has made a good start with the White Paper. However, \nthe VDMA sees the necessity to discuss the aspects of the use of artificial intelligence in \nindustry in a more differentiated manner and to evaluate the risks differently. \n \n2.  Artificial intelligence in industry: many opportunities, few risks \n \n2.1 State of the art of industrial AI use \n""Machine Learning"" as a form of ""narrow"" AI is already a reality in mechanical \nengineering and is used to answer specific technological or economic questions. \nHowever, the development is still in its infancy and the potential is enormous: The \nbreadth and variety of industrial applications promise more productivity and new \nsolutions; not only for a few pioneers, but for the whole range of companies. Especially \nthe combination of AI with the strengths of European industry is a competitive advantage \nfor Europe. \nExamples of industrial application of AI: \n•  Quality assurance with image processing methods to check surfaces or textures. AI \nhas great potential to make existing and proven image processing even more \npowerful.  \n2', '•  Process optimization of complex machines and systems: Here, sensor-based \nmachine learning can provide information on how to shorten start-up times or to  \ndiscover unknown sources of error or potential for improvement.  \n•  AI as a ""virtual digital assistant"" that helps to monitor the system or solve problems, \nfor example by detecting faults. \n•  ""Predictive Maintenance"", the evaluation of data with the aim of making maintenance \nand repair processes more efficient and predicting failures.  \n•  Optimization of internal production structures and processes, for example in \nproduction planning or procurement by evaluating ERP data  \n•  ""Smart product development"": Improvement of development and product \nmanagement, for example when products provide data during the use phase and thus \nprovide indications for innovations and improvements.  \n•  In sales and planning, AI tools for the intelligent configuration of complex systems can \nopen up considerable business value potential. \n \nIssues such as the ""black box"" effect or the consequences of ""self-learning systems"" are \nalso being discussed in industry, where trust in the reliability and safety of the technology \nare also indispensable to create acceptance among people, companies and the public. AI \nwill have effects on the cooperation between humans and machines as well as on the \ncooperation in industrial ecosystems, which have to be considered. The rights and \ninterests of employees, customers and investors must be protected.  \nHowever, the above-mentioned examples also show that the assessment of the \nopportunities and risks of industrial use must differ in some aspects from the situation in \nBusiness2Consumers (B2C) or Government2Citizens. \n \n2.2 Specifics of AI use in industry \n•  AI is mostly used as an analytical tool and under human supervision: AI technologies \nare mainly used for analysis and development purposes by operators, engineers and \ndevelopers without a direct link to the operational implementation. Examples are the \noptimization of product development or the identification of potential solutions. Human \nsupervision is therefore guaranteed. \n•  AI is part of the system function: When AI is used operationally in business and \nproduction processes, it is an element of a system that supports predefined functions. \nAI is not the determining factor for system behaviour. The high demands on industrial \nprocesses in terms of quality, safety and repeatability limit the autonomy.  \n•  Extensive existing regulation: Industrial environments have always used automated \nphysical processes that may pose a safety risk. However, this is completely \nindependent of the use of AI and is already covered by safety laws and standards. \nEvery manufacturer - with or without AI - must perform a safety compliance \nassessment for the intended use. \n•  Limited scalability: AI\'s applications in industrial production are application-specific \nand linked to physical processes, making rapid scalability difficult. The rapid spread of \nrisky or ethically questionable solutions as in B2C Internet applications is not \npossible. Furthermore, rapid success or ""killer applications"" should not be expected - \nAI development in industry is sometimes lengthy and involves a lot of effort. \n3', '•  In industry, too, there is a need to consciously deal with conflicts of interest that would \nhave remained non-transparent or unregulated without the use of AI, for example \nwhen AI is used in planning and operational processes in procurement, production \nand logistics. Although the spectrum of opportunities and risks here is not as scalable \nas with B2C Internet applications, the lack of embedding in physical processes may \nwell mean that there are similarities with B2C and the need to redesign relationships \nbetween companies or internal company processes.  \n•  Customer relations are B2B and are defined by contractual agreements. Therefore, \nthere is usually no special need for protection of AI users. \n•  In manufacturing processes, AI is a quality assurance tool that is used by people: AI \ntechnologies are mainly used for automated quality assurance purposes, or to \nsupport human decisions. Therefore, human supervision is also guaranteed.  \n \n3.  ""Ecosystem of Excellence"": aiming for broad use in all companies \n \nIn order to exploit European strengths and unlock the potential of AI for competitiveness \nand efficiency, AI must first and foremost be translated into concrete applications in \nindustry. This not only concerns the development of algorithms, but also their selection \nand adaptation as well as the context-dependent collection, selection and quality \nassurance of data. Promising applications ensure fast diffusion and effective leverage \neffects, such as industrial production, the use of AI in development, construction or new \nbusiness models.  \nAI can only become a European success story if it succeeds in bringing technology to the \nbreadth of SMEs and industrial SMEs. It is therefore important to ensure efficient \ntechnology transfer and to provide low-threshold access to technologies, projects, \nexpertise and networks. Test and competence centres in which processes and business \nmodels are tested can be very useful for this - provided they are located in an industrial \nenvironment and offer practice-oriented formats. \n \nVDMA Position \nAI must be brought into broad industrial application. A holistic view of algorithms, data \nand the application context is important. The measures proposed by the Commission, \nsuch as the initiatives on data exchange (e.g. a ""Manufacturing Data Space"") and the \nfocus on SMEs point in the right direction, but must be shaped in a practical and industry-\noriented way. \n \n \nAI is an interdisciplinary field that requires intensive cooperation between data/AI experts \nand other disciplines. Priority should therefore be given to interdisciplinary approaches \nthat promote cooperation.  \nIt is also important to make the necessary expertise available to companies in the most \nproductive form and to counter the shortage of skilled workers with efficient solutions. Not \nevery company will be able to employ data and AI experts. Concepts such as ""AI self-\nservice"" or ""guided analytics"" can help to make it easier for business experts in \ncompanies to use AI without having to be AI experts themselves. It is important to enable \ncompanies to define requirements and evaluate solutions together with AI experts or with \nthe help of suitable decision-supporting AI tools. \n4', 'VDMA Position  \nThe VDMA supports the concept presented in the White Paper, to not only address AI \ncompetencies in academic education, but also to focus on general and vocational \neducation and training as well as further education. It is important to enable companies to \nuse AI through expertise and AI tools. \n \n \n4.  ""An ecosystem of trust"": Not AI, but the application is crucial \n \n4.1 The future of AI is a task for Europe with a global dimension \nThe EU internal market plays a central role for the future of AI: only in a harmonised \nmarket with cross-border rules and initiatives can the necessary economies of scale be \nachieved and the framework conditions for investment be created. A patchwork of \nnational initiatives or even national legislation should be avoided. At the same time, care \nmust be taken to ensure that European rules remain globally compatible and that \nEuropean companies are not disadvantaged. \n \nVDMA Position  \nThe VDMA expressly supports the fact that the EU Commission wants to shape AI and \nthe exchange of data at the European level. A patchwork, including possible exemption \nclauses at national level, must be avoided. At the same time the international level must \nbe kept in mind to avoid  creating a disadvantage for European companies. \n \n \n4.2 The need for new legislation on AI has not yet been demonstrated  \nArtificial intelligence is a generic term for a range of technologies that can be used in a \nwide variety of applications. These applications can be risky, but in most cases they are \nalready subject to legal regulations. In the view of the VDMA, the Commission\'s White \nPaper does not identify any fundamental gaps in the regulation of artificial intelligence \nthat would justify additional legislation. Rather, there is a risk that new legal uncertainties \nand consistency problems will be raised. \nIn the view of the VDMA, it should be carefully examined whether the effects of artificial \nintelligence and new technologies are not already sufficiently covered by existing \nlegislation and whether, as stated in the White Paper, ""the existing legislation is equal to \nthe AI risks and can be effectively enforced. ""To speed up this process, regulatory \n""sandboxes"" could be used. If gaps are identified, they should be filled as far as possible \nby adapting existing legislation. Only if all this is not sufficient, new legislation should be \nconsidered. \n \nVDMA Position \nIn the view of the VDMA, a new, horizontal regulation for AI technologies is currently not \njustified. The White Paper does not identify fundamental regulatory gaps. Instead, the \neffects of new technologies in respective applications and existing regulation should be \nexamined. If it has been proven that deficits exist which cannot be eliminated by adapting \nexisting laws, new laws should be considered. \n5', '4.3 Focus on the risks of the application, do not regulate technologies! \nDespite the economic opportunities and the number of new questions raised: AI does not \ncreate a new class of machines and production systems. Machines are still designed by \nhumans for a defined function and in compliance with safety regulations. Moreover, many \nindustrial AI applications are exclusively technical in nature and do not raise ethical or \nlegal questions. \n \nVDMA Position  \nThe legislator should not regulate the technology, but must take into account the effects \nin the application. Otherwise there is a danger of unnecessarily impeding technological \nprogress and having to constantly enact new laws. Should horizontal legislation be \nconsidered necessary in the future, the right approach is limiting it to high-risk \napplications. AI must not be placed under general suspicion, but there must be scope for \nthe many harmless and promising AI applications. \n \n \n4.4 Formulate requirements for trustworthy AI specifically for the application \nProvided that the scope is defined in a meaningful and proportionate way, the listing of \nrequirements in the White Paper seems appropriate. For example, information on the \nlimitations and capabilities of AI can help to build expertise in dealing with AI, avoid \nmisunderstandings and increase acceptance. However, a trade-off has to be made, for \nexample whether transparency obligations jeopardise the protection of business secrets \nor intellectual property. \n \nVDMA Position \nThe requirements listed in the White Paper are useful, provided the scope and risk are \ncorrectly and clearly defined. Trade-offs, such as the balance between protection of trade \nsecrets and transparency, must be addressed. \n \n \n4.5 Create legal certainty through clear criteria for the scope \nThe VDMA supports the White Paper\'s objective of ensuring proportionality and not \nmaking the legal framework excessively prescriptive. The basic approach of linking risk \nassessment to application is also appropriate. This ensures that regulation is technology-\nneutral and remains focused. This approach should be pursued further. \nNevertheless, there is a risk that unclear criteria could create legal uncertainty. It is \ntherefore essential to develop a clear methodology to clearly define so-called ""high risk \napplications"". It is also necessary to find a simple and legally secure definition of when a \nsystem is considered to be an ""AI"" and which behaviour is to be classified as \n""autonomous"" and ""intelligent"". The current state of the discussion on this question does \nnot yet provide a sufficient basis for legal regulations. Failure to create this basis could \nlead to years of uncertainty and ultimately to a setback in the use of AI. \n \n \n \n6', 'VDMA Position \nIt will be crucial for good legislation that its scope is clear and that the criteria are easy \nand quick to interpret. It is therefore essential to develop a clear methodology to define \nso-called ""high risk applications"". The approach of focusing on critical applications is \ncorrect. It must also be clarified when an application is considered to be an “AI” and who \ndecides this. An orientation phase lasting for years, as is has been the case of GDPR, \nmust be avoided.  \n \n \n4.6 Check existing laws objectively for suitability \nAI is already covered by a large number of laws. However, AI can raise questions about \nthe suitability of existing regulations and an audit is useful. This examination must be \nbased on a realistic assessment of the possibilities and effects of AI. The VDMA \nconsiders the need to adapt existing legislation in industry to be minimal, since the \nrelevant regulations are formulated in a technology-neutral manner and the use of AI \ndoes not lead to a completely new type of product behaviour.  \n \nVDMA Position \nWhether an adaptation of existing legislation is necessary must be carefully examined in \nthe context of its application. This examination must take place in the context of \napplication and be based on the reality of the use of AI. \n \n \n4.7 Assess the real impact of AI and the autonomy of systems \nIt is true that AI technologies will enable higher degrees of autonomy of products or \nsystems in the future. The White Paper and the ""Report on the safety and liability \nimplications of Artificial Intelligence, the Internet of Things and robotics"" describe these \npossibilities in principle correctly, but ignore the actual limitations of autonomy. This gives \nthe false impression that in the near future there will be a class of physical products with \nautonomous functions controlled by AI.  \nFrom the perspective of the VDMA this is currently not the case, as the behaviour of the \nproducts follows the deterministic functionalities specified by the developer. AI elements \nare only building blocks of the overall system. The autonomous behaviour is defined \nbefore placing on the market and only takes place within narrow system limits specified \nby the manufacturer and the legislator. Moreover, an extension of autonomy does not \ntake place in a void, but within the framework of advances in automation technology, \nwhich are embedded in a context of standardization, methods, business models and in \nthe dialogue between the social partners.  \nIt is true that AI and Industry 4.0 as a whole will lead to a situation where products are \nincreasingly able to experience software-based updates or configuration changes during \nthe usage phase and where  operational functions are increasingly be determined by \ndata. How conformity can be assessed and proven in these cases is an important \nquestion, but it cannot be answered prematurely and certainly not by the legislator alone. \nPriority must be given to the development of classifications, taxonomies and methods \nthat enable agile or even automatable risk assessment and validation. However, new \nlegislation is not necessary at present, and could even lead to uncertainty. \n7', 'The White Paper mentions opacity as another AI problem It is true that the results of \nartificial intelligence do not always provide deterministic predictions. Transparency \nrequirements in the form of comprehensible rules in the narrower sense are therefore \nhard to meet. However, the concern about this characteristic of AI ignores two points: \nFirstly, the behaviour of non-AI systems is not always fully comprehensible either \n(whether due to a lack of models or disproportionate effort) and secondly, possible \nresidual risks are already being addressed in the risk assessment of technical systems. \nIn most cases, safety and acceptance is not achieved through complete traceability, but \nthrough tests, experiments, checks and impact assessments. These challenges are not \nnew for the safety and liability framework. \nIn applications where a consistent, rule-based traceability is required, certain AI \nprocedures and their advantages might have to be omitted. To systematically counteract \nsuch limitations, the question of explainability should not only be related to the processes \nin the AI algorithms themselves. Instead,it should be investigated how the input into an AI \nsystem can be designed to avoid undesired behaviour. Explainability guidelines could \nhelp to influence the behaviour of AI systems on the provider and user side. \nIf artificial intelligence is used in applications that are not limited by safety requirements \nor physical functionality - for example in planning or procurement processes - a \ndiscussion is needed on how the properties of AI change the cooperation between \nhumans and machines and in ecosystems. However, this does not currently justify any \nneed for action by the legislator. \n \nVDMA Position \nAt present, products and machines are only affected by the specific characteristics of AI \n(opacity, unpredictability and autonomous behaviour) listed in the White Paper to a \nlimited extent. A precise analysis of the actual autonomy and learning capacity is a \nprerequisite for assessing the risk and the need for legislation.  \n \n \n4.8 Machines are safe even when using AI due to existing laws \nAs described, even when using artificial intelligence, the degree of autonomy is limited - \nby the functionality and the limits defined by the developer1. In the case of physical \nproducts, these limits are also set by the safety concepts of the manufacturer. The public \nlaw regulations on product safety cover all functions of a product that operate within the \nlimits defined by the manufacturer. AI can only operate within these limits. \nFurthermore, the health and safety regulations applicable to machinery are not \nundermined by AI because they are formulated as technology-neutral protection \nobjectives. There is a legal obligation to apply the state of the art (documented in \nharmonized standards) in order to fulfil these protection goals. Functions of the machine \nthat are influenced, modified or even generated by AI must be taken into account by the \nmanufacturer during the conformity assessment procedure and must be recorded in their \n                                                           \n1 Platform Industry 4.0: ""Autonomy always takes place within system boundaries set by humans. The human \nbeing defines for which overall system a certain degree of autonomy is to be achieved, as well as within which \nareas and for which functions AI is allowed to work. The degree of autonomy of a system is not necessarily only \nsubject to technical limitations of AI, but is also influenced by other aspects such as legal framework conditions, \nestimation of the advantages and disadvantages of human actions or requirements regarding data security. \n„TECHNOLOGIESZENARIO „KÜNSTLICHE INTELLIGENZ IN DER INDUSTRIE“,2019, S.20,  \nhttps://www.plattform-i40.de/PI40/Redaktion/DE/Downloads/Publikation/KI-industrie-40.pdf?__blob=publicationFile=10 \n \n8', 'entirety. An AI machine must also not enter an uncontrolled state that would pose a \ndanger to the operator or uninvolved third parties.  \nEven the utilization of self-learning procedures during use - for example, when using \n""reinforcement learning"" procedures - does not cancel out the safety regulations: The \ndegrees of freedom of the AI can, for example, be limited by independent external \nmonitoring of the critical system parameters in such a way that conformity is guaranteed. \nIf this safety concept is abandoned by the results of the learning, a re-evaluation is \nnecessary.  \nThe Machinery Directive also covers this scenario of AI-induced change. The existing CE \nregulations on product safety define the essential requirements for product safety, which \nmust be complied with when the product is placed on the market. These requirements \nare formulated in a technology-neutral manner and are concretized by current standards. \nThis ensures that the product corresponds to the state of the art - without having to adapt \nthe legislation itself. Significant changes during the use phase - for example a new \nfunction of a machine due to modification - are also covered by the safety legislation. In \nthis case, the product is classified as new and a new assessment of conformity is \nrequired. A digital change to the characteristics of a machine - for example through a \nsoftware update - is in principle  no different from the conversion of a non-digital product; \nit is a ""digital"" conversion, so to speak. The already existing concept of ""substantial \nchange"" can therefore also be applied to digitised products. Whether or not this digital \nchange is based on an AI procedure is irrelevant. The currently prescribed conformity \nassessment procedures are suitable for this.  \nAt the level of checking whether a modified product complies with the standards and how \nvalidation is carried out, there may of course be changes and adaptations to the state of \nthe art. However, this progress is in turn updated in the standards of testing technology \nand is not subject to legislation. Instead, the state of the art must be developed further \nand there is a need for action in research, development and standardization. It is not \nnecessary to change the legislation and the essential requirements; on the contrary, it \nwould question the current clear and proven orientation. \n \nVDMA Position \nThe VDMA sees no need for action by the legislator on the subject of machine safety. \nState of the art of testing technology, standards and validation methods must be further \ndeveloped, but at the legislative level - such as the EU Machinery Directive - the safety \nrequirements are formulated in a technology-neutral manner and also apply to machines \nwith AI elements. Factories with AI are therefore just as safe as without AI, as all safety \nrequirements must be fulfilled in the same way.  \n \n \n4.9 Liability  \nIn principle, the current Product Liability Directive and national liability regimes provide a \nlegal framework within which AI problems can be solved. The Product Liability Directive \nis formulated in a technology-neutral way and the courts have applied it over the years to \na wide range of products, many of which did not exist when the Directive was adopted in \n1985. This legal framework for liability claims applies both to damage caused by a \ndefective conventional product and to damage caused by a robot or other automated \nsystem. \n9', 'The White Paper sees some need for action on the adaptation of the liability framework, \njustifying it with specific characteristics of new digital technologies such as autonomous \nbehaviour, self-learning, data dependency and opacity.  \nFrom the point of view of the VDMA, the questions raised in the White Paper are \nimportant and must be answered. However, this does not yet justify an immediate need \nfor a fundamental readjustment of the liability framework. Due to the complexity of the \nissue, further analyses, debates and observations of the real developments remain \nnecessary. When deciding on new regulations, the far-reaching effects on the innovation \nsystem in Europe must also be taken into account.  \nThe VDMA therefore recommends that sufficient time should be set aside for monitoring \nand testing the effects of artificial intelligence. For example, regulatory sandboxes could \ncontribute not only to technically validating AI, but also to specifically investigating the \nconcrete effects of AI characteristics on legal issues. In particular, the current regulations \nin the EU Member States must be analysed in detail to determine whether or not there \nare in fact liability gaps and problems of proof. \nIn the VDMA\'s view, two considerations are essential in order to conduct the debate on \nliability issues in a constructive and objective manner: Firstly, a distinction must be made \nbetween two areas: On the one hand, embedded AI, which is covered by the product \nterm and is the subject of safety legislation, and on the other hand AI as a pure software \napplication. Secondly, it is absolutely necessary to develop definitions and methodologies \nin order to transform terms like ""autonomy"", ""AI systems"" and ""AI risks"" into manageable \nand transparent concepts.  \nIn the field of embedded AI, the possibilities of the existing Product Liability Directive \nshould be carefully examined. Especially in combination with safety legislation, the \nProduct Liability Directive already offers suitable instruments to also cover properties of \nproducts with AI elements. The Product Liability Directive already provides for liability \nwithout fault:: According to the Product Liability Directive, a product is deemed to be \ndefective if it does not offer the safety that one could legitimately expect, taking all \ncircumstances into account. This is regularly the case if it does not comply with the \nrelevant safety regulations. Another example is the concept of ""substantial change"", \nwhich provides an approach on  how to deal with software updates and AI-induced \nchanges that occur after the product is placed on the market. In many places it may be \nsufficient to adapt guidelines and information requirements. \nThere should not be an insurance obligation, as current practice shows that producers \nregularly take out voluntary insurance when they are liable. \n \nVDMA Position \nThe VDMA does not consider a readjustment of the liability rules to be necessary at the \npresent time. Further analysis, observation and examination are necessary before a \nproven liability regime is changed. A distinction should be made between products with \nembedded AI (and thus covered by safety legislation) and AI software.  \n \n \n4.10  Focus on informed users and functioning markets \nThe White Paper gives the impression that users and consumers of AI are defenceless \nagainst developments. Especially in the B2B context, however, it can be assumed that \nbadly made or unethical AI will not prevail on the market. Even at present, compliance \n10', 'and conformity requirements, quality standards as well as the added value for the \ncustomer all act as selection criteria.  \nOn the other hand, detailed regulation or the use of certification can drive up costs and \nprevent innovation. Particularly for SMEs that are active in specific and less rapidly \nscalable fields of application (such as mechanical engineering), prohibitive hurdles are \nthus quickly reached and prevent the use of AI. \nThe discussion on artificial intelligence will therefore be an occasion for the legislator to \nrecall the principle of subsidiarity and technological neutrality and to only intervene \ndirectly in market processes if solutions cannot be achieved by those directly involved.  \n \nVDMA Position \nThe VDMA calls for a focus on informed users and markets rather than regulation. As a \nmatter of principle, ex-ante requirements should be avoided wherever possible. Instead, \nanalysis, monitoring and information should be the basis for further decisions. \n \n \n \n \n \n \n \n \n \n \n \nContacts VDMA:   \nKai Peters   Guido Reimann \nVDMA European Office  VDMA Software und Digitalisierung \nkai.peters@vdma.org  guido.reimann@vdma.org \n+32 2 7068219    +49 69 66 0312 58 \n \n \n11']"
F541831,10 August 2020,Dessislava Fessenko,EU-Bürger/-in,-,-,-,Bulgarien,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"Please review the attached PDF file. Feedback is provided therein. In a nutshell:

I wish to hereby submit the views that:

1.	Mandatory regulation is needed. As evident from areas such as data privacy, fake news, cyber security, tech firms (be it start-ups or large one, the latter appearing to drive and currently dominate the AI turf) cannot always be trusted to come up with and play by fair and equitable, consumer-friendly voluntary standards.

2.	The mandatory requirements should tackle first and foremost ethical and legal issues at the inception of AI solutions (e.g. regarding standards of programming, approach to decision rules, quality of input data, second and third layers of safety nets to be available in the course of operations of an AI solutions) and, as a second line, legal issues around application of AI solutions ( non-discriminatory application, avoidance of intrusion into privacy/personal life, re-allocation of liability, etc.). At the end of the day, end users would not buy an AI solution unless they are absolutely convinced from the outset – including based on regulatory mechanisms warranting this – in the uncompromised flawlessness of its conception and design from its nuts and bolts to its cognitive elements. ","['Should Artificial Intelligence be Regulated at European Union Level? \nSubmission by Dessislava Fessenko1 in the course of the European Commissions’ consultation on \nthe inception impact assessment as part of the initiative “Artificial intelligence – ethical and legal \nrequirements” \n \nIn 2015, a group of scientists from the Toulouse School of Economics, Massachusetts Institute of \nTechnology and the University of California conducted a series of online surveys2 among a large \nnumber of respondents asking them to solve a “simple” moral dilemma:  \nIn the face of an imminent accident resulting from a brake failure, should an autonomous vehicle (a) \nretain its course and, thus, hit a number of pedestrians crossing the street at this moment, (b) swerve \nleft/right and hit only one, unsuspecting passerby walking down the sidewalk, or (c) sacrifice the \npassenger in the autonomous vehicle (e.g. by swerving the car into a wall/tree) in order to avoid any \nfurther casualties.  \nThe majority of the respondents approved of the utilitarian approach whereby the passenger in the \nautonomous vehicle is sacrificed to the benefit of a greater number of others on the street. Yet, when \nasked whether they would buy an autonomous vehicle programmed to do so, respondents admitted \nthat they would not. \nThe outcome from such surveys clearly demonstrate the magnitude and complexity of the ethical issues \nthat the deployment and use of artificial intelligence (“AI”) solutions (such as autonomous vehicles) \nraise. To name just a few:  \n\uf0b7  What ethical rules (e.g. utilitarian, or stricter and more predetermined (Kantian)) should the \nprogramming of AI be based upon?  \n\uf0b7  How could the myriad of variations of the so-called “trolley problem” (to cause or let harm \nhappen, to whom at the expense of who else) be resolved?  \n\uf0b7  Who has the moral authority for all these decisions – AI developers and deployers, business \norganisations that mandate the development of AI and would ultimately use it, regulators?  \n                                                           \n1 The author is a lawyer; a member of European AI Alliance and of the Members Consultative Committee of the European Law \nInstitute’s project on Artificial Intelligence (AI) and Public Administration. All views expressed are personal. \n2 An overview available here . \nPage 1 of 3', '\uf0b7  Upon what kind of social consensus and mandate from a vast number of societies, with different \n(potentially diverging) ethical backgrounds, perceptions, and values, which would employ AI in \none form of another?  \n\uf0b7  Must the end-user of an AI solution be allowed some form of ultimate control over the operations \nof that solution assuming that the user (a human being) may have better moral knowledge (or \nat least intuition(s)) as to right/wrong?  \n\uf0b7  How is liability to be (re)allocated among developers/deployers, corporate users, end users for \nbiases, operational failures, or harm caused by AI given how deterministic the programming of \nAI is supposed – or may fail – to be?   \nAll matters that need to receive some sense of direction and, possibly, resolution long before AI \nbecomes a global commodity. Hopefully, by way of broader public and institutional debate. In the lights \nof the complexity of and the daunting ethical dimensions of AI, the question of whether the EU should \nregulate AI solutions then appears rhetorical. \nYet, such a question seems to transpire in the inception impact assessment  put up for public \nconsultation by the European Commission as part of the initiative “Artificial intelligence – ethical and \nlegal requirements” with view to gathering stakeholders’ views as to the need and possible forms of a \nlegislative initiative regarding AI at EU level.  \nTwo of the main options considered are: (a) a “soft-law” (i.e. non-legislative approach) whereby the AI \nindustry is encouraged to design, implement and enforce ethical codes; and (b) voluntary labelling of \nAI  solutions  in  order  “to  enable  customers  to  identify  AI  applications  that  comply  with  certain \nrequirements for trustworthy AI”. \nMandatory requirements for use/deployment of all or certain (higher-risk) AI solutions are considered \nas a third possible option. Their main focal points appear likely to be safeguarding fundamental rights \n(e.g. non-discrimination, privacy to the extent that it is not tackled by the GDPR), product safety and \nliability/redress mechanisms with respect to harm caused by AI solutions. \nGoing back to the question, I wish to hereby submit the views that: \n1.  Mandatory regulation is needed. As evident from areas such as data privacy, fake news, cyber \nsecurity, tech firms (be it start-ups or large one, the latter appearing to drive and currently \nPage 2 of 3', 'dominate the AI turf) cannot always be trusted to come up with and play by fair and equitable, \nconsumer-friendly voluntary standards. \n2.  The mandatory requirements should tackle first and foremost ethical and legal issues at the \ninception of AI solutions (e.g. regarding standards of programming, approach to decision rules, \nquality of input data, second and third layers of safety nets to be available in the course of \noperations of an AI solutions) and, as a second line, legal issues around application of AI \nsolutions ( non-discriminatory application, avoidance of intrusion into privacy/personal life, re-\nallocation of liability, etc.). At the end of the day, end users would not buy an AI solution unless \nthey are absolutely convinced from the outset – including based on regulatory mechanisms \nwarranting this – in the uncompromised flawlessness of its conception and design from its nuts \nand bolts to its cognitive elements.  \n \n10 August 2020 \n \n* \n*  * \n \n \n \n \nPage 3 of 3']"
F540776,04 August 2020,Nina Elzer,Wirtschaftsverband,European Association of Communications Agencies (EACA),sehr klein (1 bis 9 Beschäftigte),397482431021-09,Belgien,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"EACA welcomes the opportunity to give feedback on this roadmap where we would like to reiterate the points made during the Commission’s public consultation on its White Paper on Artificial Intelligence: 

On the trustworthy use of Artificial Intelligence   

From our industry’s point of view, concerns with AI are an extension of concerns with data use overall. AI exacerbates the risks involved in data use because of opaque decisioning, opaque use or opaque responsibility. However, the risks that are often associated with AI can be mitigated. For example, in the case of fundamental rights, through compliance with regulations, adherence to data processing standards for robustness, accuracy and security, or ethics guidelines and policies for data sets, goals and algorithms, application of decisions. Discriminatory outcomes can be mitigated through the use of bias detection software on data sets, goals and algorithms as standard. To be able to explain the rationale, record keeping for data sets, goals, algorithms is important, as well as recording decision making at the same time as decisions taking, and testing outcomes with different input data. The risk of low AI accuracy can be mitigated through the adherence to data processing standards; explicit statements of accuracy expectations; explicit statements of responsibility for accuracy (at company/team/individual level). 

On the consideration of different policy options

We think that the concerns expressed by the Commission or the High Level Expert Group can be fully addressed through applicable EU legislation. The challenge is the definition of high-risk sectors or industries. It is the purpose of the AI model use that is key, not the industry. It is impossible to predict the long term impact of behaviour, therefore it is highly difficult to assume that we can know which industries are or will be high-risk in the future. 

If new compulsory requirements had to be introduced, these should be limited to high-risk applications whereas a proper definition of high risk needs to be agreed. The challenge is the definition of high-risk sectors or industries. It is the purpose of the AI model use that is key, not the industry. It is impossible to predict the long term impact of behaviour, therefore it is highly difficult to assume that we can know which industries are or will be high-risk in the future. 

Regarding the possibility of a voluntary labelling system, we would not be opposed to such a system. However, we believe that is unlikely to work Any of the requirements suggested in the White Paper and in this consultation will add cost, effort and paperwork without providing a competitive advantage – thereby lacking an incentive. 
","['04 August 2020 \n \nEACA Feedback on “Artificial Intelligence – ethical and legal \nrequirements” (Inception Impact Assessment) \n \nThe European Association of Communications Agencies (EACA) represents more than 2,500 \ncommunications agencies and agency associations from nearly 30 European countries that directly \nemploy more than 120,000 people. EACA members include advertising, media, digital,  \nbranding and PR agencies. \n \nEACA welcomes the European Commission’s commitment to regularly consulting stakeholders at various \nstages of the decision-making process, including for roadmaps or Inception Impact Assessments. In our \nfeedback, we would like to reiterate the points made during the Commission’s public consultation on its \nWhite Paper on Artificial Intelligence:  \n \nOn the trustworthy use of Artificial Intelligence    \n \nFrom our industry’s point of view, concerns with AI are an extension of concerns with data use overall. AI \nexacerbates the risks involved in data use because of opaque decisioning, opaque use or opaque \nresponsibility. However, the risks that are often associated with AI can be mitigated. For example, in the \ncase  of  fundamental  rights,  through  compliance  with  regulations,  adherence  to  data  processing \nstandards for robustness, accuracy and security, or ethics guidelines and policies for data sets, goals and \nalgorithms, application of decisions. Discriminatory outcomes can be mitigated through the use of bias \ndetection software on data sets, goals and algorithms as standard. To be able to explain the rationale, \nrecord keeping for data sets, goals, algorithms is important, as well as recording decision making at the \nsame time as decisions taking, and testing outcomes with different input data. The risk of low AI accuracy \ncan be mitigated through the adherence to data processing standards; explicit statements of accuracy \nexpectations; explicit statements of responsibility for accuracy (at company/team/individual level).  \n \nOn the consideration of different policy options \n \nWe think that the concerns expressed by the Commission or the High Level Expert Group can be fully \naddressed  through  applicable  EU  legislation.  The  challenge  is  the  definition  of  high-risk  sectors  or \nindustries. It is the purpose of the AI model use that is key, not the industry. It is impossible to predict the \nlong term impact of behaviour, therefore it is highly difficult to assume that we can know which industries \nare or will be high-risk in the future.  \n \nIf new compulsory requirements had to be introduced, these should be limited to high-risk applications \nwhereas a proper definition of high risk needs to be agreed. The challenge is the definition of high-risk \nsectors or industries. It is the purpose of the AI model use that is key, not the industry. It is impossible to \npredict the long term impact of behaviour, therefore it is highly difficult to assume that we can know \nwhich industries are or will be high-risk in the future.  \n \nRegarding the possibility of a voluntary labelling system, we would not be opposed to such a system. \nHowever, we believe that is unlikely to work Any of the requirements suggested in the White Paper and in', 'this consultation will  add  cost,  effort  and  paperwork  without  providing  a  competitive  advantage  – \nthereby lacking an incentive.  \n \nFor more information or questions, please contact:  \n \nNina Elzer \nSenior Public Affairs Manager  \nEACA – European Association for Communications Agencies \nTel: +32 (0)2 740 07 13 \nnina.elzer@eaca.eu']"
F540758,04 August 2020,-,-,-,-,-,-,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"1.	Opportunität eines Regulierungsrahmens für KI

Der DIHK unterstützt die Idee der EU-Kommission, einen Regulierungsrahmen zu schaffen, der aber offen sein muss für weitere Entwicklungen. Die Fortschritte und weitere Marktdurchdringung sind bei KI-Anwendungen momentan nicht einzuschätzen. Daher dürfen gesetzliche Regelungen keine unnötigen Hemmnisse für die Weiterentwicklung bei KI aufbauen und sollten vielmehr innovationsfördernd wirken. Die Gefahr einer Fragmentierung des Binnenmarkts durch nationale KI-Regelungen in grenzüberschreitenden Sektoren spricht auch für eine Harmonisierung auf EU-Ebene. Es darf sich allerdings lediglich um einen Rahmen handeln, der klare und allgemeinen Leitplanken für KI regelt. 

Bei der KI-Regulierung sollte besonders für KMU vermieden werden, dass komplexe und bürokratische Regeln entstehen. Die Rechtssicherheit der Unternehmen sollte bei der Regulierung eine Priorität sein – und nicht vor allem auf den Schutz der Verbraucher fokussiert sein, wie das KI-Weißbuch an mehreren Stellen zu verstehen gibt. Dabei sollten die rechtlichen Regelungen die Risiken, die KI-Anwendungen verursachen können, berücksichtigen. Das betrifft einmal die Höhe des Risikos, aber andererseits auch die Frage der ungleichen Marktmacht im B2B-Bereich. 

Bei der Überlegung einer Differenzierung zwischen B2B und B2C muss berücksichtigt werden, dass gerade viele kleinere und mittlere Unternehmen genauso „schutzwürdig“ im Bereich KI sein können wie Verbraucher. 

2.	Risikobasierten Ansatz verfolgen

Der von der EU-Kommission gewählte risikobasierte Ansatz ist im Sinne eines verhältnismäßigen regulatorischen Eingriffs sinnvoll. Auch die Datenethikkommission in Deutschland hat einen solchen ihrem Bericht zugrunde gelegt. Die Kommission schlägt vor, KI-Anwendungen als „hohes Risiko“ einzustufen, wenn sowohl der Sektor als auch die beabsichtigte Verwendung erhebliche Risiken bergen. Dennoch scheint dieser Ansatz keine bessere Risikoeinschätzung im Einzelfall zu ermöglichen, denn die Risiken und Risikobereiche werden nicht vorhersehbar sein – dadurch könnten nicht alle Risiken erfasst werden. 

Teilweise wird vorgebracht, dass risikobehaftete Sektoren in einer Liste aufgezählt werden sollten, umso mehr Rechtssicherheit zu erhalten. Allerdings könnte eine Liste von „risikoreichen Sektoren“ ganze Sektoren unter Verdacht stellen, obwohl in jedem Sektor Anwendungen, Produkte und Dienste mit unterschiedlichsten Risikoanforderungen bestehen. Eine Liste von „risikoreichen Sektoren“ würde damit allen gelisteten Sektoren die Nutzung von KI auch für unkritische Funktionen/Dienste erheblich erschweren, etwa durch erhöhten Trainingsaufwand von ethisch unbedenklichen Anwendungsfällen. Dies wäre mit erheblichen Schäden für die Konkurrenzfähigkeit und Beschäftigung des EU-Standorts verbunden. Eine Bestimmung der risikoreichen Anwendungen nach Sektoren erscheint also nicht geeignet, um die Vielfalt der KI zu erfassen. Daher sollte eine Beurteilung des Risikos anhand allgemeiner Kriterien erfolgen. Sie sollten allerdings sehr klar festgelegt, eng gefasst und zukunftsfest sein. 

3.	Zertifizierung/Standardisierung

Die Entwicklung von KI sollte grundsätzlich den gleichen Sicherheitsstandards genügen, wie dies bei anderen Industriegütern oder Dienstleistungen heute schon geregelt ist. Um Schutzziele mit den Entwicklungszielen in Einklang zu bringen, muss auch hier das Grundprinzip Wettbewerb unter Einhalten von Sicherheitsstandards gelten. Eine Möglichkeit wäre, grundsätzliche Anforderungen ähnlich dem Prinzip der CE-Kennzeichnung für KI-Anwendungen und KI-Produkte zu definieren und Unternehmen diese nach dem Selbstverpflichtungsprinzip (Konformitätsbewertungsverfahren) unter behördlicher Überwachung umsetzen zu lassen. 

Ob die Prüfung, Zertifizierung oder auch Standardisierung von Algorithmen jedoch möglich ist, erscheint fraglich. Das Ergebnis wäre eine Momentaufnahme, die bei der schnellen technischen (...) (siehe beigefügtes Dokument)"
F540645,03 August 2020,patrick GRANT,Wirtschaftsverband,BusinessEurope,klein (10 bis 49 Beschäftigte),3978240953-79,Belgien,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"BusinessEurope welcomes the positive tone of the Commission’s White Paper acknowledging the many opportunities that AI can bring to Europe’s economy and society. We support the Commission in its venture to build ecosystems of excellence and trust in Europe for Artificial Intelligence (AI). As an all-encompassing technology, this involves cross sectoral coordination across all areas of Europe through a number of legislative and non-legislative actions. No Member State alone can support our global competitive standing in this strategic technological area. Rather than relying on a few Member States or cities to lead, the diversity of resources which Europe can bring forward jointly in this process is what will propel it to be a global competitive player in AI. In return, the full economic, environmental, workplace and societal opportunities AI derives can be evenly spread across the continent - leaving no one behind through the ongoing digital revolution.

We welcome the Commission’s pragmatic approach to excellence and trust in AI outlined in its recent White Paper. The two are likely to reinforce one another: a range of measures tackling legal challenges could support uptake by improving societal trust and offer greater clarity for consumers and businesses alike.

As a key political cornerstone of the Von der Leyen Commission, which will be fundamental to shape how AI develops in the EU, the utmost importance must be given to its transparent and open consultation before further operational steps by policy makers are taken. We repeat our call for an extension until the end of 2020 on this consultation in this regard due to the ongoing COVID-19 crisis. This will allow the fundamental debate required with stakeholders to take place. Adjusting timelines to the realities we are living within would ensure that quality prevails over speed. We suggest extending the period for input to the end of the year to ensure inclusive engagement so that useful proposals evolve from this consultation.

As a key societal stakeholder and European social partner (see latest digitalisation agreement), BusinessEurope outlines its reaction in the document attached as a response to the Commission’s ongoing consultation."
F539970,30 Juli 2020,-,-,-,-,-,-,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"I would like to express my support for the 2nd sub-option of the Option #3, as it covers all the necessary pillars of an effective AI regulation. Voluntary labeling process outlined in the option #2 might have been effective, but not in the current stage where we already have extensive use of AI applications for malicious purposes (especially outside EU). On the other hand complete regulation of AI is also not desirable as it steps in the way of development and advancement of AI technologies. "
F539630,28 Juli 2020,Bart De Witte,EU-Bürger/-in,-,-,-,Deutschland,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"As we had great concerns that the first release of the guidelines of trustworthy AI was released without going deeper into the first component, which is lawful AI.

In regards to the fundamental right as described in the EU charter for fundamental rights, we can cluster two infringements that the current GRDP regulations do not cover.

1. Right to the integrity of the person: 

Digital Twins represent a concept, where AI-models can “map” data from humans into digital duplicates of the self. Whereas the original idea of a digital twin focussed on the replication of an object in data, progression within the field of AI, can create AI-models based on observed behavior and biological data rather than just the information itself. This means that a digital twin can be a model of the human self or an evolving set of data relating to the original model. As the model used in a digital twin does not need to be a data-driven model, one can argue that they are not under the protection of GDPR. Most current data-driven business models in social media (eg. Facebook) or AI-Startups (eg. replika.ai) can be GDPR compliant, but, as the subjects have no agency on the AI-model that equals their digital replica, these companies are violating the Right to Integrity of the Person. Digital twins are an extension of ourselves. They reflect who we are as individuals, how we spend our time, who and what we like and dislike, our preferences and quirks, all of what makes us human. our fundamental rights need an upgrade, so humans have agency over their digital selves. As the digital world is a replication of our physical world (Armin Nassehi, Professor of Sociology at LMU Munich) the prohibition of the reproductive cloning of human beings as described in Article 3, needs to be extended to the concept of digital cloning. 

Secondly, data in healthcare is not a commodity, but data is human life. The current view, driven by data-driven business models are paving the way for a new stage of capitalism whose outlines we only partly see: the capitalization of human life without limit. With the current model there will be no part of human life, no layer of experience, that is not extractable for economic value. Human life will be there for mining by corporations without reserve. This process of capitalization will be the foundation for a highly unequal new social arrangement. Article 1 describes the prohibition on making the human body and its parts as such a source of financial gain (Nick Couldry, LSE);  if humans can be digitally cloned, future policies need to assure that data that is extracted out of the human body can not be used as a source of financial gains. 

2. Article 35 describes that everyone citizen of the EU has the right of access to preventive health care and the right to benefit from medical treatment under the conditions established by national laws and practices. It also described that in the definition and implementation of all the Union's policies and activities a high level of human health protection needs to be ensured. The current data-driven business models in healthcare are leading to privatization and centralisation of medical knowledge (medical knowledge is represented in AI models). In order to be compliant with Article 35, the EU need to establish policies that guarantee, that all AI-models used in healthcare are open models that are accessible to all. Only in this way one can guarantee a high level of human health protection for all, as described in article 35. 


"
F539194,24 Juli 2020,Paul MACDONNELL,NRO (Nichtregierungsorganisation),Global Digital Foundation,sehr klein (1 bis 9 Beschäftigte),352788324603-77,Irland,Künstliche Intelligenz – ethische und rechtliche Anforderungen,"Policy analysts and policymakers are responding to AI as both a threat to human rights and as a potential savour of humanity from discrimination. This response rests on two questionable ideas. The first is that AI’s supposed human-like intelligence could give it a mind or a ‘will’. The second is that decisions affecting socio-economic outcomes are, in reality, the instrumentalisation of unfair hierarchies—and that AI could make this problem worse. This paper argues that, while AI affecting safety—e.g. in transport or healthcare—should be regulated and certified prior to implementation, AI that may affect human rights or social outcomes cannot be regulated in this way. This is because the likely causes of bad human rights or socio-economic outcomes following the use of AI will be both multivariate and exogenous to AI technologies in a way that threats to safety will not. Proposals to regulate AI for ‘fairness’ ex ante rest on a univariate ‘social bias’ explanation for differential social outcomes and are, hence, likely to be both ineffective and divisive.

Therefore Option 1 a 'soft law' approach is the best way forward."
